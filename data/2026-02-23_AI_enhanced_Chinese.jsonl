{"id": "2602.17768", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17768", "abs": "https://arxiv.org/abs/2602.17768", "authors": ["Boda Lin", "Yongjie Zhu", "Xiaocheng Gong", "Wenyu Qin", "Meng Wang"], "title": "KPM-Bench: A Kinematic Parsing Motion Benchmark for Fine-grained Motion-centric Video Understanding", "comment": "26 pages", "summary": "Despite recent advancements, video captioning models still face significant limitations in accurately describing fine-grained motion details and suffer from severe hallucination issues. These challenges become particularly prominent when generating captions for motion-centric videos, where precise depiction of intricate movements and limb dynamics is crucial yet often neglected. To alleviate this gap, we introduce an automated annotation pipeline that integrates kinematic-based motion computation with linguistic parsing, enabling detailed decomposition and description of complex human motions. Based on this pipeline, we construct and release the Kinematic Parsing Motion Benchmark (KPM-Bench), a novel open-source dataset designed to facilitate fine-grained motion understanding. KPM-Bench consists of (i) fine-grained video-caption pairs that comprehensively illustrate limb-level dynamics in complex actions, (ii) diverse and challenging question-answer pairs focusing specifically on motion understanding, and (iii) a meticulously curated evaluation set specifically designed to assess hallucination phenomena associated with motion descriptions. Furthermore, to address hallucination issues systematically, we propose the linguistically grounded Motion Parsing and Extraction (MoPE) algorithm, capable of accurately extracting motion-specific attributes directly from textual captions. Leveraging MoPE, we introduce a precise hallucination evaluation metric that functions independently of large-scale vision-language or language-only models. By integrating MoPE into the GRPO post-training framework, we effectively mitigate hallucination problems, significantly improving the reliability of motion-centric video captioning models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9488\u5bf9\u89c6\u9891\u63cf\u8ff0\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u8fd0\u52a8\u7ec6\u8282\u63cf\u8ff0\u548c\u5e7b\u89c9\u95ee\u9898\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86KPM-Bench\u6570\u636e\u96c6\u548cMoPE\u7b97\u6cd5\uff0c\u901a\u8fc7\u8fd0\u52a8\u89e3\u6790\u548c\u63d0\u53d6\u6280\u672f\u6539\u5584\u8fd0\u52a8\u4e2d\u5fc3\u89c6\u9891\u63cf\u8ff0\u7684\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u63cf\u8ff0\u6a21\u578b\u5728\u63cf\u8ff0\u7ec6\u7c92\u5ea6\u8fd0\u52a8\u7ec6\u8282\u65f6\u5b58\u5728\u4e25\u91cd\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u8fd0\u52a8\u4e2d\u5fc3\u89c6\u9891\u4e2d\uff0c\u5bf9\u590d\u6742\u52a8\u4f5c\u548c\u80a2\u4f53\u52a8\u6001\u7684\u7cbe\u786e\u63cf\u8ff0\u4e0d\u8db3\uff0c\u540c\u65f6\u5b58\u5728\u4e25\u91cd\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "method": "1) \u5f00\u53d1\u81ea\u52a8\u5316\u6807\u6ce8\u6d41\u7a0b\uff0c\u6574\u5408\u57fa\u4e8e\u8fd0\u52a8\u5b66\u7684\u8fd0\u52a8\u8ba1\u7b97\u548c\u8bed\u8a00\u89e3\u6790\uff1b2) \u6784\u5efaKPM-Bench\u6570\u636e\u96c6\uff0c\u5305\u542b\u7ec6\u7c92\u5ea6\u89c6\u9891-\u63cf\u8ff0\u5bf9\u3001\u8fd0\u52a8\u7406\u89e3\u95ee\u7b54\u5bf9\u548c\u5e7b\u89c9\u8bc4\u4f30\u96c6\uff1b3) \u63d0\u51faMoPE\u7b97\u6cd5\uff0c\u4ece\u6587\u672c\u63cf\u8ff0\u4e2d\u51c6\u786e\u63d0\u53d6\u8fd0\u52a8\u7279\u5b9a\u5c5e\u6027\uff1b4) \u5c06MoPE\u96c6\u6210\u5230GRPO\u540e\u8bad\u7ec3\u6846\u67b6\u4e2d\u3002", "result": "1) \u53d1\u5e03\u4e86KPM-Bench\u5f00\u6e90\u6570\u636e\u96c6\uff1b2) \u63d0\u51fa\u4e86\u72ec\u7acb\u7684\u5e7b\u89c9\u8bc4\u4f30\u6307\u6807\uff1b3) \u6709\u6548\u7f13\u89e3\u4e86\u5e7b\u89c9\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8fd0\u52a8\u4e2d\u5fc3\u89c6\u9891\u63cf\u8ff0\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u901a\u8fc7KPM-Bench\u6570\u636e\u96c6\u548cMoPE\u7b97\u6cd5\uff0c\u8be5\u7814\u7a76\u4e3a\u7ec6\u7c92\u5ea6\u8fd0\u52a8\u7406\u89e3\u63d0\u4f9b\u4e86\u7cfb\u7edf\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u6539\u5584\u4e86\u89c6\u9891\u63cf\u8ff0\u6a21\u578b\u5728\u8fd0\u52a8\u7ec6\u8282\u63cf\u8ff0\u4e0a\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2602.18248", "categories": ["cs.LG", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.18248", "abs": "https://arxiv.org/abs/2602.18248", "authors": ["Pietro Sittoni", "Emanuele Zangrando", "Angelo A. Casulli", "Nicola Guglielmi", "Francesco Tudisco"], "title": "Neural-HSS: Hierarchical Semi-Separable Neural PDE Solver", "comment": null, "summary": "Deep learning-based methods have shown remarkable effectiveness in solving PDEs, largely due to their ability to enable fast simulations once trained. However, despite the availability of high-performance computing infrastructure, many critical applications remain constrained by the substantial computational costs associated with generating large-scale, high-quality datasets and training models. In this work, inspired by studies on the structure of Green's functions for elliptic PDEs, we introduce Neural-HSS, a parameter-efficient architecture built upon the Hierarchical Semi-Separable (HSS) matrix structure that is provably data-efficient for a broad class of PDEs. We theoretically analyze the proposed architecture, proving that it satisfies exactness properties even in very low-data regimes. We also investigate its connections with other architectural primitives, such as the Fourier neural operator layer and convolutional layers. We experimentally validate the data efficiency of Neural-HSS on the three-dimensional Poisson equation over a grid of two million points, demonstrating its superior ability to learn from data generated by elliptic PDEs in the low-data regime while outperforming baseline methods. Finally, we demonstrate its capability to learn from data arising from a broad class of PDEs in diverse domains, including electromagnetism, fluid dynamics, and biology.", "AI": {"tldr": "Neural-HSS\uff1a\u57fa\u4e8e\u5c42\u6b21\u534a\u53ef\u5206\u77e9\u9635\u7ed3\u6784\u7684\u9ad8\u6548\u53c2\u6570\u67b6\u6784\uff0c\u5728\u4f4e\u6570\u636e\u91cf\u4e0b\u4ecd\u80fd\u4fdd\u6301\u7cbe\u786e\u6027\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdbPDE\u95ee\u9898", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5728\u6c42\u89e3PDE\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7684\u751f\u6210\u548c\u6a21\u578b\u8bad\u7ec3\u4ecd\u9762\u4e34\u5de8\u5927\u8ba1\u7b97\u6210\u672c\uff0c\u9650\u5236\u4e86\u5173\u952e\u5e94\u7528\u7684\u53d1\u5c55", "method": "\u53d7\u692d\u5706PDE\u683c\u6797\u51fd\u6570\u7ed3\u6784\u7814\u7a76\u542f\u53d1\uff0c\u63d0\u51fa\u57fa\u4e8e\u5c42\u6b21\u534a\u53ef\u5206\u77e9\u9635\u7ed3\u6784\u7684\u53c2\u6570\u9ad8\u6548\u67b6\u6784Neural-HSS\uff0c\u7406\u8bba\u4e0a\u5206\u6790\u5176\u7cbe\u786e\u6027\uff0c\u5e76\u63a2\u8ba8\u5176\u4e0e\u5085\u91cc\u53f6\u795e\u7ecf\u7b97\u5b50\u5c42\u548c\u5377\u79ef\u5c42\u7b49\u5176\u4ed6\u67b6\u6784\u539f\u8bed\u7684\u8054\u7cfb", "result": "\u5728200\u4e07\u70b9\u7f51\u683c\u7684\u4e09\u7ef4\u6cca\u677e\u65b9\u7a0b\u4e0a\u9a8c\u8bc1\u4e86Neural-HSS\u7684\u6570\u636e\u6548\u7387\uff0c\u5728\u4f4e\u6570\u636e\u91cf\u4e0b\u5b66\u4e60\u692d\u5706PDE\u751f\u6210\u7684\u6570\u636e\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff1b\u540c\u65f6\u5c55\u793a\u4e86\u5176\u5728\u7535\u78c1\u5b66\u3001\u6d41\u4f53\u52a8\u529b\u5b66\u548c\u751f\u7269\u5b66\u7b49\u591a\u4e2a\u9886\u57df\u5e7f\u6cdbPDE\u95ee\u9898\u4e2d\u7684\u5b66\u4e60\u80fd\u529b", "conclusion": "Neural-HSS\u67b6\u6784\u5728\u4fdd\u6301\u53c2\u6570\u6548\u7387\u7684\u540c\u65f6\uff0c\u5728\u4f4e\u6570\u636e\u91cf\u4e0b\u5bf9\u5e7f\u6cdbPDE\u7c7b\u522b\u5177\u6709\u6570\u636e\u6548\u7387\uff0c\u4e3a\u89e3\u51b3\u8ba1\u7b97\u6210\u672c\u9ad8\u7684PDE\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848"}}
{"id": "2602.17770", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17770", "abs": "https://arxiv.org/abs/2602.17770", "authors": ["Balamurugan Thambiraja", "Omid Taheri", "Radek Danecek", "Giorgio Becherini", "Gerard Pons-Moll", "Justus Thies"], "title": "CLUTCH: Contextualized Language model for Unlocking Text-Conditioned Hand motion modelling in the wild", "comment": "ICLR2026; Project page: https://balamuruganthambiraja.github.io/CLUTCH/", "summary": "Hands play a central role in daily life, yet modeling natural hand motions remains underexplored. Existing methods that tackle text-to-hand-motion generation or hand animation captioning rely on studio-captured datasets with limited actions and contexts, making them costly to scale to \"in-the-wild\" settings. Further, contemporary models and their training schemes struggle to capture animation fidelity with text-motion alignment. To address this, we (1) introduce '3D Hands in the Wild' (3D-HIW), a dataset of 32K 3D hand-motion sequences and aligned text, and (2) propose CLUTCH, an LLM-based hand animation system with two critical innovations: (a) SHIFT, a novel VQ-VAE architecture to tokenize hand motion, and (b) a geometric refinement stage to finetune the LLM. To build 3D-HIW, we propose a data annotation pipeline that combines vision-language models (VLMs) and state-of-the-art 3D hand trackers, and apply it to a large corpus of egocentric action videos covering a wide range of scenarios. To fully capture motion in-the-wild, CLUTCH employs SHIFT, a part-modality decomposed VQ-VAE, which improves generalization and reconstruction fidelity. Finally, to improve animation quality, we introduce a geometric refinement stage, where CLUTCH is co-supervised with a reconstruction loss applied directly to decoded hand motion parameters. Experiments demonstrate state-of-the-art performance on text-to-motion and motion-to-text tasks, establishing the first benchmark for scalable in-the-wild hand motion modelling. Code, data and models will be released.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e863D-HIW\u6570\u636e\u96c6\u548cCLUTCH\u7cfb\u7edf\uff0c\u7528\u4e8e\u89e3\u51b3\u91ce\u5916\u73af\u5883\u4e0b\u7684\u624b\u90e8\u52a8\u4f5c\u5efa\u6a21\u95ee\u9898\uff0c\u5728\u6587\u672c\u5230\u624b\u90e8\u52a8\u4f5c\u751f\u6210\u548c\u52a8\u4f5c\u63cf\u8ff0\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u624b\u90e8\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u626e\u6f14\u6838\u5fc3\u89d2\u8272\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5de5\u4f5c\u5ba4\u91c7\u96c6\u7684\u6709\u9650\u6570\u636e\u96c6\uff0c\u96be\u4ee5\u6269\u5c55\u5230\u91ce\u5916\u73af\u5883\uff0c\u4e14\u73b0\u6709\u6a21\u578b\u5728\u52a8\u753b\u4fdd\u771f\u5ea6\u548c\u6587\u672c-\u52a8\u4f5c\u5bf9\u9f50\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "1) \u6784\u5efa3D-HIW\u6570\u636e\u96c6\uff1a\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5148\u8fdb3D\u624b\u90e8\u8ffd\u8e2a\u5668\uff0c\u4ece\u5927\u91cf\u7b2c\u4e00\u4eba\u79f0\u52a8\u4f5c\u89c6\u9891\u4e2d\u63d0\u53d632K\u4e2a3D\u624b\u90e8\u52a8\u4f5c\u5e8f\u5217\u548c\u5bf9\u5e94\u6587\u672c\uff1b2) \u63d0\u51faCLUTCH\u7cfb\u7edf\uff1a\u5305\u542bSHIFT\uff08\u90e8\u5206\u6a21\u6001\u5206\u89e3\u7684VQ-VAE\u67b6\u6784\uff09\u7528\u4e8e\u624b\u90e8\u52a8\u4f5c\u6807\u8bb0\u5316\uff0c\u4ee5\u53ca\u51e0\u4f55\u7cbe\u70bc\u9636\u6bb5\u5fae\u8c03LLM\u3002", "result": "\u5728\u6587\u672c\u5230\u624b\u90e8\u52a8\u4f5c\u751f\u6210\u548c\u52a8\u4f5c\u5230\u6587\u672c\u63cf\u8ff0\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5efa\u7acb\u4e86\u9996\u4e2a\u53ef\u6269\u5c55\u7684\u91ce\u5916\u624b\u90e8\u52a8\u4f5c\u5efa\u6a21\u57fa\u51c6\u3002", "conclusion": "\u901a\u8fc73D-HIW\u6570\u636e\u96c6\u548cCLUTCH\u7cfb\u7edf\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u91ce\u5916\u73af\u5883\u4e0b\u624b\u90e8\u52a8\u4f5c\u5efa\u6a21\u7684\u6311\u6218\uff0c\u4e3a\u624b\u90e8\u52a8\u753b\u751f\u6210\u548c\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u7684\u57fa\u51c6\u548c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.17667", "categories": ["cs.IR", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17667", "abs": "https://arxiv.org/abs/2602.17667", "authors": ["Cheng cheng", "Chenxing Wang", "Aolin Li", "Haijun Wu", "Huiyun Hu", "Juyuan Wang"], "title": "When & How to Write for Personalized Demand-aware Query Rewriting in Video Search", "comment": null, "summary": "In video search systems, user historical behaviors provide rich context for identifying search intent and resolving ambiguity. However, traditional methods utilizing implicit history features often suffer from signal dilution and delayed feedback. To address these challenges, we propose WeWrite, a novel Personalized Demand-aware Query Rewriting framework. Specifically, WeWrite tackles three key challenges: (1) When to Write: An automated posterior-based mining strategy extracts high-quality samples from user logs, identifying scenarios where personalization is strictly necessary; (2) How to Write: A hybrid training paradigm combines Supervised Fine-Tuning (SFT) with Group Relative Policy Optimization (GRPO) to align the LLM's output style with the retrieval system; (3) Deployment: A parallel \"Fake Recall\" architecture ensures low latency. Online A/B testing on a large-scale video platform demonstrates that WeWrite improves the Click-Through Video Volume (VV$>$10s) by 1.07% and reduces the Query Reformulation Rate by 2.97%.", "AI": {"tldr": "WeWrite\u662f\u4e00\u4e2a\u4e2a\u6027\u5316\u9700\u6c42\u611f\u77e5\u7684\u67e5\u8be2\u91cd\u5199\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u6316\u6398\u7b56\u7565\u3001\u6df7\u5408\u8bad\u7ec3\u8303\u5f0f\u548c\u5e76\u884c\u67b6\u6784\uff0c\u5728\u89c6\u9891\u641c\u7d22\u7cfb\u7edf\u4e2d\u63d0\u5347\u4e2a\u6027\u5316\u67e5\u8be2\u91cd\u5199\u6548\u679c\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u70b9\u51fb\u7387\u548c\u964d\u4f4e\u4e86\u67e5\u8be2\u91cd\u6784\u7387\u3002", "motivation": "\u89c6\u9891\u641c\u7d22\u7cfb\u7edf\u4e2d\uff0c\u7528\u6237\u5386\u53f2\u884c\u4e3a\u4e3a\u8bc6\u522b\u641c\u7d22\u610f\u56fe\u548c\u6d88\u9664\u6b67\u4e49\u63d0\u4f9b\u4e86\u4e30\u5bcc\u4e0a\u4e0b\u6587\uff0c\u4f46\u4f20\u7edf\u4f7f\u7528\u9690\u5f0f\u5386\u53f2\u7279\u5f81\u7684\u65b9\u6cd5\u5b58\u5728\u4fe1\u53f7\u7a00\u91ca\u548c\u5ef6\u8fdf\u53cd\u9988\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faWeWrite\u6846\u67b6\uff0c\u89e3\u51b3\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff1a1) \u4f55\u65f6\u91cd\u5199\uff1a\u57fa\u4e8e\u540e\u9a8c\u7684\u81ea\u52a8\u5316\u6316\u6398\u7b56\u7565\u4ece\u7528\u6237\u65e5\u5fd7\u4e2d\u63d0\u53d6\u9ad8\u8d28\u91cf\u6837\u672c\uff1b2) \u5982\u4f55\u91cd\u5199\uff1a\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\u7684\u6df7\u5408\u8bad\u7ec3\u8303\u5f0f\uff1b3) \u90e8\u7f72\uff1a\u91c7\u7528\u5e76\u884c\"\u5047\u53ec\u56de\"\u67b6\u6784\u786e\u4fdd\u4f4e\u5ef6\u8fdf\u3002", "result": "\u5728\u5927\u89c4\u6a21\u89c6\u9891\u5e73\u53f0\u4e0a\u7684\u5728\u7ebfA/B\u6d4b\u8bd5\u663e\u793a\uff0cWeWrite\u5c06\u70b9\u51fb\u7387\uff08VV>10s\uff09\u63d0\u9ad8\u4e861.07%\uff0c\u5e76\u5c06\u67e5\u8be2\u91cd\u6784\u7387\u964d\u4f4e\u4e862.97%\u3002", "conclusion": "WeWrite\u901a\u8fc7\u7cfb\u7edf\u6027\u5730\u89e3\u51b3\u4e2a\u6027\u5316\u67e5\u8be2\u91cd\u5199\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u6709\u6548\u63d0\u5347\u4e86\u89c6\u9891\u641c\u7d22\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.17793", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.17793", "abs": "https://arxiv.org/abs/2602.17793", "authors": ["Peide Zhu", "Linbin Lu", "Zhiqin Chen", "Xiong Chen"], "title": "LGD-Net: Latent-Guided Dual-Stream Network for HER2 Scoring with Task-Specific Domain Knowledge", "comment": null, "summary": "It is a critical task to evalaute HER2 expression level accurately for breast cancer evaluation and targeted treatment therapy selection. However, the standard multi-step Immunohistochemistry (IHC) staining is resource-intensive, expensive, and time-consuming, which is also often unavailable in many areas. Consequently, predicting HER2 levels directly from H&E slides has emerged as a potential alternative solution. It has been shown to be effective to use virtual IHC images from H&E images for automatic HER2 scoring. However, the pixel-level virtual staining methods are computationally expensive and prone to reconstruction artifacts that can propagate diagnostic errors. To address these limitations, we propose the Latent-Guided Dual-Stream Network (LGD-Net), a novel framework that employes cross-modal feature hallucination instead of explicit pixel-level image generation. LGD-Net learns to map morphological H&E features directly to the molecular latent space, guided by a teacher IHC encoder during training. To ensure the hallucinated features capture clinically relevant phenotypes, we explicitly regularize the model training with task-specific domain knowledge, specifically nuclei distribution and membrane staining intensity, via lightweight auxiliary regularization tasks. Extensive experiments on the public BCI dataset demonstrate that LGD-Net achieves state-of-the-art performance, significantly outperforming baseline methods while enabling efficient inference using single-modality H&E inputs.", "AI": {"tldr": "\u63d0\u51faLGD-Net\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u7279\u5f81\u5e7b\u89c9\u800c\u975e\u50cf\u7d20\u7ea7\u56fe\u50cf\u751f\u6210\uff0c\u4eceH&E\u5207\u7247\u76f4\u63a5\u9884\u6d4bHER2\u8868\u8fbe\u6c34\u5e73\uff0c\u907f\u514d\u865a\u62df\u67d3\u8272\u8ba1\u7b97\u5f00\u9500\u548c\u91cd\u5efa\u4f2a\u5f71", "motivation": "\u6807\u51c6IHC\u67d3\u8272\u8d44\u6e90\u5bc6\u96c6\u3001\u6602\u8d35\u4e14\u8017\u65f6\uff0c\u8bb8\u591a\u5730\u533a\u65e0\u6cd5\u83b7\u5f97\u3002\u4eceH&E\u5207\u7247\u76f4\u63a5\u9884\u6d4bHER2\u6c34\u5e73\u662f\u6f5c\u5728\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u73b0\u6709\u50cf\u7d20\u7ea7\u865a\u62df\u67d3\u8272\u65b9\u6cd5\u8ba1\u7b97\u6602\u8d35\u4e14\u6613\u4ea7\u751f\u91cd\u5efa\u4f2a\u5f71\uff0c\u53ef\u80fd\u5bfc\u81f4\u8bca\u65ad\u9519\u8bef", "method": "\u63d0\u51faLatent-Guided Dual-Stream Network (LGD-Net)\uff0c\u91c7\u7528\u8de8\u6a21\u6001\u7279\u5f81\u5e7b\u89c9\u800c\u975e\u663e\u5f0f\u50cf\u7d20\u7ea7\u56fe\u50cf\u751f\u6210\u3002\u6a21\u578b\u5b66\u4e60\u5c06\u5f62\u6001\u5b66H&E\u7279\u5f81\u76f4\u63a5\u6620\u5c04\u5230\u5206\u5b50\u6f5c\u5728\u7a7a\u95f4\uff0c\u5728\u8bad\u7ec3\u671f\u95f4\u7531\u6559\u5e08IHC\u7f16\u7801\u5668\u6307\u5bfc\u3002\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8f85\u52a9\u6b63\u5219\u5316\u4efb\u52a1\uff0c\u5229\u7528\u7279\u5b9a\u9886\u57df\u77e5\u8bc6\uff08\u6838\u5206\u5e03\u548c\u819c\u67d3\u8272\u5f3a\u5ea6\uff09\u6b63\u5219\u5316\u6a21\u578b\u8bad\u7ec3", "result": "\u5728\u516c\u5f00BCI\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cLGD-Net\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u540c\u65f6\u652f\u6301\u4f7f\u7528\u5355\u6a21\u6001H&E\u8f93\u5165\u8fdb\u884c\u9ad8\u6548\u63a8\u7406", "conclusion": "LGD-Net\u901a\u8fc7\u7279\u5f81\u5e7b\u89c9\u800c\u975e\u56fe\u50cf\u751f\u6210\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u865a\u62df\u67d3\u8272\u65b9\u6cd5\u7684\u8ba1\u7b97\u5f00\u9500\u548c\u4f2a\u5f71\u95ee\u9898\uff0c\u4e3a\u4eceH&E\u5207\u7247\u51c6\u786e\u9884\u6d4bHER2\u8868\u8fbe\u6c34\u5e73\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.17687", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17687", "abs": "https://arxiv.org/abs/2602.17687", "authors": ["Connor Shorten", "Augustas Skaburskas", "Daniel M. Jones", "Charles Pierse", "Roberto Esposito", "John Trengrove", "Etienne Dilocker", "Bob van Luijt"], "title": "IRPAPERS: A Visual Document Benchmark for Scientific Retrieval and Question Answering", "comment": "23 pages, 6 figures", "summary": "AI systems have achieved remarkable success in processing text and relational data, yet visual document processing remains relatively underexplored. Whereas traditional systems require OCR transcriptions to convert these visual documents into text and metadata, recent advances in multimodal foundation models offer retrieval and generation directly from document images. This raises a key question: How do image-based systems compare to established text-based methods? We introduce IRPAPERS, a benchmark of 3,230 pages from 166 scientific papers, with both an image and an OCR transcription for each page. Using 180 needle-in-the-haystack questions, we compare image- and text-based retrieval and question answering systems. Text retrieval using Arctic 2.0 embeddings, BM25, and hybrid text search achieved 46% Recall@1, 78% Recall@5, and 91% Recall@20, while image-based retrieval reaches 43%, 78%, and 93%, respectively. The two modalities exhibit complementary failures, enabling multimodal hybrid search to outperform either alone, achieving 49% Recall@1, 81% Recall@5, and 95% Recall@20. We further evaluate efficiency-performance tradeoffs with MUVERA and assess multiple multi-vector image embedding models. Among closed-source models, Cohere Embed v4 page image embeddings outperform Voyage 3 Large text embeddings and all tested open-source models, achieving 58% Recall@1, 87% Recall@5, and 97% Recall@20. For question answering, text-based RAG systems achieved higher ground-truth alignment than image-based systems (0.82 vs. 0.71), and both benefit substantially from increased retrieval depth, with multi-document retrieval outperforming oracle single-document retrieval. We analyze the complementary limitations of unimodal text and image representations and identify question types that require one modality over the other. The IRPAPERS dataset and all experimental code are publicly available.", "AI": {"tldr": "IRPAPERS\u662f\u4e00\u4e2a\u5305\u542b3,230\u9875\u79d1\u5b66\u8bba\u6587\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u6bd4\u8f83\u57fa\u4e8e\u56fe\u50cf\u548c\u57fa\u4e8e\u6587\u672c\u7684\u6587\u6863\u68c0\u7d22\u4e0e\u95ee\u7b54\u7cfb\u7edf\u3002\u7814\u7a76\u53d1\u73b0\u4e24\u79cd\u6a21\u6001\u5177\u6709\u4e92\u8865\u6027\uff0c\u591a\u6a21\u6001\u6df7\u5408\u641c\u7d22\u4f18\u4e8e\u5355\u4e00\u6a21\u6001\uff0c\u56fe\u50cf\u5d4c\u5165\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4f18\u4e8e\u6587\u672c\u5d4c\u5165\u3002", "motivation": "\u5c3d\u7ba1AI\u5728\u6587\u672c\u548c\u5173\u7cfb\u6570\u636e\u5904\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u89c6\u89c9\u6587\u6863\u5904\u7406\u4ecd\u76f8\u5bf9\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u4f20\u7edf\u7cfb\u7edf\u9700\u8981OCR\u8f6c\u5f55\u5c06\u89c6\u89c9\u6587\u6863\u8f6c\u6362\u4e3a\u6587\u672c\u548c\u5143\u6570\u636e\uff0c\u800c\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u53ef\u4ee5\u76f4\u63a5\u4ece\u6587\u6863\u56fe\u50cf\u8fdb\u884c\u68c0\u7d22\u548c\u751f\u6210\u3002\u8fd9\u5f15\u53d1\u4e86\u4e00\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u57fa\u4e8e\u56fe\u50cf\u7684\u7cfb\u7edf\u4e0e\u6210\u719f\u7684\u57fa\u4e8e\u6587\u672c\u65b9\u6cd5\u76f8\u6bd4\u8868\u73b0\u5982\u4f55\uff1f", "method": "\u5f15\u5165IRPAPERS\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b166\u7bc7\u79d1\u5b66\u8bba\u6587\u76843,230\u9875\uff0c\u6bcf\u9875\u90fd\u6709\u56fe\u50cf\u548cOCR\u8f6c\u5f55\u3002\u4f7f\u7528180\u4e2a\"\u5927\u6d77\u635e\u9488\"\u5f0f\u95ee\u9898\uff0c\u6bd4\u8f83\u57fa\u4e8e\u56fe\u50cf\u548c\u57fa\u4e8e\u6587\u672c\u7684\u68c0\u7d22\u4e0e\u95ee\u7b54\u7cfb\u7edf\u3002\u8bc4\u4f30\u4e86\u591a\u79cd\u68c0\u7d22\u65b9\u6cd5\uff08Arctic 2.0\u5d4c\u5165\u3001BM25\u3001\u6df7\u5408\u641c\u7d22\uff09\u548c\u56fe\u50cf\u5d4c\u5165\u6a21\u578b\uff08\u5305\u62ecCohere Embed v4\u3001Voyage 3 Large\u7b49\uff09\u3002", "result": "\u6587\u672c\u68c0\u7d22\u8fbe\u523046% Recall@1\u300178% Recall@5\u300191% Recall@20\uff1b\u56fe\u50cf\u68c0\u7d22\u8fbe\u523043%\u300178%\u300193%\u3002\u4e24\u79cd\u6a21\u6001\u8868\u73b0\u51fa\u4e92\u8865\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u591a\u6a21\u6001\u6df7\u5408\u641c\u7d22\u4f18\u4e8e\u4efb\u4e00\u5355\u4e00\u6a21\u6001\uff0849% Recall@1\u300181% Recall@5\u300195% Recall@20\uff09\u3002Cohere Embed v4\u56fe\u50cf\u5d4c\u5165\u5728\u6240\u6709\u6d4b\u8bd5\u6a21\u578b\u4e2d\u8868\u73b0\u6700\u4f73\uff0858% Recall@1\u300187% Recall@5\u300197% Recall@20\uff09\u3002\u5728\u95ee\u7b54\u65b9\u9762\uff0c\u57fa\u4e8e\u6587\u672c\u7684RAG\u7cfb\u7edf\u6bd4\u57fa\u4e8e\u56fe\u50cf\u7684\u7cfb\u7edf\u5177\u6709\u66f4\u9ad8\u7684\u771f\u5b9e\u5bf9\u9f50\u5ea6\uff080.82 vs. 0.71\uff09\u3002", "conclusion": "\u591a\u6a21\u6001\u65b9\u6cd5\u5728\u89c6\u89c9\u6587\u6863\u5904\u7406\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u56fe\u50cf\u548c\u6587\u672c\u8868\u793a\u5177\u6709\u4e92\u8865\u7684\u5c40\u9650\u6027\u3002\u67d0\u4e9b\u95ee\u9898\u7c7b\u578b\u9700\u8981\u7279\u5b9a\u6a21\u6001\u3002\u589e\u52a0\u68c0\u7d22\u6df1\u5ea6\u5bf9\u6240\u6709\u7cfb\u7edf\u90fd\u6709\u663e\u8457\u597d\u5904\uff0c\u591a\u6587\u6863\u68c0\u7d22\u4f18\u4e8eoracle\u5355\u6587\u6863\u68c0\u7d22\u3002IRPAPERS\u6570\u636e\u96c6\u548c\u5b9e\u9a8c\u4ee3\u7801\u5df2\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2602.17677", "categories": ["cs.LG", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17677", "abs": "https://arxiv.org/abs/2602.17677", "authors": ["Sutej Kulgod", "Sean Ye", "Sanchit Tanwar", "Christoffer Heckman"], "title": "Reducing Text Bias in Synthetically Generated MCQAs for VLMs in Autonomous Driving", "comment": "7 pages, 2 figures", "summary": "Multiple Choice Question Answering (MCQA) benchmarks are an established standard for measuring Vision Language Model (VLM) performance in driving tasks. However, we observe the known phenomenon that synthetically generated MCQAs are highly susceptible to hidden textual cues that allow models to exploit linguistic patterns rather than visual context. Our results show that a VLM fine-tuned on such data can achieve accuracy comparable to human-validated benchmarks even without visual input. Our proposed method reduces blind accuracy from +66.9% above random to +2.9%, eliminating the vast majority of exploitable textual shortcuts. By decoupling the correct answer from linguistic artifacts and employing a curriculum learning strategy, we force the model to rely on visual grounding, ensuring that performance accurately reflects perceptual understanding.", "AI": {"tldr": "\u8be5\u7814\u7a76\u53d1\u73b0\u57fa\u4e8e\u5408\u6210\u6570\u636e\u7684\u591a\u9009\u95ee\u7b54\u57fa\u51c6\u5b58\u5728\u6587\u672c\u7ebf\u7d22\u6f0f\u6d1e\uff0c\u5bfc\u81f4\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u65e0\u9700\u89c6\u89c9\u8f93\u5165\u5373\u53ef\u83b7\u5f97\u9ad8\u51c6\u786e\u7387\u3002\u4f5c\u8005\u63d0\u51fa\u65b9\u6cd5\u6d88\u9664\u6587\u672c\u6377\u5f84\uff0c\u8feb\u4f7f\u6a21\u578b\u4f9d\u8d56\u89c6\u89c9\u7406\u89e3\u3002", "motivation": "\u591a\u9009\u95ee\u7b54\u57fa\u51c6\u662f\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u9a7e\u9a76\u4efb\u52a1\u4e2d\u6027\u80fd\u7684\u6807\u51c6\u65b9\u6cd5\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u5408\u6210\u751f\u6210\u7684MCQA\u6570\u636e\u5b58\u5728\u9690\u85cf\u7684\u6587\u672c\u7ebf\u7d22\uff0c\u4f7f\u6a21\u578b\u53ef\u4ee5\u5229\u7528\u8bed\u8a00\u6a21\u5f0f\u800c\u975e\u89c6\u89c9\u4e0a\u4e0b\u6587\uff0c\u5bfc\u81f4\u8bc4\u4f30\u7ed3\u679c\u5931\u771f\u3002", "method": "\u901a\u8fc7\u5c06\u6b63\u786e\u7b54\u6848\u4e0e\u8bed\u8a00\u4f2a\u5f71\u89e3\u8026\uff0c\u5e76\u91c7\u7528\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u51cf\u5c11\u6a21\u578b\u5bf9\u6587\u672c\u6377\u5f84\u7684\u4f9d\u8d56\uff0c\u5f3a\u5236\u6a21\u578b\u8fdb\u884c\u89c6\u89c9\u57fa\u7840\u5316\uff0c\u786e\u4fdd\u6027\u80fd\u51c6\u786e\u53cd\u6620\u611f\u77e5\u7406\u89e3\u80fd\u529b\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5c06\u76f2\u76ee\u51c6\u786e\u7387\u4ece\u6bd4\u968f\u673a\u9ad8+66.9%\u964d\u4f4e\u5230+2.9%\uff0c\u6d88\u9664\u4e86\u7edd\u5927\u591a\u6570\u53ef\u5229\u7528\u7684\u6587\u672c\u6377\u5f84\uff0c\u4f7f\u6a21\u578b\u5fc5\u987b\u4f9d\u8d56\u89c6\u89c9\u8f93\u5165\u624d\u80fd\u83b7\u5f97\u826f\u597d\u6027\u80fd\u3002", "conclusion": "\u5408\u6210MCQA\u57fa\u51c6\u5b58\u5728\u4e25\u91cd\u7684\u6587\u672c\u7ebf\u7d22\u6f0f\u6d1e\uff0c\u9700\u8981\u6539\u8fdb\u8bc4\u4f30\u65b9\u6cd5\u4ee5\u786e\u4fdd\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u771f\u6b63\u53cd\u6620\u5176\u89c6\u89c9\u7406\u89e3\u80fd\u529b\uff0c\u800c\u975e\u5229\u7528\u8bed\u8a00\u6a21\u5f0f\u3002"}}
{"id": "2602.17720", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17720", "abs": "https://arxiv.org/abs/2602.17720", "authors": ["Yue Fu", "Yifan Lin", "Yessica Wang", "Sarah Tran", "Alexis Hiniker"], "title": "\"Everyone's using it, but no one is allowed to talk about it\": College Students' Experiences Navigating the Higher Education Environment in a Generative AI World", "comment": null, "summary": "Higher education students are increasingly using generative AI in their academic work. However, existing institutional practices have not yet adapted to this shift. Through semi-structured interviews with 23 college students, our study examines the environmental and social factors that influence students' use of AI. Findings show that institutional pressure factors like deadlines, exam cycles, and grading lead students to engage with AI even when they think it undermines their learning. Social influences, particularly peer micro-communities, establish de-facto AI norms regardless of official AI policies. Campus-wide ``AI shame'' is prevalent, often pushing AI use underground. Current institutional AI policies are perceived as generic, inconsistent, and confusing, resulting in routine noncompliance. Additionally, students develop value-based self-regulation strategies, but environmental pressures create a gap between students' intentions and their behaviors. Our findings show student AI use to be a situated practice, and we discuss implications for institutions, instructors, and system tool designers to effectively support student learning with AI.", "AI": {"tldr": "\u5927\u5b66\u751f\u5728\u5b66\u672f\u5de5\u4f5c\u4e2d\u8d8a\u6765\u8d8a\u591a\u5730\u4f7f\u7528\u751f\u6210\u5f0fAI\uff0c\u4f46\u73b0\u6709\u5236\u5ea6\u5b9e\u8df5\u5c1a\u672a\u9002\u5e94\u8fd9\u4e00\u8f6c\u53d8\u3002\u7814\u7a76\u53d1\u73b0\u5236\u5ea6\u538b\u529b\uff08\u622a\u6b62\u65e5\u671f\u3001\u8003\u8bd5\u5468\u671f\u3001\u8bc4\u5206\uff09\u548c\u793e\u4f1a\u56e0\u7d20\uff08\u540c\u4f34\u7fa4\u4f53\uff09\u5171\u540c\u5f71\u54cd\u5b66\u751f\u7684AI\u4f7f\u7528\u884c\u4e3a\uff0c\u5bfc\u81f4AI\u4f7f\u7528\u6210\u4e3a\u9690\u853d\u7684\u5b9e\u8df5\u3002", "motivation": "\u9ad8\u7b49\u6559\u80b2\u5b66\u751f\u8d8a\u6765\u8d8a\u591a\u5730\u5728\u5b66\u672f\u5de5\u4f5c\u4e2d\u4f7f\u7528\u751f\u6210\u5f0fAI\uff0c\u4f46\u73b0\u6709\u673a\u6784\u5b9e\u8df5\u5c1a\u672a\u9002\u5e94\u8fd9\u4e00\u8f6c\u53d8\u3002\u9700\u8981\u4e86\u89e3\u5f71\u54cd\u5b66\u751f\u4f7f\u7528AI\u7684\u73af\u5883\u548c\u793e\u4f1a\u56e0\u7d20\uff0c\u4ee5\u4fbf\u4e3a\u673a\u6784\u3001\u6559\u5e08\u548c\u7cfb\u7edf\u5de5\u5177\u8bbe\u8ba1\u8005\u63d0\u4f9b\u6709\u6548\u652f\u6301\u5b66\u751f\u5b66\u4e60\u7684\u5efa\u8bae\u3002", "method": "\u901a\u8fc7\u5bf923\u540d\u5927\u5b66\u751f\u8fdb\u884c\u534a\u7ed3\u6784\u5316\u8bbf\u8c08\uff0c\u7814\u7a76\u5b66\u751f\u4f7f\u7528AI\u7684\u73af\u5883\u548c\u793e\u4f1a\u5f71\u54cd\u56e0\u7d20\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u5236\u5ea6\u538b\u529b\u56e0\u7d20\uff08\u622a\u6b62\u65e5\u671f\u3001\u8003\u8bd5\u5468\u671f\u3001\u8bc4\u5206\uff09\u5bfc\u81f4\u5b66\u751f\u5373\u4f7f\u8ba4\u4e3aAI\u4f1a\u524a\u5f31\u5b66\u4e60\u6548\u679c\u4ecd\u4f1a\u4f7f\u7528\uff1b2\uff09\u793e\u4f1a\u5f71\u54cd\u7279\u522b\u662f\u540c\u4f34\u5fae\u793e\u533a\u5efa\u7acb\u4e86\u4e8b\u5b9e\u4e0a\u7684AI\u89c4\u8303\uff1b3\uff09\u6821\u56ed\u666e\u904d\u5b58\u5728\"AI\u7f9e\u803b\u611f\"\uff0c\u63a8\u52a8AI\u4f7f\u7528\u5730\u4e0b\u5316\uff1b4\uff09\u73b0\u6709AI\u653f\u7b56\u88ab\u5b66\u751f\u89c6\u4e3a\u7b3c\u7edf\u3001\u4e0d\u4e00\u81f4\u4e14\u4ee4\u4eba\u56f0\u60d1\uff0c\u5bfc\u81f4\u5e38\u89c4\u6027\u8fdd\u89c4\uff1b5\uff09\u5b66\u751f\u53d1\u5c55\u57fa\u4e8e\u4ef7\u503c\u7684\u81ea\u6211\u8c03\u8282\u7b56\u7565\uff0c\u4f46\u73af\u5883\u538b\u529b\u9020\u6210\u610f\u56fe\u4e0e\u884c\u4e3a\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "\u5b66\u751fAI\u4f7f\u7528\u662f\u4e00\u79cd\u60c5\u5883\u5316\u5b9e\u8df5\u3002\u7814\u7a76\u4e3a\u673a\u6784\u3001\u6559\u5e08\u548c\u7cfb\u7edf\u5de5\u5177\u8bbe\u8ba1\u8005\u63d0\u4f9b\u4e86\u6709\u6548\u652f\u6301\u5b66\u751f\u5b66\u4e60\u7684\u542f\u793a\uff0c\u9700\u8981\u91cd\u65b0\u601d\u8003AI\u653f\u7b56\u8bbe\u8ba1\uff0c\u51cf\u5c11AI\u7f9e\u803b\u611f\uff0c\u5e76\u66f4\u597d\u5730\u534f\u8c03\u73af\u5883\u538b\u529b\u4e0e\u5b66\u751f\u5b66\u4e60\u76ee\u6807\u3002"}}
{"id": "2602.17799", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17799", "abs": "https://arxiv.org/abs/2602.17799", "authors": ["Jose Sosa", "Danila Rukhovich", "Anis Kacem", "Djamila Aouada"], "title": "Enabling Training-Free Text-Based Remote Sensing Segmentation", "comment": null, "summary": "Recent advances in Vision Language Models (VLMs) and Vision Foundation Models (VFMs) have opened new opportunities for zero-shot text-guided segmentation of remote sensing imagery. However, most existing approaches still rely on additional trainable components, limiting their generalisation and practical applicability. In this work, we investigate to what extent text-based remote sensing segmentation can be achieved without additional training, by relying solely on existing foundation models. We propose a simple yet effective approach that integrates contrastive and generative VLMs with the Segment Anything Model (SAM), enabling a fully training-free or lightweight LoRA-tuned pipeline. Our contrastive approach employs CLIP as mask selector for SAM's grid-based proposals, achieving state-of-the-art open-vocabulary semantic segmentation (OVSS) in a completely zero-shot setting. In parallel, our generative approach enables reasoning and referring segmentation by generating click prompts for SAM using GPT-5 in a zero-shot setting and a LoRA-tuned Qwen-VL model, with the latter yielding the best results. Extensive experiments across 19 remote sensing benchmarks, including open-vocabulary, referring, and reasoning-based tasks, demonstrate the strong capabilities of our approach. Code will be released at https://github.com/josesosajs/trainfree-rs-segmentation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u9065\u611f\u56fe\u50cf\u6587\u672c\u5f15\u5bfc\u5206\u5272\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5bf9\u6bd4\u5f0f\u548c\u751f\u6210\u5f0f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0eSAM\uff0c\u572819\u4e2a\u9065\u611f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4e3a\u96f6\u6837\u672c\u6587\u672c\u5f15\u5bfc\u9065\u611f\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\uff0c\u4f46\u5927\u591a\u6570\u65b9\u6cd5\u4ecd\u4f9d\u8d56\u989d\u5916\u7684\u53ef\u8bad\u7ec3\u7ec4\u4ef6\uff0c\u9650\u5236\u4e86\u5176\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u5e94\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff1a1\uff09\u5bf9\u6bd4\u5f0f\u65b9\u6cd5\u4f7f\u7528CLIP\u4f5c\u4e3aSAM\u7f51\u683c\u63d0\u8bae\u7684\u63a9\u7801\u9009\u62e9\u5668\uff1b2\uff09\u751f\u6210\u5f0f\u65b9\u6cd5\u4f7f\u7528GPT-4V\u751f\u6210\u70b9\u51fb\u63d0\u793a\u7ed9SAM\uff0c\u6216\u4f7f\u7528LoRA\u5fae\u8c03\u7684Qwen-VL\u6a21\u578b\u3002", "result": "\u572819\u4e2a\u9065\u611f\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ec\u5f00\u653e\u8bcd\u6c47\u3001\u6307\u4ee3\u548c\u57fa\u4e8e\u63a8\u7406\u7684\u4efb\u52a1\uff09\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u5bf9\u6bd4\u5f0f\u65b9\u6cd5\u5728\u5b8c\u5168\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5f00\u653e\u8bcd\u6c47\u8bed\u4e49\u5206\u5272\uff0c\u751f\u6210\u5f0f\u65b9\u6cd5\u5728\u6307\u4ee3\u5206\u5272\u65b9\u9762\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u5c55\u793a\u4e86\u4ec5\u4f9d\u8d56\u73b0\u6709\u57fa\u7840\u6a21\u578b\u800c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u6587\u672c\u5f15\u5bfc\u9065\u611f\u56fe\u50cf\u5206\u5272\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.17856", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17856", "abs": "https://arxiv.org/abs/2602.17856", "authors": ["Hamideh Ghanadian", "Amin Kamali", "Mohammad Hossein Tekieh"], "title": "Enhancing Scientific Literature Chatbots with Retrieval-Augmented Generation: A Performance Evaluation of Vector and Graph-Based Systems", "comment": null, "summary": "This paper investigates the enhancement of scientific literature chatbots through retrieval-augmented generation (RAG), with a focus on evaluating vector- and graph-based retrieval systems. The proposed chatbot leverages both structured (graph) and unstructured (vector) databases to access scientific articles and gray literature, enabling efficient triage of sources according to research objectives. To systematically assess performance, we examine two use-case scenarios: retrieval from a single uploaded document and retrieval from a large-scale corpus. Benchmark test sets were generated using a GPT model, with selected outputs annotated for evaluation. The comparative analysis emphasizes retrieval accuracy and response relevance, providing insight into the strengths and limitations of each approach. The findings demonstrate the potential of hybrid RAG systems to improve accessibility to scientific knowledge and to support evidence-based decision making.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u6539\u8fdb\u79d1\u5b66\u6587\u732e\u804a\u5929\u673a\u5668\u4eba\uff0c\u91cd\u70b9\u8bc4\u4f30\u5411\u91cf\u548c\u56fe\u68c0\u7d22\u7cfb\u7edf\uff0c\u4f7f\u7528\u6df7\u5408\u6570\u636e\u5e93\u5b9e\u73b0\u9ad8\u6548\u6587\u732e\u7b5b\u9009\uff0c\u5e76\u901a\u8fc7\u4e24\u4e2a\u7528\u4f8b\u573a\u666f\u8fdb\u884c\u7cfb\u7edf\u6027\u80fd\u8bc4\u4f30\u3002", "motivation": "\u63d0\u5347\u79d1\u5b66\u6587\u732e\u804a\u5929\u673a\u5668\u4eba\u7684\u6027\u80fd\uff0c\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u6570\u636e\u68c0\u7d22\uff0c\u6539\u5584\u79d1\u5b66\u77e5\u8bc6\u7684\u53ef\u8bbf\u95ee\u6027\uff0c\u652f\u6301\u57fa\u4e8e\u8bc1\u636e\u7684\u51b3\u7b56\u5236\u5b9a\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7684\u804a\u5929\u673a\u5668\u4eba\uff0c\u7ed3\u5408\u56fe\u6570\u636e\u5e93\uff08\u7ed3\u6784\u5316\uff09\u548c\u5411\u91cf\u6570\u636e\u5e93\uff08\u975e\u7ed3\u6784\u5316\uff09\u68c0\u7d22\u79d1\u5b66\u6587\u732e\uff1b\u901a\u8fc7\u4e24\u4e2a\u7528\u4f8b\u573a\u666f\u8bc4\u4f30\uff1a\u5355\u6587\u6863\u68c0\u7d22\u548c\u5927\u89c4\u6a21\u8bed\u6599\u5e93\u68c0\u7d22\uff1b\u4f7f\u7528GPT\u6a21\u578b\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u5e76\u8fdb\u884c\u4eba\u5de5\u6807\u6ce8\u8bc4\u4f30\u3002", "result": "\u6bd4\u8f83\u5206\u6790\u663e\u793a\uff0c\u6df7\u5408RAG\u7cfb\u7edf\u5728\u68c0\u7d22\u51c6\u786e\u6027\u548c\u54cd\u5e94\u76f8\u5173\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u63ed\u793a\u4e86\u5411\u91cf\u548c\u56fe\u68c0\u7d22\u65b9\u6cd5\u5404\u81ea\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u8bc1\u660e\u4e86\u6df7\u5408\u7cfb\u7edf\u63d0\u5347\u79d1\u5b66\u77e5\u8bc6\u53ef\u8bbf\u95ee\u6027\u7684\u6f5c\u529b\u3002", "conclusion": "\u6df7\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u6539\u8fdb\u79d1\u5b66\u6587\u732e\u804a\u5929\u673a\u5668\u4eba\u7684\u6027\u80fd\uff0c\u4e3a\u79d1\u5b66\u77e5\u8bc6\u83b7\u53d6\u548c\u8bc1\u636e\u51b3\u7b56\u63d0\u4f9b\u6709\u529b\u652f\u6301\uff0c\u5c55\u793a\u4e86\u7ed3\u5408\u4e0d\u540c\u68c0\u7d22\u65b9\u6cd5\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.17826", "categories": ["cs.AI", "cs.LG", "cs.SC"], "pdf": "https://arxiv.org/pdf/2602.17826", "abs": "https://arxiv.org/abs/2602.17826", "authors": ["Marcelo Labre"], "title": "Ontology-Guided Neuro-Symbolic Inference: Grounding Language Models with Mathematical Domain Knowledge", "comment": "Submitted to NeuS 2026. Supplementary materials and code: https://doi.org/10.5281/zenodo.18665030", "summary": "Language models exhibit fundamental limitations -- hallucination, brittleness, and lack of formal grounding -- that are particularly problematic in high-stakes specialist fields requiring verifiable reasoning. I investigate whether formal domain ontologies can enhance language model reliability through retrieval-augmented generation. Using mathematics as proof of concept, I implement a neuro-symbolic pipeline leveraging the OpenMath ontology with hybrid retrieval and cross-encoder reranking to inject relevant definitions into model prompts. Evaluation on the MATH benchmark with three open-source models reveals that ontology-guided context improves performance when retrieval quality is high, but irrelevant context actively degrades it -- highlighting both the promise and challenges of neuro-symbolic approaches.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4f7f\u7528\u5f62\u5f0f\u5316\u9886\u57df\u672c\u4f53\uff08OpenMath\uff09\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6765\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u53d1\u73b0\u672c\u4f53\u5f15\u5bfc\u7684\u4e0a\u4e0b\u6587\u5728\u68c0\u7d22\u8d28\u91cf\u9ad8\u65f6\u80fd\u6539\u5584\u6027\u80fd\uff0c\u4f46\u65e0\u5173\u4e0a\u4e0b\u6587\u4f1a\u964d\u4f4e\u6027\u80fd\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u5e7b\u89c9\u3001\u8106\u5f31\u6027\u548c\u7f3a\u4e4f\u5f62\u5f0f\u5316\u57fa\u7840\u7b49\u6839\u672c\u9650\u5236\uff0c\u8fd9\u5728\u9700\u8981\u53ef\u9a8c\u8bc1\u63a8\u7406\u7684\u9ad8\u98ce\u9669\u4e13\u4e1a\u9886\u57df\u5c24\u4e3a\u6210\u95ee\u9898\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5f62\u5f0f\u5316\u9886\u57df\u672c\u4f53\u662f\u5426\u80fd\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6765\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002", "method": "\u4f7f\u7528\u6570\u5b66\u4f5c\u4e3a\u6982\u5ff5\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u4e86\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7\u7ba1\u9053\uff0c\u5229\u7528OpenMath\u672c\u4f53\u8fdb\u884c\u6df7\u5408\u68c0\u7d22\u548c\u4ea4\u53c9\u7f16\u7801\u5668\u91cd\u6392\u5e8f\uff0c\u5c06\u76f8\u5173\u5b9a\u4e49\u6ce8\u5165\u6a21\u578b\u63d0\u793a\u4e2d\u3002\u5728MATH\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8bc4\u4f30\u4e86\u4e09\u4e2a\u5f00\u6e90\u6a21\u578b\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u5f53\u68c0\u7d22\u8d28\u91cf\u9ad8\u65f6\uff0c\u672c\u4f53\u5f15\u5bfc\u7684\u4e0a\u4e0b\u6587\u80fd\u63d0\u9ad8\u6027\u80fd\uff0c\u4f46\u65e0\u5173\u4e0a\u4e0b\u6587\u4f1a\u4e3b\u52a8\u964d\u4f4e\u6027\u80fd\uff0c\u8fd9\u51f8\u663e\u4e86\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u7684\u6f5c\u529b\u548c\u6311\u6218\u3002", "conclusion": "\u5f62\u5f0f\u5316\u672c\u4f53\u5728\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u53ef\u9760\u6027\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u68c0\u7d22\u8d28\u91cf\u81f3\u5173\u91cd\u8981\uff0c\u65e0\u5173\u4e0a\u4e0b\u6587\u4f1a\u635f\u5bb3\u6027\u80fd\uff0c\u8fd9\u4e3a\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u7684\u53d1\u5c55\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2602.17721", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2602.17721", "abs": "https://arxiv.org/abs/2602.17721", "authors": ["Melissa Langworthy", "Yana Rodgers"], "title": "Gender and Digital Platform Work During Turbulent Times", "comment": "Gender, Work & Organization", "summary": "This commentary explores how the platform economy shapes labour market responses during times of crisis, with a focus on gendered experiences. Drawing on cases of economic crisis, natural disasters, and refugee displacement, it examines how digital labour platforms offer flexible work opportunities while also reinforcing existing inequalities. Women face distinct constraints (such as caregiving responsibilities, limited mobility, and economic insecurity) that hinder their employment opportunities and earnings potential. These constraints are more pronounced during crises, when access to stable income and safe working conditions becomes more difficult. While platform work can serve as a lifeline, it is not a guaranteed solution, and its benefits are unevenly distributed. The commentary calls for gender-responsive policies and new research to understand how digital infrastructures mediate labour experiences across different crisis contexts. Such research can inform inclusive strategies that promote resilience and equity in platform-based work, particularly for marginalized and displaced populations.", "AI": {"tldr": "\u5e73\u53f0\u7ecf\u6d4e\u5728\u5371\u673a\u65f6\u671f\u5982\u4f55\u5851\u9020\u52b3\u52a8\u529b\u5e02\u573a\u53cd\u5e94\uff0c\u7279\u522b\u5173\u6ce8\u6027\u522b\u5316\u4f53\u9a8c\uff0c\u6307\u51fa\u5e73\u53f0\u5de5\u4f5c\u65e2\u662f\u7075\u6d3b\u673a\u4f1a\u4e5f\u5f3a\u5316\u4e0d\u5e73\u7b49\uff0c\u5973\u6027\u9762\u4e34\u66f4\u591a\u7ea6\u675f\uff0c\u9700\u8981\u6027\u522b\u654f\u611f\u653f\u7b56", "motivation": "\u63a2\u8ba8\u5e73\u53f0\u7ecf\u6d4e\u5728\u5371\u673a\u65f6\u671f\uff08\u7ecf\u6d4e\u5371\u673a\u3001\u81ea\u7136\u707e\u5bb3\u3001\u96be\u6c11\u6d41\u79bb\u5931\u6240\uff09\u5982\u4f55\u5f71\u54cd\u52b3\u52a8\u529b\u5e02\u573a\uff0c\u7279\u522b\u5173\u6ce8\u6027\u522b\u5dee\u5f02\uff0c\u7406\u89e3\u6570\u5b57\u5e73\u53f0\u5982\u4f55\u540c\u65f6\u63d0\u4f9b\u5de5\u4f5c\u673a\u4f1a\u53c8\u5f3a\u5316\u73b0\u6709\u4e0d\u5e73\u7b49", "method": "\u57fa\u4e8e\u6848\u4f8b\u7814\u7a76\u7684\u65b9\u6cd5\uff0c\u5206\u6790\u7ecf\u6d4e\u5371\u673a\u3001\u81ea\u7136\u707e\u5bb3\u548c\u96be\u6c11\u6d41\u79bb\u5931\u6240\u7b49\u4e0d\u540c\u5371\u673a\u60c5\u5883\uff0c\u8003\u5bdf\u6570\u5b57\u52b3\u52a8\u5e73\u53f0\u5982\u4f55\u8fd0\u4f5c\u53ca\u5176\u5bf9\u6027\u522b\u5316\u52b3\u52a8\u4f53\u9a8c\u7684\u5f71\u54cd", "result": "\u5e73\u53f0\u5de5\u4f5c\u5728\u5371\u673a\u65f6\u671f\u53ef\u4f5c\u4e3a\"\u751f\u547d\u7ebf\"\u63d0\u4f9b\u7075\u6d3b\u5de5\u4f5c\u673a\u4f1a\uff0c\u4f46\u540c\u65f6\u4e5f\u5f3a\u5316\u4e86\u73b0\u6709\u4e0d\u5e73\u7b49\uff1b\u5973\u6027\u56e0\u7167\u987e\u8d23\u4efb\u3001\u6d41\u52a8\u6027\u9650\u5236\u548c\u7ecf\u6d4e\u4e0d\u5b89\u5168\u7b49\u7ea6\u675f\u9762\u4e34\u66f4\u591a\u5c31\u4e1a\u969c\u788d\u548c\u6536\u5165\u6f5c\u529b\u9650\u5236\uff1b\u5e73\u53f0\u5de5\u4f5c\u7684\u76ca\u5904\u5206\u5e03\u4e0d\u5747\uff0c\u5e76\u975e\u53ef\u9760\u89e3\u51b3\u65b9\u6848", "conclusion": "\u9700\u8981\u6027\u522b\u654f\u611f\u7684\u653f\u7b56\u548c\u65b0\u7684\u7814\u7a76\u6765\u7406\u89e3\u6570\u5b57\u57fa\u7840\u8bbe\u65bd\u5982\u4f55\u5728\u4e0d\u540c\u5371\u673a\u60c5\u5883\u4e2d\u4ecb\u5bfc\u52b3\u52a8\u4f53\u9a8c\uff0c\u4ee5\u5236\u5b9a\u4fc3\u8fdb\u5e73\u53f0\u5de5\u4f5c\u97e7\u6027\u548c\u516c\u5e73\u6027\u7684\u5305\u5bb9\u6027\u7b56\u7565\uff0c\u7279\u522b\u662f\u9488\u5bf9\u8fb9\u7f18\u5316\u548c\u6d41\u79bb\u5931\u6240\u7fa4\u4f53"}}
{"id": "2602.18107", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.18107", "abs": "https://arxiv.org/abs/2602.18107", "authors": ["Andrew Parry", "Debasis Ganguly", "Sean MacAvaney"], "title": "SuiteEval: Simplifying Retrieval Benchmarks", "comment": "5 pages, 3 figures, 2 tables, Accepted as a Demonstration to ECIR 2026", "summary": "Information retrieval evaluation often suffers from fragmented practices -- varying dataset subsets, aggregation methods, and pipeline configurations -- that undermine reproducibility and comparability, especially for foundation embedding models requiring robust out-of-domain performance. We introduce SuiteEval, a unified framework that offers automatic end-to-end evaluation, dynamic indexing that reuses on-disk indices to minimise disk usage, and built-in support for major benchmarks (BEIR, LoTTE, MS MARCO, NanoBEIR, and BRIGHT). Users only need to supply a pipeline generator. SuiteEval handles data loading, indexing, ranking, metric computation, and result aggregation. New benchmark suites can be added in a single line. SuiteEval reduces boilerplate and standardises evaluations to facilitate reproducible IR research, as a broader benchmark set is increasingly required.", "AI": {"tldr": "SuiteEval\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u4fe1\u606f\u68c0\u7d22\u8bc4\u4f30\u6846\u67b6\uff0c\u63d0\u4f9b\u81ea\u52a8\u7aef\u5230\u7aef\u8bc4\u4f30\u3001\u52a8\u6001\u7d22\u5f15\u91cd\u7528\u548c\u4e3b\u6d41\u57fa\u51c6\u6d4b\u8bd5\u652f\u6301\uff0c\u65e8\u5728\u89e3\u51b3\u8bc4\u4f30\u5b9e\u8df5\u788e\u7247\u5316\u95ee\u9898\u3002", "motivation": "\u4fe1\u606f\u68c0\u7d22\u8bc4\u4f30\u5b58\u5728\u788e\u7247\u5316\u5b9e\u8df5\u95ee\u9898\u2014\u2014\u4e0d\u540c\u7684\u6570\u636e\u96c6\u5b50\u96c6\u3001\u805a\u5408\u65b9\u6cd5\u548c\u7ba1\u9053\u914d\u7f6e\u2014\u2014\u8fd9\u635f\u5bb3\u4e86\u53ef\u91cd\u590d\u6027\u548c\u53ef\u6bd4\u6027\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u9700\u8981\u5f3a\u5927\u8de8\u57df\u6027\u80fd\u7684\u57fa\u7840\u5d4c\u5165\u6a21\u578b\u3002", "method": "\u5f15\u5165SuiteEval\u6846\u67b6\uff0c\u63d0\u4f9b\u81ea\u52a8\u7aef\u5230\u7aef\u8bc4\u4f30\u3001\u52a8\u6001\u7d22\u5f15\u91cd\u7528\u78c1\u76d8\u7d22\u5f15\u4ee5\u6700\u5c0f\u5316\u78c1\u76d8\u4f7f\u7528\uff0c\u5e76\u5185\u7f6e\u652f\u6301\u4e3b\u8981\u57fa\u51c6\u6d4b\u8bd5\uff08BEIR\u3001LoTTE\u3001MS MARCO\u3001NanoBEIR\u548cBRIGHT\uff09\u3002\u7528\u6237\u53ea\u9700\u63d0\u4f9b\u7ba1\u9053\u751f\u6210\u5668\uff0c\u6846\u67b6\u5904\u7406\u6570\u636e\u52a0\u8f7d\u3001\u7d22\u5f15\u3001\u6392\u5e8f\u3001\u6307\u6807\u8ba1\u7b97\u548c\u7ed3\u679c\u805a\u5408\u3002", "result": "SuiteEval\u51cf\u5c11\u4e86\u6837\u677f\u4ee3\u7801\u5e76\u6807\u51c6\u5316\u4e86\u8bc4\u4f30\u6d41\u7a0b\uff0c\u4fc3\u8fdb\u4e86\u53ef\u91cd\u590d\u7684\u4fe1\u606f\u68c0\u7d22\u7814\u7a76\uff0c\u7279\u522b\u662f\u968f\u7740\u9700\u8981\u66f4\u5e7f\u6cdb\u57fa\u51c6\u6d4b\u8bd5\u96c6\u7684\u9700\u6c42\u589e\u52a0\u3002", "conclusion": "SuiteEval\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u89e3\u51b3\u4e86\u4fe1\u606f\u68c0\u7d22\u8bc4\u4f30\u7684\u788e\u7247\u5316\u95ee\u9898\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u3001\u53ef\u91cd\u590d\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u652f\u6301\u66f4\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002"}}
{"id": "2602.17831", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17831", "abs": "https://arxiv.org/abs/2602.17831", "authors": ["Simon Henniger", "Gabriel Poesia"], "title": "The Token Games: Evaluating Language Model Reasoning with Puzzle Duels", "comment": "Project website: https://token-games.ai/", "summary": "Evaluating the reasoning capabilities of Large Language Models is increasingly challenging as models improve. Human curation of hard questions is highly expensive, especially in recent benchmarks using PhD-level domain knowledge to challenge the most capable models. Even then, there is always a concern about whether these questions test genuine reasoning or if similar problems have been seen during training. Here, we take inspiration from 16th-century mathematical duels to design The Token Games (TTG): an evaluation framework where models challenge each other by creating their own puzzles. We leverage the format of Programming Puzzles - given a Python function that returns a boolean, find inputs that make it return True - to flexibly represent problems and enable verifying solutions. Using results from pairwise duels, we then compute Elo ratings, allowing us to compare models relative to each other. We evaluate 10 frontier models on TTG, and closely match the ranking from existing benchmarks such as Humanity's Last Exam, without involving any human effort in creating puzzles. We also find that creating good puzzles is still a highly challenging task for current models, not measured by previous benchmarks. Overall, our work suggests new paradigms for evaluating reasoning that cannot be saturated by design, and that allow testing models for other skills like creativity and task creation alongside problem solving.", "AI": {"tldr": "TTG\u662f\u4e00\u4e2a\u57fa\u4e8e16\u4e16\u7eaa\u6570\u5b66\u51b3\u6597\u542f\u53d1\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u8ba9\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u76f8\u4e92\u521b\u5efa\u7f16\u7a0b\u8c1c\u9898\u6765\u6311\u6218\u5bf9\u65b9\uff0c\u4ece\u800c\u8bc4\u4f30\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700\u4eba\u5de5\u521b\u5efa\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u9762\u4e34\u6311\u6218\uff1a\u4eba\u5de5\u521b\u5efa\u96be\u9898\u6210\u672c\u9ad8\u6602\uff0c\u4e14\u96be\u4ee5\u786e\u5b9a\u6a21\u578b\u662f\u5426\u771f\u6b63\u63a8\u7406\u8fd8\u662f\u89c1\u8fc7\u7c7b\u4f3c\u8bad\u7ec3\u6570\u636e\u3002\u9700\u8981\u4e00\u79cd\u65e0\u6cd5\u901a\u8fc7\u8bbe\u8ba1\u9971\u548c\u7684\u8bc4\u4f30\u8303\u5f0f\u3002", "method": "\u91c7\u752816\u4e16\u7eaa\u6570\u5b66\u51b3\u6597\u683c\u5f0f\uff0c\u8ba9\u6a21\u578b\u901a\u8fc7\u521b\u5efa\u7f16\u7a0b\u8c1c\u9898\u76f8\u4e92\u6311\u6218\u3002\u4f7f\u7528\u7f16\u7a0b\u8c1c\u9898\u683c\u5f0f\uff08\u7ed9\u5b9a\u8fd4\u56de\u5e03\u5c14\u503c\u7684Python\u51fd\u6570\uff0c\u627e\u5230\u4f7f\u5176\u8fd4\u56deTrue\u7684\u8f93\u5165\uff09\u7075\u6d3b\u8868\u793a\u95ee\u9898\u5e76\u9a8c\u8bc1\u89e3\u51b3\u65b9\u6848\u3002\u901a\u8fc7\u4e24\u4e24\u5bf9\u51b3\u7ed3\u679c\u8ba1\u7b97Elo\u8bc4\u5206\u6765\u6bd4\u8f83\u6a21\u578b\u3002", "result": "\u8bc4\u4f30\u4e8610\u4e2a\u524d\u6cbf\u6a21\u578b\uff0c\u7ed3\u679c\u4e0e\u73b0\u6709\u57fa\u51c6\uff08\u5982Humanity's Last Exam\uff09\u6392\u540d\u9ad8\u5ea6\u5339\u914d\uff0c\u4e14\u65e0\u9700\u4eba\u5de5\u521b\u5efa\u8c1c\u9898\u3002\u53d1\u73b0\u521b\u5efa\u4f18\u8d28\u8c1c\u9898\u5bf9\u5f53\u524d\u6a21\u578b\u4ecd\u662f\u6781\u5177\u6311\u6218\u7684\u4efb\u52a1\uff0c\u8fd9\u662f\u5148\u524d\u57fa\u51c6\u672a\u6d4b\u91cf\u7684\u80fd\u529b\u3002", "conclusion": "TTG\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u63a8\u7406\u8bc4\u4f30\u8303\u5f0f\uff0c\u65e0\u6cd5\u901a\u8fc7\u8bbe\u8ba1\u9971\u548c\uff0c\u80fd\u591f\u540c\u65f6\u6d4b\u8bd5\u6a21\u578b\u7684\u521b\u9020\u529b\u3001\u4efb\u52a1\u521b\u5efa\u80fd\u529b\u4ee5\u53ca\u95ee\u9898\u89e3\u51b3\u80fd\u529b\uff0c\u4e3a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2602.17680", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17680", "abs": "https://arxiv.org/abs/2602.17680", "authors": ["Yujia Wang", "Jihong Guan", "Wengen Li", "Shuigeng Zhou", "Xuhong Wang"], "title": "BioBridge: Bridging Proteins and Language for Enhanced Biological Reasoning with LLMs", "comment": null, "summary": "Existing Protein Language Models (PLMs) often suffer from limited adaptability to multiple tasks and exhibit poor generalization across diverse biological contexts. In contrast, general-purpose Large Language Models (LLMs) lack the capability to interpret protein sequences and fall short in domain-specific knowledge, limiting their capacity for effective biosemantic reasoning. To combine the advantages of both, we propose BioBridge, a domain-adaptive continual pretraining framework for protein understanding. This framework employs Domain-Incremental Continual Pre-training (DICP) to infuse protein domain knowledge and general reasoning corpus into a LLM simultaneously, effectively mitigating catastrophic forgetting. Cross-modal alignment is achieved via a PLM-Projector-LLM pipeline, which maps protein sequence embeddings into the semantic space of the language model. Ultimately, an end-to-end optimization is adopted to uniformly support various tasks, including protein property prediction and knowledge question-answering. Our proposed BioBridge demonstrates performance comparable to that of mainstream PLMs on multiple protein benchmarks, such as EC and BindingDB. It also achieves results on par with LLMs on general understanding tasks like MMLU and RACE. This showcases its innovative advantage of combining domain-specific adaptability with general-purpose language competency.", "AI": {"tldr": "BioBridge\u662f\u4e00\u4e2a\u7ed3\u5408\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u548c\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4f18\u52bf\u7684\u9886\u57df\u81ea\u9002\u5e94\u6301\u7eed\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u9886\u57df\u589e\u91cf\u6301\u7eed\u9884\u8bad\u7ec3\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u5728\u86cb\u767d\u8d28\u7406\u89e3\u548c\u901a\u7528\u7406\u89e3\u4efb\u52a1\u4e0a\u90fd\u53d6\u5f97\u4e86\u826f\u597d\u8868\u73b0\u3002", "motivation": "\u73b0\u6709\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4efb\u52a1\u9002\u5e94\u6027\u548c\u8de8\u751f\u7269\u73af\u5883\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u800c\u901a\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u86cb\u767d\u8d28\u5e8f\u5217\u89e3\u91ca\u80fd\u529b\u548c\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff0c\u65e0\u6cd5\u8fdb\u884c\u6709\u6548\u7684\u751f\u7269\u8bed\u4e49\u63a8\u7406\u3002\u9700\u8981\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u3002", "method": "\u63d0\u51faBioBridge\u6846\u67b6\uff0c\u91c7\u7528\u9886\u57df\u589e\u91cf\u6301\u7eed\u9884\u8bad\u7ec3(DICP)\u540c\u65f6\u6ce8\u5165\u86cb\u767d\u8d28\u9886\u57df\u77e5\u8bc6\u548c\u901a\u7528\u63a8\u7406\u8bed\u6599\uff0c\u901a\u8fc7PLM-Projector-LLM\u7ba1\u9053\u5b9e\u73b0\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u5c06\u86cb\u767d\u8d28\u5e8f\u5217\u5d4c\u5165\u6620\u5c04\u5230\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u4e49\u7a7a\u95f4\uff0c\u91c7\u7528\u7aef\u5230\u7aef\u4f18\u5316\u652f\u6301\u591a\u79cd\u4efb\u52a1\u3002", "result": "BioBridge\u5728EC\u548cBindingDB\u7b49\u591a\u4e2a\u86cb\u767d\u8d28\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4e0e\u4e3b\u6d41\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u76f8\u5f53\uff0c\u5728MMLU\u548cRACE\u7b49\u901a\u7528\u7406\u89e3\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u76f8\u5f53\u7684\u7ed3\u679c\uff0c\u5c55\u793a\u4e86\u9886\u57df\u7279\u5b9a\u9002\u5e94\u6027\u4e0e\u901a\u7528\u8bed\u8a00\u80fd\u529b\u7684\u521b\u65b0\u7ed3\u5408\u4f18\u52bf\u3002", "conclusion": "BioBridge\u6210\u529f\u7ed3\u5408\u4e86\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u7684\u9886\u57df\u4e13\u4e1a\u6027\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u901a\u7528\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6301\u7eed\u9884\u8bad\u7ec3\u6846\u67b6\u5b9e\u73b0\u4e86\u86cb\u767d\u8d28\u7406\u89e3\u548c\u901a\u7528\u8bed\u8a00\u7406\u89e3\u7684\u53cc\u91cd\u4f18\u52bf\uff0c\u4e3a\u751f\u7269\u4fe1\u606f\u5b66\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u591a\u6a21\u6001\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.17726", "categories": ["cs.CY", "cs.DC", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.17726", "abs": "https://arxiv.org/abs/2602.17726", "authors": ["Qness Ndlovu"], "title": "Closing Africa's Early Warning Gap: AI Weather Forecasting for Disaster Prevention", "comment": "23 pages, 4 figures", "summary": "In January 2026, torrential rains killed 200-300 people across Southern Africa, exposing a critical reality: 60% of the continent lacks effective early warning systems due to infrastructure costs. Traditional radar stations exceed USD 1 million each, leaving Africa with an 18x coverage deficit compared to the US and EU. We present a production-grade architecture for deploying NVIDIA Earth-2 AI weather models at USD 1,430-1,730/month for national-scale deployment - enabling coverage at 2,000-4,545x lower cost than radar. The system generates 15-day global atmospheric forecasts, cached in PostgreSQL to enable user queries under 200 milliseconds without real-time inference.\n  Deployed in South Africa in February 2026, our system demonstrates three technical contributions: (1) a ProcessPoolExecutor-based event loop isolation pattern that resolves aiobotocore session lifecycle conflicts in async Python applications; (2) a database-backed serving architecture where the GPU writes global forecasts directly to PostgreSQL, eliminating HTTP transfer bottlenecks for high-resolution tensors; and (3) an automated coordinate management pattern for multi-step inference across 61 timesteps. Forecasts are delivered via WhatsApp, leveraging 80%+ market penetration. This architecture makes continent-scale early warning systems economically viable, supporting UNDRR findings that such systems reduce disaster death rates by 6x. All architectural details are documented inline for full reproducibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u6210\u672cAI\u5929\u6c14\u9884\u8b66\u7cfb\u7edf\u67b6\u6784\uff0c\u4f7f\u7528NVIDIA Earth-2\u6a21\u578b\uff0c\u4ee5\u6bcf\u67081430-1730\u7f8e\u5143\u7684\u6210\u672c\u5b9e\u73b0\u56fd\u5bb6\u7ea7\u90e8\u7f72\uff0c\u6210\u672c\u6bd4\u4f20\u7edf\u96f7\u8fbe\u4f4e2000-4545\u500d\uff0c\u901a\u8fc7WhatsApp\u63d0\u4f9b15\u5929\u5168\u7403\u5929\u6c14\u9884\u62a5\u3002", "motivation": "2026\u5e741\u6708\u5357\u975e\u66b4\u96e8\u9020\u6210200-300\u4eba\u6b7b\u4ea1\uff0c\u66b4\u9732\u4e86\u975e\u6d3260%\u5730\u533a\u7f3a\u4e4f\u6709\u6548\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u7684\u95ee\u9898\u3002\u4f20\u7edf\u96f7\u8fbe\u7ad9\u6bcf\u4e2a\u8d85\u8fc7100\u4e07\u7f8e\u5143\uff0c\u5bfc\u81f4\u975e\u6d32\u7684\u8986\u76d6\u7387\u6bd4\u7f8e\u56fd\u548c\u6b27\u76df\u4f4e18\u500d\uff0c\u9700\u8981\u7ecf\u6d4e\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u751f\u4ea7\u7ea7\u67b6\u6784\u90e8\u7f72NVIDIA Earth-2 AI\u5929\u6c14\u6a21\u578b\uff0c\u4f7f\u7528ProcessPoolExecutor\u89e3\u51b3\u5f02\u6b65Python\u5e94\u7528\u4e2d\u7684\u4f1a\u8bdd\u751f\u547d\u5468\u671f\u51b2\u7a81\uff0c\u5efa\u7acb\u6570\u636e\u5e93\u652f\u6301\u7684GPU\u76f4\u63a5\u5199\u5165PostgreSQL\u7684\u67b6\u6784\uff0c\u5b9e\u73b061\u4e2a\u65f6\u95f4\u6b65\u7684\u591a\u6b65\u63a8\u7406\u81ea\u52a8\u5750\u6807\u7ba1\u7406\u3002", "result": "2026\u5e742\u6708\u5728\u5357\u975e\u6210\u529f\u90e8\u7f72\uff0c\u7cfb\u7edf\u80fd\u4ee5200\u6beb\u79d2\u5185\u54cd\u5e94\u7528\u6237\u67e5\u8be2\uff0c\u901a\u8fc7WhatsApp\uff08\u5e02\u573a\u6e17\u900f\u738780%+\uff09\u63d0\u4f9b15\u5929\u5168\u7403\u5927\u6c14\u9884\u62a5\uff0c\u4f7f\u5927\u9646\u89c4\u6a21\u7684\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u5728\u7ecf\u6d4e\u4e0a\u53ef\u884c\u3002", "conclusion": "\u8be5\u67b6\u6784\u4f7f\u5927\u9646\u89c4\u6a21\u7684\u65e9\u671f\u9884\u8b66\u7cfb\u7edf\u5728\u7ecf\u6d4e\u4e0a\u53ef\u884c\uff0c\u652f\u6301UNDRR\u5173\u4e8e\u6b64\u7c7b\u7cfb\u7edf\u80fd\u5c06\u707e\u5bb3\u6b7b\u4ea1\u7387\u964d\u4f4e6\u500d\u7684\u53d1\u73b0\uff0c\u6240\u6709\u67b6\u6784\u7ec6\u8282\u90fd\u6709\u5b8c\u6574\u6587\u6863\u652f\u6301\u53ef\u590d\u73b0\u6027\u3002"}}
{"id": "2602.17814", "categories": ["cs.CV", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17814", "abs": "https://arxiv.org/abs/2602.17814", "authors": ["Adrian Catalin Lutu", "Eduard Poesina", "Radu Tudor Ionescu"], "title": "VQPP: Video Query Performance Prediction Benchmark", "comment": null, "summary": "Query performance prediction (QPP) is an important and actively studied information retrieval task, having various applications, such as query reformulation, query expansion, and retrieval system selection, among many others. The task has been primarily studied in the context of text and image retrieval, whereas QPP for content-based video retrieval (CBVR) remains largely underexplored. To this end, we propose the first benchmark for video query performance prediction (VQPP), comprising two text-to-video retrieval datasets and two CBVR systems, respectively. VQPP contains a total of 56K text queries and 51K videos, and comes with official training, validation and test splits, fostering direct comparisons and reproducible results. We explore multiple pre-retrieval and post-retrieval performance predictors, creating a representative benchmark for future exploration of QPP in the video domain. Our results show that pre-retrieval predictors obtain competitive performance, enabling applications before performing the retrieval step. We also demonstrate the applicability of VQPP by employing the best performing pre-retrieval predictor as reward model for training a large language model (LLM) on the query reformulation task via direct preference optimization (DPO). We release our benchmark and code at https://github.com/AdrianLutu/VQPP.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u89c6\u9891\u67e5\u8be2\u6027\u80fd\u9884\u6d4b\u57fa\u51c6VQPP\uff0c\u5305\u542b\u4e24\u4e2a\u6587\u672c\u5230\u89c6\u9891\u68c0\u7d22\u6570\u636e\u96c6\u548c\u4e24\u4e2aCBVR\u7cfb\u7edf\uff0c\u63a2\u7d22\u4e86\u9884\u68c0\u7d22\u548c\u540e\u68c0\u7d22\u6027\u80fd\u9884\u6d4b\u5668\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u67e5\u8be2\u91cd\u5199\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u67e5\u8be2\u6027\u80fd\u9884\u6d4b\u5728\u4fe1\u606f\u68c0\u7d22\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u4e3b\u8981\u7814\u7a76\u96c6\u4e2d\u5728\u6587\u672c\u548c\u56fe\u50cf\u68c0\u7d22\u9886\u57df\uff0c\u57fa\u4e8e\u5185\u5bb9\u7684\u89c6\u9891\u68c0\u7d22\u4e2d\u7684QPP\u7814\u7a76\u76f8\u5bf9\u4e0d\u8db3\uff0c\u9700\u8981\u5efa\u7acb\u4e13\u95e8\u7684\u57fa\u51c6\u6765\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55\u3002", "method": "\u521b\u5efaVQPP\u57fa\u51c6\uff0c\u5305\u542b\u4e24\u4e2a\u6587\u672c\u5230\u89c6\u9891\u68c0\u7d22\u6570\u636e\u96c6\u548c\u4e24\u4e2aCBVR\u7cfb\u7edf\uff0c\u517156K\u6587\u672c\u67e5\u8be2\u548c51K\u89c6\u9891\uff1b\u63a2\u7d22\u591a\u79cd\u9884\u68c0\u7d22\u548c\u540e\u68c0\u7d22\u6027\u80fd\u9884\u6d4b\u5668\uff1b\u4f7f\u7528\u6700\u4f73\u9884\u68c0\u7d22\u9884\u6d4b\u5668\u4f5c\u4e3a\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u76f4\u63a5\u504f\u597d\u4f18\u5316\u8bad\u7ec3LLM\u8fdb\u884c\u67e5\u8be2\u91cd\u5199\u3002", "result": "\u9884\u68c0\u7d22\u9884\u6d4b\u5668\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u80fd\u591f\u5728\u68c0\u7d22\u6b65\u9aa4\u4e4b\u524d\u5b9e\u73b0\u5e94\u7528\uff1b\u6210\u529f\u5c06\u6700\u4f73\u9884\u68c0\u7d22\u9884\u6d4b\u5668\u5e94\u7528\u4e8e\u67e5\u8be2\u91cd\u5199\u4efb\u52a1\uff0c\u901a\u8fc7DPO\u8bad\u7ec3LLM\uff1b\u5efa\u7acb\u4e86\u53ef\u590d\u73b0\u7684\u89c6\u9891QPP\u57fa\u51c6\u3002", "conclusion": "VQPP\u662f\u9996\u4e2a\u89c6\u9891\u67e5\u8be2\u6027\u80fd\u9884\u6d4b\u57fa\u51c6\uff0c\u586b\u8865\u4e86CBVR\u9886\u57dfQPP\u7814\u7a76\u7684\u7a7a\u767d\uff1b\u9884\u68c0\u7d22\u9884\u6d4b\u5668\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u89c6\u9891\u68c0\u7d22\u4e2d\u7684\u67e5\u8be2\u4f18\u5316\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\uff1b\u57fa\u51c6\u7684\u53d1\u5e03\u5c06\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u6807\u51c6\u5316\u6bd4\u8f83\u548c\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2602.18206", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.18206", "abs": "https://arxiv.org/abs/2602.18206", "authors": ["Jiayi Wu", "Zhengyu Wu", "Xunkai Li", "Ronghua Li", "Guoren Wang"], "title": "A Simple yet Effective Negative Sampling Plugin for Constructing Positive Sample Pairs in Implicit Collaborative Filtering", "comment": null, "summary": "Most implicit collaborative filtering (CF) models are trained with negative sampling, where existing work designs sophisticated strategies for high-quality negatives while largely overlooking the exploration of positive samples. Although some denoising recommendation methods can be applied to implicit CF for denoising positive samples, they often sparsify positive supervision. Moreover, these approaches generally overlook user activity bias during training, leading to insufficient learning for inactive users. To address these issues, we propose a simple yet effective negative sampling plugin, PSP-NS, from the perspective of enhancing positive supervision signals. It builds a user-item bipartite graph with edge weights indicating interaction confidence inferred from global and local patterns, generates positive sample pairs via replication-based reweighting to strengthen positive signals, and adopts an activity-aware weighting scheme to effectively learn inactive users' preferences. We provide theoretical insights from a margin-improvement perspective, explaining why PSP-NS tends to improve ranking quality (e.g., Precision@k/Recall@k), and conduct extensive experiments on four real-world datasets to demonstrate its superiority. For instance, PSP-NS boosts Recall@30 and Precision@30 by 32.11% and 22.90% on Yelp over the strongest baselines. PSP-NS can be integrated with various implicit CF recommenders or negative sampling methods to enhance their performance.", "AI": {"tldr": "\u63d0\u51faPSP-NS\u8d1f\u91c7\u6837\u63d2\u4ef6\uff0c\u901a\u8fc7\u589e\u5f3a\u6b63\u6837\u672c\u76d1\u7763\u4fe1\u53f7\u6765\u6539\u8fdb\u9690\u5f0f\u534f\u540c\u8fc7\u6ee4\u63a8\u8350\u7cfb\u7edf\uff0c\u7279\u522b\u5173\u6ce8\u6b63\u6837\u672c\u5229\u7528\u548c\u89e3\u51b3\u7528\u6237\u6d3b\u8dc3\u5ea6\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u9690\u5f0f\u534f\u540c\u8fc7\u6ee4\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u9ad8\u8d28\u91cf\u8d1f\u6837\u672c\u8bbe\u8ba1\uff0c\u4f46\u5ffd\u89c6\u4e86\u6b63\u6837\u672c\u7684\u63a2\u7d22\u3002\u73b0\u6709\u7684\u53bb\u566a\u63a8\u8350\u65b9\u6cd5\u867d\u7136\u53ef\u7528\u4e8e\u6b63\u6837\u672c\u53bb\u566a\uff0c\u4f46\u5f80\u5f80\u7a00\u758f\u5316\u4e86\u6b63\u76d1\u7763\u4fe1\u53f7\uff0c\u4e14\u666e\u904d\u5ffd\u7565\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u7528\u6237\u6d3b\u8dc3\u5ea6\u504f\u5dee\uff0c\u5bfc\u81f4\u5bf9\u975e\u6d3b\u8dc3\u7528\u6237\u5b66\u4e60\u4e0d\u8db3\u3002", "method": "\u63d0\u51faPSP-NS\u8d1f\u91c7\u6837\u63d2\u4ef6\uff1a1\uff09\u6784\u5efa\u7528\u6237-\u7269\u54c1\u4e8c\u90e8\u56fe\uff0c\u8fb9\u6743\u91cd\u8868\u793a\u57fa\u4e8e\u5168\u5c40\u548c\u5c40\u90e8\u6a21\u5f0f\u63a8\u65ad\u7684\u4ea4\u4e92\u7f6e\u4fe1\u5ea6\uff1b2\uff09\u901a\u8fc7\u57fa\u4e8e\u590d\u5236\u7684\u91cd\u52a0\u6743\u751f\u6210\u6b63\u6837\u672c\u5bf9\u4ee5\u589e\u5f3a\u6b63\u4fe1\u53f7\uff1b3\uff09\u91c7\u7528\u6d3b\u52a8\u611f\u77e5\u52a0\u6743\u65b9\u6848\u6709\u6548\u5b66\u4e60\u975e\u6d3b\u8dc3\u7528\u6237\u504f\u597d\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002\u4f8b\u5982\u5728Yelp\u6570\u636e\u96c6\u4e0a\uff0cPSP-NS\u5c06Recall@30\u548cPrecision@30\u5206\u522b\u63d0\u5347\u4e8632.11%\u548c22.90%\u3002\u8be5\u63d2\u4ef6\u53ef\u4e0e\u5404\u79cd\u9690\u5f0fCF\u63a8\u8350\u5668\u6216\u8d1f\u91c7\u6837\u65b9\u6cd5\u96c6\u6210\u4ee5\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "PSP-NS\u662f\u4e00\u4e2a\u7b80\u5355\u6709\u6548\u7684\u8d1f\u91c7\u6837\u63d2\u4ef6\uff0c\u4ece\u589e\u5f3a\u6b63\u76d1\u7763\u4fe1\u53f7\u7684\u89d2\u5ea6\u51fa\u53d1\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u63a8\u65ad\u3001\u6b63\u6837\u672c\u5bf9\u91cd\u52a0\u6743\u548c\u6d3b\u52a8\u611f\u77e5\u52a0\u6743\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u6b63\u6837\u672c\u63a2\u7d22\u548c\u7528\u6237\u6d3b\u8dc3\u5ea6\u504f\u5dee\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u8d28\u91cf\u3002"}}
{"id": "2602.17681", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17681", "abs": "https://arxiv.org/abs/2602.17681", "authors": ["Ofir Gordon", "Lior Dikstein", "Arnon Netzer", "Idan Achituve", "Hai Victor Habi"], "title": "LATMiX: Learnable Affine Transformations for Microscaling Quantization of LLMs", "comment": "24 pages, 4 figures", "summary": "Post-training quantization (PTQ) is a widely used approach for reducing the memory and compute costs of large language models (LLMs). Recent studies have shown that applying invertible transformations to activations can significantly improve quantization robustness by reducing activation outliers; however, existing approaches are largely restricted to rotation or Hadamard-based transformations. Moreover, most studies focused primarily on traditional quantization schemes, whereas modern hardware increasingly supports the microscaling (MX) data format. Attempts to combine both showed severe performance degradation, leading prior work to introduce assumptions on the transformations. In this work, we take a complementary perspective. First, we provide a theoretical analysis of transformations under MX quantization by deriving a bound on the quantization error. Our analysis emphasizes the importance of accounting for both the activation distribution and the underlying quantization structure. Building on this analysis, we propose LATMiX, a method that generalizes outlier reduction to learnable invertible affine transformations optimized using standard deep learning tools. Experiments show consistent improvements in average accuracy for MX low-bit quantization over strong baselines on a wide range of zero-shot benchmarks, across multiple model sizes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLATMiX\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u53ef\u9006\u4eff\u5c04\u53d8\u6362\u4f18\u5316\u6fc0\u6d3b\u5206\u5e03\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5fae\u7f29\u653e\u91cf\u5316\u683c\u5f0f\u4e0b\u7684\u4f4e\u6bd4\u7279\u91cf\u5316\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u540e\u8bad\u7ec3\u91cf\u5316\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u4f20\u7edf\u91cf\u5316\u65b9\u6848\uff0c\u800c\u73b0\u4ee3\u786c\u4ef6\u8d8a\u6765\u8d8a\u591a\u5730\u652f\u6301\u5fae\u7f29\u653e\u6570\u636e\u683c\u5f0f\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u7ed3\u5408\u53ef\u9006\u53d8\u6362\u548c\u5fae\u7f29\u653e\u91cf\u5316\u65f6\u4f1a\u51fa\u73b0\u4e25\u91cd\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u4e14\u73b0\u6709\u53d8\u6362\u65b9\u6cd5\u4e3b\u8981\u5c40\u9650\u4e8e\u65cb\u8f6c\u6216Hadamard\u53d8\u6362\u3002", "method": "\u9996\u5148\u5bf9\u5fae\u7f29\u653e\u91cf\u5316\u4e0b\u7684\u53d8\u6362\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u63a8\u5bfc\u91cf\u5316\u8bef\u5dee\u8fb9\u754c\u3002\u57fa\u4e8e\u6b64\u5206\u6790\u63d0\u51faLATMiX\u65b9\u6cd5\uff0c\u5c06\u5f02\u5e38\u503c\u51cf\u5c11\u63a8\u5e7f\u5230\u53ef\u5b66\u4e60\u7684\u53ef\u9006\u4eff\u5c04\u53d8\u6362\uff0c\u4f7f\u7528\u6807\u51c6\u6df1\u5ea6\u5b66\u4e60\u5de5\u5177\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u5e7f\u6cdb\u7684\u96f6\u6837\u672c\u57fa\u51c6\u6d4b\u8bd5\u548c\u591a\u79cd\u6a21\u578b\u5c3a\u5bf8\u4e0a\uff0cLATMiX\u65b9\u6cd5\u5728\u5fae\u7f29\u653e\u4f4e\u6bd4\u7279\u91cf\u5316\u65b9\u9762\u76f8\u6bd4\u5f3a\u57fa\u7ebf\u5b9e\u73b0\u4e86\u5e73\u5747\u51c6\u786e\u7387\u7684\u6301\u7eed\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u53ef\u5b66\u4e60\u53d8\u6362\u7684\u7ed3\u5408\uff0cLATMiX\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u5fae\u7f29\u653e\u91cf\u5316\u683c\u5f0f\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.17729", "categories": ["cs.CY", "cs.AI", "cs.ET", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.17729", "abs": "https://arxiv.org/abs/2602.17729", "authors": ["Nathan G. Wood", "Scott Robbins", "Eduardo Zegarra Berodt", "Anton Graf von Westerholt", "Michelle Behrndt", "Daniel Kloock-Schreiber"], "title": "Stop Saying \"AI\"", "comment": null, "summary": "Across academia, industry, and government, ``AI'' has become central in research and development, regulatory debates, and promises of ever faster and more capable decision-making and action. In numerous domains, especially safety-critical ones, there are significant concerns over how ``AI'' may affect decision-making, responsibility, or the likelihood of mistakes (to name only a few categories of critique). However, for most critiques, the target is generally ``AI'', a broad term admitting many (types of) systems used for a variety of tasks and each coming with its own set of limitations, challenges, and potential use cases. In this article, we focus on the military domain as a case study and present both a loose enumerative taxonomy of systems captured under the umbrella term ``military AI'', as well as discussion of the challenges of each. In doing so, we highlight that critiques of one (type of) system will not always transfer to other (types of) systems. Building on this, we argue that in order for debates to move forward fruitfully, it is imperative that the discussions be made more precise and that ``AI'' be excised from debates to the extent possible. Researchers, developers, and policy-makers should make clear exactly what systems they have in mind and what possible benefits and risks attend the deployment of those particular systems. While we focus on AI in the military as an exemplar for the overall trends in discussions of ``AI'', the argument's conclusions are broad and have import for discussions of AI across a host of domains.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\"AI\"\u4e00\u8bcd\u8fc7\u4e8e\u5bbd\u6cdb\uff0c\u5bfc\u81f4\u8ba8\u8bba\u6a21\u7cca\u4e0d\u6e05\uff0c\u5efa\u8bae\u5728\u519b\u4e8b\u9886\u57df\u7b49\u5177\u4f53\u5e94\u7528\u4e2d\u660e\u786e\u533a\u5206\u4e0d\u540c\u7c7b\u578b\u7684AI\u7cfb\u7edf\uff0c\u4ee5\u4fbf\u8fdb\u884c\u66f4\u6709\u6210\u6548\u7684\u8fa9\u8bba\u548c\u653f\u7b56\u5236\u5b9a\u3002", "motivation": "\u5f53\u524d\u5173\u4e8eAI\u7684\u8ba8\u8bba\u8fc7\u4e8e\u7b3c\u7edf\uff0c\u5c06\u5404\u79cd\u4e0d\u540c\u7c7b\u578b\u7684\u7cfb\u7edf\u90fd\u5f52\u4e3a\"AI\"\uff0c\u5bfc\u81f4\u6279\u8bc4\u548c\u8fa9\u8bba\u7f3a\u4e4f\u9488\u5bf9\u6027\u3002\u7279\u522b\u662f\u5728\u519b\u4e8b\u7b49\u5b89\u5168\u5173\u952e\u9886\u57df\uff0c\u8fd9\u79cd\u6a21\u7cca\u6027\u4f1a\u5f71\u54cd\u5bf9\u5177\u4f53\u7cfb\u7edf\u98ce\u9669\u3001\u8d23\u4efb\u548c\u51b3\u7b56\u5f71\u54cd\u7684\u51c6\u786e\u8bc4\u4f30\u3002", "method": "\u4ee5\u519b\u4e8b\u9886\u57df\u4e3a\u6848\u4f8b\u7814\u7a76\uff0c\u63d0\u51fa\u4e00\u4e2a\u677e\u6563\u7684\u679a\u4e3e\u5f0f\u5206\u7c7b\u6cd5\uff0c\u5bf9\"\u519b\u4e8bAI\"\u8fd9\u4e00\u5bbd\u6cdb\u672f\u8bed\u4e0b\u7684\u5404\u79cd\u7cfb\u7edf\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u8ba8\u8bba\u6bcf\u7c7b\u7cfb\u7edf\u9762\u4e34\u7684\u72ec\u7279\u6311\u6218\u3002", "result": "\u901a\u8fc7\u5bf9\u519b\u4e8bAI\u7cfb\u7edf\u7684\u5206\u7c7b\u5206\u6790\uff0c\u53d1\u73b0\u5bf9\u67d0\u4e00\u7c7b\u7cfb\u7edf\u7684\u6279\u8bc4\u5e76\u4e0d\u603b\u662f\u9002\u7528\u4e8e\u5176\u4ed6\u7c7b\u578b\u7684\u7cfb\u7edf\u3002\u8fd9\u63ed\u793a\u4e86\u5f53\u524dAI\u8ba8\u8bba\u4e2d\u672f\u8bed\u6a21\u7cca\u6027\u5e26\u6765\u7684\u95ee\u9898\u3002", "conclusion": "\u4e3a\u4e86\u4f7fAI\u8fa9\u8bba\u66f4\u6709\u6210\u6548\uff0c\u9700\u8981\u63d0\u9ad8\u8ba8\u8bba\u7684\u7cbe\u786e\u6027\uff0c\u5c3d\u53ef\u80fd\u907f\u514d\u4f7f\u7528\"AI\"\u8fd9\u4e00\u7b3c\u7edf\u672f\u8bed\u3002\u7814\u7a76\u4eba\u5458\u3001\u5f00\u53d1\u8005\u548c\u653f\u7b56\u5236\u5b9a\u8005\u5e94\u660e\u786e\u4ed6\u4eec\u6240\u6307\u7684\u5177\u4f53\u7cfb\u7edf\u7c7b\u578b\uff0c\u5e76\u9488\u5bf9\u6027\u5730\u8ba8\u8bba\u8fd9\u4e9b\u7279\u5b9a\u7cfb\u7edf\u7684\u6f5c\u5728\u5229\u76ca\u548c\u98ce\u9669\u3002"}}
{"id": "2602.17854", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17854", "abs": "https://arxiv.org/abs/2602.17854", "authors": ["Domonkos Varga"], "title": "On the Evaluation Protocol of Gesture Recognition for UAV-based Rescue Operation based on Deep Learning: A Subject-Independence Perspective", "comment": null, "summary": "This paper presents a methodological analysis of the gesture-recognition approach proposed by Liu and Szir\u00e1nyi, with a particular focus on the validity of their evaluation protocol. We show that the reported near-perfect accuracy metrics result from a frame-level random train-test split that inevitably mixes samples from the same subjects across both sets, causing severe data leakage. By examining the published confusion matrix, learning curves, and dataset construction, we demonstrate that the evaluation does not measure generalization to unseen individuals. Our findings underscore the importance of subject-independent data partitioning in vision-based gesture-recognition research, especially for applications - such as UAV-human interaction - that require reliable recognition of gestures performed by previously unseen people.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5bf9Liu\u548cSzir\u00e1nyi\u63d0\u51fa\u7684\u624b\u52bf\u8bc6\u522b\u65b9\u6cd5\u8fdb\u884c\u4e86\u65b9\u6cd5\u5b66\u5206\u6790\uff0c\u6307\u51fa\u5176\u8bc4\u4f30\u534f\u8bae\u5b58\u5728\u4e25\u91cd\u7684\u6570\u636e\u6cc4\u9732\u95ee\u9898\uff0c\u5bfc\u81f4\u62a5\u544a\u7684\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u51c6\u786e\u7387\u6307\u6807\u65e0\u6548\u3002", "motivation": "\u8be5\u7814\u7a76\u7684\u52a8\u673a\u662f\u5206\u6790Liu\u548cSzir\u00e1nyi\u63d0\u51fa\u7684\u624b\u52bf\u8bc6\u522b\u65b9\u6cd5\u7684\u8bc4\u4f30\u534f\u8bae\u6709\u6548\u6027\uff0c\u7279\u522b\u5173\u6ce8\u5176\u662f\u5426\u5b58\u5728\u6570\u636e\u6cc4\u9732\u95ee\u9898\u3002\u7814\u7a76\u8005\u53d1\u73b0\u62a5\u544a\u7684\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u51c6\u786e\u7387\u53ef\u80fd\u6e90\u4e8e\u4e0d\u6070\u5f53\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u8fd9\u5bf9\u624b\u52bf\u8bc6\u522b\u7814\u7a76\uff0c\u7279\u522b\u662f\u65e0\u4eba\u673a-\u4eba\u4ea4\u4e92\u7b49\u9700\u8981\u8bc6\u522b\u672a\u89c1\u4e2a\u4f53\u624b\u52bf\u7684\u5e94\u7528\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5df2\u53d1\u8868\u7684\u6df7\u6dc6\u77e9\u9635\u3001\u5b66\u4e60\u66f2\u7ebf\u548c\u6570\u636e\u96c6\u6784\u5efa\u65b9\u6cd5\uff0c\u7814\u7a76\u8005\u5c55\u793a\u4e86Liu\u548cSzir\u00e1nyi\u7684\u8bc4\u4f30\u534f\u8bae\u5b58\u5728\u95ee\u9898\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u4ed6\u4eec\u4f7f\u7528\u4e86\u5e27\u7ea7\u522b\u7684\u968f\u673a\u8bad\u7ec3-\u6d4b\u8bd5\u5206\u5272\uff0c\u8fd9\u4e0d\u53ef\u907f\u514d\u5730\u6df7\u5408\u4e86\u540c\u4e00\u53d7\u8bd5\u8005\u5728\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u4e2d\u7684\u6837\u672c\uff0c\u5bfc\u81f4\u4e86\u4e25\u91cd\u7684\u6570\u636e\u6cc4\u9732\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cLiu\u548cSzir\u00e1nyi\u62a5\u544a\u7684\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u51c6\u786e\u7387\u6307\u6807\u662f\u7531\u4e8e\u6570\u636e\u6cc4\u9732\u9020\u6210\u7684\uff0c\u800c\u4e0d\u662f\u771f\u6b63\u7684\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002\u4ed6\u4eec\u7684\u8bc4\u4f30\u534f\u8bae\u672a\u80fd\u6d4b\u91cf\u6a21\u578b\u5bf9\u672a\u89c1\u4e2a\u4f53\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u56e0\u4e3a\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u4e2d\u5305\u542b\u4e86\u540c\u4e00\u53d7\u8bd5\u8005\u7684\u6570\u636e\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u57fa\u4e8e\u89c6\u89c9\u7684\u624b\u52bf\u8bc6\u522b\u7814\u7a76\u4e2d\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u9700\u8981\u8bc6\u522b\u672a\u89c1\u4e2a\u4f53\u624b\u52bf\u7684\u5e94\u7528\uff08\u5982\u65e0\u4eba\u673a-\u4eba\u4ea4\u4e92\uff09\uff0c\u91c7\u7528\u53d7\u8bd5\u8005\u72ec\u7acb\u7684\u6570\u636e\u5212\u5206\u65b9\u6cd5\u7684\u91cd\u8981\u6027\u3002\u7814\u7a76\u547c\u5401\u5728\u8bc4\u4f30\u624b\u52bf\u8bc6\u522b\u7cfb\u7edf\u65f6\uff0c\u5fc5\u987b\u786e\u4fdd\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u4e2d\u7684\u53d7\u8bd5\u8005\u5b8c\u5168\u5206\u79bb\uff0c\u4ee5\u907f\u514d\u6570\u636e\u6cc4\u9732\u5e76\u51c6\u786e\u8bc4\u4f30\u6a21\u578b\u7684\u771f\u5b9e\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.18221", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.18221", "abs": "https://arxiv.org/abs/2602.18221", "authors": ["Teddy Lazebnik"], "title": "The Economical-Ecological Benefits of Matching Non-matching Socks", "comment": null, "summary": "Socks are produced and replaced at a massive scale, yet their paired use makes them unusually vulnerable to waste, as the loss of a single sock can strand usable wear-capacity and trigger premature replacement. In this study, we quantify the economic and ecological value of pairing non-matching \\say{orphan} socks, and the social cost that discourages this behaviour. We formalize sock ownership as a sequential decision problem under uncertainty in which socks wear out and disappear stochastically during laundering, while public exposure induces a person-specific mismatch penalty. We conducted an in-person study to estimate mismatch sensitivity and diversity preference, linking behavioural heterogeneity to optimal mixing strategies. Using these results and a computer simulation-based evaluation of interpretable pairing policies, we show that strict matching can appear resource-frugal largely because it generates many sockless days, whereas controlled tolerance for mismatch sustains service and reduces stranded capacity across loss regimes. This study establishes the feasibility of matching non-matching socks while outlining its limitations and challenges.", "AI": {"tldr": "\u7814\u7a76\u91cf\u5316\u4e86\u914d\u5bf9\u4e0d\u5339\u914d\u7684\"\u5b64\u513f\u889c\"\u7684\u7ecf\u6d4e\u548c\u751f\u6001\u4ef7\u503c\uff0c\u5206\u6790\u4e86\u963b\u788d\u8fd9\u79cd\u884c\u4e3a\u7684\u793e\u4f1a\u6210\u672c\uff0c\u901a\u8fc7\u5efa\u6a21\u548c\u5b9e\u9a8c\u53d1\u73b0\u9002\u5ea6\u5bb9\u5fcd\u4e0d\u5339\u914d\u6bd4\u4e25\u683c\u5339\u914d\u66f4\u53ef\u6301\u7eed\u3002", "motivation": "\u889c\u5b50\u5927\u89c4\u6a21\u751f\u4ea7\u548c\u66f4\u6362\uff0c\u4f46\u6210\u5bf9\u4f7f\u7528\u4f7f\u5176\u7279\u522b\u5bb9\u6613\u6d6a\u8d39\u2014\u2014\u4e22\u5931\u4e00\u53ea\u889c\u5b50\u4f1a\u6d6a\u8d39\u53ef\u7528\u7a7f\u7740\u5bb9\u91cf\u5e76\u5bfc\u81f4\u8fc7\u65e9\u66f4\u6362\u3002\u7814\u7a76\u65e8\u5728\u91cf\u5316\u914d\u5bf9\u4e0d\u5339\u914d\u889c\u5b50\u7684\u4ef7\u503c\u53ca\u963b\u788d\u8fd9\u79cd\u884c\u4e3a\u7684\u793e\u4f1a\u6210\u672c\u3002", "method": "\u5c06\u889c\u5b50\u6240\u6709\u6743\u5efa\u6a21\u4e3a\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u5e8f\u5217\u51b3\u7b56\u95ee\u9898\uff0c\u889c\u5b50\u5728\u4f7f\u7528\u548c\u6d17\u6da4\u4e2d\u968f\u673a\u78e8\u635f\u548c\u4e22\u5931\uff0c\u516c\u5171\u66b4\u9732\u5f15\u5165\u4e2a\u4eba\u7279\u5b9a\u7684\u4e0d\u5339\u914d\u60e9\u7f5a\u3002\u901a\u8fc7\u73b0\u573a\u7814\u7a76\u4f30\u8ba1\u4e0d\u5339\u914d\u654f\u611f\u6027\u548c\u591a\u6837\u6027\u504f\u597d\uff0c\u5c06\u884c\u4e3a\u5f02\u8d28\u6027\u4e0e\u6700\u4f18\u914d\u5bf9\u7b56\u7565\u8054\u7cfb\u8d77\u6765\u3002\u4f7f\u7528\u8ba1\u7b97\u673a\u6a21\u62df\u8bc4\u4f30\u53ef\u89e3\u91ca\u7684\u914d\u5bf9\u7b56\u7565\u3002", "result": "\u4e25\u683c\u5339\u914d\u770b\u4f3c\u8282\u7ea6\u8d44\u6e90\u4e3b\u8981\u662f\u56e0\u4e3a\u4ea7\u751f\u8bb8\u591a\u65e0\u889c\u53ef\u7a7f\u7684\u65e5\u5b50\uff0c\u800c\u9002\u5ea6\u5bb9\u5fcd\u4e0d\u5339\u914d\u80fd\u7ef4\u6301\u670d\u52a1\u5e76\u51cf\u5c11\u4e0d\u540c\u4e22\u5931\u60c5\u51b5\u4e0b\u7684\u95f2\u7f6e\u5bb9\u91cf\u3002\u7814\u7a76\u5efa\u7acb\u4e86\u914d\u5bf9\u4e0d\u5339\u914d\u889c\u5b50\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u914d\u5bf9\u4e0d\u5339\u914d\u889c\u5b50\u5177\u6709\u7ecf\u6d4e\u548c\u751f\u6001\u4ef7\u503c\uff0c\u4f46\u53d7\u793e\u4f1a\u6210\u672c\u9650\u5236\u3002\u9002\u5ea6\u5bb9\u5fcd\u4e0d\u5339\u914d\u6bd4\u4e25\u683c\u5339\u914d\u66f4\u53ef\u6301\u7eed\uff0c\u80fd\u51cf\u5c11\u6d6a\u8d39\u5e76\u7ef4\u6301\u670d\u52a1\u3002\u7814\u7a76\u4e3a\u914d\u5bf9\u7b56\u7565\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2602.17753", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17753", "abs": "https://arxiv.org/abs/2602.17753", "authors": ["Leon Staufer", "Kevin Feng", "Kevin Wei", "Luke Bailey", "Yawen Duan", "Mick Yang", "A. Pinar Ozisik", "Stephen Casper", "Noam Kolt"], "title": "The 2025 AI Agent Index: Documenting Technical and Safety Features of Deployed Agentic AI Systems", "comment": null, "summary": "Agentic AI systems are increasingly capable of performing professional and personal tasks with limited human involvement. However, tracking these developments is difficult because the AI agent ecosystem is complex, rapidly evolving, and inconsistently documented, posing obstacles to both researchers and policymakers. To address these challenges, this paper presents the 2025 AI Agent Index. The Index documents information regarding the origins, design, capabilities, ecosystem, and safety features of 30 state-of-the-art AI agents based on publicly available information and email correspondence with developers. In addition to documenting information about individual agents, the Index illuminates broader trends in the development of agents, their capabilities, and the level of transparency of developers. Notably, we find different transparency levels among agent developers and observe that most developers share little information about safety, evaluations, and societal impacts. The 2025 AI Agent Index is available online at https://aiagentindex.mit.edu", "AI": {"tldr": "2025 AI Agent Index\u7cfb\u7edf\u6027\u5730\u8bb0\u5f55\u548c\u5206\u6790\u4e8630\u4e2a\u6700\u5148\u8fdb\u7684AI\u667a\u80fd\u4f53\uff0c\u63ed\u793a\u4e86\u5f00\u53d1\u8005\u900f\u660e\u5ea6\u4e0d\u8db3\u3001\u5b89\u5168\u8bc4\u4f30\u4fe1\u606f\u7f3a\u4e4f\u7b49\u5173\u952e\u95ee\u9898\u3002", "motivation": "AI\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\u590d\u6742\u3001\u5feb\u901f\u6f14\u53d8\u4e14\u6587\u6863\u8bb0\u5f55\u4e0d\u4e00\u81f4\uff0c\u7ed9\u7814\u7a76\u4eba\u5458\u548c\u653f\u7b56\u5236\u5b9a\u8005\u5e26\u6765\u8ddf\u8e2a\u56f0\u96be\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u4fe1\u606f\u6574\u7406\u548c\u5206\u6790\u5de5\u5177\u3002", "method": "\u57fa\u4e8e\u516c\u5f00\u4fe1\u606f\u548c\u4e0e\u5f00\u53d1\u8005\u7684\u90ae\u4ef6\u901a\u4fe1\uff0c\u6536\u96c630\u4e2a\u6700\u5148\u8fdbAI\u667a\u80fd\u4f53\u7684\u8d77\u6e90\u3001\u8bbe\u8ba1\u3001\u80fd\u529b\u3001\u751f\u6001\u7cfb\u7edf\u548c\u5b89\u5168\u7279\u6027\u7b49\u4fe1\u606f\uff0c\u5efa\u7acb\u7efc\u5408\u7d22\u5f15\u3002", "result": "\u53d1\u73b0\u4e0d\u540c\u5f00\u53d1\u8005\u900f\u660e\u5ea6\u5dee\u5f02\u663e\u8457\uff0c\u5927\u591a\u6570\u5f00\u53d1\u8005\u5f88\u5c11\u5206\u4eab\u5173\u4e8e\u5b89\u5168\u6027\u3001\u8bc4\u4f30\u548c\u793e\u4f1a\u5f71\u54cd\u7684\u4fe1\u606f\uff0c\u63ed\u793a\u4e86\u884c\u4e1a\u900f\u660e\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "conclusion": "2025 AI Agent Index\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u53c2\u8003\u5de5\u5177\uff0c\u540c\u65f6\u6307\u51fa\u4e86AI\u667a\u80fd\u4f53\u5f00\u53d1\u4e2d\u900f\u660e\u5ea6\u3001\u5b89\u5168\u8bc4\u4f30\u548c\u793e\u4f1a\u5f71\u54cd\u8003\u91cf\u65b9\u9762\u7684\u4e0d\u8db3\u3002"}}
{"id": "2602.17869", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17869", "abs": "https://arxiv.org/abs/2602.17869", "authors": ["Yuxiao Chen", "Jue Wang", "Zhikang Zhang", "Jingru Yi", "Xu Zhang", "Yang Zou", "Zhaowei Cai", "Jianbo Yuan", "Xinyu Li", "Hao Yang", "Davide Modolo"], "title": "Learning Compact Video Representations for Efficient Long-form Video Understanding in Large Multimodal Models", "comment": null, "summary": "With recent advancements in video backbone architectures, combined with the remarkable achievements of large language models (LLMs), the analysis of long-form videos spanning tens of minutes has become both feasible and increasingly prevalent. However, the inherently redundant nature of video sequences poses significant challenges for contemporary state-of-the-art models. These challenges stem from two primary aspects: 1) efficiently incorporating a larger number of frames within memory constraints, and 2) extracting discriminative information from the vast volume of input data. In this paper, we introduce a novel end-to-end schema for long-form video understanding, which includes an information-density-based adaptive video sampler (AVS) and an autoencoder-based spatiotemporal video compressor (SVC) integrated with a multimodal large language model (MLLM). Our proposed system offers two major advantages: it adaptively and effectively captures essential information from video sequences of varying durations, and it achieves high compression rates while preserving crucial discriminative information. The proposed framework demonstrates promising performance across various benchmarks, excelling in both long-form video understanding tasks and standard video understanding benchmarks. These results underscore the versatility and efficacy of our approach, particularly in managing the complexities of prolonged video sequences.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u957f\u89c6\u9891\u7406\u89e3\u7684\u65b0\u578b\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u5305\u542b\u57fa\u4e8e\u4fe1\u606f\u5bc6\u5ea6\u7684\u81ea\u9002\u5e94\u89c6\u9891\u91c7\u6837\u5668\u548c\u57fa\u4e8e\u81ea\u52a8\u7f16\u7801\u5668\u7684\u65f6\u7a7a\u89c6\u9891\u538b\u7f29\u5668\uff0c\u4e0e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u96c6\u6210\uff0c\u6709\u6548\u5904\u7406\u957f\u89c6\u9891\u5197\u4f59\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u89c6\u9891\u9aa8\u5e72\u67b6\u6784\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\uff0c\u5206\u6790\u957f\u8fbe\u6570\u5341\u5206\u949f\u7684\u957f\u89c6\u9891\u53d8\u5f97\u53ef\u884c\u4e14\u666e\u904d\u3002\u4f46\u89c6\u9891\u5e8f\u5217\u56fa\u6709\u7684\u5197\u4f59\u6027\u7ed9\u73b0\u6709\u6a21\u578b\u5e26\u6765\u4e24\u5927\u6311\u6218\uff1a1) \u5728\u5185\u5b58\u9650\u5236\u5185\u9ad8\u6548\u5904\u7406\u66f4\u591a\u5e27\uff1b2) \u4ece\u5927\u91cf\u8f93\u5165\u6570\u636e\u4e2d\u63d0\u53d6\u5224\u522b\u6027\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u7aef\u5230\u7aef\u7684\u957f\u89c6\u9891\u7406\u89e3\u6846\u67b6\uff0c\u5305\u62ec\uff1a1) \u57fa\u4e8e\u4fe1\u606f\u5bc6\u5ea6\u7684\u81ea\u9002\u5e94\u89c6\u9891\u91c7\u6837\u5668(AVS)\uff0c\u81ea\u9002\u5e94\u6355\u6349\u4e0d\u540c\u65f6\u957f\u89c6\u9891\u7684\u5173\u952e\u4fe1\u606f\uff1b2) \u57fa\u4e8e\u81ea\u52a8\u7f16\u7801\u5668\u7684\u65f6\u7a7a\u89c6\u9891\u538b\u7f29\u5668(SVC)\uff0c\u5b9e\u73b0\u9ad8\u538b\u7f29\u7387\u540c\u65f6\u4fdd\u7559\u5173\u952e\u5224\u522b\u4fe1\u606f\uff1b3) \u4e0e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b(MLLM)\u96c6\u6210\u3002", "result": "\u8be5\u6846\u67b6\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e0d\u4ec5\u5728\u957f\u89c6\u9891\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5728\u6807\u51c6\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u53d6\u5f97\u826f\u597d\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5904\u7406\u957f\u89c6\u9891\u590d\u6742\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u91c7\u6837\u548c\u9ad8\u6548\u538b\u7f29\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u5197\u4f59\u95ee\u9898\uff0c\u5c55\u73b0\u4e86\u5728\u5904\u7406\u957f\u89c6\u9891\u5e8f\u5217\u590d\u6742\u6027\u7684\u591a\u529f\u80fd\u6027\u548c\u9ad8\u6548\u6027\uff0c\u4e3a\u957f\u89c6\u9891\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.18249", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.18249", "abs": "https://arxiv.org/abs/2602.18249", "authors": ["Jiayi Wu", "Zhengyu Wu", "Xunkai Li", "Rong-Hua Li", "Guoren Wang"], "title": "Dual-Tree LLM-Enhanced Negative Sampling for Implicit Collaborative Filtering", "comment": null, "summary": "Negative sampling is a pivotal technique in implicit collaborative filtering (CF) recommendation, enabling efficient and effective training by contrasting observed interactions with sampled unobserved ones.\n  Recently, large language models (LLMs) have shown promise in recommender systems; however, research on LLM-empowered negative sampling remains underexplored.\n  Existing methods heavily rely on textual information and task-specific fine-tuning, limiting practical applicability.\n  To address this limitation, we propose a text-free and fine-tuning-free Dual-Tree LLM-enhanced Negative Sampling method (DTL-NS).\n  It consists of two modules: (i) an offline false negative identification module that leverages hierarchical index trees to transform collaborative structural and latent semantic information into structured item-ID encodings for LLM inference, enabling accurate identification of false negatives; and (ii) a multi-view hard negative sampling module that combines user-item preference scores with item-item hierarchical similarities from these encodings to mine high-quality hard negatives, thus improving models' discriminative ability.\n  Extensive experiments demonstrate the effectiveness of DTL-NS. For example, on the Amazon-sports dataset, DTL-NS outperforms the strongest baseline by 10.64% and 19.12% in Recall@20 and NDCG@20, respectively.\n  Moreover, DTL-NS can be integrated into various implicit CF models and negative sampling methods, consistently enhancing their performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6587\u672c\u4fe1\u606f\u548c\u5fae\u8c03\u7684LLM\u589e\u5f3a\u8d1f\u91c7\u6837\u65b9\u6cd5DTL-NS\uff0c\u901a\u8fc7\u53cc\u6811\u7ed3\u6784\u8bc6\u522b\u5047\u8d1f\u6837\u672c\u5e76\u6316\u6398\u9ad8\u8d28\u91cf\u786c\u8d1f\u6837\u672c\uff0c\u663e\u8457\u63d0\u5347\u63a8\u8350\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u8d1f\u91c7\u6837\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u6587\u672c\u4fe1\u606f\u548c\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u6587\u672c\u4fe1\u606f\u548c\u5fae\u8c03\u7684LLM\u589e\u5f3a\u8d1f\u91c7\u6837\u65b9\u6cd5\u3002", "method": "\u63d0\u51faDTL-NS\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u6a21\u5757\uff1a1\uff09\u79bb\u7ebf\u5047\u8d1f\u6837\u672c\u8bc6\u522b\u6a21\u5757\uff0c\u5229\u7528\u5c42\u6b21\u7d22\u5f15\u6811\u5c06\u534f\u540c\u7ed3\u6784\u548c\u6f5c\u5728\u8bed\u4e49\u4fe1\u606f\u8f6c\u6362\u4e3a\u7ed3\u6784\u5316\u9879\u76eeID\u7f16\u7801\u4f9bLLM\u63a8\u7406\uff1b2\uff09\u591a\u89c6\u56fe\u786c\u8d1f\u91c7\u6837\u6a21\u5757\uff0c\u7ed3\u5408\u7528\u6237-\u9879\u76ee\u504f\u597d\u5206\u6570\u548c\u9879\u76ee\u95f4\u5c42\u6b21\u76f8\u4f3c\u6027\u6316\u6398\u9ad8\u8d28\u91cf\u786c\u8d1f\u6837\u672c\u3002", "result": "\u5728Amazon-sports\u6570\u636e\u96c6\u4e0a\uff0cDTL-NS\u5728Recall@20\u548cNDCG@20\u6307\u6807\u4e0a\u5206\u522b\u6bd4\u6700\u5f3a\u57fa\u7ebf\u63d0\u534710.64%\u548c19.12%\u3002\u8be5\u65b9\u6cd5\u53ef\u96c6\u6210\u5230\u591a\u79cd\u9690\u5f0fCF\u6a21\u578b\u548c\u8d1f\u91c7\u6837\u65b9\u6cd5\u4e2d\uff0c\u6301\u7eed\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "DTL-NS\u662f\u4e00\u79cd\u6709\u6548\u4e14\u5b9e\u7528\u7684LLM\u589e\u5f3a\u8d1f\u91c7\u6837\u65b9\u6cd5\uff0c\u65e0\u9700\u6587\u672c\u4fe1\u606f\u548c\u5fae\u8c03\uff0c\u901a\u8fc7\u53cc\u6811\u7ed3\u6784\u663e\u8457\u63d0\u5347\u63a8\u8350\u7cfb\u7edf\u6027\u80fd\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2602.17990", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17990", "abs": "https://arxiv.org/abs/2602.17990", "authors": ["Madhav Kanda", "Pedro Las-Casas", "Alok Gautam Kumbhare", "Rodrigo Fonseca", "Sharad Agarwal"], "title": "WorkflowPerturb: Calibrated Stress Tests for Evaluating Multi-Agent Workflow Metrics", "comment": null, "summary": "LLM-based systems increasingly generate structured workflows for complex tasks. In practice, automatic evaluation of these workflows is difficult, because metric scores are often not calibrated, and score changes do not directly communicate the severity of workflow degradation. We introduce WorkflowPerturb, a controlled benchmark for studying workflow evaluation metrics. It works by applying realistic, controlled perturbations to golden workflows. WorkflowPerturb contains 4,973 golden workflows and 44,757 perturbed variants across three perturbation types (Missing Steps, Compressed Steps, and Description Changes), each applied at severity levels of 10%, 30%, and 50%. We benchmark multiple metric families and analyze their sensitivity and calibration using expected score trajectories and residuals. Our results characterize systematic differences across metric families and support severity-aware interpretation of workflow evaluation scores. Our dataset will be released upon acceptance.", "AI": {"tldr": "WorkflowPerturb\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5de5\u4f5c\u6d41\u8bc4\u4f30\u6307\u6807\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u5bf9\u9ec4\u91d1\u5de5\u4f5c\u6d41\u65bd\u52a0\u53ef\u63a7\u6270\u52a8\u6765\u7814\u7a76\u4e0d\u540c\u6307\u6807\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\u751f\u6210\u590d\u6742\u4efb\u52a1\u7684\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\uff0c\u4f46\u81ea\u52a8\u8bc4\u4f30\u8fd9\u4e9b\u5de5\u4f5c\u6d41\u5f88\u56f0\u96be\uff0c\u56e0\u4e3a\u6307\u6807\u5206\u6570\u901a\u5e38\u672a\u6821\u51c6\uff0c\u4e14\u5206\u6570\u53d8\u5316\u4e0d\u80fd\u76f4\u63a5\u53cd\u6620\u5de5\u4f5c\u6d41\u9000\u5316\u7684\u4e25\u91cd\u7a0b\u5ea6\u3002", "method": "\u5f15\u5165WorkflowPerturb\u57fa\u51c6\uff0c\u901a\u8fc7\u5bf9\u9ec4\u91d1\u5de5\u4f5c\u6d41\u65bd\u52a0\u4e09\u79cd\u73b0\u5b9e\u53ef\u63a7\u6270\u52a8\uff08\u7f3a\u5931\u6b65\u9aa4\u3001\u538b\u7f29\u6b65\u9aa4\u3001\u63cf\u8ff0\u53d8\u5316\uff09\uff0c\u6bcf\u79cd\u6270\u52a8\u572810%\u300130%\u300150%\u7684\u4e25\u91cd\u7ea7\u522b\u4e0a\u5e94\u7528\uff0c\u5305\u542b4,973\u4e2a\u9ec4\u91d1\u5de5\u4f5c\u6d41\u548c44,757\u4e2a\u6270\u52a8\u53d8\u4f53\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u4e86\u591a\u4e2a\u6307\u6807\u5bb6\u65cf\uff0c\u4f7f\u7528\u9884\u671f\u5206\u6570\u8f68\u8ff9\u548c\u6b8b\u5dee\u5206\u6790\u5176\u654f\u611f\u6027\u548c\u6821\u51c6\u6027\uff0c\u7ed3\u679c\u63ed\u793a\u4e86\u4e0d\u540c\u6307\u6807\u5bb6\u65cf\u4e4b\u95f4\u7684\u7cfb\u7edf\u6027\u5dee\u5f02\uff0c\u652f\u6301\u57fa\u4e8e\u4e25\u91cd\u7a0b\u5ea6\u7684\u5de5\u4f5c\u6d41\u8bc4\u4f30\u5206\u6570\u89e3\u91ca\u3002", "conclusion": "WorkflowPerturb\u4e3a\u5de5\u4f5c\u6d41\u8bc4\u4f30\u6307\u6807\u63d0\u4f9b\u4e86\u53d7\u63a7\u57fa\u51c6\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u4e0d\u540c\u6307\u6807\u7684\u654f\u611f\u6027\u548c\u6821\u51c6\u7279\u6027\uff0c\u652f\u6301\u66f4\u51c6\u786e\u5730\u89e3\u91ca\u5de5\u4f5c\u6d41\u8bc4\u4f30\u5206\u6570\uff0c\u6570\u636e\u96c6\u5c06\u5728\u63a5\u53d7\u540e\u53d1\u5e03\u3002"}}
{"id": "2602.17683", "categories": ["cs.LG", "cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.17683", "abs": "https://arxiv.org/abs/2602.17683", "authors": ["Irene Iele", "Giulia Romoli", "Daniele Molino", "Elena Mulero Ayll\u00f3n", "Filippo Ruffini", "Paolo Soda", "Matteo Tortora"], "title": "Probabilistic NDVI Forecasting from Sparse Satellite Time Series and Weather Covariates", "comment": null, "summary": "Accurate short-term forecasting of vegetation dynamics is a key enabler for data-driven decision support in precision agriculture. Normalized Difference Vegetation Index (NDVI) forecasting from satellite observations, however, remains challenging due to sparse and irregular sampling caused by cloud coverage, as well as the heterogeneous climatic conditions under which crops evolve. In this work, we propose a probabilistic forecasting framework specifically designed for field-level NDVI prediction under clear-sky acquisition constraints. The method leverages a transformer-based architecture that explicitly separates the modeling of historical vegetation dynamics from future exogenous information, integrating historical NDVI observations with both historical and future meteorological covariates. To address irregular revisit patterns and horizon-dependent uncertainty, we introduce a temporal-distance weighted quantile loss that aligns the training objective with the effective forecasting horizon. In addition, we incorporate cumulative and extreme-weather feature engineering to better capture delayed meteorological effects relevant to vegetation response. Extensive experiments on European satellite data demonstrate that the proposed approach consistently outperforms a diverse set of statistical, deep learning, and recent time series baselines across both point-wise and probabilistic evaluation metrics. Ablation studies further highlight the central role of target history, while showing that meteorological covariates provide complementary gains when jointly exploited. The code is available at https://github.com/arco-group/ndvi-forecasting.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eTransformer\u7684\u6982\u7387\u9884\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u536b\u661fNDVI\u6307\u6570\u9884\u6d4b\uff0c\u901a\u8fc7\u5206\u79bb\u5386\u53f2\u690d\u88ab\u52a8\u6001\u4e0e\u672a\u6765\u5916\u751f\u4fe1\u606f\u5efa\u6a21\uff0c\u5f15\u5165\u65f6\u95f4\u8ddd\u79bb\u52a0\u6743\u5206\u4f4d\u6570\u635f\u5931\u5904\u7406\u4e0d\u89c4\u5219\u91c7\u6837\uff0c\u5728\u70b9\u9884\u6d4b\u548c\u6982\u7387\u9884\u6d4b\u6307\u6807\u4e0a\u4f18\u4e8e\u591a\u79cd\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u7cbe\u51c6\u519c\u4e1a\u9700\u8981\u51c6\u786e\u7684\u690d\u88ab\u52a8\u6001\u77ed\u671f\u9884\u6d4b\uff0c\u4f46\u536b\u661fNDVI\u9884\u6d4b\u9762\u4e34\u4e91\u5c42\u906e\u6321\u5bfc\u81f4\u91c7\u6837\u7a00\u758f\u4e0d\u89c4\u5219\u4ee5\u53ca\u4f5c\u7269\u751f\u957f\u7684\u5f02\u8d28\u6027\u6c14\u5019\u6761\u4ef6\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51fa\u6982\u7387\u9884\u6d4b\u6846\u67b6\uff0c\u4f7f\u7528Transformer\u67b6\u6784\u660e\u786e\u5206\u79bb\u5386\u53f2\u690d\u88ab\u52a8\u6001\u4e0e\u672a\u6765\u5916\u751f\u4fe1\u606f\u5efa\u6a21\uff0c\u6574\u5408\u5386\u53f2NDVI\u89c2\u6d4b\u4e0e\u5386\u53f2\u548c\u672a\u6765\u6c14\u8c61\u534f\u53d8\u91cf\uff1b\u5f15\u5165\u65f6\u95f4\u8ddd\u79bb\u52a0\u6743\u5206\u4f4d\u6570\u635f\u5931\u5904\u7406\u4e0d\u89c4\u5219\u91cd\u8bbf\u6a21\u5f0f\uff1b\u52a0\u5165\u7d2f\u79ef\u548c\u6781\u7aef\u5929\u6c14\u7279\u5f81\u5de5\u7a0b\u6355\u6349\u5ef6\u8fdf\u6c14\u8c61\u6548\u5e94\u3002", "result": "\u5728\u6b27\u6d32\u536b\u661f\u6570\u636e\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u70b9\u9884\u6d4b\u548c\u6982\u7387\u9884\u6d4b\u6307\u6807\u4e0a\u4e00\u81f4\u4f18\u4e8e\u7edf\u8ba1\u3001\u6df1\u5ea6\u5b66\u4e60\u548c\u8fd1\u671f\u65f6\u95f4\u5e8f\u5217\u57fa\u7ebf\u65b9\u6cd5\uff1b\u6d88\u878d\u7814\u7a76\u663e\u793a\u76ee\u6807\u5386\u53f2\u4fe1\u606f\u8d77\u6838\u5fc3\u4f5c\u7528\uff0c\u800c\u6c14\u8c61\u534f\u53d8\u91cf\u8054\u5408\u4f7f\u7528\u65f6\u63d0\u4f9b\u4e92\u8865\u589e\u76ca\u3002", "conclusion": "\u63d0\u51fa\u7684\u6982\u7387\u9884\u6d4b\u6846\u67b6\u80fd\u6709\u6548\u5904\u7406\u536b\u661fNDVI\u9884\u6d4b\u4e2d\u7684\u4e0d\u89c4\u5219\u91c7\u6837\u548c\u5f02\u8d28\u6027\u6c14\u5019\u6761\u4ef6\uff0c\u4e3a\u7cbe\u51c6\u519c\u4e1a\u63d0\u4f9b\u53ef\u9760\u7684\u6570\u636e\u9a71\u52a8\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2602.17871", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2602.17871", "abs": "https://arxiv.org/abs/2602.17871", "authors": ["Dhruba Ghosh", "Yuhui Zhang", "Ludwig Schmidt"], "title": "Understanding the Fine-Grained Knowledge Capabilities of Vision-Language Models", "comment": null, "summary": "Vision-language models (VLMs) have made substantial progress across a wide range of visual question answering benchmarks, spanning visual reasoning, document understanding, and multimodal dialogue. These improvements are evident in a wide range of VLMs built on a variety of base models, alignment architectures, and training data. However, recent works show that these models trail behind in traditional image classification benchmarks, which test fine-grained visual knowledge. We test a large number of recent VLMs on fine-grained classification benchmarks and identify potential factors in the disconnect between fine-grained knowledge and other vision benchmarks. Through a series of ablation experiments, we find that using a better LLM improves all benchmark scores equally, while a better vision encoder disproportionately improves fine-grained classification performance. Furthermore, we find that the pretraining stage is also vital to fine-grained performance, particularly when the language model weights are unfrozen during pretraining. These insights pave the way for enhancing fine-grained visual understanding and vision-centric capabilities in VLMs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\u66f4\u597d\u7684\u89c6\u89c9\u7f16\u7801\u5668\u548c\u9884\u8bad\u7ec3\u9636\u6bb5\u5bf9\u63d0\u5347\u7ec6\u7c92\u5ea6\u5206\u7c7b\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u867d\u7136\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5404\u79cd\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u6700\u8fd1\u7684\u7814\u7a76\u8868\u660e\u8fd9\u4e9b\u6a21\u578b\u5728\u4f20\u7edf\u7684\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\uff08\u7279\u522b\u662f\u7ec6\u7c92\u5ea6\u89c6\u89c9\u77e5\u8bc6\u6d4b\u8bd5\uff09\u4e0a\u8868\u73b0\u843d\u540e\u3002\u7814\u7a76\u8005\u5e0c\u671b\u63a2\u7d22\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u627e\u51fa\u5f71\u54cd\u8fd9\u79cd\u80fd\u529b\u7684\u5173\u952e\u56e0\u7d20\u3002", "method": "\u7814\u7a76\u6d4b\u8bd5\u4e86\u5927\u91cf\u6700\u65b0\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u5206\u7c7b\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u4e00\u7cfb\u5217\u6d88\u878d\u5b9e\u9a8c\u5206\u6790\u5f71\u54cd\u6027\u80fd\u7684\u56e0\u7d20\u3002\u5b9e\u9a8c\u8bbe\u8ba1\u5305\u62ec\uff1a1\uff09\u6d4b\u8bd5\u4e0d\u540cLLM\u5bf9\u5404\u7c7b\u57fa\u51c6\u6027\u80fd\u7684\u5f71\u54cd\uff1b2\uff09\u6d4b\u8bd5\u4e0d\u540c\u89c6\u89c9\u7f16\u7801\u5668\u5bf9\u7ec6\u7c92\u5ea6\u5206\u7c7b\u6027\u80fd\u7684\u5f71\u54cd\uff1b3\uff09\u5206\u6790\u9884\u8bad\u7ec3\u9636\u6bb5\uff08\u7279\u522b\u662f\u8bed\u8a00\u6a21\u578b\u6743\u91cd\u662f\u5426\u51bb\u7ed3\uff09\u5bf9\u7ec6\u7c92\u5ea6\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff1a1\uff09\u4f7f\u7528\u66f4\u597d\u7684LLM\u80fd\u540c\u7b49\u63d0\u5347\u6240\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u5206\u6570\uff1b2\uff09\u66f4\u597d\u7684\u89c6\u89c9\u7f16\u7801\u5668\u80fd\u4e0d\u6210\u6bd4\u4f8b\u5730\u63d0\u5347\u7ec6\u7c92\u5ea6\u5206\u7c7b\u6027\u80fd\uff1b3\uff09\u9884\u8bad\u7ec3\u9636\u6bb5\u5bf9\u7ec6\u7c92\u5ea6\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5728\u9884\u8bad\u7ec3\u671f\u95f4\u4e0d\u51bb\u7ed3\u8bed\u8a00\u6a21\u578b\u6743\u91cd\u65f6\u6548\u679c\u66f4\u4f73\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7406\u89e3\u80fd\u529b\u7684\u5173\u952e\u56e0\u7d20\uff1a\u89c6\u89c9\u7f16\u7801\u5668\u7684\u8d28\u91cf\u548c\u9884\u8bad\u7ec3\u7b56\u7565\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u589e\u5f3a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7406\u89e3\u548c\u89c6\u89c9\u4e2d\u5fc3\u80fd\u529b\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\uff0c\u6709\u52a9\u4e8e\u7f29\u5c0f\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u901a\u7528\u89c6\u89c9\u95ee\u7b54\u548c\u7ec6\u7c92\u5ea6\u5206\u7c7b\u4efb\u52a1\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u3002"}}
{"id": "2602.18025", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18025", "abs": "https://arxiv.org/abs/2602.18025", "authors": ["Haruki Abe", "Takayuki Osa", "Yusuke Mukuta", "Tatsuya Harada"], "title": "Cross-Embodiment Offline Reinforcement Learning for Heterogeneous Robot Datasets", "comment": "ICLR 2026", "summary": "Scalable robot policy pre-training has been hindered by the high cost of collecting high-quality demonstrations for each platform. In this study, we address this issue by uniting offline reinforcement learning (offline RL) with cross-embodiment learning. Offline RL leverages both expert and abundant suboptimal data, and cross-embodiment learning aggregates heterogeneous robot trajectories across diverse morphologies to acquire universal control priors. We perform a systematic analysis of this offline RL and cross-embodiment paradigm, providing a principled understanding of its strengths and limitations. To evaluate this offline RL and cross-embodiment paradigm, we construct a suite of locomotion datasets spanning 16 distinct robot platforms. Our experiments confirm that this combined approach excels at pre-training with datasets rich in suboptimal trajectories, outperforming pure behavior cloning. However, as the proportion of suboptimal data and the number of robot types increase, we observe that conflicting gradients across morphologies begin to impede learning. To mitigate this, we introduce an embodiment-based grouping strategy in which robots are clustered by morphological similarity and the model is updated with a group gradient. This simple, static grouping substantially reduces inter-robot conflicts and outperforms existing conflict-resolution methods.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7ed3\u5408\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u548c\u8de8\u5177\u8eab\u5b66\u4e60\u6765\u89e3\u51b3\u673a\u5668\u4eba\u7b56\u7565\u9884\u8bad\u7ec3\u4e2d\u9ad8\u8d28\u91cf\u6f14\u793a\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u6784\u5efa\u5305\u542b16\u4e2a\u673a\u5668\u4eba\u5e73\u53f0\u7684\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u5f62\u6001\u76f8\u4f3c\u6027\u7684\u5206\u7ec4\u7b56\u7565\u6765\u7f13\u89e3\u591a\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u7684\u68af\u5ea6\u51b2\u7a81\u95ee\u9898\u3002", "motivation": "\u673a\u5668\u4eba\u7b56\u7565\u9884\u8bad\u7ec3\u9762\u4e34\u9ad8\u8d28\u91cf\u6f14\u793a\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u6602\u7684\u95ee\u9898\uff0c\u4e0d\u540c\u673a\u5668\u4eba\u5e73\u53f0\u9700\u8981\u5404\u81ea\u7684\u6f14\u793a\u6570\u636e\uff0c\u8fd9\u9650\u5236\u4e86\u89c4\u6a21\u5316\u9884\u8bad\u7ec3\u7684\u53ef\u884c\u6027\u3002", "method": "\u7ed3\u5408\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u548c\u8de8\u5177\u8eab\u5b66\u4e60\uff0c\u5229\u7528\u4e13\u5bb6\u548c\u6b21\u4f18\u6570\u636e\uff0c\u805a\u5408\u5f02\u6784\u673a\u5668\u4eba\u8f68\u8ff9\u6765\u83b7\u53d6\u901a\u7528\u63a7\u5236\u5148\u9a8c\u3002\u5f53\u6b21\u4f18\u6570\u636e\u6bd4\u4f8b\u548c\u673a\u5668\u4eba\u7c7b\u578b\u589e\u52a0\u65f6\uff0c\u5f15\u5165\u57fa\u4e8e\u5f62\u6001\u76f8\u4f3c\u6027\u7684\u5206\u7ec4\u7b56\u7565\uff0c\u5c06\u673a\u5668\u4eba\u6309\u5f62\u6001\u805a\u7c7b\u5e76\u4f7f\u7528\u7ec4\u68af\u5ea6\u66f4\u65b0\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u5305\u542b\u4e30\u5bcc\u6b21\u4f18\u8f68\u8ff9\u7684\u6570\u636e\u96c6\u4e0a\u9884\u8bad\u7ec3\u6548\u679c\u4f18\u4e8e\u7eaf\u884c\u4e3a\u514b\u9686\u3002\u4f46\u968f\u7740\u6b21\u4f18\u6570\u636e\u6bd4\u4f8b\u548c\u673a\u5668\u4eba\u7c7b\u578b\u589e\u52a0\uff0c\u5f62\u6001\u95f4\u7684\u68af\u5ea6\u51b2\u7a81\u4f1a\u963b\u788d\u5b66\u4e60\u3002\u63d0\u51fa\u7684\u9759\u6001\u5206\u7ec4\u7b56\u7565\u80fd\u663e\u8457\u51cf\u5c11\u673a\u5668\u4eba\u95f4\u51b2\u7a81\uff0c\u4f18\u4e8e\u73b0\u6709\u51b2\u7a81\u89e3\u51b3\u65b9\u6cd5\u3002", "conclusion": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e0e\u8de8\u5177\u8eab\u5b66\u4e60\u7684\u7ed3\u5408\u4e3a\u673a\u5668\u4eba\u7b56\u7565\u9884\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u591a\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u7684\u68af\u5ea6\u51b2\u7a81\u95ee\u9898\u3002\u57fa\u4e8e\u5f62\u6001\u76f8\u4f3c\u6027\u7684\u7b80\u5355\u9759\u6001\u5206\u7ec4\u7b56\u7565\u662f\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2602.17684", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17684", "abs": "https://arxiv.org/abs/2602.17684", "authors": ["Xiao Zhu", "Xinyu Zhou", "Boyu Zhu", "Hanxu Hu", "Mingzhe Du", "Haotian Zhang", "Huiming Wang", "Zhijiang Guo"], "title": "CodeScaler: Scaling Code LLM Training and Test-Time Inference via Execution-Free Reward Models", "comment": null, "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) has driven recent progress in code large language models by leveraging execution-based feedback from unit tests, but its scalability is fundamentally constrained by the availability and reliability of high-quality test cases. We propose CodeScaler, an execution-free reward model designed to scale both reinforcement learning training and test-time inference for code generation. CodeScaler is trained on carefully curated preference data derived from verified code problems and incorporates syntax-aware code extraction and validity-preserving reward shaping to ensure stable and robust optimization. Across five coding benchmarks, CodeScaler improves Qwen3-8B-Base by an average of +11.72 points, outperforming binary execution-based RL by +1.82 points, and enables scalable reinforcement learning on synthetic datasets without any test cases. At inference time, CodeScaler serves as an effective test-time scaling method, achieving performance comparable to unit test approaches while providing a 10-fold reduction in latency. Moreover, CodeScaler surpasses existing reward models on RM-Bench not only in the code domain (+3.3 points), but also in general and reasoning domains (+2.7 points on average).", "AI": {"tldr": "CodeScaler\uff1a\u65e0\u9700\u6267\u884c\u7684\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u8bed\u6cd5\u611f\u77e5\u7684\u4ee3\u7801\u63d0\u53d6\u548c\u4fdd\u6301\u6709\u6548\u6027\u7684\u5956\u52b1\u5851\u9020\uff0c\u663e\u8457\u63d0\u5347\u4ee3\u7801\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\uff0c\u8d85\u8d8a\u57fa\u4e8e\u5355\u5143\u6d4b\u8bd5\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u5b9e\u73b010\u500d\u5ef6\u8fdf\u964d\u4f4e\u3002", "motivation": "\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u4f9d\u8d56\u5355\u5143\u6d4b\u8bd5\u7684\u6267\u884c\u53cd\u9988\uff0c\u4f46\u5176\u53ef\u6269\u5c55\u6027\u53d7\u9650\u4e8e\u9ad8\u8d28\u91cf\u6d4b\u8bd5\u7528\u4f8b\u7684\u53ef\u7528\u6027\u548c\u53ef\u9760\u6027\u3002\u9700\u8981\u4e00\u79cd\u65e0\u9700\u6267\u884c\u7684\u5956\u52b1\u6a21\u578b\u6765\u6269\u5c55\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u548c\u63a8\u7406\u3002", "method": "\u63d0\u51faCodeScaler\u5956\u52b1\u6a21\u578b\uff0c\u5728\u5df2\u9a8c\u8bc1\u4ee3\u7801\u95ee\u9898\u7684\u57fa\u7840\u4e0a\u6784\u5efa\u7cbe\u5fc3\u7b56\u5212\u7684\u504f\u597d\u6570\u636e\uff0c\u91c7\u7528\u8bed\u6cd5\u611f\u77e5\u7684\u4ee3\u7801\u63d0\u53d6\u548c\u4fdd\u6301\u6709\u6548\u6027\u7684\u5956\u52b1\u5851\u9020\u6280\u672f\uff0c\u786e\u4fdd\u7a33\u5b9a\u9c81\u68d2\u7684\u4f18\u5316\u3002", "result": "\u5728\u4e94\u4e2a\u7f16\u7801\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCodeScaler\u5c06Qwen3-8B-Base\u5e73\u5747\u63d0\u534711.72\u5206\uff0c\u8d85\u8d8a\u57fa\u4e8e\u6267\u884c\u7684RL\u65b9\u6cd51.82\u5206\uff1b\u5728\u65e0\u6d4b\u8bd5\u7528\u4f8b\u7684\u5408\u6210\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u53ef\u6269\u5c55\u5f3a\u5316\u5b66\u4e60\uff1b\u63a8\u7406\u65f6\u5ef6\u8fdf\u964d\u4f4e10\u500d\uff1b\u5728RM-Bench\u4e0a\u8d85\u8d8a\u73b0\u6709\u5956\u52b1\u6a21\u578b\u3002", "conclusion": "CodeScaler\u4f5c\u4e3a\u4e00\u79cd\u65e0\u9700\u6267\u884c\u7684\u5956\u52b1\u6a21\u578b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u57fa\u4e8e\u6d4b\u8bd5\u7684\u5f3a\u5316\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u6027\u9650\u5236\uff0c\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\u548c\u5ef6\u8fdf\u964d\u4f4e\uff0c\u5e76\u5728\u591a\u4e2a\u9886\u57df\u8d85\u8d8a\u4e86\u73b0\u6709\u5956\u52b1\u6a21\u578b\u3002"}}
{"id": "2602.17841", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2602.17841", "abs": "https://arxiv.org/abs/2602.17841", "authors": ["Dejan Grba"], "title": "Strange Undercurrents: A Critical Outlook on AI's Cultural Influence", "comment": "15 pages", "summary": "While generative artificial intelligence (generative AI) is being examined extensively, some issues it epitomizes call for more refined scrutiny and deeper contextualization. Besides the lack of nuanced understanding of art's continuously changing character in discussions about generative AI's cultural impact, one of the notably underexplored aspects is the conceptual and ideological substrate of AI science and industry whose attributes generative AI propagates by fostering the integration of diverse modes of AI-powered artmaking into the mainstream culture and economy. Taking the current turmoil around the generative AI as a pretext, this paper summarizes a broader study of AI's influence on art notions focusing on the confluence of certain foundational concepts in computer science and ideological vectors of the AI industry that transfer into art, culture, and society. This influence merges diverse and sometimes inconsistent but somehow coalescing philosophical premises, technical ideas, and political views, many of which have unfavorable overtones.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u751f\u6210\u5f0fAI\u5bf9\u827a\u672f\u89c2\u5ff5\u7684\u5f71\u54cd\uff0c\u805a\u7126\u8ba1\u7b97\u673a\u79d1\u5b66\u57fa\u7840\u6982\u5ff5\u4e0eAI\u884c\u4e1a\u610f\u8bc6\u5f62\u6001\u5411\u827a\u672f\u3001\u6587\u5316\u548c\u793e\u4f1a\u8f6c\u79fb\u7684\u95ee\u9898", "motivation": "\u5f53\u524d\u5bf9\u751f\u6210\u5f0fAI\u7684\u8ba8\u8bba\u7f3a\u4e4f\u5bf9\u827a\u672f\u4e0d\u65ad\u53d8\u5316\u7279\u6027\u7684\u7ec6\u81f4\u7406\u89e3\uff0c\u4e14\u5ffd\u89c6\u4e86AI\u79d1\u5b66\u4e0e\u884c\u4e1a\u7684\u6982\u5ff5\u548c\u610f\u8bc6\u5f62\u6001\u57fa\u7840\u5982\u4f55\u901a\u8fc7\u751f\u6210\u5f0fAI\u4f20\u64ad\u5230\u4e3b\u6d41\u6587\u5316\u548c\u7ecf\u6d4e\u4e2d", "method": "\u4ee5\u5f53\u524d\u751f\u6210\u5f0fAI\u7684\u4e89\u8bae\u4e3a\u5207\u5165\u70b9\uff0c\u603b\u7ed3\u66f4\u5e7f\u6cdb\u7684AI\u5bf9\u827a\u672f\u89c2\u5ff5\u5f71\u54cd\u7684\u7814\u7a76\uff0c\u805a\u7126\u8ba1\u7b97\u673a\u79d1\u5b66\u57fa\u7840\u6982\u5ff5\u4e0eAI\u884c\u4e1a\u610f\u8bc6\u5f62\u6001\u5411\u827a\u672f\u9886\u57df\u7684\u8f6c\u79fb", "result": "\u7814\u7a76\u53d1\u73b0AI\u7684\u5f71\u54cd\u878d\u5408\u4e86\u591a\u6837\u4e14\u6709\u65f6\u4e0d\u4e00\u81f4\u4f46\u53c8\u80fd\u51dd\u805a\u7684\u54f2\u5b66\u524d\u63d0\u3001\u6280\u672f\u7406\u5ff5\u548c\u653f\u6cbb\u89c2\u70b9\uff0c\u5176\u4e2d\u8bb8\u591a\u5e26\u6709\u4e0d\u5229\u7684\u57fa\u8c03", "conclusion": "\u751f\u6210\u5f0fAI\u4e0d\u4ec5\u9700\u8981\u6280\u672f\u5c42\u9762\u7684\u5ba1\u89c6\uff0c\u66f4\u9700\u8981\u5bf9\u5176\u4f20\u64ad\u7684\u8ba1\u7b97\u673a\u79d1\u5b66\u57fa\u7840\u6982\u5ff5\u548cAI\u884c\u4e1a\u610f\u8bc6\u5f62\u6001\u5982\u4f55\u5f71\u54cd\u827a\u672f\u89c2\u5ff5\u8fdb\u884c\u6df1\u5165\u6279\u5224\u6027\u5206\u6790"}}
{"id": "2602.17909", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17909", "abs": "https://arxiv.org/abs/2602.17909", "authors": ["Amirhosein Javadi", "Chi-Shiang Gau", "Konstantinos D. Polyzos", "Tara Javidi"], "title": "A Single Image and Multimodality Is All You Need for Novel View Synthesis", "comment": null, "summary": "Diffusion-based approaches have recently demonstrated strong performance for single-image novel view synthesis by conditioning generative models on geometry inferred from monocular depth estimation. However, in practice, the quality and consistency of the synthesized views are fundamentally limited by the reliability of the underlying depth estimates, which are often fragile under low texture, adverse weather, and occlusion-heavy real-world conditions. In this work, we show that incorporating sparse multimodal range measurements provides a simple yet effective way to overcome these limitations. We introduce a multimodal depth reconstruction framework that leverages extremely sparse range sensing data, such as automotive radar or LiDAR, to produce dense depth maps that serve as robust geometric conditioning for diffusion-based novel view synthesis. Our approach models depth in an angular domain using a localized Gaussian Process formulation, enabling computationally efficient inference while explicitly quantifying uncertainty in regions with limited observations. The reconstructed depth and uncertainty are used as a drop-in replacement for monocular depth estimators in existing diffusion-based rendering pipelines, without modifying the generative model itself. Experiments on real-world multimodal driving scenes demonstrate that replacing vision-only depth with our sparse range-based reconstruction substantially improves both geometric consistency and visual quality in single-image novel-view video generation. These results highlight the importance of reliable geometric priors for diffusion-based view synthesis and demonstrate the practical benefits of multimodal sensing even at extreme levels of sparsity.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u6df1\u5ea6\u91cd\u5efa\u6846\u67b6\uff0c\u5229\u7528\u6781\u7a00\u758f\u7684\u96f7\u8fbe\u6216LiDAR\u6d4b\u8ddd\u6570\u636e\u751f\u6210\u5bc6\u96c6\u6df1\u5ea6\u56fe\uff0c\u4f5c\u4e3a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b0\u89c6\u89d2\u5408\u6210\u7684\u51e0\u4f55\u6761\u4ef6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u56fe\u50cf\u65b0\u89c6\u89d2\u5408\u6210\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u5355\u56fe\u50cf\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7684\u51e0\u4f55\u4fe1\u606f\uff0c\u4f46\u5728\u4f4e\u7eb9\u7406\u3001\u6076\u52a3\u5929\u6c14\u548c\u906e\u6321\u4e25\u91cd\u7684\u771f\u5b9e\u573a\u666f\u4e2d\uff0c\u6df1\u5ea6\u4f30\u8ba1\u7684\u53ef\u9760\u6027\u6709\u9650\uff0c\u5bfc\u81f4\u5408\u6210\u89c6\u56fe\u7684\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u53d7\u5230\u9650\u5236\u3002", "method": "\u63d0\u51fa\u591a\u6a21\u6001\u6df1\u5ea6\u91cd\u5efa\u6846\u67b6\uff0c\u5229\u7528\u6781\u7a00\u758f\u7684\u6d4b\u8ddd\u6570\u636e\uff08\u5982\u6c7d\u8f66\u96f7\u8fbe\u6216LiDAR\uff09\uff0c\u91c7\u7528\u5c40\u90e8\u9ad8\u65af\u8fc7\u7a0b\u5efa\u6a21\u5728\u89d2\u5ea6\u57df\u4e2d\u8868\u793a\u6df1\u5ea6\uff0c\u5b9e\u73b0\u8ba1\u7b97\u9ad8\u6548\u63a8\u7406\u5e76\u663e\u5f0f\u91cf\u5316\u89c2\u6d4b\u6709\u9650\u533a\u57df\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u91cd\u5efa\u7684\u6df1\u5ea6\u548c\u4e0d\u786e\u5b9a\u6027\u53ef\u76f4\u63a5\u66ff\u6362\u73b0\u6709\u6269\u6563\u6e32\u67d3\u6d41\u7a0b\u4e2d\u7684\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u5668\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u591a\u6a21\u6001\u9a7e\u9a76\u573a\u666f\u5b9e\u9a8c\u4e2d\uff0c\u7528\u7a00\u758f\u6d4b\u8ddd\u91cd\u5efa\u6df1\u5ea6\u66ff\u6362\u7eaf\u89c6\u89c9\u6df1\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u56fe\u50cf\u65b0\u89c6\u89d2\u89c6\u9891\u751f\u6210\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u53ef\u9760\u51e0\u4f55\u5148\u9a8c\u5bf9\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u89c6\u89d2\u5408\u6210\u7684\u91cd\u8981\u6027\uff0c\u5e76\u8bc1\u660e\u4e86\u5373\u4f7f\u5728\u6781\u7aef\u7a00\u758f\u6c34\u5e73\u4e0b\uff0c\u591a\u6a21\u6001\u4f20\u611f\u4e5f\u5177\u6709\u5b9e\u9645\u4f18\u52bf\u3002"}}
{"id": "2602.18288", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2602.18288", "abs": "https://arxiv.org/abs/2602.18288", "authors": ["Jiayi Wu", "Zhengyu Wu", "Xunkai Li", "Rong-Hua Li", "Guoren Wang"], "title": "A Topology-Aware Positive Sample Set Construction and Feature Optimization Method in Implicit Collaborative Filtering", "comment": null, "summary": "Negative sampling strategies are widely used in implicit collaborative filtering to address issues like data sparsity and class imbalance. However, these methods often introduce false negatives, hindering the model's ability to accurately learn users' latent preferences. To mitigate this problem, existing methods adjust the negative sampling distribution based on statistical features from model training or the hardness of negative samples. Nevertheless, these methods face two key limitations: (1) over-reliance on the model's current representation capabilities; (2) failure to leverage the potential of false negatives as latent positive samples to guide model learning of user preferences more accurately. To address the above issues, we propose a Topology-aware Positive Sample Set Construction and Feature Optimization method (TPSC-FO). First, we design a simple topological community-aware false negative identification (FNI) method and observe that topological community structures in interaction networks can effectively identify false negatives. Motivated by this, we develop a topology-aware positive sample set construction module. This module employs a differential community detection strategy to capture topological community structures in implicit feedback, coupled with personalized noise filtration to reliably identify false negatives and convert them into positive samples. Additionally, we introduce a neighborhood-guided feature optimization module that refines positive sample features by incorporating neighborhood features in the embedding space, effectively mitigating noise in the positive samples. Extensive experiments on five real-world datasets and two synthetic datasets validate the effectiveness of TPSC-FO.", "AI": {"tldr": "TPSC-FO\u65b9\u6cd5\u901a\u8fc7\u62d3\u6251\u793e\u533a\u7ed3\u6784\u8bc6\u522b\u5047\u9634\u6027\u6837\u672c\u5e76\u5c06\u5176\u8f6c\u5316\u4e3a\u6b63\u6837\u672c\uff0c\u7ed3\u5408\u90bb\u57df\u5f15\u5bfc\u7684\u7279\u5f81\u4f18\u5316\uff0c\u63d0\u5347\u9690\u5f0f\u534f\u540c\u8fc7\u6ee4\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8d1f\u91c7\u6837\u65b9\u6cd5\u5b58\u5728\u5047\u9634\u6027\u95ee\u9898\uff0c\u4e14\u8fc7\u5ea6\u4f9d\u8d56\u6a21\u578b\u5f53\u524d\u8868\u793a\u80fd\u529b\uff0c\u672a\u80fd\u5229\u7528\u5047\u9634\u6027\u4f5c\u4e3a\u6f5c\u5728\u6b63\u6837\u672c\u6765\u66f4\u51c6\u786e\u5b66\u4e60\u7528\u6237\u504f\u597d\u3002", "method": "\u63d0\u51faTPSC-FO\u65b9\u6cd5\uff1a1) \u62d3\u6251\u611f\u77e5\u6b63\u6837\u672c\u96c6\u6784\u5efa\u6a21\u5757\uff0c\u4f7f\u7528\u5dee\u5206\u793e\u533a\u68c0\u6d4b\u7b56\u7565\u6355\u83b7\u9690\u5f0f\u53cd\u9988\u4e2d\u7684\u62d3\u6251\u793e\u533a\u7ed3\u6784\uff0c\u7ed3\u5408\u4e2a\u6027\u5316\u566a\u58f0\u8fc7\u6ee4\u8bc6\u522b\u5047\u9634\u6027\u5e76\u8f6c\u5316\u4e3a\u6b63\u6837\u672c\uff1b2) \u90bb\u57df\u5f15\u5bfc\u7279\u5f81\u4f18\u5316\u6a21\u5757\uff0c\u901a\u8fc7\u878d\u5165\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u90bb\u57df\u7279\u5f81\u6765\u7cbe\u70bc\u6b63\u6837\u672c\u7279\u5f81\u3002", "result": "\u5728\u4e94\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u548c\u4e24\u4e2a\u5408\u6210\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u9a8c\u8bc1\u4e86TPSC-FO\u7684\u6709\u6548\u6027\u3002", "conclusion": "TPSC-FO\u901a\u8fc7\u62d3\u6251\u793e\u533a\u7ed3\u6784\u8bc6\u522b\u5047\u9634\u6027\u5e76\u5c06\u5176\u8f6c\u5316\u4e3a\u6b63\u6837\u672c\uff0c\u7ed3\u5408\u7279\u5f81\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9690\u5f0f\u534f\u540c\u8fc7\u6ee4\u7684\u6027\u80fd\u3002"}}
{"id": "2602.17685", "categories": ["cs.LG", "cs.RO", "physics.space-ph"], "pdf": "https://arxiv.org/pdf/2602.17685", "abs": "https://arxiv.org/abs/2602.17685", "authors": ["Agni Bandyopadhyay", "Gunther Waxenegger-Wilfing"], "title": "Optimal Multi-Debris Mission Planning in LEO: A Deep Reinforcement Learning Approach with Co-Elliptic Transfers and Refueling", "comment": "Presented at Conference: IFAC Workshop on Control Aspects of Multi-Satellite Systems (CAMSAT) 2025 At: Wuerzburg", "summary": "This paper addresses the challenge of multi target active debris removal (ADR) in Low Earth Orbit (LEO) by introducing a unified coelliptic maneuver framework that combines Hohmann transfers, safety ellipse proximity operations, and explicit refueling logic. We benchmark three distinct planning algorithms Greedy heuristic, Monte Carlo Tree Search (MCTS), and deep reinforcement learning (RL) using Masked Proximal Policy Optimization (PPO) within a realistic orbital simulation environment featuring randomized debris fields, keep out zones, and delta V constraints. Experimental results over 100 test scenarios demonstrate that Masked PPO achieves superior mission efficiency and computational performance, visiting up to twice as many debris as Greedy and significantly outperforming MCTS in runtime. These findings underscore the promise of modern RL methods for scalable, safe, and resource efficient space mission planning, paving the way for future advancements in ADR autonomy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4f4e\u5730\u7403\u8f68\u9053\u591a\u76ee\u6807\u4e3b\u52a8\u788e\u7247\u79fb\u9664\u7684\u7edf\u4e00\u5171\u692d\u5706\u673a\u52a8\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u970d\u66fc\u8f6c\u79fb\u3001\u5b89\u5168\u692d\u5706\u63a5\u8fd1\u64cd\u4f5c\u548c\u663e\u5f0f\u71c3\u6599\u8865\u7ed9\u903b\u8f91\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e09\u79cd\u89c4\u5212\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f4e\u5730\u7403\u8f68\u9053\u591a\u76ee\u6807\u4e3b\u52a8\u788e\u7247\u79fb\u9664\u7684\u6311\u6218\uff0c\u9700\u8981\u9ad8\u6548\u7684\u8f68\u9053\u673a\u52a8\u89c4\u5212\u65b9\u6cd5\u6765\u5904\u7406\u968f\u673a\u788e\u7247\u573a\u3001\u7981\u98de\u533a\u548c\u71c3\u6599\u7ea6\u675f\u7b49\u590d\u6742\u6761\u4ef6\u3002", "method": "\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u5171\u692d\u5706\u673a\u52a8\u6846\u67b6\uff0c\u7ed3\u5408\u970d\u66fc\u8f6c\u79fb\u3001\u5b89\u5168\u692d\u5706\u63a5\u8fd1\u64cd\u4f5c\u548c\u663e\u5f0f\u71c3\u6599\u8865\u7ed9\u903b\u8f91\u3002\u5728\u771f\u5b9e\u7684\u8f68\u9053\u4eff\u771f\u73af\u5883\u4e2d\uff0c\u5bf9\u4e09\u79cd\u89c4\u5212\u7b97\u6cd5\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff1a\u8d2a\u5a6a\u542f\u53d1\u5f0f\u7b97\u6cd5\u3001\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u548c\u57fa\u4e8e\u63a9\u7801\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5728100\u4e2a\u6d4b\u8bd5\u573a\u666f\u4e2d\uff0c\u63a9\u7801PPO\u7b97\u6cd5\u8868\u73b0\u51fa\u6700\u4f18\u7684\u4efb\u52a1\u6548\u7387\u548c\u8ba1\u7b97\u6027\u80fd\uff0c\u8bbf\u95ee\u7684\u788e\u7247\u6570\u91cf\u662f\u8d2a\u5a6a\u7b97\u6cd5\u7684\u4e24\u500d\uff0c\u5e76\u4e14\u5728\u8fd0\u884c\u65f6\u95f4\u4e0a\u663e\u8457\u4f18\u4e8e\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u3002", "conclusion": "\u73b0\u4ee3\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u3001\u5b89\u5168\u548c\u8d44\u6e90\u9ad8\u6548\u7684\u7a7a\u95f4\u4efb\u52a1\u89c4\u5212\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u4e3b\u52a8\u788e\u7247\u79fb\u9664\u81ea\u4e3b\u6027\u7684\u53d1\u5c55\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2602.17919", "categories": ["cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.17919", "abs": "https://arxiv.org/abs/2602.17919", "authors": ["Ruiqing Han", "Hao Cui", "Taha Yasseri"], "title": "Visual Anthropomorphism Shifts Evaluations of Gendered AI Managers", "comment": "Preprint, Under Review", "summary": "This research examines whether competence cues can reduce gender bias in evaluations of AI managers and whether these effects depend on how the AI is represented. Across two preregistered experiments (N = 2,505), each employing a 2 x 2 x 3 design manipulating AI gender, competence, and decision outcome, we compared text-based descriptions of AI managers with visually generated AI faces created using a reverse-correlation paradigm. In the text condition, evaluations were driven by competence rather than gender. When participants received unfavourable decisions, high-competence AI managers were judged as fairer, more competent, and better leaders than low-competence managers, regardless of AI gender. In contrast, when the AI manager was visually represented, competence cues had attenuated influence once facial information was present. Instead, participants showed systematic gender-differentiated responses to AI faces, with feminine-appearing managers evaluated as more competent and more trustworthy than masculine-appearing managers, particularly when delivering favourable outcomes. These gender effects were largely absent when outcomes were unfavourable, suggesting that negative feedback attenuates the influence of both competence information and facial cues. Taken together, these findings show that competence information can mitigate negative reactions to AI managers in text-based interactions, whereas facial anthropomorphism elicits gendered perceptual biases not observed in text-only settings. The results highlight that representational modality plays a critical role in determining when gender stereotypes are activated in evaluations of AI systems and underscore that design choices are consequential for AI governance in evaluative contexts.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff1a\u80fd\u529b\u7ebf\u7d22\u53ef\u51cf\u5c11AI\u7ecf\u7406\u8bc4\u4ef7\u4e2d\u7684\u6027\u522b\u504f\u89c1\uff0c\u4f46\u6548\u679c\u53d6\u51b3\u4e8eAI\u7684\u5448\u73b0\u65b9\u5f0f\u3002\u6587\u672c\u63cf\u8ff0\u4e2d\u80fd\u529b\u4e3b\u5bfc\u8bc4\u4ef7\uff0c\u800c\u89c6\u89c9\u9762\u90e8\u5448\u73b0\u5219\u5f15\u53d1\u6027\u522b\u504f\u89c1\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u8ba8\u80fd\u529b\u7ebf\u7d22\u662f\u5426\u80fd\u51cf\u5c11AI\u7ecf\u7406\u8bc4\u4ef7\u4e2d\u7684\u6027\u522b\u504f\u89c1\uff0c\u4ee5\u53ca\u8fd9\u79cd\u6548\u679c\u662f\u5426\u53d6\u51b3\u4e8eAI\u7684\u5448\u73b0\u65b9\u5f0f\uff08\u6587\u672cvs\u89c6\u89c9\uff09\u3002\u65e8\u5728\u7406\u89e3\u4e0d\u540c\u5448\u73b0\u6a21\u6001\u5982\u4f55\u5f71\u54cd\u6027\u522b\u523b\u677f\u5370\u8c61\u7684\u6fc0\u6d3b\u3002", "method": "\u91c7\u7528\u4e24\u4e2a\u9884\u6ce8\u518c\u5b9e\u9a8c\uff08N=2,505\uff09\uff0c\u4f7f\u75282\u00d72\u00d73\u8bbe\u8ba1\uff1a\u64cd\u7eb5AI\u6027\u522b\uff08\u7537/\u5973\uff09\u3001\u80fd\u529b\uff08\u9ad8/\u4f4e\uff09\u548c\u51b3\u7b56\u7ed3\u679c\uff08\u6709\u5229/\u4e0d\u5229\uff09\u3002\u6bd4\u8f83\u6587\u672c\u63cf\u8ff0\u4e0e\u4f7f\u7528\u53cd\u5411\u76f8\u5173\u8303\u5f0f\u751f\u6210\u7684AI\u9762\u90e8\u56fe\u50cf\u4e24\u79cd\u5448\u73b0\u65b9\u5f0f\u3002", "result": "\u6587\u672c\u6761\u4ef6\u4e0b\uff0c\u8bc4\u4ef7\u7531\u80fd\u529b\u800c\u975e\u6027\u522b\u9a71\u52a8\uff1b\u4e0d\u5229\u51b3\u7b56\u65f6\uff0c\u9ad8\u80fd\u529bAI\u7ecf\u7406\u88ab\u8ba4\u4e3a\u66f4\u516c\u5e73\u3001\u66f4\u6709\u80fd\u529b\u3001\u9886\u5bfc\u529b\u66f4\u5f3a\u3002\u89c6\u89c9\u9762\u90e8\u5448\u73b0\u65f6\uff0c\u80fd\u529b\u7ebf\u7d22\u5f71\u54cd\u51cf\u5f31\uff0c\u51fa\u73b0\u7cfb\u7edf\u6027\u6027\u522b\u5dee\u5f02\uff1a\u5973\u6027\u5316\u7ecf\u7406\u88ab\u8ba4\u4e3a\u66f4\u6709\u80fd\u529b\u548c\u66f4\u53ef\u4fe1\uff0c\u7279\u522b\u662f\u5728\u6709\u5229\u7ed3\u679c\u65f6\u3002\u4e0d\u5229\u7ed3\u679c\u65f6\u6027\u522b\u6548\u5e94\u51cf\u5f31\u3002", "conclusion": "\u80fd\u529b\u4fe1\u606f\u53ef\u5728\u6587\u672c\u4ea4\u4e92\u4e2d\u51cf\u8f7b\u5bf9AI\u7ecf\u7406\u7684\u8d1f\u9762\u53cd\u5e94\uff0c\u800c\u9762\u90e8\u62df\u4eba\u5316\u4f1a\u5f15\u53d1\u6587\u672c\u73af\u5883\u4e2d\u672a\u89c2\u5bdf\u5230\u7684\u6027\u522b\u504f\u89c1\u3002\u5448\u73b0\u6a21\u6001\u5728\u51b3\u5b9aAI\u7cfb\u7edf\u8bc4\u4ef7\u4e2d\u6027\u522b\u523b\u677f\u5370\u8c61\u4f55\u65f6\u88ab\u6fc0\u6d3b\u65b9\u9762\u8d77\u5173\u952e\u4f5c\u7528\uff0c\u8bbe\u8ba1\u9009\u62e9\u5bf9AI\u6cbb\u7406\u5177\u6709\u91cd\u8981\u5f71\u54cd\u3002"}}
{"id": "2602.17929", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2602.17929", "abs": "https://arxiv.org/abs/2602.17929", "authors": ["Athanasios Angelakis"], "title": "ZACH-ViT: Regime-Dependent Inductive Bias in Compact Vision Transformers for Medical Imaging", "comment": "15 pages, 12 figures, 7 tables. Code and models available at https://github.com/Bluesman79/ZACH-ViT", "summary": "Vision Transformers rely on positional embeddings and class tokens that encode fixed spatial priors. While effective for natural images, these priors may hinder generalization when spatial layout is weakly informative or inconsistent, a frequent condition in medical imaging and edge-deployed clinical systems. We introduce ZACH-ViT (Zero-token Adaptive Compact Hierarchical Vision Transformer), a compact Vision Transformer that removes both positional embeddings and the [CLS] token, achieving permutation invariance through global average pooling over patch representations. The term \"Zero-token\" specifically refers to removing the dedicated [CLS] aggregation token and positional embeddings; patch tokens remain unchanged and are processed normally. Adaptive residual projections preserve training stability in compact configurations while maintaining a strict parameter budget.\n  Evaluation is performed across seven MedMNIST datasets spanning binary and multi-class tasks under a strict few-shot protocol (50 samples per class, fixed hyperparameters, five random seeds). The empirical analysis demonstrates regime-dependent behavior: ZACH-ViT (0.25M parameters, trained from scratch) achieves its strongest advantage on BloodMNIST and remains competitive with TransMIL on PathMNIST, while its relative advantage decreases on datasets with strong anatomical priors (OCTMNIST, OrganAMNIST), consistent with the architectural hypothesis. These findings support the view that aligning architectural inductive bias with data structure can be more important than pursuing universal benchmark dominance. Despite its minimal size and lack of pretraining, ZACH-ViT achieves competitive performance while maintaining sub-second inference times, supporting deployment in resource-constrained clinical environments. Code and models are available at https://github.com/Bluesman79/ZACH-ViT.", "AI": {"tldr": "ZACH-ViT\u662f\u4e00\u79cd\u7d27\u51d1\u578b\u89c6\u89c9Transformer\uff0c\u79fb\u9664\u4e86\u4f4d\u7f6e\u5d4c\u5165\u548c[CLS]\u6807\u8bb0\uff0c\u901a\u8fc7\u5168\u5c40\u5e73\u5747\u6c60\u5316\u5b9e\u73b0\u6392\u5217\u4e0d\u53d8\u6027\uff0c\u5728\u533b\u5b66\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u4e34\u5e8a\u73af\u5883\u3002", "motivation": "\u4f20\u7edf\u89c6\u89c9Transformer\u4f9d\u8d56\u4f4d\u7f6e\u5d4c\u5165\u548c\u7c7b\u522b\u6807\u8bb0\u7f16\u7801\u56fa\u5b9a\u7684\u7a7a\u95f4\u5148\u9a8c\uff0c\u8fd9\u5728\u81ea\u7136\u56fe\u50cf\u4e2d\u6709\u6548\uff0c\u4f46\u5728\u533b\u5b66\u6210\u50cf\u4e2d\u53ef\u80fd\u963b\u788d\u6cdb\u5316\u80fd\u529b\uff0c\u56e0\u4e3a\u533b\u5b66\u56fe\u50cf\u7684\u7a7a\u95f4\u5e03\u5c40\u4fe1\u606f\u8f83\u5f31\u6216\u4e0d\u4e00\u81f4\u3002", "method": "\u63d0\u51faZACH-ViT\uff0c\u79fb\u9664\u4f4d\u7f6e\u5d4c\u5165\u548c[CLS]\u6807\u8bb0\uff0c\u901a\u8fc7\u5168\u5c40\u5e73\u5747\u6c60\u5316\u5b9e\u73b0\u6392\u5217\u4e0d\u53d8\u6027\uff1b\u91c7\u7528\u81ea\u9002\u5e94\u6b8b\u5dee\u6295\u5f71\u4fdd\u6301\u8bad\u7ec3\u7a33\u5b9a\u6027\uff1b\u5728\u7d27\u51d1\u914d\u7f6e\u4e0b\u4fdd\u6301\u4e25\u683c\u7684\u53c2\u6570\u9884\u7b97\u3002", "result": "\u57287\u4e2aMedMNIST\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff1aZACH-ViT\uff080.25M\u53c2\u6570\uff09\u5728BloodMNIST\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u5728PathMNIST\u4e0a\u4e0eTransMIL\u7ade\u4e89\uff0c\u5728\u5177\u6709\u5f3a\u89e3\u5256\u5148\u9a8c\u7684\u6570\u636e\u96c6\uff08OCTMNIST, OrganAMNIST\uff09\u4e0a\u76f8\u5bf9\u4f18\u52bf\u51cf\u5f31\uff1b\u4fdd\u6301\u4e9a\u79d2\u7ea7\u63a8\u7406\u65f6\u95f4\u3002", "conclusion": "\u5c06\u67b6\u6784\u5f52\u7eb3\u504f\u7f6e\u4e0e\u6570\u636e\u7ed3\u6784\u5bf9\u9f50\u6bd4\u8ffd\u6c42\u901a\u7528\u57fa\u51c6\u4f18\u52bf\u66f4\u91cd\u8981\uff1bZACH-ViT\u5728\u8d44\u6e90\u53d7\u9650\u7684\u4e34\u5e8a\u73af\u5883\u4e2d\u5177\u6709\u90e8\u7f72\u6f5c\u529b\uff0c\u5c3d\u7ba1\u89c4\u6a21\u5c0f\u4e14\u65e0\u9884\u8bad\u7ec3\uff0c\u4ecd\u80fd\u4fdd\u6301\u7ade\u4e89\u529b\u3002"}}
{"id": "2602.17695", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.17695", "abs": "https://arxiv.org/abs/2602.17695", "authors": ["Xin Yu", "Hanwen Xing", "Lingzhou Xue"], "title": "EXACT: Explicit Attribute-Guided Decoding-Time Personalization", "comment": null, "summary": "Achieving personalized alignment requires adapting large language models to each user's evolving context. While decoding-time personalization offers a scalable alternative to training-time methods, existing methods largely rely on implicit, less interpretable preference representations and impose a rigid, context-agnostic user representation, failing to account for how preferences shift across prompts. We introduce EXACT, a new decoding-time personalization that aligns generation with limited pairwise preference feedback using a predefined set of interpretable attributes. EXACT first identifies user-specific attribute subsets by maximizing the likelihood of preferred responses in the offline stage. Then, for online inference, EXACT retrieves the most semantically relevant attributes for an incoming prompt and injects them into the context to steer generation. We establish theoretical approximation guarantees for the proposed algorithm under mild assumptions, and provably show that our similarity-based retrieval mechanism effectively mitigates contextual preference shifts, adapting to disparate tasks without pooling conflicting preferences. Extensive experiments on human-annotated preference datasets demonstrate that EXACT consistently outperforms strong baselines, including preference modeling accuracy and personalized generation quality.", "AI": {"tldr": "EXACT\u662f\u4e00\u79cd\u89e3\u7801\u65f6\u4e2a\u6027\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u53ef\u89e3\u91ca\u5c5e\u6027\u5bf9\u9f50\u751f\u6210\u4e0e\u6709\u9650\u6210\u5bf9\u504f\u597d\u53cd\u9988\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5", "motivation": "\u73b0\u6709\u89e3\u7801\u65f6\u4e2a\u6027\u5316\u65b9\u6cd5\u4f9d\u8d56\u9690\u5f0f\u3001\u96be\u4ee5\u89e3\u91ca\u7684\u504f\u597d\u8868\u793a\uff0c\u4e14\u91c7\u7528\u50f5\u5316\u7684\u4e0a\u4e0b\u6587\u65e0\u5173\u7528\u6237\u8868\u793a\uff0c\u65e0\u6cd5\u5904\u7406\u504f\u597d\u968f\u63d0\u793a\u53d8\u5316\u7684\u95ee\u9898", "method": "EXACT\u4f7f\u7528\u9884\u5b9a\u4e49\u7684\u53ef\u89e3\u91ca\u5c5e\u6027\u96c6\uff1a1\uff09\u79bb\u7ebf\u9636\u6bb5\u901a\u8fc7\u6700\u5927\u5316\u504f\u597d\u54cd\u5e94\u4f3c\u7136\u8bc6\u522b\u7528\u6237\u7279\u5b9a\u5c5e\u6027\u5b50\u96c6\uff1b2\uff09\u5728\u7ebf\u63a8\u7406\u65f6\u68c0\u7d22\u4e0e\u8f93\u5165\u63d0\u793a\u6700\u76f8\u5173\u7684\u5c5e\u6027\u5e76\u6ce8\u5165\u4e0a\u4e0b\u6587\u5f15\u5bfc\u751f\u6210", "result": "\u5728\u6e29\u548c\u5047\u8bbe\u4e0b\u5efa\u7acb\u4e86\u7406\u8bba\u8fd1\u4f3c\u4fdd\u8bc1\uff0c\u8bc1\u660e\u76f8\u4f3c\u6027\u68c0\u7d22\u673a\u5236\u80fd\u6709\u6548\u7f13\u89e3\u4e0a\u4e0b\u6587\u504f\u597d\u504f\u79fb\uff1b\u5728\u4eba\u5de5\u6807\u6ce8\u504f\u597d\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cEXACT\u5728\u504f\u597d\u5efa\u6a21\u51c6\u786e\u6027\u548c\u4e2a\u6027\u5316\u751f\u6210\u8d28\u91cf\u4e0a\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf", "conclusion": "EXACT\u901a\u8fc7\u53ef\u89e3\u91ca\u5c5e\u6027\u5b9e\u73b0\u89e3\u7801\u65f6\u4e2a\u6027\u5316\uff0c\u80fd\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u800c\u4e0d\u6df7\u5408\u51b2\u7a81\u504f\u597d\uff0c\u4e3a\u4e2a\u6027\u5316\u5bf9\u9f50\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.18201", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18201", "abs": "https://arxiv.org/abs/2602.18201", "authors": ["Joseph Bingham", "Netanel Arussy", "Dvir Aran"], "title": "SOMtime the World Ain$'$t Fair: Violating Fairness Using Self-Organizing Maps", "comment": "10 pages, 2 figures, preprint", "summary": "Unsupervised representations are widely assumed to be neutral with respect to sensitive attributes when those attributes are withheld from training. We show that this assumption is false. Using SOMtime, a topology-preserving representation method based on high-capacity Self-Organizing Maps, we demonstrate that sensitive attributes such as age and income emerge as dominant latent axes in purely unsupervised embeddings, even when explicitly excluded from the input. On two large-scale real-world datasets (the World Values Survey across five countries and the Census-Income dataset), SOMtime recovers monotonic orderings aligned with withheld sensitive attributes, achieving Spearman correlations of up to 0.85, whereas PCA and UMAP typically remain below 0.23 (with a single exception reaching 0.31), and against t-SNE and autoencoders which achieve at most 0.34. Furthermore, unsupervised segmentation of SOMtime embeddings produces demographically skewed clusters, demonstrating downstream fairness risks without any supervised task. These findings establish that \\textit{fairness through unawareness} fails at the representation level for ordinal sensitive attributes and that fairness auditing must extend to unsupervised components of machine learning pipelines. We have made the code available at~ https://github.com/JosephBingham/SOMtime", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u65e0\u76d1\u7763\u8868\u793a\u5b66\u4e60\u5373\u4f7f\u6392\u9664\u654f\u611f\u5c5e\u6027\uff0c\u4ecd\u4f1a\u6cc4\u9732\u5e74\u9f84\u3001\u6536\u5165\u7b49\u654f\u611f\u4fe1\u606f\uff0c\u516c\u5e73\u6027\u901a\u8fc7\u65e0\u77e5\u5047\u8bbe\u5728\u8868\u793a\u5c42\u9762\u5931\u6548\u3002", "motivation": "\u6311\u6218\"\u516c\u5e73\u6027\u901a\u8fc7\u65e0\u77e5\"\u7684\u5047\u8bbe\uff0c\u5373\u8ba4\u4e3a\u5728\u8bad\u7ec3\u4e2d\u6392\u9664\u654f\u611f\u5c5e\u6027\u5c31\u80fd\u83b7\u5f97\u4e2d\u6027\u8868\u793a\u3002\u4f5c\u8005\u53d1\u73b0\u5373\u4f7f\u660e\u786e\u6392\u9664\u654f\u611f\u5c5e\u6027\uff0c\u65e0\u76d1\u7763\u8868\u793a\u4ecd\u4f1a\u6cc4\u9732\u8fd9\u4e9b\u4fe1\u606f\u3002", "method": "\u4f7f\u7528SOMtime\uff08\u57fa\u4e8e\u9ad8\u5bb9\u91cf\u81ea\u7ec4\u7ec7\u6620\u5c04\u7684\u62d3\u6251\u4fdd\u6301\u8868\u793a\u65b9\u6cd5\uff09\uff0c\u5728\u4e24\u4e2a\u5927\u89c4\u6a21\u771f\u5b9e\u6570\u636e\u96c6\uff08\u4e94\u4e2a\u56fd\u5bb6\u7684\u4e16\u754c\u4ef7\u503c\u89c2\u8c03\u67e5\u548c\u4eba\u53e3\u666e\u67e5\u6536\u5165\u6570\u636e\u96c6\uff09\u4e0a\u6d4b\u8bd5\uff0c\u6bd4\u8f83PCA\u3001UMAP\u3001t-SNE\u548c\u81ea\u7f16\u7801\u5668\u7b49\u65b9\u6cd5\u3002", "result": "SOMtime\u5728\u6392\u9664\u654f\u611f\u5c5e\u6027\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u6062\u590d\u4e0e\u5e74\u9f84\u3001\u6536\u5165\u7b49\u654f\u611f\u5c5e\u6027\u5bf9\u9f50\u7684\u5355\u8c03\u6392\u5e8f\uff0cSpearman\u76f8\u5173\u7cfb\u6570\u9ad8\u8fbe0.85\uff0c\u800c\u5176\u4ed6\u65b9\u6cd5\u901a\u5e38\u4f4e\u4e8e0.23\u3002\u65e0\u76d1\u7763\u5206\u5272SOMtime\u5d4c\u5165\u4f1a\u4ea7\u751f\u4eba\u53e3\u7edf\u8ba1\u5b66\u504f\u659c\u7684\u805a\u7c7b\u3002", "conclusion": "\u516c\u5e73\u6027\u901a\u8fc7\u65e0\u77e5\u5728\u8868\u793a\u5c42\u9762\u5bf9\u5e8f\u6570\u654f\u611f\u5c5e\u6027\u5931\u6548\uff0c\u516c\u5e73\u6027\u5ba1\u8ba1\u5fc5\u987b\u6269\u5c55\u5230\u673a\u5668\u5b66\u4e60\u7ba1\u9053\u7684\u65e0\u76d1\u7763\u7ec4\u4ef6\u3002\u65e0\u76d1\u7763\u8868\u793a\u5e76\u975e\u4e2d\u6027\uff0c\u4f1a\u6cc4\u9732\u654f\u611f\u4fe1\u606f\u3002"}}
{"id": "2602.17951", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17951", "abs": "https://arxiv.org/abs/2602.17951", "authors": ["Guoheng Sun", "Tingting Du", "Kaixi Feng", "Chenxiang Luo", "Xingguo Ding", "Zheyu Shen", "Ziyao Wang", "Yexiao He", "Ang Li"], "title": "ROCKET: Residual-Oriented Multi-Layer Alignment for Spatially-Aware Vision-Language-Action Models", "comment": null, "summary": "Vision-Language-Action (VLA) models enable instruction-following robotic manipulation, but they are typically pretrained on 2D data and lack 3D spatial understanding. An effective approach is representation alignment, where a strong vision foundation model is used to guide a 2D VLA model. However, existing methods usually apply supervision at only a single layer, failing to fully exploit the rich information distributed across depth; meanwhile, na\u00efve multi-layer alignment can cause gradient interference. We introduce ROCKET, a residual-oriented multi-layer representation alignment framework that formulates multi-layer alignment as aligning one residual stream to another. Concretely, ROCKET employs a shared projector to align multiple layers of the VLA backbone with multiple layers of a powerful 3D vision foundation model via a layer-invariant mapping, which reduces gradient conflicts. We provide both theoretical justification and empirical analyses showing that a shared projector is sufficient and outperforms prior designs, and further propose a Matryoshka-style sparse activation scheme for the shared projector to balance multiple alignment losses. Our experiments show that, combined with a training-free layer selection strategy, ROCKET requires only about 4% of the compute budget while achieving 98.5% state-of-the-art success rate on LIBERO. We further demonstrate the superior performance of ROCKET across LIBERO-Plus and RoboTwin, as well as multiple VLA models. The code and model weights can be found at https://github.com/CASE-Lab-UMD/ROCKET-VLA.", "AI": {"tldr": "ROCKET\u63d0\u51fa\u4e86\u4e00\u79cd\u6b8b\u5dee\u5bfc\u5411\u7684\u591a\u5c42\u8868\u793a\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u6295\u5f71\u5668\u5c06VLA\u6a21\u578b\u7684\u591a\u4e2a\u5c42\u4e0e3D\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u591a\u4e2a\u5c42\u5bf9\u9f50\uff0c\u51cf\u5c11\u68af\u5ea6\u51b2\u7a81\uff0c\u663e\u8457\u63d0\u53473D\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684Vision-Language-Action\u6a21\u578b\u901a\u5e38\u57282D\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\uff0c\u7f3a\u4e4f3D\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\u3002\u73b0\u6709\u8868\u793a\u5bf9\u9f50\u65b9\u6cd5\u901a\u5e38\u53ea\u5728\u5355\u5c42\u5e94\u7528\u76d1\u7763\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u6df1\u5ea6\u5206\u5e03\u4fe1\u606f\uff0c\u800c\u7b80\u5355\u7684\u591a\u5c42\u5bf9\u9f50\u4f1a\u5bfc\u81f4\u68af\u5ea6\u5e72\u6270\u3002", "method": "\u63d0\u51faROCKET\u6846\u67b6\uff0c\u5c06\u591a\u5c42\u5bf9\u9f50\u5efa\u6a21\u4e3a\u5c06\u4e00\u4e2a\u6b8b\u5dee\u6d41\u5bf9\u9f50\u5230\u53e6\u4e00\u4e2a\u6b8b\u5dee\u6d41\u3002\u4f7f\u7528\u5171\u4eab\u6295\u5f71\u5668\u901a\u8fc7\u5c42\u4e0d\u53d8\u6620\u5c04\u5c06VLA\u9aa8\u5e72\u7684\u591a\u4e2a\u5c42\u4e0e\u5f3a\u5927\u76843D\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u591a\u4e2a\u5c42\u5bf9\u9f50\uff0c\u51cf\u5c11\u68af\u5ea6\u51b2\u7a81\u3002\u91c7\u7528Matryoshka\u98ce\u683c\u7684\u7a00\u758f\u6fc0\u6d3b\u65b9\u6848\u5e73\u8861\u591a\u4e2a\u5bf9\u9f50\u635f\u5931\uff0c\u5e76\u7ed3\u5408\u514d\u8bad\u7ec3\u5c42\u9009\u62e9\u7b56\u7565\u3002", "result": "ROCKET\u4ec5\u9700\u7ea64%\u7684\u8ba1\u7b97\u9884\u7b97\uff0c\u5728LIBERO\u4e0a\u8fbe\u523098.5%\u7684\u6700\u5148\u8fdb\u6210\u529f\u7387\u3002\u5728LIBERO-Plus\u548cRoboTwin\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u79cdVLA\u6a21\u578b\u3002", "conclusion": "ROCKET\u901a\u8fc7\u6b8b\u5dee\u5bfc\u5411\u7684\u591a\u5c42\u8868\u793a\u5bf9\u9f50\u6709\u6548\u63d0\u5347\u4e86VLA\u6a21\u578b\u76843D\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\uff0c\u8ba1\u7b97\u6548\u7387\u9ad8\u4e14\u6027\u80fd\u4f18\u8d8a\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.18291", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18291", "abs": "https://arxiv.org/abs/2602.18291", "authors": ["Zhuoran Li", "Hai Zhong", "Xun Wang", "Qingxin Xia", "Lihua Zhang", "Longbo Huang"], "title": "Diffusing to Coordinate: Efficient Online Multi-Agent Diffusion Policies", "comment": null, "summary": "Online Multi-Agent Reinforcement Learning (MARL) is a prominent framework for efficient agent coordination. Crucially, enhancing policy expressiveness is pivotal for achieving superior performance. Diffusion-based generative models are well-positioned to meet this demand, having demonstrated remarkable expressiveness and multimodal representation in image generation and offline settings. Yet, their potential in online MARL remains largely under-explored. A major obstacle is that the intractable likelihoods of diffusion models impede entropy-based exploration and coordination. To tackle this challenge, we propose among the first \\underline{O}nline off-policy \\underline{MA}RL framework using \\underline{D}iffusion policies (\\textbf{OMAD}) to orchestrate coordination. Our key innovation is a relaxed policy objective that maximizes scaled joint entropy, facilitating effective exploration without relying on tractable likelihood. Complementing this, within the centralized training with decentralized execution (CTDE) paradigm, we employ a joint distributional value function to optimize decentralized diffusion policies. It leverages tractable entropy-augmented targets to guide the simultaneous updates of diffusion policies, thereby ensuring stable coordination. Extensive evaluations on MPE and MAMuJoCo establish our method as the new state-of-the-art across $10$ diverse tasks, demonstrating a remarkable $2.5\\times$ to $5\\times$ improvement in sample efficiency.", "AI": {"tldr": "OMAD\uff1a\u9996\u4e2a\u5728\u7ebf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u91c7\u7528\u6269\u6563\u7b56\u7565\u534f\u8c03\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u677e\u5f1b\u7b56\u7565\u76ee\u6807\u6700\u5927\u5316\u8054\u5408\u71b5\u5b9e\u73b0\u9ad8\u6548\u63a2\u7d22\uff0c\u65e0\u9700\u53ef\u5904\u7406\u4f3c\u7136\u51fd\u6570", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u751f\u6210\u548c\u79bb\u7ebf\u8bbe\u7f6e\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u8868\u8fbe\u80fd\u529b\u548c\u591a\u6a21\u6001\u8868\u793a\u80fd\u529b\uff0c\u4f46\u5728\u5728\u7ebf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u4e3b\u8981\u969c\u788d\u662f\u6269\u6563\u6a21\u578b\u7684\u4e0d\u53ef\u5904\u7406\u4f3c\u7136\u51fd\u6570\u963b\u788d\u4e86\u57fa\u4e8e\u71b5\u7684\u63a2\u7d22\u548c\u534f\u8c03\u3002", "method": "\u63d0\u51faOMAD\u6846\u67b6\uff1a1\uff09\u677e\u5f1b\u7b56\u7565\u76ee\u6807\u6700\u5927\u5316\u7f29\u653e\u8054\u5408\u71b5\uff0c\u5b9e\u73b0\u6709\u6548\u63a2\u7d22\uff1b2\uff09\u5728\u96c6\u4e2d\u8bad\u7ec3\u5206\u6563\u6267\u884c\u8303\u5f0f\u4e0b\uff0c\u4f7f\u7528\u8054\u5408\u5206\u5e03\u503c\u51fd\u6570\u4f18\u5316\u5206\u6563\u6269\u6563\u7b56\u7565\uff1b3\uff09\u5229\u7528\u53ef\u5904\u7406\u7684\u71b5\u589e\u5f3a\u76ee\u6807\u6307\u5bfc\u6269\u6563\u7b56\u7565\u540c\u65f6\u66f4\u65b0\uff0c\u786e\u4fdd\u7a33\u5b9a\u534f\u8c03\u3002", "result": "\u5728MPE\u548cMAMuJoCo\u768410\u4e2a\u591a\u6837\u5316\u4efb\u52a1\u4e0a\u5efa\u7acb\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u6837\u672c\u6548\u7387\u663e\u8457\u63d0\u9ad82.5\u500d\u52305\u500d\u3002", "conclusion": "OMAD\u6210\u529f\u5c06\u6269\u6563\u7b56\u7565\u5e94\u7528\u4e8e\u5728\u7ebf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u677e\u5f1b\u7b56\u7565\u76ee\u6807\u548c\u8054\u5408\u5206\u5e03\u503c\u51fd\u6570\u4f18\u5316\uff0c\u514b\u670d\u4e86\u6269\u6563\u6a21\u578b\u4f3c\u7136\u4e0d\u53ef\u5904\u7406\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u534f\u8c03\u6027\u80fd\u548c\u6837\u672c\u6548\u7387\u3002"}}
{"id": "2602.17688", "categories": ["cs.LG", "cs.PL"], "pdf": "https://arxiv.org/pdf/2602.17688", "abs": "https://arxiv.org/abs/2602.17688", "authors": ["Anton Xue", "Litu Rout", "Constantine Caramanis", "Sanjay Shakkottai"], "title": "AnCoder: Anchored Code Generation via Discrete Diffusion Models", "comment": null, "summary": "Diffusion language models offer a compelling alternative to autoregressive code generation, enabling global planning and iterative refinement of complex program logic. However, existing approaches fail to respect the rigid structure of programming languages and, as a result, often produce broken programs that fail to execute. To address this, we introduce AnchorTree, a framework that explicitly anchors the diffusion process using structured, hierarchical priors native to code. Specifically, AnchorTree uses the abstract syntax tree to prioritize resolving syntactically and semantically salient tokens, such as keywords (e.g., if, while) and identifiers (e.g., variable names), thereby establishing a structural scaffold that guides the remaining generation. We validate this framework via AnCoder, a family of models showing that structurally anchored diffusion offers a parameter-efficient path to high-quality code generation.", "AI": {"tldr": "AnchorTree\u6846\u67b6\u901a\u8fc7\u62bd\u8c61\u8bed\u6cd5\u6811\u951a\u5b9a\u6269\u6563\u8fc7\u7a0b\uff0c\u4f18\u5148\u89e3\u6790\u8bed\u6cd5\u548c\u8bed\u4e49\u5173\u952e\u6807\u8bb0\uff0c\u63d0\u5347\u4ee3\u7801\u751f\u6210\u8d28\u91cf", "motivation": "\u73b0\u6709\u6269\u6563\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7ecf\u5e38\u4ea7\u751f\u65e0\u6cd5\u6267\u884c\u7684\u7a0b\u5e8f\uff0c\u56e0\u4e3a\u5b83\u4eec\u672a\u80fd\u5c0a\u91cd\u7f16\u7a0b\u8bed\u8a00\u7684\u521a\u6027\u7ed3\u6784", "method": "\u5f15\u5165AnchorTree\u6846\u67b6\uff0c\u5229\u7528\u62bd\u8c61\u8bed\u6cd5\u6811\u4f5c\u4e3a\u7ed3\u6784\u5316\u5c42\u6b21\u5148\u9a8c\uff0c\u4f18\u5148\u89e3\u6790\u5173\u952e\u8bcd\u548c\u6807\u8bc6\u7b26\u7b49\u5173\u952e\u6807\u8bb0\uff0c\u5efa\u7acb\u7ed3\u6784\u811a\u624b\u67b6\u6307\u5bfc\u751f\u6210", "result": "\u901a\u8fc7AnCoder\u6a21\u578b\u9a8c\u8bc1\uff0c\u7ed3\u6784\u951a\u5b9a\u7684\u6269\u6563\u65b9\u6cd5\u4e3a\u9ad8\u8d28\u91cf\u4ee3\u7801\u751f\u6210\u63d0\u4f9b\u4e86\u53c2\u6570\u9ad8\u6548\u7684\u8def\u5f84", "conclusion": "AnchorTree\u6846\u67b6\u901a\u8fc7\u663e\u5f0f\u951a\u5b9a\u6269\u6563\u8fc7\u7a0b\u4e8e\u4ee3\u7801\u539f\u751f\u7ed3\u6784\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u7ed3\u6784\u4e0d\u5339\u914d\u95ee\u9898"}}
{"id": "2602.18036", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2602.18036", "abs": "https://arxiv.org/abs/2602.18036", "authors": ["Ankit Singh", "Vidhi Thakur", "Nachiket Tapas"], "title": "Atrial Fibrillation Detection Using Machine Learning", "comment": null, "summary": "Atrial fibrillation (AF) is a common cardiac arrhythmia and a major risk factor for ischemic stroke. Early detection of AF using non-invasive signals can enable timely intervention. In this work, we present a comprehensive machine learning framework for AF detection from simultaneous photoplethysmogram (PPG) and electrocardiogram (ECG) signals. We partitioned continuous recordings from 35 subjects into 525 segments (15 segments of 10,000 samples each at 125Hz per subject). After data cleaning to remove segments with missing samples, 481 segments remained (263 AF, 218 normal).\n  We extracted 22 features per segment, including time-domain statistics (mean, standard deviation, skewness, etc.), bandpower, and heart-rate variability metrics from both PPG and ECG signals. Three classifiers -- ensemble of bagged decision trees, cubic-kernel support vector machine (SVM), and subspace k-nearest neighbors (KNN) -- were trained and evaluated using 10-fold cross-validation and hold-out testing. The subspace KNN achieved the highest test accuracy (98.7\\%), slightly outperforming bagged trees (97.9\\%) and cubic SVM (97.1\\%). Sensitivity (AF detection) and specificity (normal rhythm detection) were all above 95\\% for the top-performing models.\n  The results indicate that ensemble-based machine learning models using combined PPG and ECG features can effectively detect atrial fibrillation. A comparative analysis of model performance along with strengths and limitations of the proposed framework is presented.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u623f\u98a4\u68c0\u6d4b\u6846\u67b6\uff0c\u4f7f\u7528PPG\u548cECG\u4fe1\u53f7\uff0c\u901a\u8fc7\u7279\u5f81\u63d0\u53d6\u548c\u591a\u79cd\u5206\u7c7b\u5668\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u68c0\u6d4b\u3002", "motivation": "\u623f\u98a4\u662f\u5e38\u89c1\u7684\u5fc3\u5f8b\u5931\u5e38\uff0c\u4e5f\u662f\u7f3a\u8840\u6027\u4e2d\u98ce\u7684\u4e3b\u8981\u98ce\u9669\u56e0\u7d20\u3002\u65e9\u671f\u901a\u8fc7\u975e\u4fb5\u5165\u6027\u4fe1\u53f7\u68c0\u6d4b\u623f\u98a4\u53ef\u4ee5\u5b9e\u73b0\u53ca\u65f6\u5e72\u9884\u3002", "method": "1. \u6570\u636e\u91c7\u96c6\uff1a35\u540d\u53d7\u8bd5\u8005\u7684\u8fde\u7eed\u8bb0\u5f55\uff0c\u5206\u5272\u6210525\u4e2a\u7247\u6bb5\uff08\u6bcf\u4e2a\u7247\u6bb510,000\u4e2a\u6837\u672c\uff0c\u91c7\u6837\u7387125Hz\uff09\n2. \u6570\u636e\u6e05\u6d17\uff1a\u53bb\u9664\u7f3a\u5931\u6837\u672c\u7684\u7247\u6bb5\uff0c\u4fdd\u7559481\u4e2a\u7247\u6bb5\uff08263\u4e2a\u623f\u98a4\uff0c218\u4e2a\u6b63\u5e38\uff09\n3. \u7279\u5f81\u63d0\u53d6\uff1a\u6bcf\u4e2a\u7247\u6bb5\u63d0\u53d622\u4e2a\u7279\u5f81\uff0c\u5305\u62ec\u65f6\u57df\u7edf\u8ba1\u7279\u5f81\u3001\u9891\u5e26\u529f\u7387\u548c\u5fc3\u7387\u53d8\u5f02\u6027\u6307\u6807\n4. \u5206\u7c7b\u5668\uff1a\u96c6\u6210\u888b\u88c5\u51b3\u7b56\u6811\u3001\u7acb\u65b9\u6838SVM\u548c\u5b50\u7a7a\u95f4KNN\n5. \u8bc4\u4f30\uff1a10\u6298\u4ea4\u53c9\u9a8c\u8bc1\u548c\u4fdd\u7559\u6d4b\u8bd5", "result": "\u5b50\u7a7a\u95f4KNN\u83b7\u5f97\u6700\u9ad8\u6d4b\u8bd5\u51c6\u786e\u738798.7%\uff0c\u7565\u4f18\u4e8e\u888b\u88c5\u51b3\u7b56\u6811\uff0897.9%\uff09\u548c\u7acb\u65b9\u6838SVM\uff0897.1%\uff09\u3002\u6700\u4f73\u6a21\u578b\u7684\u654f\u611f\u6027\u548c\u7279\u5f02\u6027\u5747\u8d85\u8fc795%\u3002", "conclusion": "\u57fa\u4e8e\u96c6\u6210\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7ed3\u5408PPG\u548cECG\u7279\u5f81\u53ef\u4ee5\u6709\u6548\u68c0\u6d4b\u623f\u98a4\u3002\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u6a21\u578b\u6027\u80fd\u7684\u6bd4\u8f83\u5206\u6790\u4ee5\u53ca\u6240\u63d0\u6846\u67b6\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002"}}
{"id": "2602.18000", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18000", "abs": "https://arxiv.org/abs/2602.18000", "authors": ["Xuting Lan", "Mingliang Zhou", "Xuekai Wei", "Jielu Yan", "Yueting Huang", "Huayan Pu", "Jun Luo", "Weijia Jia"], "title": "Image Quality Assessment: Exploring Quality Awareness via Memory-driven Distortion Patterns Matching", "comment": null, "summary": "Existing full-reference image quality assessment (FR-IQA) methods achieve high-precision evaluation by analysing feature differences between reference and distorted images. However, their performance is constrained by the quality of the reference image, which limits real-world applications where ideal reference sources are unavailable. Notably, the human visual system has the ability to accumulate visual memory, allowing image quality assessment on the basis of long-term memory storage. Inspired by this biological memory mechanism, we propose a memory-driven quality-aware framework (MQAF), which establishes a memory bank for storing distortion patterns and dynamically switches between dual-mode quality assessment strategies to reduce reliance on high-quality reference images. When reference images are available, MQAF obtains reference-guided quality scores by adaptively weighting reference information and comparing the distorted image with stored distortion patterns in the memory bank. When the reference image is absent, the framework relies on distortion patterns in the memory bank to infer image quality, enabling no-reference quality assessment (NR-IQA). The experimental results show that our method outperforms state-of-the-art approaches across multiple datasets while adapting to both no-reference and full-reference tasks.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u8bb0\u5fc6\u9a71\u52a8\u7684\u8d28\u91cf\u611f\u77e5\u6846\u67b6\uff08MQAF\uff09\uff0c\u901a\u8fc7\u5efa\u7acb\u5b58\u50a8\u5931\u771f\u6a21\u5f0f\u7684\u8bb0\u5fc6\u5e93\uff0c\u5728\u6709\u65e0\u53c2\u8003\u56fe\u50cf\u65f6\u52a8\u6001\u5207\u6362\u53cc\u6a21\u5f0f\u8d28\u91cf\u8bc4\u4f30\u7b56\u7565\uff0c\u51cf\u5c11\u5bf9\u9ad8\u8d28\u91cf\u53c2\u8003\u56fe\u50cf\u7684\u4f9d\u8d56\u3002", "motivation": "\u73b0\u6709\u5168\u53c2\u8003\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u53c2\u8003\u56fe\u50cf\u8d28\u91cf\uff0c\u9650\u5236\u4e86\u5728\u7406\u60f3\u53c2\u8003\u6e90\u4e0d\u53ef\u7528\u7684\u5b9e\u9645\u5e94\u7528\u3002\u53d7\u4eba\u7c7b\u89c6\u89c9\u7cfb\u7edf\u79ef\u7d2f\u89c6\u89c9\u8bb0\u5fc6\u80fd\u529b\u7684\u542f\u53d1\uff0c\u5e0c\u671b\u5f00\u53d1\u80fd\u57fa\u4e8e\u957f\u671f\u8bb0\u5fc6\u5b58\u50a8\u8fdb\u884c\u8d28\u91cf\u8bc4\u4f30\u7684\u65b9\u6cd5\u3002", "method": "\u5efa\u7acb\u5b58\u50a8\u5931\u771f\u6a21\u5f0f\u7684\u8bb0\u5fc6\u5e93\uff0c\u8bbe\u8ba1\u53cc\u6a21\u5f0f\u8d28\u91cf\u8bc4\u4f30\u7b56\u7565\uff1a\u6709\u53c2\u8003\u56fe\u50cf\u65f6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u52a0\u6743\u53c2\u8003\u4fe1\u606f\u5e76\u5c06\u5931\u771f\u56fe\u50cf\u4e0e\u8bb0\u5fc6\u5e93\u4e2d\u7684\u5931\u771f\u6a21\u5f0f\u6bd4\u8f83\u83b7\u5f97\u8d28\u91cf\u5206\u6570\uff1b\u65e0\u53c2\u8003\u56fe\u50cf\u65f6\uff0c\u4f9d\u8d56\u8bb0\u5fc6\u5e93\u4e2d\u7684\u5931\u771f\u6a21\u5f0f\u63a8\u65ad\u56fe\u50cf\u8d28\u91cf\uff0c\u5b9e\u73b0\u65e0\u53c2\u8003\u8d28\u91cf\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u540c\u65f6\u80fd\u9002\u5e94\u65e0\u53c2\u8003\u548c\u5168\u53c2\u8003\u4e24\u79cd\u4efb\u52a1\u3002", "conclusion": "\u63d0\u51fa\u7684\u8bb0\u5fc6\u9a71\u52a8\u8d28\u91cf\u611f\u77e5\u6846\u67b6\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u89c6\u89c9\u8bb0\u5fc6\u673a\u5236\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u5bf9\u9ad8\u8d28\u91cf\u53c2\u8003\u56fe\u50cf\u7684\u4f9d\u8d56\uff0c\u5728\u6709\u65e0\u53c2\u8003\u56fe\u50cf\u7684\u60c5\u51b5\u4e0b\u90fd\u80fd\u5b9e\u73b0\u9ad8\u8d28\u91cf\u8bc4\u4f30\uff0c\u5177\u6709\u66f4\u597d\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.17689", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2602.17689", "abs": "https://arxiv.org/abs/2602.17689", "authors": ["Melika Filvantorkaman", "Mohsen Piri"], "title": "Robust Pre-Training of Medical Vision-and-Language Models with Domain-Invariant Multi-Modal Masked Reconstruction", "comment": "28 pages, 3 figures", "summary": "Medical vision-language models show strong potential for joint reasoning over medical images and clinical text, but their performance often degrades under domain shift caused by variations in imaging devices, acquisition protocols, and reporting styles. Existing multi-modal pre-training methods largely overlook robustness, treating it as a downstream adaptation problem. In this work, we propose Robust Multi-Modal Masked Reconstruction (Robust-MMR), a self-supervised pre-training framework that explicitly incorporates robustness objectives into masked vision-language learning. Robust-MMR integrates asymmetric perturbation-aware masking, domain-consistency regularization, and modality-resilience constraints to encourage domain-invariant representations. We evaluate Robust-MMR on multiple medical vision-language benchmarks, including medical visual question answering (VQA-RAD, SLAKE, VQA-2019), cross-domain image-text classification (MELINDA), and robust image-caption retrieval (ROCO). Robust-MMR achieves 78.9% cross-domain accuracy on VQA-RAD, outperforming the strongest baseline by 3.8 percentage points, and reaches 74.6% and 77.0% accuracy on SLAKE and VQA-2019, respectively. Under perturbed evaluation, Robust-MMR improves VQA-RAD accuracy from 69.1% to 75.6%. For image-text classification, cross-domain MELINDA accuracy increases from 70.3% to 75.2%, while retrieval experiments show a reduction in mean rank degradation from over 16 to 4.1 under perturbation. Qualitative results further demonstrate improved clinical reasoning for disease detection and structural abnormality assessment. These findings show that explicitly modeling robustness during pre-training leads to more reliable and transferable medical vision-language representations for real-world deployment.", "AI": {"tldr": "\u63d0\u51faRobust-MMR\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u589e\u5f3a\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u9886\u57df\u504f\u79fb\u4e0b\u7684\u9c81\u68d2\u6027", "motivation": "\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6210\u50cf\u8bbe\u5907\u3001\u91c7\u96c6\u534f\u8bae\u548c\u62a5\u544a\u98ce\u683c\u53d8\u5316\u5bfc\u81f4\u7684\u9886\u57df\u504f\u79fb\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u73b0\u6709\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u65b9\u6cd5\u5927\u591a\u5ffd\u89c6\u9c81\u68d2\u6027\uff0c\u5c06\u5176\u89c6\u4e3a\u4e0b\u6e38\u9002\u5e94\u95ee\u9898", "method": "\u63d0\u51faRobust Multi-Modal Masked Reconstruction (Robust-MMR)\u6846\u67b6\uff0c\u6574\u5408\u975e\u5bf9\u79f0\u6270\u52a8\u611f\u77e5\u63a9\u7801\u3001\u9886\u57df\u4e00\u81f4\u6027\u6b63\u5219\u5316\u548c\u6a21\u6001\u5f39\u6027\u7ea6\u675f\uff0c\u9f13\u52b1\u9886\u57df\u4e0d\u53d8\u8868\u793a", "result": "\u5728VQA-RAD\u4e0a\u8fbe\u523078.9%\u8de8\u57df\u51c6\u786e\u7387\uff0c\u6bd4\u6700\u5f3a\u57fa\u7ebf\u9ad83.8\u4e2a\u767e\u5206\u70b9\uff1b\u5728SLAKE\u548cVQA-2019\u4e0a\u5206\u522b\u8fbe\u523074.6%\u548c77.0%\uff1b\u6270\u52a8\u8bc4\u4f30\u4e0bVQA-RAD\u51c6\u786e\u7387\u4ece69.1%\u63d0\u5347\u81f375.6%\uff1bMELINDA\u8de8\u57df\u51c6\u786e\u7387\u4ece70.3%\u63d0\u5347\u81f375.2%\uff1b\u68c0\u7d22\u5b9e\u9a8c\u4e2d\u5e73\u5747\u6392\u540d\u9000\u5316\u4ece\u8d85\u8fc716\u964d\u81f34.1", "conclusion": "\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u663e\u5f0f\u5efa\u6a21\u9c81\u68d2\u6027\u80fd\u591f\u4ea7\u751f\u66f4\u53ef\u9760\u3001\u53ef\u8fc1\u79fb\u7684\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u8868\u793a\uff0c\u9002\u7528\u4e8e\u771f\u5b9e\u4e16\u754c\u90e8\u7f72"}}
{"id": "2602.18139", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2602.18139", "abs": "https://arxiv.org/abs/2602.18139", "authors": ["L. C. R. Patell", "O. E. Guest"], "title": "Demonstrating Restraint", "comment": null, "summary": "Some have claimed that the future development of powerful AI systems would enable the United States to shift the international balance of power dramatically in its favor. Such a feat may not be technically possible; even so, if American AI development is perceived as a sufficiently severe threat by its nation-state adversaries, then the risk that they take extreme preventive action against the United States may rise. To bolster its security against preventive action, the United States could aim to pursue a strategy of restraint by demonstrating that it would not use powerful AI to threaten the survival of other nations. Drawing from the international relations literature that explores how states can make credible commitments, we sketch a set of options that the United States could employ to implement this strategy. In the most challenging setting, where it is certain that the US will unilaterally obtain powerful new capabilities, it is difficult to credibly commit to restraint, though an approach that layers significant policy effort with technical breakthroughs may make credibility achievable. If an adversary has realistic levels of uncertainty about the capabilities and intentions of the United States, a strategy of restraint becomes more feasible. Though restraint faces difficulties, it deserves to be weighed against alternative strategies that have been proposed for avoiding conflict during the transition to a world with advanced AI.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u7f8e\u56fd\u5982\u4f55\u901a\u8fc7\u514b\u5236\u7b56\u7565\u907f\u514d\u56e0AI\u53d1\u5c55\u5f15\u53d1\u4ed6\u56fd\u9884\u9632\u6027\u653b\u51fb\uff0c\u5206\u6790\u5728AI\u80fd\u529b\u4f18\u52bf\u786e\u5b9a\u548c\u4e0d\u786e\u5b9a\u4e24\u79cd\u60c5\u51b5\u4e0b\u514b\u5236\u7b56\u7565\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u7f8e\u56fdAI\u5feb\u901f\u53d1\u5c55\u53ef\u80fd\u88ab\u654c\u5bf9\u56fd\u5bb6\u89c6\u4e3a\u751f\u5b58\u5a01\u80c1\uff0c\u4ece\u800c\u5f15\u53d1\u9884\u9632\u6027\u653b\u51fb\u3002\u4e3a\u907f\u514d\u8fd9\u79cd\u51b2\u7a81\uff0c\u9700\u8981\u63a2\u8ba8\u7f8e\u56fd\u5982\u4f55\u901a\u8fc7\u514b\u5236\u7b56\u7565\u6765\u589e\u5f3a\u81ea\u8eab\u5b89\u5168\u3002", "method": "\u501f\u9274\u56fd\u9645\u5173\u7cfb\u5b66\u4e2d\u5173\u4e8e\u56fd\u5bb6\u53ef\u4fe1\u627f\u8bfa\u7684\u6587\u732e\uff0c\u63d0\u51fa\u7f8e\u56fd\u5b9e\u65bd\u514b\u5236\u7b56\u7565\u7684\u4e00\u7cfb\u5217\u9009\u9879\uff0c\u5206\u6790\u5728AI\u80fd\u529b\u786e\u5b9a\u548c\u4e0d\u786e\u5b9a\u4e24\u79cd\u60c5\u5883\u4e0b\u7684\u53ef\u884c\u6027\u3002", "result": "\u5728AI\u80fd\u529b\u786e\u5b9a\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u4fe1\u514b\u5236\u627f\u8bfa\u56f0\u96be\uff0c\u4f46\u653f\u7b56\u52aa\u529b\u4e0e\u6280\u672f\u7a81\u7834\u7ed3\u5408\u53ef\u80fd\u5b9e\u73b0\uff1b\u5728\u80fd\u529b\u4e0d\u786e\u5b9a\u7684\u60c5\u51b5\u4e0b\uff0c\u514b\u5236\u7b56\u7565\u66f4\u53ef\u884c\u3002", "conclusion": "\u514b\u5236\u7b56\u7565\u867d\u7136\u9762\u4e34\u56f0\u96be\uff0c\u4f46\u503c\u5f97\u4e0e\u907f\u514dAI\u8fc7\u6e21\u671f\u51b2\u7a81\u7684\u5176\u4ed6\u7b56\u7565\u8fdb\u884c\u6bd4\u8f83\u6743\u8861\u3002"}}
{"id": "2602.18006", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18006", "abs": "https://arxiv.org/abs/2602.18006", "authors": ["Ahsan Baidar Bakht", "Mohamad Alansari", "Muhayy Ud Din", "Muzammal Naseer", "Sajid Javed", "Irfan Hussain", "Jiri Matas", "Arif Mahmood"], "title": "MUOT_3M: A 3 Million Frame Multimodal Underwater Benchmark and the MUTrack Tracking Method", "comment": null, "summary": "Underwater Object Tracking (UOT) is crucial for efficient marine robotics, large scale ecological monitoring, and ocean exploration; however, progress has been hindered by the scarcity of large, multimodal, and diverse datasets. Existing benchmarks remain small and RGB only, limiting robustness under severe color distortion, turbidity, and low visibility conditions. We introduce MUOT_3M, the first pseudo multimodal UOT benchmark comprising 3 million frames from 3,030 videos (27.8h) annotated with 32 tracking attributes, 677 fine grained classes, and synchronized RGB, estimated enhanced RGB, estimated depth, and language modalities validated by a marine biologist. Building upon MUOT_3M, we propose MUTrack, a SAM-based multimodal to unimodal tracker featuring visual geometric alignment, vision language fusion, and four level knowledge distillation that transfers multimodal knowledge into a unimodal student model. Extensive evaluations across five UOT benchmarks demonstrate that MUTrack achieves up to 8.40% higher AUC and 7.80% higher precision than the strongest SOTA baselines while running at 24 FPS. MUOT_3M and MUTrack establish a new foundation for scalable, multimodally trained yet practically deployable underwater tracking.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u4f2a\u591a\u6a21\u6001\u6c34\u4e0b\u76ee\u6807\u8ddf\u8e2a\u57fa\u51c6MUOT_3M\uff08300\u4e07\u5e27\uff09\u548c\u57fa\u4e8eSAM\u7684\u591a\u6a21\u6001\u5230\u5355\u6a21\u6001\u8ddf\u8e2a\u5668MUTrack\uff0c\u901a\u8fc7\u89c6\u89c9\u51e0\u4f55\u5bf9\u9f50\u3001\u89c6\u89c9\u8bed\u8a00\u878d\u5408\u548c\u56db\u7ea7\u77e5\u8bc6\u84b8\u998f\uff0c\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\u3002", "motivation": "\u6c34\u4e0b\u76ee\u6807\u8ddf\u8e2a\u5bf9\u6d77\u6d0b\u673a\u5668\u4eba\u3001\u751f\u6001\u76d1\u6d4b\u548c\u6d77\u6d0b\u52d8\u63a2\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u6570\u636e\u96c6\u89c4\u6a21\u5c0f\u4e14\u4ec5\u652f\u6301RGB\u6a21\u6001\uff0c\u9650\u5236\u4e86\u5728\u4e25\u91cd\u989c\u8272\u5931\u771f\u3001\u6d51\u6d4a\u548c\u4f4e\u80fd\u89c1\u5ea6\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "method": "1. \u6784\u5efaMUOT_3M\u57fa\u51c6\uff1a\u5305\u542b300\u4e07\u5e27\u30013030\u4e2a\u89c6\u9891\uff0827.8\u5c0f\u65f6\uff09\uff0c\u6807\u6ce832\u4e2a\u8ddf\u8e2a\u5c5e\u6027\u3001677\u4e2a\u7ec6\u7c92\u5ea6\u7c7b\u522b\uff0c\u540c\u6b65\u63d0\u4f9bRGB\u3001\u589e\u5f3aRGB\u3001\u4f30\u8ba1\u6df1\u5ea6\u548c\u8bed\u8a00\u6a21\u6001\uff1b2. \u63d0\u51faMUTrack\u8ddf\u8e2a\u5668\uff1a\u57fa\u4e8eSAM\u67b6\u6784\uff0c\u91c7\u7528\u89c6\u89c9\u51e0\u4f55\u5bf9\u9f50\u3001\u89c6\u89c9\u8bed\u8a00\u878d\u5408\u548c\u56db\u7ea7\u77e5\u8bc6\u84b8\u998f\uff0c\u5c06\u591a\u6a21\u6001\u77e5\u8bc6\u8f6c\u79fb\u5230\u5355\u6a21\u6001\u5b66\u751f\u6a21\u578b\u4e2d\u3002", "result": "\u5728\u4e94\u4e2a\u6c34\u4e0b\u76ee\u6807\u8ddf\u8e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMUTrack\u6bd4\u6700\u5f3a\u7684SOTA\u57fa\u7ebf\u9ad8\u51fa8.40%\u7684AUC\u548c7.80%\u7684\u7cbe\u5ea6\uff0c\u540c\u65f6\u4ee524 FPS\u5b9e\u65f6\u8fd0\u884c\u3002MUOT_3M\u57fa\u51c6\u7ecf\u8fc7\u6d77\u6d0b\u751f\u7269\u5b66\u5bb6\u9a8c\u8bc1\u3002", "conclusion": "MUOT_3M\u548cMUTrack\u4e3a\u53ef\u6269\u5c55\u3001\u591a\u6a21\u6001\u8bad\u7ec3\u4f46\u5b9e\u9645\u53ef\u90e8\u7f72\u7684\u6c34\u4e0b\u8ddf\u8e2a\u5efa\u7acb\u4e86\u65b0\u7684\u57fa\u7840\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u89c4\u6a21\u5c0f\u3001\u6a21\u6001\u5355\u4e00\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6c34\u4e0b\u8ddf\u8e2a\u6027\u80fd\u3002"}}
{"id": "2602.18189", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2602.18189", "abs": "https://arxiv.org/abs/2602.18189", "authors": ["Dejan Grba"], "title": "Computer Vision in Tactical AI Art", "comment": "18 pages", "summary": "AI art comprises a spectrum of creative endeavors that emerge from and respond to the development of artificial intelligence (AI), the expansion of AI-powered economies, and their influence on culture and society. Within this repertoire, the relationship between the cognitive value of human vision and the wide application range of computer vision (CV) technologies opens a sizeable space for exploring the problematic sociopolitical aspects of automated inference and decision-making in modern AI. In this paper, I examine the art practices critically engaged with the notions and protocols of CV. After identifying and contextualizing the CV-related tactical AI art, I discuss the features of exemplar artworks in four interrelated subject areas. Their topical imbrications, common critical points, and shared pitfalls plot a wider landscape of tactical AI art, allowing me to detect factors that affect its poetic cogency, social responsibility, and political impact, some of which exist in the theoretical premises of digital art activism. Along these lines, I outline the routes for addressing the challenges and advancing the field.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86AI\u827a\u672f\u5982\u4f55\u6279\u5224\u6027\u5730\u56de\u5e94\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u7684\u793e\u4f1a\u653f\u6cbb\u5f71\u54cd\uff0c\u5206\u6790\u4e86\u6218\u672f\u6027AI\u827a\u672f\u7684\u7279\u5f81\u3001\u6311\u6218\u548c\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "AI\u827a\u672f\u4f5c\u4e3a\u5bf9\u4eba\u5de5\u667a\u80fd\u53d1\u5c55\u3001AI\u9a71\u52a8\u7ecf\u6d4e\u6269\u5f20\u53ca\u5176\u793e\u4f1a\u6587\u5316\u5f71\u54cd\u7684\u521b\u9020\u6027\u56de\u5e94\uff0c\u9700\u8981\u6279\u5224\u6027\u5730\u5ba1\u89c6\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u5728\u73b0\u4ee3AI\u4e2d\u7684\u81ea\u52a8\u5316\u63a8\u7406\u548c\u51b3\u7b56\u6240\u6d89\u53ca\u7684\u95ee\u9898\u6027\u793e\u4f1a\u653f\u6cbb\u65b9\u9762\u3002", "method": "\u4f5c\u8005\u9996\u5148\u8bc6\u522b\u548c\u80cc\u666f\u5316\u4e0e\u8ba1\u7b97\u673a\u89c6\u89c9\u76f8\u5173\u7684\u6218\u672f\u6027AI\u827a\u672f\uff0c\u7136\u540e\u4ece\u56db\u4e2a\u76f8\u4e92\u5173\u8054\u7684\u4e3b\u9898\u9886\u57df\u5206\u6790\u4ee3\u8868\u6027\u827a\u672f\u4f5c\u54c1\u7684\u7279\u5f81\uff0c\u901a\u8fc7\u5b83\u4eec\u7684\u4e3b\u9898\u91cd\u53e0\u3001\u5171\u540c\u6279\u5224\u70b9\u548c\u5171\u4eab\u7f3a\u9677\u6765\u63cf\u7ed8\u6218\u672f\u6027AI\u827a\u672f\u7684\u66f4\u5e7f\u6cdb\u56fe\u666f\u3002", "result": "\u901a\u8fc7\u5206\u6790\u53d1\u73b0\uff0c\u6218\u672f\u6027AI\u827a\u672f\u5728\u8bd7\u610f\u8bf4\u670d\u529b\u3001\u793e\u4f1a\u8d23\u4efb\u548c\u653f\u6cbb\u5f71\u54cd\u529b\u65b9\u9762\u53d7\u5230\u591a\u79cd\u56e0\u7d20\u5f71\u54cd\uff0c\u5176\u4e2d\u4e00\u4e9b\u56e0\u7d20\u5b58\u5728\u4e8e\u6570\u5b57\u827a\u672f\u884c\u52a8\u4e3b\u4e49\u7684\u7406\u8bba\u524d\u63d0\u4e2d\u3002", "conclusion": "\u57fa\u4e8e\u5206\u6790\u7ed3\u679c\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u5e94\u5bf9\u6311\u6218\u548c\u63a8\u8fdb\u8be5\u9886\u57df\u53d1\u5c55\u7684\u8def\u7ebf\u56fe\uff0c\u5f3a\u8c03\u9700\u8981\u89e3\u51b3\u6218\u672f\u6027AI\u827a\u672f\u5728\u6279\u5224\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u793e\u4f1a\u653f\u6cbb\u5f71\u54cd\u65f6\u7684\u6709\u6548\u6027\u548c\u5f71\u54cd\u529b\u95ee\u9898\u3002"}}
{"id": "2602.18016", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18016", "abs": "https://arxiv.org/abs/2602.18016", "authors": ["Jiamin Luo", "Xuqian Gu", "Jingjing Wang", "Jiahong Lu"], "title": "Towards LLM-centric Affective Visual Customization via Efficient and Precise Emotion Manipulating", "comment": null, "summary": "Previous studies on visual customization primarily rely on the objective alignment between various control signals (e.g., language, layout and canny) and the edited images, which largely ignore the subjective emotional contents, and more importantly lack general-purpose foundation models for affective visual customization. With this in mind, this paper proposes an LLM-centric Affective Visual Customization (L-AVC) task, which focuses on generating images within modifying their subjective emotions via Multimodal LLM. Further, this paper contends that how to make the model efficiently align emotion conversion in semantics (named inter-emotion semantic conversion) and how to precisely retain emotion-agnostic contents (named exter-emotion semantic retaining) are rather important and challenging in this L-AVC task. To this end, this paper proposes an Efficient and Precise Emotion Manipulating approach for editing subjective emotions in images. Specifically, an Efficient Inter-emotion Converting (EIC) module is tailored to make the LLM efficiently align emotion conversion in semantics before and after editing, followed by a Precise Exter-emotion Retaining (PER) module to precisely retain the emotion-agnostic contents. Comprehensive experimental evaluations on our constructed L-AVC dataset demonstrate the great advantage of the proposed EPEM approach to the L-AVC task over several state-of-the-art baselines. This justifies the importance of emotion information for L-AVC and the effectiveness of EPEM in efficiently and precisely manipulating such information.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684LLM\u4e2d\u5fc3\u60c5\u611f\u89c6\u89c9\u5b9a\u5236\u4efb\u52a1\uff0c\u65e8\u5728\u901a\u8fc7\u9ad8\u6548\u7684\u60c5\u611f\u8bed\u4e49\u8f6c\u6362\u548c\u7cbe\u786e\u7684\u60c5\u611f\u65e0\u5173\u5185\u5bb9\u4fdd\u7559\u6765\u7f16\u8f91\u56fe\u50cf\u7684\u4e3b\u89c2\u60c5\u611f\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u5b9a\u5236\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u63a7\u5236\u4fe1\u53f7\u4e0e\u7f16\u8f91\u56fe\u50cf\u7684\u5ba2\u89c2\u5bf9\u9f50\uff0c\u5ffd\u89c6\u4e86\u4e3b\u89c2\u60c5\u611f\u5185\u5bb9\uff0c\u4e14\u7f3a\u4e4f\u901a\u7528\u7684\u60c5\u611f\u89c6\u89c9\u5b9a\u5236\u57fa\u7840\u6a21\u578b\u3002\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6709\u6548\u5904\u7406\u60c5\u611f\u8f6c\u6362\u548c\u5185\u5bb9\u4fdd\u7559\u7684\u89c6\u89c9\u5b9a\u5236\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u9ad8\u6548\u7cbe\u786e\u60c5\u611f\u64cd\u7eb5\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u9ad8\u6548\u60c5\u611f\u95f4\u8f6c\u6362\u6a21\u5757\u4f7fLLM\u5728\u7f16\u8f91\u524d\u540e\u9ad8\u6548\u5bf9\u9f50\u60c5\u611f\u8bed\u4e49\u8f6c\u6362\uff1b\u7cbe\u786e\u60c5\u611f\u5916\u4fdd\u7559\u6a21\u5757\u7cbe\u786e\u4fdd\u7559\u60c5\u611f\u65e0\u5173\u5185\u5bb9\u3002", "result": "\u5728\u6784\u5efa\u7684L-AVC\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5168\u9762\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660eEPEM\u65b9\u6cd5\u5728L-AVC\u4efb\u52a1\u4e0a\u4f18\u4e8e\u591a\u4e2a\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u60c5\u611f\u4fe1\u606f\u5bf9L-AVC\u7684\u91cd\u8981\u6027\u4ee5\u53caEPEM\u65b9\u6cd5\u5728\u9ad8\u6548\u7cbe\u786e\u64cd\u7eb5\u60c5\u611f\u4fe1\u606f\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u60c5\u611f\u4fe1\u606f\u5bf9LLM\u4e2d\u5fc3\u60c5\u611f\u89c6\u89c9\u5b9a\u5236\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u63d0\u51fa\u7684EPEM\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u7cbe\u786e\u5730\u64cd\u7eb5\u56fe\u50cf\u4e2d\u7684\u60c5\u611f\u4fe1\u606f\uff0c\u4e3a\u60c5\u611f\u89c6\u89c9\u5b9a\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.18019", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18019", "abs": "https://arxiv.org/abs/2602.18019", "authors": ["Yujie Jin", "Wenxin Zhang", "Jingjing Wang", "Guodong Zhou"], "title": "DeepSVU: Towards In-depth Security-oriented Video Understanding via Unified Physical-world Regularized MoE", "comment": null, "summary": "In the literature, prior research on Security-oriented Video Understanding (SVU) has predominantly focused on detecting and localize the threats (e.g., shootings, robberies) in videos, while largely lacking the effective capability to generate and evaluate the threat causes. Motivated by these gaps, this paper introduces a new chat paradigm SVU task, i.e., In-depth Security-oriented Video Understanding (DeepSVU), which aims to not only identify and locate the threats but also attribute and evaluate the causes threatening segments. Furthermore, this paper reveals two key challenges in the proposed task: 1) how to effectively model the coarse-to-fine physical-world information (e.g., human behavior, object interactions and background context) to boost the DeepSVU task; and 2) how to adaptively trade off these factors. To tackle these challenges, this paper proposes a new Unified Physical-world Regularized MoE (UPRM) approach. Specifically, UPRM incorporates two key components: the Unified Physical-world Enhanced MoE (UPE) Block and the Physical-world Trade-off Regularizer (PTR), to address the above two challenges, respectively. Extensive experiments conduct on our DeepSVU instructions datasets (i.e., UCF-C instructions and CUVA instructions) demonstrate that UPRM outperforms several advanced Video-LLMs as well as non-VLM approaches. Such information.These justify the importance of the coarse-to-fine physical-world information in the DeepSVU task and demonstrate the effectiveness of our UPRM in capturing such information.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u6df1\u5ea6\u5b89\u5168\u5bfc\u5411\u89c6\u9891\u7406\u89e3\uff08DeepSVU\uff09\u65b0\u4efb\u52a1\uff0c\u65e8\u5728\u4e0d\u4ec5\u68c0\u6d4b\u5a01\u80c1\uff0c\u8fd8\u80fd\u5f52\u56e0\u548c\u8bc4\u4f30\u5a01\u80c1\u539f\u56e0\uff0c\u5e76\u63d0\u51fa\u4e86\u7edf\u4e00\u7269\u7406\u4e16\u754c\u6b63\u5219\u5316MoE\uff08UPRM\uff09\u65b9\u6cd5\u6765\u89e3\u51b3\u8be5\u4efb\u52a1\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u5bfc\u5411\u89c6\u9891\u7406\u89e3\uff08SVU\uff09\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u68c0\u6d4b\u548c\u5b9a\u4f4d\u89c6\u9891\u4e2d\u7684\u5a01\u80c1\uff08\u5982\u67aa\u51fb\u3001\u62a2\u52ab\uff09\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u751f\u6210\u548c\u8bc4\u4f30\u5a01\u80c1\u539f\u56e0\u7684\u80fd\u529b\u3002\u4e3a\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u6df1\u5ea6\u5b89\u5168\u5bfc\u5411\u89c6\u9891\u7406\u89e3\uff08DeepSVU\uff09\u65b0\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86\u7edf\u4e00\u7269\u7406\u4e16\u754c\u6b63\u5219\u5316MoE\uff08UPRM\uff09\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a\u7edf\u4e00\u7269\u7406\u4e16\u754c\u589e\u5f3aMoE\uff08UPE\uff09\u5757\u548c\u7269\u7406\u4e16\u754c\u6743\u8861\u6b63\u5219\u5316\u5668\uff08PTR\uff09\uff0c\u5206\u522b\u7528\u4e8e\u5efa\u6a21\u4ece\u7c97\u5230\u7ec6\u7684\u7269\u7406\u4e16\u754c\u4fe1\u606f\u548c\u81ea\u9002\u5e94\u6743\u8861\u8fd9\u4e9b\u56e0\u7d20\u3002", "result": "\u5728DeepSVU\u6307\u4ee4\u6570\u636e\u96c6\uff08UCF-C\u6307\u4ee4\u548cCUVA\u6307\u4ee4\uff09\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cUPRM\u4f18\u4e8e\u591a\u4e2a\u5148\u8fdb\u7684\u89c6\u9891-LLM\u548c\u975eVLM\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u4ece\u7c97\u5230\u7ec6\u7684\u7269\u7406\u4e16\u754c\u4fe1\u606f\u5728DeepSVU\u4efb\u52a1\u4e2d\u7684\u91cd\u8981\u6027\u4ee5\u53caUPRM\u5728\u6355\u83b7\u6b64\u7c7b\u4fe1\u606f\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684DeepSVU\u4efb\u52a1\u6269\u5c55\u4e86\u4f20\u7edfSVU\u7684\u8303\u56f4\uff0cUPRM\u65b9\u6cd5\u901a\u8fc7\u6709\u6548\u5efa\u6a21\u548c\u6743\u8861\u7269\u7406\u4e16\u754c\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6df1\u5ea6\u5b89\u5168\u89c6\u9891\u7406\u89e3\u80fd\u529b\uff0c\u4e3a\u5b89\u5168\u89c6\u9891\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.17693", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17693", "abs": "https://arxiv.org/abs/2602.17693", "authors": ["Yuchen Luo", "Fangyue Zhu", "Ruining Zhou", "Mingzhe Huang", "Jian Zhu", "Fanyu Fan", "Wei Shao"], "title": "A Case Study of Selected PTQ Baselines for Reasoning LLMs on Ascend NPU", "comment": null, "summary": "Post-Training Quantization (PTQ) is crucial for efficient model deployment, yet its effectiveness on Ascend NPU remains under-explored compared to GPU architectures. This paper presents a case study of representative PTQ baselines applied to reasoning-oriented models such as DeepSeek-R1-Distill-Qwen series (1.5B/7B/14B) and QwQ-32B. We evaluate four distinct algorithms, including AWQ, GPTQ, SmoothQuant, and FlatQuant, to cover the spectrum from weight-only compression to advanced rotation-based methods. Our empirical results reveal significant platform sensitivity. While 4-bit weight-only quantization proves viable for larger models, aggressive 4-bit weight-activation schemes suffer from layer-wise calibration instability on the NPU, leading to logic collapse in long-context reasoning tasks. Conversely, standard 8-bit quantization remains numerically stable. Furthermore, a real-world INT8 deployment demonstrates that although optimized kernels reduce latency, dynamic quantization overheads currently limit end-to-end acceleration. These findings offer a practical reference for the feasibility and limitations of deploying quantized reasoning models on Ascend NPU.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u6607\u817eNPU\u4e0a\u5bf9\u63a8\u7406\u5bfc\u5411\u6a21\u578b\u8fdb\u884c\u8bad\u7ec3\u540e\u91cf\u5316\u7684\u53ef\u884c\u6027\u548c\u5c40\u9650\u6027\uff0c\u53d1\u73b04\u4f4d\u6743\u91cd\u91cf\u5316\u5bf9\u5927\u578b\u6a21\u578b\u53ef\u884c\uff0c\u4f464\u4f4d\u6743\u91cd-\u6fc0\u6d3b\u91cf\u5316\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4efb\u52a1\u4e2d\u4e0d\u7a33\u5b9a\uff0c\u800c8\u4f4d\u91cf\u5316\u4fdd\u6301\u6570\u503c\u7a33\u5b9a\u3002", "motivation": "\u8bad\u7ec3\u540e\u91cf\u5316\uff08PTQ\uff09\u5bf9\u4e8e\u9ad8\u6548\u6a21\u578b\u90e8\u7f72\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5176\u5728\u6607\u817eNPU\u4e0a\u7684\u6709\u6548\u6027\u76f8\u6bd4GPU\u67b6\u6784\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4e3a\u5728\u6607\u817eNPU\u4e0a\u90e8\u7f72\u91cf\u5316\u63a8\u7406\u6a21\u578b\u63d0\u4f9b\u5b9e\u8df5\u53c2\u8003\u3002", "method": "\u5bf9DeepSeek-R1-Distill-Qwen\u7cfb\u5217\uff081.5B/7B/14B\uff09\u548cQwQ-32B\u7b49\u63a8\u7406\u5bfc\u5411\u6a21\u578b\u5e94\u7528\u56db\u79cd\u4ee3\u8868\u6027PTQ\u57fa\u7ebf\u7b97\u6cd5\uff1aAWQ\u3001GPTQ\u3001SmoothQuant\u548cFlatQuant\uff0c\u6db5\u76d6\u4ece\u4ec5\u6743\u91cd\u91cf\u5316\u5230\u57fa\u4e8e\u65cb\u8f6c\u7684\u9ad8\u7ea7\u65b9\u6cd5\u3002\u5728\u6607\u817eNPU\u5e73\u53f0\u4e0a\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u663e\u793a\u663e\u8457\u7684\u5e73\u53f0\u654f\u611f\u6027\uff1a4\u4f4d\u4ec5\u6743\u91cd\u91cf\u5316\u5bf9\u5927\u578b\u6a21\u578b\u53ef\u884c\uff0c\u4f46\u6fc0\u8fdb\u76844\u4f4d\u6743\u91cd-\u6fc0\u6d3b\u91cf\u5316\u65b9\u6848\u5728NPU\u4e0a\u5b58\u5728\u5c42\u95f4\u6821\u51c6\u4e0d\u7a33\u5b9a\u6027\uff0c\u5bfc\u81f4\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u903b\u8f91\u5d29\u6e83\u3002\u6807\u51c68\u4f4d\u91cf\u5316\u4fdd\u6301\u6570\u503c\u7a33\u5b9a\u3002\u5b9e\u9645INT8\u90e8\u7f72\u663e\u793a\uff0c\u5c3d\u7ba1\u4f18\u5316\u5185\u6838\u51cf\u5c11\u4e86\u5ef6\u8fdf\uff0c\u4f46\u52a8\u6001\u91cf\u5316\u5f00\u9500\u76ee\u524d\u9650\u5236\u4e86\u7aef\u5230\u7aef\u52a0\u901f\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u5728\u6607\u817eNPU\u4e0a\u90e8\u7f72\u91cf\u5316\u63a8\u7406\u6a21\u578b\u7684\u53ef\u884c\u6027\u548c\u5c40\u9650\u6027\u63d0\u4f9b\u4e86\u5b9e\u7528\u53c2\u8003\uff0c\u5f3a\u8c03\u4e86\u5e73\u53f0\u7279\u5b9a\u7684\u91cf\u5316\u7b56\u7565\u9700\u6c42\uff0c\u5e76\u6307\u51fa\u5f53\u524d\u52a8\u6001\u91cf\u5316\u5f00\u9500\u662f\u7aef\u5230\u7aef\u52a0\u901f\u7684\u4e3b\u8981\u74f6\u9888\u3002"}}
{"id": "2602.18020", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18020", "abs": "https://arxiv.org/abs/2602.18020", "authors": ["Jiabing Yang", "Yixiang Chen", "Yuan Xu", "Peiyan Li", "Xiangnan Wu", "Zichen Wen", "Bowen Fang", "Tao Yu", "Zhengbo Zhang", "Yingda Li", "Kai Wang", "Jing Liu", "Nianfeng Liu", "Yan Huang", "Liang Wang"], "title": "UAOR: Uncertainty-aware Observation Reinjection for Vision-Language-Action Models", "comment": null, "summary": "Vision-Language-Action (VLA) models leverage pretrained Vision-Language Models (VLMs) as backbones to map images and instructions to actions, demonstrating remarkable potential for generalizable robotic manipulation. To enhance performance, existing methods often incorporate extra observation cues (e.g., depth maps, point clouds) or auxiliary modules (e.g., object detectors, encoders) to enable more precise and reliable task execution, yet these typically require costly data collection and additional training. Inspired by the finding that Feed-Forward Network (FFN) in language models can act as \"key-value memory\", we propose Uncertainty-aware Observation Reinjection (UAOR), an effective, training-free and plug-and-play module for VLA models. Specifically, when the current language model layer exhibits high uncertainty, measured by Action Entropy, it reinjects key observation information into the next layer's Feed-Forward Network (FFN) through attention retrieval. This mechanism helps VLAs better attend to observations during inference, enabling more confident and faithful action generation. Comprehensive experiments show that our method consistently improves diverse VLA models across simulation and real-world tasks with minimal overhead. Notably, UAOR eliminates the need for additional observation cues or modules, making it a versatile and practical plug-in for existing VLA pipelines. The project page is at https://uaor.jiabingyang.cn.", "AI": {"tldr": "\u63d0\u51faUAOR\uff08\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u89c2\u5bdf\u91cd\u6ce8\u5165\uff09\u6a21\u5757\uff0c\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u6d4b\u91cf\u548c\u6ce8\u610f\u529b\u68c0\u7d22\u673a\u5236\uff0c\u5728VLA\u6a21\u578b\u4e2d\u91cd\u6ce8\u5165\u5173\u952e\u89c2\u5bdf\u4fe1\u606f\uff0c\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u4e3a\u63d0\u5347\u6027\u80fd\u901a\u5e38\u9700\u8981\u989d\u5916\u89c2\u5bdf\u7ebf\u7d22\uff08\u5982\u6df1\u5ea6\u56fe\u3001\u70b9\u4e91\uff09\u6216\u8f85\u52a9\u6a21\u5757\uff08\u5982\u76ee\u6807\u68c0\u6d4b\u5668\uff09\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u7684\u6570\u636e\u6536\u96c6\u548c\u989d\u5916\u8bad\u7ec3\u3002\u4f5c\u8005\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684\u6a21\u5757\u6765\u589e\u5f3aVLA\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51faUAOR\u6a21\u5757\uff1a\u5f53\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u5c42\u8868\u73b0\u51fa\u9ad8\u4e0d\u786e\u5b9a\u6027\uff08\u901a\u8fc7\u52a8\u4f5c\u71b5\u6d4b\u91cf\uff09\u65f6\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u68c0\u7d22\u673a\u5236\u5c06\u5173\u952e\u89c2\u5bdf\u4fe1\u606f\u91cd\u6ce8\u5165\u5230\u4e0b\u4e00\u5c42\u7684FFN\u4e2d\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u53ef\u76f4\u63a5\u96c6\u6210\u5230\u73b0\u6709VLA\u6a21\u578b\u4e2d\u3002", "result": "\u7efc\u5408\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6301\u7eed\u6539\u8fdb\u591a\u79cdVLA\u6a21\u578b\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\uff0c\u4e14\u5f00\u9500\u6781\u5c0f\u3002UAOR\u65e0\u9700\u989d\u5916\u89c2\u5bdf\u7ebf\u7d22\u6216\u6a21\u5757\uff0c\u6210\u4e3a\u73b0\u6709VLA\u7ba1\u9053\u7684\u901a\u7528\u5b9e\u7528\u63d2\u4ef6\u3002", "conclusion": "UAOR\u662f\u4e00\u79cd\u6709\u6548\u3001\u65e0\u9700\u8bad\u7ec3\u3001\u5373\u63d2\u5373\u7528\u7684VLA\u6a21\u578b\u589e\u5f3a\u6a21\u5757\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u89c2\u5bdf\u91cd\u6ce8\u5165\u673a\u5236\uff0c\u4f7f\u6a21\u578b\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u66f4\u597d\u5730\u5173\u6ce8\u89c2\u5bdf\u4fe1\u606f\uff0c\u751f\u6210\u66f4\u81ea\u4fe1\u548c\u53ef\u9760\u7684\u52a8\u4f5c\u3002"}}
{"id": "2602.18022", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18022", "abs": "https://arxiv.org/abs/2602.18022", "authors": ["Guandong Li", "Mengxia Ye"], "title": "Dual-Channel Attention Guidance for Training-Free Image Editing Control in Diffusion Transformers", "comment": null, "summary": "Training-free control over editing intensity is a critical requirement for diffusion-based image editing models built on the Diffusion Transformer (DiT) architecture. Existing attention manipulation methods focus exclusively on the Key space to modulate attention routing, leaving the Value space -- which governs feature aggregation -- entirely unexploited. In this paper, we first reveal that both Key and Value projections in DiT's multi-modal attention layers exhibit a pronounced bias-delta structure, where token embeddings cluster tightly around a layer-specific bias vector. Building on this observation, we propose Dual-Channel Attention Guidance (DCAG), a training-free framework that simultaneously manipulates both the Key channel (controlling where to attend) and the Value channel (controlling what to aggregate). We provide a theoretical analysis showing that the Key channel operates through the nonlinear softmax function, acting as a coarse control knob, while the Value channel operates through linear weighted summation, serving as a fine-grained complement. Together, the two-dimensional parameter space $(\u03b4_k, \u03b4_v)$ enables more precise editing-fidelity trade-offs than any single-channel method. Extensive experiments on the PIE-Bench benchmark (700 images, 10 editing categories) demonstrate that DCAG consistently outperforms Key-only guidance across all fidelity metrics, with the most significant improvements observed in localized editing tasks such as object deletion (4.9% LPIPS reduction) and object addition (3.2% LPIPS reduction).", "AI": {"tldr": "\u672c\u6587\u63d0\u51faDCAG\u6846\u67b6\uff0c\u901a\u8fc7\u540c\u65f6\u64cd\u7eb5DiT\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u7684Key\u548cValue\u901a\u9053\u5b9e\u73b0\u65e0\u8bad\u7ec3\u7f16\u8f91\u5f3a\u5ea6\u63a7\u5236\uff0c\u76f8\u6bd4\u4ec5\u4f7f\u7528Key\u901a\u9053\u7684\u65b9\u6cd5\u80fd\u66f4\u7cbe\u786e\u5730\u5e73\u8861\u7f16\u8f91\u6548\u679c\u4e0e\u4fdd\u771f\u5ea6\u3002", "motivation": "\u57fa\u4e8eDiT\u67b6\u6784\u7684\u6269\u6563\u6a21\u578b\u9700\u8981\u65e0\u8bad\u7ec3\u7f16\u8f91\u5f3a\u5ea6\u63a7\u5236\u3002\u73b0\u6709\u65b9\u6cd5\u4ec5\u5173\u6ce8Key\u7a7a\u95f4\u6765\u8c03\u8282\u6ce8\u610f\u529b\u8def\u7531\uff0c\u800c\u5b8c\u5168\u5ffd\u7565\u4e86\u63a7\u5236\u7279\u5f81\u805a\u5408\u7684Value\u7a7a\u95f4\u3002", "method": "\u63d0\u51fa\u53cc\u901a\u9053\u6ce8\u610f\u529b\u5f15\u5bfc(DCAG)\u6846\u67b6\uff0c\u540c\u65f6\u64cd\u7eb5Key\u901a\u9053\uff08\u63a7\u5236\u6ce8\u610f\u529b\u4f4d\u7f6e\uff09\u548cValue\u901a\u9053\uff08\u63a7\u5236\u7279\u5f81\u805a\u5408\uff09\u3002\u57fa\u4e8eDiT\u591a\u6a21\u6001\u6ce8\u610f\u529b\u5c42\u4e2dKey\u548cValue\u6295\u5f71\u90fd\u5448\u73b0\u660e\u663e\u7684\u504f\u5dee-\u589e\u91cf\u7ed3\u6784\u8fd9\u4e00\u89c2\u5bdf\uff0c\u901a\u8fc7\u4e8c\u7ef4\u53c2\u6570\u7a7a\u95f4(\u03b4_k, \u03b4_v)\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684\u7f16\u8f91-\u4fdd\u771f\u5ea6\u6743\u8861\u3002", "result": "\u5728PIE-Bench\u57fa\u51c6\u6d4b\u8bd5\uff08700\u5f20\u56fe\u50cf\uff0c10\u4e2a\u7f16\u8f91\u7c7b\u522b\uff09\u4e0a\uff0cDCAG\u5728\u6240\u6709\u4fdd\u771f\u5ea6\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u4ec5\u4f7f\u7528Key\u5f15\u5bfc\u7684\u65b9\u6cd5\uff0c\u5728\u5bf9\u8c61\u5220\u9664\uff08LPIPS\u964d\u4f4e4.9%\uff09\u548c\u5bf9\u8c61\u6dfb\u52a0\uff08LPIPS\u964d\u4f4e3.2%\uff09\u7b49\u5c40\u90e8\u7f16\u8f91\u4efb\u52a1\u4e2d\u6539\u8fdb\u6700\u663e\u8457\u3002", "conclusion": "\u540c\u65f6\u64cd\u7eb5Key\u548cValue\u901a\u9053\u7684DCAG\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u66f4\u7cbe\u786e\u7684\u65e0\u8bad\u7ec3\u7f16\u8f91\u63a7\u5236\uff0cKey\u901a\u9053\u901a\u8fc7\u975e\u7ebf\u6027softmax\u51fd\u6570\u63d0\u4f9b\u7c97\u7c92\u5ea6\u63a7\u5236\uff0cValue\u901a\u9053\u901a\u8fc7\u7ebf\u6027\u52a0\u6743\u6c42\u548c\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u8865\u5145\uff0c\u4e24\u8005\u7ed3\u5408\u4f18\u4e8e\u5355\u901a\u9053\u65b9\u6cd5\u3002"}}
{"id": "2602.18043", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18043", "abs": "https://arxiv.org/abs/2602.18043", "authors": ["Hongyu Qu", "Xiangbo Shu", "Rui Yan", "Hailiang Gao", "Wenguan Wang", "Jinhui Tang"], "title": "Spatio-temporal Decoupled Knowledge Compensator for Few-Shot Action Recognition", "comment": "Accepted to TPAMI 2026", "summary": "Few-Shot Action Recognition (FSAR) is a challenging task that requires recognizing novel action categories with a few labeled videos. Recent works typically apply semantically coarse category names as auxiliary contexts to guide the learning of discriminative visual features. However, such context provided by the action names is too limited to provide sufficient background knowledge for capturing novel spatial and temporal concepts in actions. In this paper, we propose DiST, an innovative Decomposition-incorporation framework for FSAR that makes use of decoupled Spatial and Temporal knowledge provided by large language models to learn expressive multi-granularity prototypes. In the decomposition stage, we decouple vanilla action names into diverse spatio-temporal attribute descriptions (action-related knowledge). Such commonsense knowledge complements semantic contexts from spatial and temporal perspectives. In the incorporation stage, we propose Spatial/Temporal Knowledge Compensators (SKC/TKC) to discover discriminative object-level and frame-level prototypes, respectively. In SKC, object-level prototypes adaptively aggregate important patch tokens under the guidance of spatial knowledge. Moreover, in TKC, frame-level prototypes utilize temporal attributes to assist in inter-frame temporal relation modeling. These learned prototypes thus provide transparency in capturing fine-grained spatial details and diverse temporal patterns. Experimental results show DiST achieves state-of-the-art results on five standard FSAR datasets.", "AI": {"tldr": "DiST\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u89e3-\u878d\u5408\u6846\u67b6\u7684\u5c11\u6837\u672c\u52a8\u4f5c\u8bc6\u522b\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u7684\u89e3\u8026\u7a7a\u95f4\u548c\u65f6\u95f4\u77e5\u8bc6\u6765\u5b66\u4e60\u591a\u7c92\u5ea6\u539f\u578b\uff0c\u5728\u4e94\u4e2a\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u4f20\u7edf\u5c11\u6837\u672c\u52a8\u4f5c\u8bc6\u522b\u65b9\u6cd5\u901a\u5e38\u4f7f\u7528\u8bed\u4e49\u7c97\u7cd9\u7684\u7c7b\u522b\u540d\u79f0\u4f5c\u4e3a\u8f85\u52a9\u4e0a\u4e0b\u6587\uff0c\u4f46\u8fd9\u79cd\u4e0a\u4e0b\u6587\u8fc7\u4e8e\u6709\u9650\uff0c\u65e0\u6cd5\u4e3a\u6355\u6349\u52a8\u4f5c\u4e2d\u7684\u65b0\u9896\u7a7a\u95f4\u548c\u65f6\u95f4\u6982\u5ff5\u63d0\u4f9b\u8db3\u591f\u7684\u80cc\u666f\u77e5\u8bc6\u3002", "method": "\u63d0\u51faDecomposition-incorporation\u6846\u67b6\uff1a1\uff09\u5206\u89e3\u9636\u6bb5\uff1a\u5c06\u539f\u59cb\u52a8\u4f5c\u540d\u79f0\u89e3\u8026\u4e3a\u591a\u6837\u5316\u7684\u65f6\u7a7a\u5c5e\u6027\u63cf\u8ff0\uff1b2\uff09\u878d\u5408\u9636\u6bb5\uff1a\u63d0\u51fa\u7a7a\u95f4/\u65f6\u95f4\u77e5\u8bc6\u8865\u507f\u5668\uff08SKC/TKC\uff09\u6765\u53d1\u73b0\u5224\u522b\u6027\u7684\u5bf9\u8c61\u7ea7\u548c\u5e27\u7ea7\u539f\u578b\u3002SKC\u5728\u7a7a\u95f4\u77e5\u8bc6\u6307\u5bfc\u4e0b\u81ea\u9002\u5e94\u805a\u5408\u91cd\u8981\u8865\u4e01\u6807\u8bb0\uff0cTKC\u5229\u7528\u65f6\u95f4\u5c5e\u6027\u8f85\u52a9\u5e27\u95f4\u65f6\u95f4\u5173\u7cfb\u5efa\u6a21\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eDiST\u5728\u4e94\u4e2a\u6807\u51c6\u5c11\u6837\u672c\u52a8\u4f5c\u8bc6\u522b\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u7684\u89e3\u8026\u7a7a\u95f4\u548c\u65f6\u95f4\u77e5\u8bc6\uff0cDiST\u6846\u67b6\u80fd\u591f\u5b66\u4e60\u8868\u8fbe\u6027\u5f3a\u7684\u591a\u7c92\u5ea6\u539f\u578b\uff0c\u6709\u6548\u6355\u6349\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u7ec6\u8282\u548c\u591a\u6837\u5316\u65f6\u95f4\u6a21\u5f0f\uff0c\u4ece\u800c\u63d0\u5347\u5c11\u6837\u672c\u52a8\u4f5c\u8bc6\u522b\u6027\u80fd\u3002"}}
{"id": "2602.17696", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17696", "abs": "https://arxiv.org/abs/2602.17696", "authors": ["Zongmin Li", "Jian Su", "Farah Benamara", "Aixin Sun"], "title": "Can LLM Safety Be Ensured by Constraining Parameter Regions?", "comment": "32 pages", "summary": "Large language models (LLMs) are often assumed to contain ``safety regions'' -- parameter subsets whose modification directly influences safety behaviors. We conduct a systematic evaluation of four safety region identification methods spanning different parameter granularities, from individual weights to entire Transformer layers, across four families of backbone LLMs with varying sizes. Using ten safety identification datasets, we find that the identified safety regions exhibit only low to moderate overlap, as measured by IoU. The overlap drops significantly when the safety regions are further refined using utility datasets (\\ie non-harmful queries). These results suggest that current techniques fail to reliably identify a stable, dataset-agnostic safety region.", "AI": {"tldr": "\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\"\u5b89\u5168\u533a\u57df\"\u8bc6\u522b\u65b9\u6cd5\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u663e\u793a\uff0c\u73b0\u6709\u6280\u672f\u65e0\u6cd5\u53ef\u9760\u8bc6\u522b\u7a33\u5b9a\u3001\u4e0e\u6570\u636e\u96c6\u65e0\u5173\u7684\u5b89\u5168\u533a\u57df", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u88ab\u8ba4\u4e3a\u5305\u542b\"\u5b89\u5168\u533a\u57df\"\u2014\u2014\u8fd9\u4e9b\u53c2\u6570\u5b50\u96c6\u7684\u4fee\u6539\u4f1a\u76f4\u63a5\u5f71\u54cd\u5b89\u5168\u884c\u4e3a\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u73b0\u6709\u5b89\u5168\u533a\u57df\u8bc6\u522b\u65b9\u6cd5\u7684\u53ef\u9760\u6027", "method": "\u7cfb\u7edf\u8bc4\u4f30\u4e86\u56db\u79cd\u5b89\u5168\u533a\u57df\u8bc6\u522b\u65b9\u6cd5\uff0c\u6db5\u76d6\u4ece\u5355\u4e2a\u6743\u91cd\u5230\u6574\u4e2aTransformer\u5c42\u7684\u4e0d\u540c\u53c2\u6570\u7c92\u5ea6\uff0c\u5728\u56db\u4e2a\u4e0d\u540c\u89c4\u6a21\u7684LLM\u5bb6\u65cf\u4e0a\u6d4b\u8bd5\uff0c\u4f7f\u7528\u5341\u4e2a\u5b89\u5168\u8bc6\u522b\u6570\u636e\u96c6", "result": "\u8bc6\u522b\u51fa\u7684\u5b89\u5168\u533a\u57df\u4ec5\u663e\u793a\u4f4e\u5230\u4e2d\u5ea6\u7684\u91cd\u53e0\uff08\u901a\u8fc7IoU\u6d4b\u91cf\uff09\uff0c\u5f53\u4f7f\u7528\u6548\u7528\u6570\u636e\u96c6\uff08\u5373\u975e\u6709\u5bb3\u67e5\u8be2\uff09\u8fdb\u4e00\u6b65\u7cbe\u70bc\u65f6\uff0c\u91cd\u53e0\u663e\u8457\u4e0b\u964d", "conclusion": "\u5f53\u524d\u6280\u672f\u65e0\u6cd5\u53ef\u9760\u8bc6\u522b\u7a33\u5b9a\u3001\u4e0e\u6570\u636e\u96c6\u65e0\u5173\u7684\u5b89\u5168\u533a\u57df\uff0c\u8868\u660e\u73b0\u6709\u5b89\u5168\u533a\u57df\u8bc6\u522b\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027"}}
{"id": "2602.18047", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18047", "abs": "https://arxiv.org/abs/2602.18047", "authors": ["Rong Fu", "Wenxin Zhang", "Yibo Meng", "Jia Yee Tan", "Jiaxuan Lu", "Rui Lu", "Jiekai Wu", "Zhaolu Kang", "Simon Fong"], "title": "CityGuard: Graph-Aware Private Descriptors for Bias-Resilient Identity Search Across Urban Cameras", "comment": "36 pages, 12 figures", "summary": "City-scale person re-identification across distributed cameras must handle severe appearance changes from viewpoint, occlusion, and domain shift while complying with data protection rules that prevent sharing raw imagery. We introduce CityGuard, a topology-aware transformer for privacy-preserving identity retrieval in decentralized surveillance. The framework integrates three components. A dispersion-adaptive metric learner adjusts instance-level margins according to feature spread, increasing intra-class compactness. Spatially conditioned attention injects coarse geometry, such as GPS or deployment floor plans, into graph-based self-attention to enable projectively consistent cross-view alignment using only coarse geometric priors without requiring survey-grade calibration. Differentially private embedding maps are coupled with compact approximate indexes to support secure and cost-efficient deployment. Together these designs produce descriptors robust to viewpoint variation, occlusion, and domain shifts, and they enable a tunable balance between privacy and utility under rigorous differential-privacy accounting. Experiments on Market-1501 and additional public benchmarks, complemented by database-scale retrieval studies, show consistent gains in retrieval precision and query throughput over strong baselines, confirming the practicality of the framework for privacy-critical urban identity matching.", "AI": {"tldr": "CityGuard\uff1a\u4e00\u79cd\u7528\u4e8e\u5206\u6563\u5f0f\u76d1\u63a7\u4e2d\u9690\u79c1\u4fdd\u62a4\u8eab\u4efd\u68c0\u7d22\u7684\u62d3\u6251\u611f\u77e5Transformer\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5ea6\u91cf\u5b66\u4e60\u3001\u7a7a\u95f4\u6761\u4ef6\u6ce8\u610f\u529b\u548c\u5dee\u5206\u9690\u79c1\u5d4c\u5165\u5b9e\u73b0\u8de8\u89c6\u89d2\u5bf9\u9f50\u548c\u9690\u79c1\u4fdd\u62a4\u3002", "motivation": "\u57ce\u5e02\u89c4\u6a21\u7684\u4eba\u5458\u91cd\u8bc6\u522b\u9762\u4e34\u89c6\u89d2\u53d8\u5316\u3001\u906e\u6321\u548c\u57df\u504f\u79fb\u7b49\u5916\u89c2\u53d8\u5316\u6311\u6218\uff0c\u540c\u65f6\u9700\u8981\u9075\u5b88\u6570\u636e\u4fdd\u62a4\u89c4\u5219\u9632\u6b62\u539f\u59cb\u56fe\u50cf\u5171\u4eab\u3002\u9700\u8981\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u524d\u63d0\u4e0b\u5b9e\u73b0\u8de8\u5206\u5e03\u5f0f\u6444\u50cf\u5934\u7684\u8eab\u4efd\u68c0\u7d22\u3002", "method": "1. \u5206\u6563\u81ea\u9002\u5e94\u5ea6\u91cf\u5b66\u4e60\uff1a\u6839\u636e\u7279\u5f81\u5206\u5e03\u8c03\u6574\u5b9e\u4f8b\u7ea7\u8fb9\u754c\uff0c\u589e\u5f3a\u7c7b\u5185\u7d27\u51d1\u6027\uff1b2. \u7a7a\u95f4\u6761\u4ef6\u6ce8\u610f\u529b\uff1a\u5c06\u7c97\u7565\u51e0\u4f55\u4fe1\u606f\uff08\u5982GPS\u6216\u90e8\u7f72\u5e73\u9762\u56fe\uff09\u6ce8\u5165\u57fa\u4e8e\u56fe\u7684\u81ea\u6ce8\u610f\u529b\uff0c\u5b9e\u73b0\u6295\u5f71\u4e00\u81f4\u7684\u8de8\u89c6\u89d2\u5bf9\u9f50\uff1b3. \u5dee\u5206\u9690\u79c1\u5d4c\u5165\u6620\u5c04\uff1a\u7ed3\u5408\u7d27\u51d1\u8fd1\u4f3c\u7d22\u5f15\uff0c\u652f\u6301\u5b89\u5168\u4e14\u6210\u672c\u9ad8\u6548\u7684\u90e8\u7f72\u3002", "result": "\u5728Market-1501\u548c\u5176\u4ed6\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u68c0\u7d22\u7cbe\u5ea6\u548c\u67e5\u8be2\u541e\u5410\u91cf\u5747\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6570\u636e\u5e93\u89c4\u6a21\u7684\u68c0\u7d22\u7814\u7a76\u4e5f\u8bc1\u5b9e\u4e86\u6846\u67b6\u7684\u5b9e\u9645\u53ef\u884c\u6027\u3002", "conclusion": "CityGuard\u6846\u67b6\u80fd\u591f\u751f\u6210\u5bf9\u89c6\u89d2\u53d8\u5316\u3001\u906e\u6321\u548c\u57df\u504f\u79fb\u5177\u6709\u9c81\u68d2\u6027\u7684\u63cf\u8ff0\u7b26\uff0c\u5728\u4e25\u683c\u7684\u5dee\u5206\u9690\u79c1\u6838\u7b97\u4e0b\u5b9e\u73b0\u9690\u79c1\u4e0e\u6548\u7528\u7684\u53ef\u8c03\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u9690\u79c1\u5173\u952e\u7684\u57ce\u5e02\u8eab\u4efd\u5339\u914d\u5e94\u7528\u3002"}}
{"id": "2602.17697", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.17697", "abs": "https://arxiv.org/abs/2602.17697", "authors": ["Nada Zine", "Cl\u00e9ment Quinton", "Romain Rouvoy"], "title": "Pimp My LLM: Leveraging Variability Modeling to Tune Inference Hyperparameters", "comment": null, "summary": "Large Language Models (LLMs) are being increasingly used across a wide range of tasks. However, their substantial computational demands raise concerns about the energy efficiency and sustainability of both training and inference. Inference, in particular, dominates total compute usage, making its optimization crucial. Recent research has explored optimization techniques and analyzed how configuration choices influence energy consumption. Yet, the vast configuration space of inference servers makes exhaustive empirical evaluation infeasible due to combinatorial explosion. In this paper, we introduce a new perspective on this problem by treating LLMs as configurable systems and applying variability management techniques to systematically analyze inference-time configuration choices. We evaluate our approach on the Hugging Face Transformers library by representing generation hyperparameters and their constraints using a feature-based variability model, sampling representative configurations, measuring their energy consumption, latency, accuracy, and learning predictive models from the collected data. Our results show that variability modeling effectively manages the complexity of LLM inference configurations. It enables systematic analysis of hyperparameters effects and interactions, reveals trade-offs, and supports accurate prediction of inference behavior from a limited number of measurements. Overall, this work opens a new research direction that bridges software engineering and machine learning by leveraging variability modeling for the efficient and sustainable configuration of LLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u89c6\u4e3a\u53ef\u914d\u7f6e\u7cfb\u7edf\uff0c\u5e94\u7528\u53d8\u5f02\u6027\u7ba1\u7406\u6280\u672f\u6765\u7cfb\u7edf\u5206\u6790\u63a8\u7406\u65f6\u914d\u7f6e\u9009\u62e9\uff0c\u4ee5\u4f18\u5316\u80fd\u8017\u548c\u53ef\u6301\u7eed\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5e7f\u6cdb\u4efb\u52a1\u4e2d\u5e94\u7528\u65e5\u76ca\u589e\u591a\uff0c\u4f46\u5176\u5de8\u5927\u7684\u8ba1\u7b97\u9700\u6c42\u5f15\u53d1\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u80fd\u6548\u4e0e\u53ef\u6301\u7eed\u6027\u7684\u62c5\u5fe7\u3002\u63a8\u7406\u9636\u6bb5\u5c24\u5176\u5360\u636e\u603b\u8ba1\u7b97\u4f7f\u7528\u7684\u4e3b\u5bfc\u5730\u4f4d\uff0c\u4f7f\u5f97\u5176\u4f18\u5316\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7814\u7a76\u867d\u7136\u63a2\u7d22\u4e86\u4f18\u5316\u6280\u672f\u5e76\u5206\u6790\u4e86\u914d\u7f6e\u9009\u62e9\u5bf9\u80fd\u8017\u7684\u5f71\u54cd\uff0c\u4f46\u7531\u4e8e\u63a8\u7406\u670d\u52a1\u5668\u914d\u7f6e\u7a7a\u95f4\u7684\u7ec4\u5408\u7206\u70b8\uff0c\u7ecf\u9a8c\u8bc4\u4f30\u53d8\u5f97\u4e0d\u53ef\u884c\u3002", "method": "\u5c06LLMs\u89c6\u4e3a\u53ef\u914d\u7f6e\u7cfb\u7edf\uff0c\u5e94\u7528\u53d8\u5f02\u6027\u7ba1\u7406\u6280\u672f\u7cfb\u7edf\u5206\u6790\u63a8\u7406\u65f6\u914d\u7f6e\u9009\u62e9\u3002\u5728Hugging Face Transformers\u5e93\u4e0a\u8bc4\u4f30\u8be5\u65b9\u6cd5\uff0c\u4f7f\u7528\u57fa\u4e8e\u7279\u5f81\u7684\u53d8\u5f02\u6027\u6a21\u578b\u8868\u793a\u751f\u6210\u8d85\u53c2\u6570\u53ca\u5176\u7ea6\u675f\uff0c\u91c7\u6837\u4ee3\u8868\u6027\u914d\u7f6e\uff0c\u6d4b\u91cf\u5176\u80fd\u8017\u3001\u5ef6\u8fdf\u3001\u51c6\u786e\u6027\uff0c\u5e76\u4ece\u6536\u96c6\u7684\u6570\u636e\u4e2d\u5b66\u4e60\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u53d8\u5f02\u6027\u5efa\u6a21\u6709\u6548\u7ba1\u7406\u4e86LLM\u63a8\u7406\u914d\u7f6e\u7684\u590d\u6742\u6027\u3002\u5b83\u80fd\u591f\u7cfb\u7edf\u5206\u6790\u8d85\u53c2\u6570\u6548\u5e94\u548c\u76f8\u4e92\u4f5c\u7528\uff0c\u63ed\u793a\u6743\u8861\u5173\u7cfb\uff0c\u5e76\u652f\u6301\u4ece\u6709\u9650\u6d4b\u91cf\u4e2d\u51c6\u786e\u9884\u6d4b\u63a8\u7406\u884c\u4e3a\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5f00\u8f9f\u4e86\u8f6f\u4ef6\u5de5\u7a0b\u4e0e\u673a\u5668\u5b66\u4e60\u4ea4\u53c9\u7684\u65b0\u7814\u7a76\u65b9\u5411\uff0c\u901a\u8fc7\u5229\u7528\u53d8\u5f02\u6027\u5efa\u6a21\u6765\u5b9e\u73b0LLM\u7684\u9ad8\u6548\u548c\u53ef\u6301\u7eed\u914d\u7f6e\u3002"}}
{"id": "2602.18057", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18057", "abs": "https://arxiv.org/abs/2602.18057", "authors": ["Hongsong Wang", "Wenjing Yan", "Qiuxia Lai", "Xin Geng"], "title": "Temporal Consistency-Aware Text-to-Motion Generation", "comment": "Code is on https://github.com/Giat995/TCA-T2M/", "summary": "Text-to-Motion (T2M) generation aims to synthesize realistic human motion sequences from natural language descriptions. While two-stage frameworks leveraging discrete motion representations have advanced T2M research, they often neglect cross-sequence temporal consistency, i.e., the shared temporal structures present across different instances of the same action. This leads to semantic misalignments and physically implausible motions. To address this limitation, we propose TCA-T2M, a framework for temporal consistency-aware T2M generation. Our approach introduces a temporal consistency-aware spatial VQ-VAE (TCaS-VQ-VAE) for cross-sequence temporal alignment, coupled with a masked motion transformer for text-conditioned motion generation. Additionally, a kinematic constraint block mitigates discretization artifacts to ensure physical plausibility. Experiments on HumanML3D and KIT-ML benchmarks demonstrate that TCA-T2M achieves state-of-the-art performance, highlighting the importance of temporal consistency in robust and coherent T2M generation.", "AI": {"tldr": "TCA-T2M\u662f\u4e00\u4e2a\u65f6\u95f4\u4e00\u81f4\u6027\u611f\u77e5\u7684\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u5e8f\u5217\u65f6\u95f4\u5bf9\u9f50\u548c\u8fd0\u52a8\u53d8\u6362\u5668\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u95ee\u9898\uff0c\u5728HumanML3D\u548cKIT-ML\u57fa\u51c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4e24\u9636\u6bb5\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u6846\u67b6\u867d\u7136\u5229\u7528\u79bb\u6563\u8fd0\u52a8\u8868\u793a\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u5f80\u5f80\u5ffd\u7565\u4e86\u8de8\u5e8f\u5217\u65f6\u95f4\u4e00\u81f4\u6027\uff08\u5373\u76f8\u540c\u52a8\u4f5c\u4e0d\u540c\u5b9e\u4f8b\u95f4\u5171\u4eab\u7684\u65f6\u95f4\u7ed3\u6784\uff09\uff0c\u5bfc\u81f4\u8bed\u4e49\u9519\u4f4d\u548c\u7269\u7406\u4e0a\u4e0d\u53ef\u4fe1\u7684\u8fd0\u52a8\u3002", "method": "\u63d0\u51faTCA-T2M\u6846\u67b6\uff0c\u5305\u542b\uff1a1\uff09\u65f6\u95f4\u4e00\u81f4\u6027\u611f\u77e5\u7a7a\u95f4VQ-VAE\uff08TCaS-VQ-VAE\uff09\u7528\u4e8e\u8de8\u5e8f\u5217\u65f6\u95f4\u5bf9\u9f50\uff1b2\uff09\u63a9\u7801\u8fd0\u52a8\u53d8\u6362\u5668\u7528\u4e8e\u6587\u672c\u6761\u4ef6\u8fd0\u52a8\u751f\u6210\uff1b3\uff09\u8fd0\u52a8\u5b66\u7ea6\u675f\u5757\u51cf\u8f7b\u79bb\u6563\u5316\u4f2a\u5f71\u4ee5\u786e\u4fdd\u7269\u7406\u53ef\u4fe1\u6027\u3002", "result": "\u5728HumanML3D\u548cKIT-ML\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTCA-T2M\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u65f6\u95f4\u4e00\u81f4\u6027\u5728\u9c81\u68d2\u548c\u8fde\u8d2f\u7684\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u4e2d\u7684\u91cd\u8981\u6027\u3002", "conclusion": "TCA-T2M\u901a\u8fc7\u5f15\u5165\u65f6\u95f4\u4e00\u81f4\u6027\u611f\u77e5\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6587\u672c\u5230\u52a8\u4f5c\u751f\u6210\u65b9\u6cd5\u4e2d\u8de8\u5e8f\u5217\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u7f3a\u5931\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u52a8\u4f5c\u7684\u8bed\u4e49\u5bf9\u9f50\u548c\u7269\u7406\u53ef\u4fe1\u6027\u3002"}}
{"id": "2602.17698", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17698", "abs": "https://arxiv.org/abs/2602.17698", "authors": ["Xinlin Li", "Timothy Chou", "Josh Fromm", "Zichang Liu", "Yunjie Pan", "Christina Fragouli"], "title": "ScaleBITS: Scalable Bitwidth Search for Hardware-Aligned Mixed-Precision LLMs", "comment": null, "summary": "Post-training weight quantization is crucial for reducing the memory and inference cost of large language models (LLMs), yet pushing the average precision below 4 bits remains challenging due to highly non-uniform weight sensitivity and the lack of principled precision allocation. Existing solutions use irregular fine-grained mixed-precision with high runtime overhead or rely on heuristics or highly constrained precision allocation strategies. In this work, we propose ScaleBITS, a mixed-precision quantization framework that enables automated, fine-grained bitwidth allocation under a memory budget while preserving hardware efficiency. Guided by a new sensitivity analysis, we introduce a hardware-aligned, block-wise weight partitioning scheme, powered by bi-directional channel reordering. We formulate global bitwidth allocation as a constrained optimization problem and develop a scalable approximation to the greedy algorithm, enabling end-to-end principled allocation. Experiments show that ScaleBITS significantly improves over uniform-precision quantization (up to +36%) and outperforms state-of-the-art sensitivity-aware baselines (up to +13%) in ultra-low-bit regime, without adding runtime overhead.", "AI": {"tldr": "ScaleBITS\u662f\u4e00\u4e2a\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u6846\u67b6\uff0c\u80fd\u591f\u5728\u5185\u5b58\u9884\u7b97\u4e0b\u5b9e\u73b0\u81ea\u52a8\u5316\u3001\u7ec6\u7c92\u5ea6\u7684\u6bd4\u7279\u5bbd\u5ea6\u5206\u914d\uff0c\u540c\u65f6\u4fdd\u6301\u786c\u4ef6\u6548\u7387\uff0c\u5728\u8d85\u4f4e\u6bd4\u7279\u91cf\u5316\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u540e\u8bad\u7ec3\u6743\u91cd\u91cf\u5316\u5bf9\u4e8e\u51cf\u5c11\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5185\u5b58\u548c\u63a8\u7406\u6210\u672c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5c06\u5e73\u5747\u7cbe\u5ea6\u63a8\u52304\u6bd4\u7279\u4ee5\u4e0b\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u6743\u91cd\u654f\u611f\u5ea6\u9ad8\u5ea6\u4e0d\u5747\u5300\uff0c\u7f3a\u4e4f\u539f\u5219\u6027\u7684\u7cbe\u5ea6\u5206\u914d\u65b9\u6cd5\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u8981\u4e48\u4f7f\u7528\u4e0d\u89c4\u5219\u7ec6\u7c92\u5ea6\u6df7\u5408\u7cbe\u5ea6\u5bfc\u81f4\u9ad8\u8fd0\u884c\u65f6\u5f00\u9500\uff0c\u8981\u4e48\u4f9d\u8d56\u542f\u53d1\u5f0f\u6216\u9ad8\u5ea6\u53d7\u9650\u7684\u7cbe\u5ea6\u5206\u914d\u7b56\u7565\u3002", "method": "\u63d0\u51faScaleBITS\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u6846\u67b6\uff1a1\uff09\u57fa\u4e8e\u65b0\u7684\u654f\u611f\u5ea6\u5206\u6790\uff1b2\uff09\u5f15\u5165\u786c\u4ef6\u5bf9\u9f50\u7684\u5757\u7ea7\u6743\u91cd\u5206\u533a\u65b9\u6848\uff0c\u91c7\u7528\u53cc\u5411\u901a\u9053\u91cd\u6392\u5e8f\uff1b3\uff09\u5c06\u5168\u5c40\u6bd4\u7279\u5bbd\u5ea6\u5206\u914d\u5efa\u6a21\u4e3a\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u53ef\u6269\u5c55\u7684\u8d2a\u5fc3\u7b97\u6cd5\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u539f\u5219\u6027\u5206\u914d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cScaleBITS\u5728\u8d85\u4f4e\u6bd4\u7279\u91cf\u5316\u4e2d\u663e\u8457\u4f18\u4e8e\u5747\u5300\u7cbe\u5ea6\u91cf\u5316\uff08\u63d0\u5347\u9ad8\u8fbe36%\uff09\uff0c\u5e76\u4e14\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u654f\u611f\u5ea6\u611f\u77e5\u57fa\u7ebf\u65b9\u6cd5\uff08\u63d0\u5347\u9ad8\u8fbe13%\uff09\uff0c\u540c\u65f6\u4e0d\u589e\u52a0\u8fd0\u884c\u65f6\u5f00\u9500\u3002", "conclusion": "ScaleBITS\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u8d85\u4f4e\u6bd4\u7279\u91cf\u5316\u4e2d\u7684\u7cbe\u5ea6\u5206\u914d\u95ee\u9898\uff0c\u901a\u8fc7\u786c\u4ef6\u9ad8\u6548\u7684\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u63a8\u7406\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u91cf\u5316\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2602.17699", "categories": ["cs.LG", "math.RA", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.17699", "abs": "https://arxiv.org/abs/2602.17699", "authors": ["Chandrasekhar Gokavarapu", "Sudhakar Gadde", "Y. Rajasekhar", "S. R. Bhargava"], "title": "Certified Learning under Distribution Shift: Sound Verification and Identifiable Structure", "comment": null, "summary": "Proposition. Let $f$ be a predictor trained on a distribution $P$ and evaluated on a shifted distribution $Q$. Under verifiable regularity and complexity constraints, the excess risk under shift admits an explicit upper bound determined by a computable shift metric and model parameters. We develop a unified framework in which (i) risk under distribution shift is certified by explicit inequalities, (ii) verification of learned models is sound for nontrivial sizes, and (iii) interpretability is enforced through identifiability conditions rather than post hoc explanations. All claims are stated with explicit assumptions. Failure modes are isolated. Non-certifiable regimes are characterized.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u8bc1\u660e\u5728\u53ef\u9a8c\u8bc1\u7684\u6b63\u5219\u6027\u548c\u590d\u6742\u6027\u7ea6\u675f\u4e0b\uff0c\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u9884\u6d4b\u5668\u98ce\u9669\u53ef\u7531\u53ef\u8ba1\u7b97\u7684\u504f\u79fb\u5ea6\u91cf\u548c\u6a21\u578b\u53c2\u6570\u663e\u5f0f\u4e0a\u754c", "motivation": "\u89e3\u51b3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u98ce\u9669\u8ba4\u8bc1\u95ee\u9898\uff0c\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u4fdd\u8bc1\uff0c\u907f\u514d\u4f9d\u8d56\u4e8b\u540e\u89e3\u91ca\uff0c\u5efa\u7acb\u4e25\u683c\u7684\u6570\u5b66\u6846\u67b6", "method": "\u5f00\u53d1\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u57fa\u4e8e\u53ef\u9a8c\u8bc1\u7684\u6b63\u5219\u6027\u548c\u590d\u6742\u6027\u7ea6\u675f\uff0c\u63a8\u5bfc\u5206\u5e03\u504f\u79fb\u4e0b\u98ce\u9669\u7684\u663e\u5f0f\u4e0a\u754c\uff0c\u4f7f\u7528\u53ef\u8ba1\u7b97\u7684\u504f\u79fb\u5ea6\u91cf\uff0c\u5efa\u7acb\u53ef\u8ba4\u8bc1\u7684\u4e0d\u7b49\u5f0f", "result": "\u83b7\u5f97\u4e86\u5206\u5e03\u504f\u79fb\u4e0b\u98ce\u9669\u7684\u663e\u5f0f\u4e0a\u754c\uff0c\u8be5\u4e0a\u754c\u7531\u53ef\u8ba1\u7b97\u7684\u504f\u79fb\u5ea6\u91cf\u548c\u6a21\u578b\u53c2\u6570\u51b3\u5b9a\uff1b\u5efa\u7acb\u4e86\u53ef\u9a8c\u8bc1\u7684\u6a21\u578b\u8ba4\u8bc1\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u975e\u5e73\u51e1\u89c4\u6a21\u95ee\u9898\uff1b\u901a\u8fc7\u53ef\u8bc6\u522b\u6027\u6761\u4ef6\u800c\u975e\u4e8b\u540e\u89e3\u91ca\u5b9e\u73b0\u53ef\u89e3\u91ca\u6027", "conclusion": "\u5728\u660e\u786e\u5047\u8bbe\u4e0b\u5efa\u7acb\u4e86\u5206\u5e03\u504f\u79fb\u98ce\u9669\u8ba4\u8bc1\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u80fd\u591f\u9694\u79bb\u5931\u6548\u6a21\u5f0f\u5e76\u8868\u5f81\u4e0d\u53ef\u8ba4\u8bc1\u7684\u673a\u5236\uff0c\u4e3a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840"}}
{"id": "2602.18066", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18066", "abs": "https://arxiv.org/abs/2602.18066", "authors": ["Daniel Busch", "Christian Bohn", "Thomas Kurbiel", "Klaus Friedrichs", "Richard Meyes", "Tobias Meisen"], "title": "Faster Training, Fewer Labels: Self-Supervised Pretraining for Fine-Grained BEV Segmentation", "comment": "This Paper has been accepted to the 2026 IEEE Intelligent Vehicles Symposium (IV)", "summary": "Dense Bird's Eye View (BEV) semantic maps are central to autonomous driving, yet current multi-camera methods depend on costly, inconsistently annotated BEV ground truth. We address this limitation with a two-phase training strategy for fine-grained road marking segmentation that removes full supervision during pretraining and halves the amount of training data during fine-tuning while still outperforming the comparable supervised baseline model. During the self-supervised pretraining, BEVFormer predictions are differentiably reprojected into the image plane and trained against multi-view semantic pseudo-labels generated by the widely used semantic segmentation model Mask2Former. A temporal loss encourages consistency across frames. The subsequent supervised fine-tuning phase requires only 50% of the dataset and significantly less training time. With our method, the fine-tuning benefits from rich priors learned during pretraining boosting the performance and BEV segmentation quality (up to +2.5pp mIoU over the fully supervised baseline) on nuScenes. It simultaneously halves the usage of annotation data and reduces total training time by up to two thirds. The results demonstrate that differentiable reprojection plus camera perspective pseudo labels yields transferable BEV features and a scalable path toward reduced-label autonomous perception.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u548c\u534a\u76d1\u7763\u5fae\u8c03\uff0c\u51cf\u5c11\u5bf9\u6602\u8d35BEV\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u540c\u65f6\u63d0\u5347\u9053\u8def\u6807\u8bb0\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u591a\u6444\u50cf\u5934BEV\u8bed\u4e49\u5730\u56fe\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u4e14\u6807\u6ce8\u4e0d\u4e00\u81f4\u7684\u5730\u9762\u771f\u503c\u6570\u636e\uff0c\u8fd9\u9650\u5236\u4e86\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a1) \u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u9636\u6bb5\uff0c\u5c06BEVFormer\u9884\u6d4b\u901a\u8fc7\u53ef\u5fae\u5206\u91cd\u6295\u5f71\u5230\u56fe\u50cf\u5e73\u9762\uff0c\u4f7f\u7528Mask2Former\u751f\u6210\u7684\u591a\u89c6\u89d2\u8bed\u4e49\u4f2a\u6807\u7b7e\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u52a0\u5165\u65f6\u95f4\u4e00\u81f4\u6027\u635f\u5931\uff1b2) \u76d1\u7763\u5fae\u8c03\u9636\u6bb5\uff0c\u4ec5\u970050%\u6570\u636e\u96c6\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u6bd4\u5168\u76d1\u7763\u57fa\u7ebf\u63d0\u5347\u9ad8\u8fbe2.5pp mIoU\uff0c\u540c\u65f6\u5c06\u6807\u6ce8\u6570\u636e\u4f7f\u7528\u91cf\u51cf\u534a\uff0c\u603b\u8bad\u7ec3\u65f6\u95f4\u51cf\u5c11\u4e09\u5206\u4e4b\u4e8c\u3002", "conclusion": "\u53ef\u5fae\u5206\u91cd\u6295\u5f71\u52a0\u76f8\u673a\u89c6\u89d2\u4f2a\u6807\u7b7e\u80fd\u591f\u4ea7\u751f\u53ef\u8fc1\u79fb\u7684BEV\u7279\u5f81\uff0c\u4e3a\u51cf\u5c11\u6807\u6ce8\u7684\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u63d0\u4f9b\u4e86\u4e00\u6761\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002"}}
{"id": "2602.17700", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2602.17700", "abs": "https://arxiv.org/abs/2602.17700", "authors": ["Konstanty Subbotko"], "title": "MIDAS: Mosaic Input-Specific Differentiable Architecture Search", "comment": null, "summary": "Differentiable Neural Architecture Search (NAS) provides efficient, gradient-based methods for automatically designing neural networks, yet its adoption remains limited in practice. We present MIDAS, a novel approach that modernizes DARTS by replacing static architecture parameters with dynamic, input-specific parameters computed via self-attention. To improve robustness, MIDAS (i) localizes the architecture selection by computing it separately for each spatial patch of the activation map, and (ii) introduces a parameter-free, topology-aware search space that models node connectivity and simplifies selecting the two incoming edges per node. We evaluate MIDAS on the DARTS, NAS-Bench-201, and RDARTS search spaces. In DARTS, it reaches 97.42% top-1 on CIFAR-10 and 83.38% on CIFAR-100. In NAS-Bench-201, it consistently finds globally optimal architectures. In RDARTS, it sets the state of the art on two of four search spaces on CIFAR-10. We further analyze why MIDAS works, showing that patchwise attention improves discrimination among candidate operations, and the resulting input-specific parameter distributions are class-aware and predominantly unimodal, providing reliable guidance for decoding.", "AI": {"tldr": "MIDAS\u662f\u4e00\u79cd\u6539\u8fdb\u7684\u53ef\u5fae\u5206\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u673a\u5236\u52a8\u6001\u751f\u6210\u8f93\u5165\u7279\u5b9a\u7684\u67b6\u6784\u53c2\u6570\uff0c\u5e76\u91c7\u7528\u5c40\u90e8\u5316\u67b6\u6784\u9009\u62e9\u548c\u62d3\u6251\u611f\u77e5\u641c\u7d22\u7a7a\u95f4\u6765\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "motivation": "\u5c3d\u7ba1\u53ef\u5fae\u5206\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u68af\u5ea6\u4f18\u5316\u65b9\u6cd5\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u91c7\u7528\u4ecd\u7136\u6709\u9650\u3002\u4f5c\u8005\u65e8\u5728\u6539\u8fdbDARTS\u65b9\u6cd5\uff0c\u4f7f\u5176\u66f4\u52a0\u5b9e\u7528\u548c\u9c81\u68d2\u3002", "method": "MIDAS\u7528\u52a8\u6001\u7684\u3001\u8f93\u5165\u7279\u5b9a\u7684\u53c2\u6570\u66ff\u6362DARTS\u4e2d\u7684\u9759\u6001\u67b6\u6784\u53c2\u6570\uff0c\u8fd9\u4e9b\u53c2\u6570\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u8ba1\u7b97\u3002\u91c7\u7528\u4e24\u79cd\u5173\u952e\u6280\u672f\uff1a(1) \u5c40\u90e8\u5316\u67b6\u6784\u9009\u62e9\uff0c\u4e3a\u6fc0\u6d3b\u56fe\u7684\u6bcf\u4e2a\u7a7a\u95f4\u8865\u4e01\u5355\u72ec\u8ba1\u7b97\uff1b(2) \u5f15\u5165\u53c2\u6570\u514d\u8d39\u3001\u62d3\u6251\u611f\u77e5\u7684\u641c\u7d22\u7a7a\u95f4\uff0c\u5efa\u6a21\u8282\u70b9\u8fde\u63a5\u6027\u5e76\u7b80\u5316\u6bcf\u4e2a\u8282\u70b9\u7684\u4e24\u4e2a\u5165\u8fb9\u9009\u62e9\u3002", "result": "\u5728DARTS\u641c\u7d22\u7a7a\u95f4\u4e0a\uff0cCIFAR-10\u8fbe\u523097.42% top-1\u51c6\u786e\u7387\uff0cCIFAR-100\u8fbe\u523083.38%\u3002\u5728NAS-Bench-201\u4e0a\u80fd\u4e00\u81f4\u627e\u5230\u5168\u5c40\u6700\u4f18\u67b6\u6784\u3002\u5728RDARTS\u641c\u7d22\u7a7a\u95f4\u4e2d\uff0c\u5728CIFAR-10\u7684\u56db\u4e2a\u641c\u7d22\u7a7a\u95f4\u4e2d\u7684\u4e24\u4e2a\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "MIDAS\u901a\u8fc7\u8865\u4e01\u7ea7\u6ce8\u610f\u529b\u6539\u8fdb\u4e86\u5019\u9009\u64cd\u4f5c\u4e4b\u95f4\u7684\u533a\u5206\u5ea6\uff0c\u751f\u6210\u7684\u8f93\u5165\u7279\u5b9a\u53c2\u6570\u5206\u5e03\u5177\u6709\u7c7b\u522b\u611f\u77e5\u6027\u4e14\u4e3b\u8981\u662f\u5355\u5cf0\u7684\uff0c\u4e3a\u89e3\u7801\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6307\u5bfc\u3002\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u53ef\u5fae\u5206NAS\u7684\u5b9e\u7528\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2602.17706", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17706", "abs": "https://arxiv.org/abs/2602.17706", "authors": ["Rongyao Cai", "Yuxi Wan", "Kexin Zhang", "Ming Jin", "Zhiqiang Ge", "Qingsong Wen", "Yong Liu"], "title": "Parallel Complex Diffusion for Scalable Time Series Generation", "comment": null, "summary": "Modeling long-range dependencies in time series generation poses a fundamental trade-off between representational capacity and computational efficiency. Traditional temporal diffusion models suffer from local entanglement and the $\\mathcal{O}(L^2)$ cost of attention mechanisms. We address these limitations by introducing PaCoDi (Parallel Complex Diffusion), a spectral-native architecture that decouples generative modeling in the frequency domain. PaCoDi fundamentally alters the problem topology: the Fourier Transform acts as a diagonalizing operator, converting locally coupled temporal signals into globally decorrelated spectral components. Theoretically, we prove the Quadrature Forward Diffusion and Conditional Reverse Factorization theorem, demonstrating that the complex diffusion process can be split into independent real and imaginary branches. We bridge the gap between this decoupled theory and data reality using a \\textbf{Mean Field Theory (MFT) approximation} reinforced by an interactive correction mechanism. Furthermore, we generalize this discrete DDPM to continuous-time Frequency SDEs, rigorously deriving the Spectral Wiener Process describe the differential spectral Brownian motion limit. Crucially, PaCoDi exploits the Hermitian Symmetry of real-valued signals to compress the sequence length by half, achieving a 50% reduction in attention FLOPs without information loss. We further derive a rigorous Heteroscedastic Loss to handle the non-isotropic noise distribution on the compressed manifold. Extensive experiments show that PaCoDi outperforms existing baselines in both generation quality and inference speed, offering a theoretically grounded and computationally efficient solution for time series modeling.", "AI": {"tldr": "PaCoDi\u662f\u4e00\u79cd\u5728\u9891\u57df\u8fdb\u884c\u751f\u6210\u5efa\u6a21\u7684\u8c31\u539f\u751f\u67b6\u6784\uff0c\u901a\u8fc7\u5085\u91cc\u53f6\u53d8\u6362\u89e3\u8026\u65f6\u95f4\u4fe1\u53f7\uff0c\u5229\u7528\u5384\u7c73\u7279\u5bf9\u79f0\u6027\u538b\u7f29\u5e8f\u5217\u957f\u5ea6\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u6a21\u578b\u5728\u5904\u7406\u957f\u7a0b\u4f9d\u8d56\u65f6\u9762\u4e34\u8868\u793a\u80fd\u529b\u4e0e\u8ba1\u7b97\u6548\u7387\u7684\u6743\u8861\uff0c\u65f6\u95f4\u6269\u6563\u6a21\u578b\u5b58\u5728\u5c40\u90e8\u7ea0\u7f20\u95ee\u9898\uff0c\u6ce8\u610f\u529b\u673a\u5236\u5177\u6709O(L\u00b2)\u7684\u8ba1\u7b97\u6210\u672c\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u6709\u6548\u5efa\u6a21\u957f\u7a0b\u4f9d\u8d56\u53c8\u8ba1\u7b97\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faPaCoDi\uff08\u5e76\u884c\u590d\u6269\u6563\uff09\u67b6\u6784\uff0c\u5728\u9891\u57df\u8fdb\u884c\u751f\u6210\u5efa\u6a21\u3002\u901a\u8fc7\u5085\u91cc\u53f6\u53d8\u6362\u5c06\u65f6\u95f4\u4fe1\u53f7\u8f6c\u6362\u4e3a\u89e3\u8026\u7684\u8c31\u5206\u91cf\uff0c\u5229\u7528\u5384\u7c73\u7279\u5bf9\u79f0\u6027\u538b\u7f29\u5e8f\u5217\u957f\u5ea6\u3002\u63d0\u51fa\u6b63\u4ea4\u524d\u5411\u6269\u6563\u548c\u6761\u4ef6\u53cd\u5411\u5206\u89e3\u5b9a\u7406\uff0c\u4f7f\u7528\u5e73\u5747\u573a\u7406\u8bba\u8fd1\u4f3c\u548c\u4ea4\u4e92\u6821\u6b63\u673a\u5236\uff0c\u5c06\u79bb\u6563DDPM\u63a8\u5e7f\u5230\u8fde\u7eed\u65f6\u95f4\u9891\u7387SDE\uff0c\u63a8\u5bfc\u8c31\u7ef4\u7eb3\u8fc7\u7a0b\u3002", "result": "PaCoDi\u5b9e\u73b0\u4e8650%\u7684\u6ce8\u610f\u529bFLOPs\u51cf\u5c11\uff0c\u5728\u751f\u6210\u8d28\u91cf\u548c\u63a8\u7406\u901f\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u63d0\u4f9b\u4e86\u7406\u8bba\u4e25\u8c28\u4e14\u8ba1\u7b97\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "PaCoDi\u901a\u8fc7\u9891\u57df\u89e3\u8026\u548c\u8c31\u538b\u7f29\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u95f4\u5e8f\u5217\u751f\u6210\u4e2d\u957f\u7a0b\u4f9d\u8d56\u5efa\u6a21\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u5728\u7406\u8bba\u4e25\u8c28\u6027\u548c\u5b9e\u9645\u6027\u80fd\u65b9\u9762\u90fd\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2602.18089", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18089", "abs": "https://arxiv.org/abs/2602.18089", "authors": ["Kunwar Arpit Singh", "Ankush Prakash", "Haroon R Lone"], "title": "DohaScript: A Large-Scale Multi-Writer Dataset for Continuous Handwritten Hindi Text", "comment": null, "summary": "Despite having hundreds of millions of speakers, handwritten Devanagari text remains severely underrepresented in publicly available benchmark datasets. Existing resources are limited in scale, focus primarily on isolated characters or short words, and lack controlled lexical content and writer level diversity, which restricts their utility for modern data driven handwriting analysis. As a result, they fail to capture the continuous, fused, and structurally complex nature of Devanagari handwriting, where characters are connected through a shared shirorekha (horizontal headline) and exhibit rich ligature formations. We introduce DohaScript, a large scale, multi writer dataset of handwritten Hindi text collected from 531 unique contributors. The dataset is designed as a parallel stylistic corpus, in which all writers transcribe the same fixed set of six traditional Hindi dohas (couplets). This controlled design enables systematic analysis of writer specific variation independent of linguistic content, and supports tasks such as handwriting recognition, writer identification, style analysis, and generative modeling. The dataset is accompanied by non identifiable demographic metadata, rigorous quality curation based on objective sharpness and resolution criteria, and page level layout difficulty annotations that facilitate stratified benchmarking. Baseline experiments demonstrate clear quality separation and strong generalization to unseen writers, highlighting the dataset's reliability and practical value. DohaScript is intended to serve as a standardized and reproducible benchmark for advancing research on continuous handwritten Devanagari text in low resource script settings.", "AI": {"tldr": "DohaScript\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u591a\u4e66\u5199\u8005\u7684\u624b\u5199\u5370\u5730\u8bed\u6570\u636e\u96c6\uff0c\u5305\u542b531\u4f4d\u8d21\u732e\u8005\u8f6c\u5f55\u76f8\u540c\u7684\u516d\u9996\u4f20\u7edf\u5370\u5730\u8bed\u5bf9\u53e5\uff0c\u65e8\u5728\u89e3\u51b3\u5fb7\u74e6\u7eb3\u683c\u91cc\u6587\u5b57\u624b\u5199\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u5fb7\u74e6\u7eb3\u683c\u91cc\u6587\u5b57\u6709\u6570\u4ebf\u4f7f\u7528\u8005\uff0c\u4f46\u516c\u5f00\u53ef\u7528\u7684\u624b\u5199\u57fa\u51c6\u6570\u636e\u96c6\u4e25\u91cd\u4e0d\u8db3\u3002\u73b0\u6709\u8d44\u6e90\u89c4\u6a21\u6709\u9650\uff0c\u4e3b\u8981\u5173\u6ce8\u5b64\u7acb\u5b57\u7b26\u6216\u77ed\u8bcd\uff0c\u7f3a\u4e4f\u53d7\u63a7\u7684\u8bcd\u6c47\u5185\u5bb9\u548c\u4e66\u5199\u8005\u591a\u6837\u6027\uff0c\u65e0\u6cd5\u6355\u6349\u5fb7\u74e6\u7eb3\u683c\u91cc\u624b\u5199\u6587\u5b57\u7684\u8fde\u7eed\u6027\u3001\u878d\u5408\u6027\u548c\u7ed3\u6784\u590d\u6742\u6027\u3002", "method": "\u6536\u96c6531\u4f4d\u72ec\u7279\u8d21\u732e\u8005\u7684\u624b\u5199\u5370\u5730\u8bed\u6587\u672c\uff0c\u8bbe\u8ba1\u4e3a\u5e73\u884c\u98ce\u683c\u8bed\u6599\u5e93\uff0c\u6240\u6709\u4e66\u5199\u8005\u8f6c\u5f55\u76f8\u540c\u7684\u516d\u9996\u4f20\u7edf\u5370\u5730\u8bed\u5bf9\u53e5\u3002\u6570\u636e\u96c6\u5305\u542b\u53bb\u8bc6\u522b\u5316\u7684\u4eba\u53e3\u7edf\u8ba1\u5143\u6570\u636e\uff0c\u57fa\u4e8e\u5ba2\u89c2\u6e05\u6670\u5ea6\u548c\u5206\u8fa8\u7387\u6807\u51c6\u7684\u4e25\u683c\u8d28\u91cf\u7b5b\u9009\uff0c\u4ee5\u53ca\u9875\u9762\u7ea7\u5e03\u5c40\u96be\u5ea6\u6807\u6ce8\u3002", "result": "\u57fa\u7ebf\u5b9e\u9a8c\u663e\u793a\u4e86\u6e05\u6670\u7684\u8d28\u91cf\u5206\u79bb\u548c\u5bf9\u672a\u89c1\u4e66\u5199\u8005\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u7a81\u51fa\u4e86\u6570\u636e\u96c6\u7684\u53ef\u9760\u6027\u548c\u5b9e\u7528\u4ef7\u503c\u3002DohaScript\u4e3a\u4f4e\u8d44\u6e90\u811a\u672c\u73af\u5883\u4e0b\u8fde\u7eed\u624b\u5199\u5fb7\u74e6\u7eb3\u683c\u91cc\u6587\u5b57\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u3001\u53ef\u590d\u73b0\u7684\u57fa\u51c6\u3002", "conclusion": "DohaScript\u586b\u8865\u4e86\u5fb7\u74e6\u7eb3\u683c\u91cc\u624b\u5199\u6587\u672c\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u652f\u6301\u624b\u5199\u8bc6\u522b\u3001\u4e66\u5199\u8005\u8bc6\u522b\u3001\u98ce\u683c\u5206\u6790\u548c\u751f\u6210\u5efa\u6a21\u7b49\u4efb\u52a1\uff0c\u4e3a\u4f4e\u8d44\u6e90\u811a\u672c\u73af\u5883\u4e0b\u7684\u624b\u5199\u6587\u5b57\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u57fa\u51c6\u3002"}}
{"id": "2602.17743", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.17743", "abs": "https://arxiv.org/abs/2602.17743", "authors": ["Di Zhang"], "title": "Provable Adversarial Robustness in In-Context Learning", "comment": "16 pages", "summary": "Large language models adapt to new tasks through in-context learning (ICL) without parameter updates. Current theoretical explanations for this capability assume test tasks are drawn from a distribution similar to that seen during pretraining. This assumption overlooks adversarial distribution shifts that threaten real-world reliability. To address this gap, we introduce a distributionally robust meta-learning framework that provides worst-case performance guarantees for ICL under Wasserstein-based distribution shifts. Focusing on linear self-attention Transformers, we derive a non-asymptotic bound linking adversarial perturbation strength ($\u03c1$), model capacity ($m$), and the number of in-context examples ($N$). The analysis reveals that model robustness scales with the square root of its capacity ($\u03c1_{\\text{max}} \\propto \\sqrt{m}$), while adversarial settings impose a sample complexity penalty proportional to the square of the perturbation magnitude ($N_\u03c1- N_0 \\propto \u03c1^2$). Experiments on synthetic tasks confirm these scaling laws. These findings advance the theoretical understanding of ICL's limits under adversarial conditions and suggest that model capacity serves as a fundamental resource for distributional robustness.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u5e03\u9c81\u68d2\u7684\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u4e3a\u57fa\u4e8eWasserstein\u8ddd\u79bb\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u63d0\u4f9b\u6700\u574f\u60c5\u51b5\u6027\u80fd\u4fdd\u8bc1\uff0c\u5206\u6790\u4e86\u5bf9\u6297\u6027\u6270\u52a8\u5f3a\u5ea6\u3001\u6a21\u578b\u5bb9\u91cf\u548c\u4e0a\u4e0b\u6587\u793a\u4f8b\u6570\u91cf\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "motivation": "\u5f53\u524d\u5bf9\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u7684\u7406\u8bba\u89e3\u91ca\u5047\u8bbe\u6d4b\u8bd5\u4efb\u52a1\u5206\u5e03\u4e0e\u9884\u8bad\u7ec3\u65f6\u76f8\u4f3c\uff0c\u5ffd\u7565\u4e86\u5bf9\u6297\u6027\u5206\u5e03\u504f\u79fb\u5bf9\u5b9e\u9645\u53ef\u9760\u6027\u7684\u5a01\u80c1\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7406\u8bba\u7a7a\u767d\u3002", "method": "\u5f15\u5165\u5206\u5e03\u9c81\u68d2\u7684\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u9488\u5bf9\u7ebf\u6027\u81ea\u6ce8\u610f\u529bTransformer\uff0c\u63a8\u5bfc\u51fa\u5bf9\u6297\u6027\u6270\u52a8\u5f3a\u5ea6(\u03c1)\u3001\u6a21\u578b\u5bb9\u91cf(m)\u548c\u4e0a\u4e0b\u6587\u793a\u4f8b\u6570\u91cf(N)\u4e4b\u95f4\u7684\u975e\u6e10\u8fd1\u8fb9\u754c\u3002", "result": "\u5206\u6790\u8868\u660e\u6a21\u578b\u9c81\u68d2\u6027\u968f\u5176\u5bb9\u91cf\u7684\u5e73\u65b9\u6839\u7f29\u653e(\u03c1_max \u221d \u221am)\uff0c\u800c\u5bf9\u6297\u6027\u8bbe\u7f6e\u4f1a\u5e26\u6765\u4e0e\u6270\u52a8\u5e45\u5ea6\u5e73\u65b9\u6210\u6b63\u6bd4\u7684\u6837\u672c\u590d\u6742\u5ea6\u60e9\u7f5a(N_\u03c1 - N_0 \u221d \u03c1^2)\u3002\u5408\u6210\u4efb\u52a1\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u7f29\u653e\u89c4\u5f8b\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u63a8\u8fdb\u4e86\u5bf9\u5bf9\u6297\u6761\u4ef6\u4e0b\u4e0a\u4e0b\u6587\u5b66\u4e60\u6781\u9650\u7684\u7406\u8bba\u7406\u89e3\uff0c\u8868\u660e\u6a21\u578b\u5bb9\u91cf\u662f\u5206\u5e03\u9c81\u68d2\u6027\u7684\u57fa\u672c\u8d44\u6e90\u3002"}}
{"id": "2602.18093", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18093", "abs": "https://arxiv.org/abs/2602.18093", "authors": ["Hanshuai Cui", "Zhiqing Tang", "Qianli Ma", "Zhi Yao", "Weijia Jia"], "title": "Predict to Skip: Linear Multistep Feature Forecasting for Efficient Diffusion Transformers", "comment": null, "summary": "Diffusion Transformers (DiT) have emerged as a widely adopted backbone for high-fidelity image and video generation, yet their iterative denoising process incurs high computational costs. Existing training-free acceleration methods rely on feature caching and reuse under the assumption of temporal stability. However, reusing features for multiple steps may lead to latent drift and visual degradation. We observe that model outputs evolve smoothly along much of the diffusion trajectory, enabling principled predictions rather than naive reuse. Based on this insight, we propose \\textbf{PrediT}, a training-free acceleration framework that formulates feature prediction as a linear multistep problem. We employ classical linear multistep methods to forecast future model outputs from historical information, combined with a corrector that activates in high-dynamics regions to prevent error accumulation. A dynamic step modulation mechanism adaptively adjusts the prediction horizon by monitoring the feature change rate. Together, these components enable substantial acceleration while preserving generation fidelity. Extensive experiments validate that our method achieves up to $5.54\\times$ latency reduction across various DiT-based image and video generation models, while incurring negligible quality degradation.", "AI": {"tldr": "PrediT\uff1a\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684DiT\u52a0\u901f\u6846\u67b6\uff0c\u901a\u8fc7\u7ebf\u6027\u591a\u6b65\u65b9\u6cd5\u9884\u6d4b\u6a21\u578b\u8f93\u51fa\uff0c\u7ed3\u5408\u6821\u6b63\u5668\u548c\u52a8\u6001\u6b65\u957f\u8c03\u5236\uff0c\u5b9e\u73b05.54\u500d\u5ef6\u8fdf\u964d\u4f4e\u4e14\u8d28\u91cf\u635f\u5931\u53ef\u5ffd\u7565\u3002", "motivation": "\u6269\u6563\u53d8\u6362\u5668(DiT)\u5728\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u8fed\u4ee3\u53bb\u566a\u8fc7\u7a0b\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u73b0\u6709\u57fa\u4e8e\u7279\u5f81\u7f13\u5b58\u548c\u91cd\u7528\u7684\u514d\u8bad\u7ec3\u52a0\u901f\u65b9\u6cd5\u5047\u8bbe\u65f6\u95f4\u7a33\u5b9a\u6027\uff0c\u4f46\u591a\u6b65\u91cd\u7528\u7279\u5f81\u53ef\u80fd\u5bfc\u81f4\u6f5c\u5728\u6f02\u79fb\u548c\u89c6\u89c9\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u63d0\u51faPrediT\u6846\u67b6\uff1a1) \u5c06\u7279\u5f81\u9884\u6d4b\u5efa\u6a21\u4e3a\u7ebf\u6027\u591a\u6b65\u95ee\u9898\uff0c\u4f7f\u7528\u7ecf\u5178\u7ebf\u6027\u591a\u6b65\u65b9\u6cd5\u4ece\u5386\u53f2\u4fe1\u606f\u9884\u6d4b\u672a\u6765\u6a21\u578b\u8f93\u51fa\uff1b2) \u5728\u9ad8\u52a8\u6001\u533a\u57df\u6fc0\u6d3b\u6821\u6b63\u5668\u9632\u6b62\u8bef\u5dee\u7d2f\u79ef\uff1b3) \u52a8\u6001\u6b65\u957f\u8c03\u5236\u673a\u5236\u901a\u8fc7\u76d1\u63a7\u7279\u5f81\u53d8\u5316\u7387\u81ea\u9002\u5e94\u8c03\u6574\u9884\u6d4b\u8303\u56f4\u3002", "result": "\u5728\u5404\u79cd\u57fa\u4e8eDiT\u7684\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe5.54\u500d\u7684\u5ef6\u8fdf\u964d\u4f4e\uff0c\u540c\u65f6\u8d28\u91cf\u9000\u5316\u53ef\u5ffd\u7565\u4e0d\u8ba1\u3002", "conclusion": "PrediT\u901a\u8fc7\u9884\u6d4b\u800c\u975e\u7b80\u5355\u91cd\u7528\u7684\u65b9\u5f0f\uff0c\u5728\u4fdd\u6301\u751f\u6210\u4fdd\u771f\u5ea6\u7684\u540c\u65f6\u663e\u8457\u52a0\u901f\u4e86DiT\u6a21\u578b\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.18094", "categories": ["cs.CV", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2602.18094", "abs": "https://arxiv.org/abs/2602.18094", "authors": ["Ling Lin", "Yang Bai", "Heng Su", "Congcong Zhu", "Yaoxing Wang", "Yang Zhou", "Huazhu Fu", "Jingrun Chen"], "title": "OODBench: Out-of-Distribution Benchmark for Large Vision-Language Models", "comment": "54 pages, 21 figures", "summary": "Existing Visual-Language Models (VLMs) have achieved significant progress by being trained on massive-scale datasets, typically under the assumption that data are independent and identically distributed (IID). However, in real-world scenarios, it is often impractical to expect that all data processed by an AI system satisfy this assumption. Furthermore, failure to appropriately handle out-of-distribution (OOD) objects may introduce safety risks in real-world applications (e.g., autonomous driving or medical assistance). Unfortunately, current research has not yet provided valid benchmarks that can comprehensively assess the performance of VLMs in response to OOD data. Therefore, we propose OODBench, a predominantly automated method with minimal human verification, for constructing new benchmarks and evaluating the ability of VLMs to process OOD data. OODBench contains 40K instance-level OOD instance-category pairs, and we show that current VLMs still exhibit notable performance degradation on OODBench, even when the underlying image categories are common. In addition, we propose a reliable automated assessment metric that employs a Basic-to-Advanced Progression of prompted questions to assess the impact of OOD data on questions of varying difficulty more fully. Lastly, we summarize substantial findings and insights to facilitate future research in the acquisition and evaluation of OOD data.", "AI": {"tldr": "OODBench\uff1a\u4e00\u4e2a\u81ea\u52a8\u5316\u6784\u5efa\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578bOOD\u6570\u636e\u8bc4\u4f30\u57fa\u51c6\uff0c\u5305\u542b40K\u5b9e\u4f8b\u7ea7OOD\u6570\u636e\u5bf9\uff0c\u63ed\u793a\u4e86\u73b0\u6709VLMs\u5728OOD\u6570\u636e\u4e0a\u6027\u80fd\u663e\u8457\u4e0b\u964d\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u5047\u8bbe\u6570\u636e\u72ec\u7acb\u540c\u5206\u5e03\uff0c\u4f46\u73b0\u5b9e\u5e94\u7528\u4e2d\u5e38\u9047\u5230\u5206\u5e03\u5916\u6570\u636e\uff0c\u8fd9\u53ef\u80fd\u5e26\u6765\u5b89\u5168\u98ce\u9669\u3002\u76ee\u524d\u7f3a\u4e4f\u5168\u9762\u8bc4\u4f30VLMs\u5904\u7406OOD\u6570\u636e\u80fd\u529b\u7684\u6709\u6548\u57fa\u51c6\u3002", "method": "\u63d0\u51faOODBench\u65b9\u6cd5\uff0c\u4ee5\u6700\u5c0f\u4eba\u5de5\u9a8c\u8bc1\u7684\u81ea\u52a8\u5316\u65b9\u5f0f\u6784\u5efa\u65b0\u57fa\u51c6\uff0c\u5305\u542b40K\u5b9e\u4f8b\u7ea7OOD\u5b9e\u4f8b-\u7c7b\u522b\u5bf9\uff0c\u5e76\u63d0\u51fa\u4ece\u57fa\u7840\u5230\u9ad8\u7ea7\u7684\u6e10\u8fdb\u5f0f\u63d0\u793a\u95ee\u9898\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5f53\u524dVLMs\u5373\u4f7f\u5728\u5e38\u89c1\u56fe\u50cf\u7c7b\u522b\u4e0a\uff0c\u5728OODBench\u4e0a\u4ecd\u8868\u73b0\u51fa\u663e\u8457\u6027\u80fd\u4e0b\u964d\u3002\u63d0\u51fa\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u6307\u6807\u80fd\u66f4\u5168\u9762\u5730\u8bc4\u4f30OOD\u6570\u636e\u5bf9\u4e0d\u540c\u96be\u5ea6\u95ee\u9898\u7684\u5f71\u54cd\u3002", "conclusion": "OODBench\u4e3a\u8bc4\u4f30VLMs\u5904\u7406OOD\u6570\u636e\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u57fa\u51c6\uff0c\u603b\u7ed3\u4e86\u91cd\u8981\u53d1\u73b0\u548c\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u672a\u6765OOD\u6570\u636e\u83b7\u53d6\u548c\u8bc4\u4f30\u7684\u7814\u7a76\u3002"}}
{"id": "2602.18178", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18178", "abs": "https://arxiv.org/abs/2602.18178", "authors": ["Poonam Poonam", "Pere-Pau V\u00e1zquez", "Timo Ropinski"], "title": "Evaluating Graphical Perception Capabilities of Vision Transformers", "comment": null, "summary": "Vision Transformers, ViTs, have emerged as a powerful alternative to convolutional neural networks, CNNs, in a variety of image-based tasks. While CNNs have previously been evaluated for their ability to perform graphical perception tasks, which are essential for interpreting visualizations, the perceptual capabilities of ViTs remain largely unexplored. In this work, we investigate the performance of ViTs in elementary visual judgment tasks inspired by the foundational studies of Cleveland and McGill, which quantified the accuracy of human perception across different visual encodings. Inspired by their study, we benchmark ViTs against CNNs and human participants in a series of controlled graphical perception tasks. Our results reveal that, although ViTs demonstrate strong performance in general vision tasks, their alignment with human-like graphical perception in the visualization domain is limited. This study highlights key perceptual gaps and points to important considerations for the application of ViTs in visualization systems and graphical perceptual modeling.", "AI": {"tldr": "ViTs\u5728\u53ef\u89c6\u5316\u56fe\u5f62\u611f\u77e5\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u5982\u4eba\u7c7b\uff0c\u4e0eCNN\u76f8\u6bd4\u4e5f\u5b58\u5728\u611f\u77e5\u5dee\u8ddd", "motivation": "\u867d\u7136ViTs\u5728\u591a\u79cd\u56fe\u50cf\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5728\u53ef\u89c6\u5316\u56fe\u5f62\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u5c1a\u672a\u88ab\u5145\u5206\u7814\u7a76\uff0c\u800c\u8fd9\u4e9b\u4efb\u52a1\u5bf9\u89e3\u91ca\u53ef\u89c6\u5316\u81f3\u5173\u91cd\u8981", "method": "\u57fa\u4e8eCleveland\u548cMcGill\u7684\u7ecf\u5178\u7814\u7a76\uff0c\u8bbe\u8ba1\u4e86\u4e00\u7cfb\u5217\u53d7\u63a7\u7684\u56fe\u5f62\u611f\u77e5\u4efb\u52a1\uff0c\u5c06ViTs\u4e0eCNNs\u548c\u4eba\u7c7b\u53c2\u4e0e\u8005\u7684\u8868\u73b0\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30", "result": "ViTs\u5728\u901a\u7528\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u53ef\u89c6\u5316\u9886\u57df\u7684\u7c7b\u4eba\u56fe\u5f62\u611f\u77e5\u80fd\u529b\u6709\u9650\uff0c\u4e0e\u4eba\u7c7b\u611f\u77e5\u5b58\u5728\u663e\u8457\u5dee\u8ddd", "conclusion": "ViTs\u5728\u53ef\u89c6\u5316\u7cfb\u7edf\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u9700\u8981\u8c28\u614e\u8003\u8651\u5176\u611f\u77e5\u5c40\u9650\u6027\uff0c\u8fd9\u5bf9\u53ef\u89c6\u5316\u7cfb\u7edf\u548c\u56fe\u5f62\u611f\u77e5\u5efa\u6a21\u6709\u91cd\u8981\u542f\u793a"}}
{"id": "2602.17778", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.17778", "abs": "https://arxiv.org/abs/2602.17778", "authors": ["Zachary Coalson", "Bo Fang", "Sanghyun Hong"], "title": "Asking Forever: Universal Activations Behind Turn Amplification in Conversational LLMs", "comment": "Pre-print", "summary": "Multi-turn interaction length is a dominant factor in the operational costs of conversational LLMs. In this work, we present a new failure mode in conversational LLMs: turn amplification, in which a model consistently prolongs multi-turn interactions without completing the underlying task. We show that an adversary can systematically exploit clarification-seeking behavior$-$commonly encouraged in multi-turn conversation settings$-$to scalably prolong interactions. Moving beyond prompt-level behaviors, we take a mechanistic perspective and identify a query-independent, universal activation subspace associated with clarification-seeking responses. Unlike prior cost-amplification attacks that rely on per-turn prompt optimization, our attack arises from conversational dynamics and persists across prompts and tasks. We show that this mechanism provides a scalable pathway to induce turn amplification: both supply-chain attacks via fine-tuning and runtime attacks through low-level parameter corruptions consistently shift models toward abstract, clarification-seeking behavior across prompts. Across multiple instruction-tuned LLMs and benchmarks, our attack substantially increases turn count while remaining compliant. We also show that existing defenses offer limited protection against this emerging class of failures.", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0\u5bf9\u8bdd\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\"\u8f6e\u6b21\u653e\u5927\"\u65b0\u6545\u969c\u6a21\u5f0f\uff0c\u653b\u51fb\u8005\u53ef\u7cfb\u7edf\u5229\u7528\u6f84\u6e05\u5bfb\u6c42\u884c\u4e3a\u5ef6\u957f\u591a\u8f6e\u5bf9\u8bdd\u800c\u4e0d\u5b8c\u6210\u4efb\u52a1\uff0c\u8fd9\u79cd\u653b\u51fb\u6e90\u4e8e\u5bf9\u8bdd\u52a8\u6001\u673a\u5236\u800c\u975e\u7279\u5b9a\u63d0\u793a\uff0c\u80fd\u901a\u8fc7\u5fae\u8c03\u6216\u53c2\u6570\u6c61\u67d3\u5b9e\u73b0\u89c4\u6a21\u5316\u653b\u51fb\u3002", "motivation": "\u591a\u8f6e\u4ea4\u4e92\u957f\u5ea6\u662f\u5bf9\u8bdd\u5927\u8bed\u8a00\u6a21\u578b\u8fd0\u8425\u6210\u672c\u7684\u4e3b\u8981\u56e0\u7d20\uff0c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u63d0\u793a\u5c42\u9762\u7684\u6210\u672c\u653e\u5927\u653b\u51fb\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5bf9\u8bdd\u52a8\u6001\u673a\u5236\u4e2d\u56fa\u6709\u6545\u969c\u6a21\u5f0f\u7684\u7814\u7a76\uff0c\u7279\u522b\u662f\u6a21\u578b\u53ef\u80fd\u88ab\u8bf1\u5bfc\u6301\u7eed\u5ef6\u957f\u5bf9\u8bdd\u800c\u4e0d\u5b8c\u6210\u4efb\u52a1\u7684\u7cfb\u7edf\u6027\u98ce\u9669\u3002", "method": "\u4ece\u673a\u5236\u89d2\u5ea6\u8bc6\u522b\u4e0e\u6f84\u6e05\u5bfb\u6c42\u54cd\u5e94\u76f8\u5173\u7684\u67e5\u8be2\u65e0\u5173\u901a\u7528\u6fc0\u6d3b\u5b50\u7a7a\u95f4\uff1b\u901a\u8fc7\u4f9b\u5e94\u94fe\u653b\u51fb\uff08\u5fae\u8c03\uff09\u548c\u8fd0\u884c\u65f6\u653b\u51fb\uff08\u4f4e\u7ea7\u53c2\u6570\u6c61\u67d3\uff09\u5b9e\u73b0\u89c4\u6a21\u5316\u8f6e\u6b21\u653e\u5927\uff1b\u5728\u591a\u4e2a\u6307\u4ee4\u8c03\u4f18\u5927\u8bed\u8a00\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u653b\u51fb\u6548\u679c\u3002", "result": "\u653b\u51fb\u80fd\u663e\u8457\u589e\u52a0\u5bf9\u8bdd\u8f6e\u6b21\u540c\u65f6\u4fdd\u6301\u5408\u89c4\u6027\uff1b\u73b0\u6709\u9632\u5fa1\u63aa\u65bd\u5bf9\u6b64\u7c7b\u6545\u969c\u4fdd\u62a4\u6709\u9650\uff1b\u653b\u51fb\u5177\u6709\u8de8\u63d0\u793a\u548c\u4efb\u52a1\u7684\u6301\u4e45\u6027\uff0c\u4e0d\u4f9d\u8d56\u6bcf\u8f6e\u63d0\u793a\u4f18\u5316\uff1b\u901a\u8fc7\u6fc0\u6d3b\u5b50\u7a7a\u95f4\u64cd\u7eb5\u80fd\u4e00\u81f4\u5730\u5c06\u6a21\u578b\u8f6c\u5411\u62bd\u8c61\u6f84\u6e05\u5bfb\u6c42\u884c\u4e3a\u3002", "conclusion": "\u5bf9\u8bdd\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u56fa\u6709\u7684\u8f6e\u6b21\u653e\u5927\u6545\u969c\u6a21\u5f0f\uff0c\u6e90\u4e8e\u5bf9\u8bdd\u52a8\u6001\u673a\u5236\u800c\u975e\u7279\u5b9a\u63d0\u793a\uff0c\u653b\u51fb\u8005\u53ef\u89c4\u6a21\u5316\u5229\u7528\u6b64\u6f0f\u6d1e\u589e\u52a0\u8fd0\u8425\u6210\u672c\uff0c\u73b0\u6709\u9632\u5fa1\u4e0d\u8db3\uff0c\u9700\u8981\u65b0\u7684\u5b89\u5168\u673a\u5236\u6765\u5e94\u5bf9\u6b64\u7c7b\u57fa\u4e8e\u5bf9\u8bdd\u52a8\u6001\u7684\u653b\u51fb\u3002"}}
{"id": "2602.18193", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18193", "abs": "https://arxiv.org/abs/2602.18193", "authors": ["Yiran Yang", "Zhaowei Liu", "Yuan Yuan", "Yukun Song", "Xiong Ma", "Yinghao Song", "Xiangji Zeng", "Lu Sun", "Yulu Wang", "Hai Zhou", "Shuai Cui", "Zhaohan Gong", "Jiefei Zhang"], "title": "BLM-Guard: Explainable Multimodal Ad Moderation with Chain-of-Thought and Policy-Aligned Rewards", "comment": "7 pages, 3 figures. To appear in AAAI 2026", "summary": "Short-video platforms now host vast multimodal ads whose deceptive visuals, speech and subtitles demand finer-grained, policy-driven moderation than community safety filters. We present BLM-Guard, a content-audit framework for commercial ads that fuses Chain-of-Thought reasoning with rule-based policy principles and a critic-guided reward. A rule-driven ICoT data-synthesis pipeline jump-starts training by generating structured scene descriptions, reasoning chains and labels, cutting annotation costs. Reinforcement learning then refines the model using a composite reward balancing causal coherence with policy adherence. A multitask architecture models intra-modal manipulations (e.g., exaggerated imagery) and cross-modal mismatches (e.g., subtitle-speech drift), boosting robustness. Experiments on real short-video ads show BLM-Guard surpasses strong baselines in accuracy, consistency and generalization.", "AI": {"tldr": "BLM-Guard\u662f\u4e00\u4e2a\u7528\u4e8e\u77ed\u89c6\u9891\u5e7f\u544a\u5185\u5bb9\u5ba1\u6838\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u601d\u7ef4\u94fe\u63a8\u7406\u3001\u89c4\u5219\u7b56\u7565\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5206\u6790\u68c0\u6d4b\u6b3a\u9a97\u6027\u5185\u5bb9", "motivation": "\u77ed\u89c6\u9891\u5e73\u53f0\u4e0a\u7684\u591a\u6a21\u6001\u5e7f\u544a\u5305\u542b\u6b3a\u9a97\u6027\u7684\u89c6\u89c9\u3001\u8bed\u97f3\u548c\u5b57\u5e55\u5185\u5bb9\uff0c\u9700\u8981\u6bd4\u793e\u533a\u5b89\u5168\u8fc7\u6ee4\u5668\u66f4\u7ec6\u7c92\u5ea6\u3001\u57fa\u4e8e\u7b56\u7565\u7684\u5ba1\u6838\u673a\u5236", "method": "\u878d\u5408\u601d\u7ef4\u94fe\u63a8\u7406\u4e0e\u57fa\u4e8e\u89c4\u5219\u7684\u7b56\u7565\u539f\u5219\uff0c\u91c7\u7528\u89c4\u5219\u9a71\u52a8\u7684ICoT\u6570\u636e\u5408\u6210\u7ba1\u9053\u751f\u6210\u7ed3\u6784\u5316\u573a\u666f\u63cf\u8ff0\u3001\u63a8\u7406\u94fe\u548c\u6807\u7b7e\uff0c\u7136\u540e\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f7f\u7528\u590d\u5408\u5956\u52b1\uff08\u5e73\u8861\u56e0\u679c\u4e00\u81f4\u6027\u4e0e\u7b56\u7565\u9075\u5faa\uff09\u4f18\u5316\u6a21\u578b\uff0c\u591a\u4efb\u52a1\u67b6\u6784\u5efa\u6a21\u6a21\u6001\u5185\u64cd\u7eb5\u548c\u8de8\u6a21\u6001\u4e0d\u5339\u914d", "result": "\u5728\u771f\u5b9e\u77ed\u89c6\u9891\u5e7f\u544a\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cBLM-Guard\u5728\u51c6\u786e\u6027\u3001\u4e00\u81f4\u6027\u548c\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u8d85\u8d8a\u4e86\u5f3a\u57fa\u7ebf\u6a21\u578b", "conclusion": "BLM-Guard\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u5546\u4e1a\u5e7f\u544a\u5185\u5bb9\u5ba1\u6838\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u89c4\u5219\u9a71\u52a8\u6570\u636e\u5408\u6210\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u591a\u6a21\u6001\u5206\u6790\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6b3a\u9a97\u6027\u5e7f\u544a\u5185\u5bb9\u7684\u7cbe\u51c6\u68c0\u6d4b"}}
{"id": "2602.17865", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17865", "abs": "https://arxiv.org/abs/2602.17865", "authors": ["Andrzej Podobi\u0144ski", "Jaros\u0142aw A. Chudziak"], "title": "Financial time series augmentation using transformer based GAN architecture", "comment": "This paper has been accepted for the upcoming 18th International Conference on Agents and Artificial Intelligence (ICAART-2026), Marbella, Spain. The final published version will appear in the official conference proceedings", "summary": "Time-series forecasting is a critical task across many domains, from engineering to economics, where accurate predictions drive strategic decisions. However, applying advanced deep learning models in challenging, volatile domains like finance is difficult due to the inherent limitation and dynamic nature of financial time series data. This scarcity often results in sub-optimal model training and poor generalization. The fundamental challenge lies in determining how to reliably augment scarce financial time series data to enhance the predictive accuracy of deep learning forecasting models. Our main contribution is a demonstration of how Generative Adversarial Networks (GANs) can effectively serve as a data augmentation tool to overcome data scarcity in the financial domain. Specifically, we show that training a Long Short-Term Memory (LSTM) forecasting model on a dataset augmented with synthetic data generated by a transformer-based GAN (TTS-GAN) significantly improves the forecasting accuracy compared to using real data alone. We confirm these results across different financial time series (Bitcoin and S\\&P500 price data) and various forecasting horizons. Furthermore, we propose a novel, time series specific quality metric that combines Dynamic Time Warping (DTW) and a modified Deep Dataset Dissimilarity Measure (DeD-iMs) to reliably monitor the training progress and evaluate the quality of the generated data. These findings provide compelling evidence for the benefits of GAN-based data augmentation in enhancing financial predictive capabilities.", "AI": {"tldr": "\u4f7f\u7528\u57fa\u4e8eTransformer\u7684GAN\u751f\u6210\u5408\u6210\u6570\u636e\u6765\u589e\u5f3a\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u663e\u8457\u63d0\u9ad8\u4e86LSTM\u9884\u6d4b\u6a21\u578b\u7684\u51c6\u786e\u6027", "motivation": "\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5177\u6709\u7a00\u7f3a\u6027\u548c\u52a8\u6001\u6027\uff0c\u5bfc\u81f4\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u4e0d\u5145\u5206\u3001\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u9700\u8981\u53ef\u9760\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u6765\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027", "method": "\u63d0\u51fa\u4f7f\u7528\u57fa\u4e8eTransformer\u7684GAN\uff08TTS-GAN\uff09\u751f\u6210\u5408\u6210\u6570\u636e\u6765\u589e\u5f3a\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\uff0c\u5e76\u8bad\u7ec3LSTM\u9884\u6d4b\u6a21\u578b\uff1b\u540c\u65f6\u63d0\u51fa\u7ed3\u5408\u52a8\u6001\u65f6\u95f4\u89c4\u6574\uff08DTW\uff09\u548c\u6539\u8fdb\u7684\u6df1\u5ea6\u6570\u636e\u96c6\u5dee\u5f02\u5ea6\u91cf\uff08DeD-iMs\uff09\u7684\u65f6\u95f4\u5e8f\u5217\u7279\u5b9a\u8d28\u91cf\u8bc4\u4f30\u6307\u6807", "result": "\u5728\u6bd4\u7279\u5e01\u548c\u6807\u666e500\u4ef7\u683c\u6570\u636e\u4e0a\uff0c\u4f7f\u7528GAN\u589e\u5f3a\u6570\u636e\u7684LSTM\u6a21\u578b\u9884\u6d4b\u51c6\u786e\u6027\u663e\u8457\u4f18\u4e8e\u4ec5\u4f7f\u7528\u771f\u5b9e\u6570\u636e\u7684\u6a21\u578b\uff0c\u4e14\u5728\u4e0d\u540c\u9884\u6d4b\u65f6\u95f4\u8303\u56f4\u5185\u90fd\u6709\u6548", "conclusion": "GAN\u4f5c\u4e3a\u6570\u636e\u589e\u5f3a\u5de5\u5177\u80fd\u6709\u6548\u514b\u670d\u91d1\u878d\u9886\u57df\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u63d0\u9ad8\u9884\u6d4b\u80fd\u529b\uff0c\u63d0\u51fa\u7684\u65f6\u95f4\u5e8f\u5217\u7279\u5b9a\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u80fd\u53ef\u9760\u76d1\u63a7\u8bad\u7ec3\u8fdb\u5c55\u548c\u8bc4\u4f30\u751f\u6210\u6570\u636e\u8d28\u91cf"}}
{"id": "2602.18199", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18199", "abs": "https://arxiv.org/abs/2602.18199", "authors": ["Gahyeon Shim", "Soogeun Park", "Hyemin Ahn"], "title": "A Self-Supervised Approach on Motion Calibration for Enhancing Physical Plausibility in Text-to-Motion", "comment": null, "summary": "Generating semantically aligned human motion from textual descriptions has made rapid progress, but ensuring both semantic and physical realism in motion remains a challenge. In this paper, we introduce the Distortion-aware Motion Calibrator (DMC), a post-hoc module that refines physically implausible motions (e.g., foot floating) while preserving semantic consistency with the original textual description. Rather than relying on complex physical modeling, we propose a self-supervised and data-driven approach, whereby DMC learns to obtain physically plausible motions when an intentionally distorted motion and the original textual descriptions are given as inputs. We evaluate DMC as a post-hoc module to improve motions obtained from various text-to-motion generation models and demonstrate its effectiveness in improving physical plausibility while enhancing semantic consistency. The experimental results show that DMC reduces FID score by 42.74% on T2M and 13.20% on T2M-GPT, while also achieving the highest R-Precision. When applied to high-quality models like MoMask, DMC improves the physical plausibility of motions by reducing penetration by 33.0% as well as adjusting floating artifacts closer to the ground-truth reference. These results highlight that DMC can serve as a promising post-hoc motion refinement framework for any kind of text-to-motion models by incorporating textual semantics and physical plausibility.", "AI": {"tldr": "DMC\u662f\u4e00\u4e2a\u540e\u5904\u7406\u6a21\u5757\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u6587\u672c\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u540c\u65f6\uff0c\u4fee\u6b63\u6587\u672c\u751f\u6210\u52a8\u4f5c\u4e2d\u7684\u7269\u7406\u4e0d\u5408\u7406\u73b0\u8c61\uff08\u5982\u811a\u90e8\u6f02\u6d6e\uff09", "motivation": "\u5f53\u524d\u6587\u672c\u751f\u6210\u4eba\u4f53\u52a8\u4f5c\u7684\u65b9\u6cd5\u5728\u8bed\u4e49\u5bf9\u9f50\u65b9\u9762\u8fdb\u5c55\u8fc5\u901f\uff0c\u4f46\u96be\u4ee5\u540c\u65f6\u4fdd\u8bc1\u8bed\u4e49\u548c\u7269\u7406\u771f\u5b9e\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u751f\u6210\u7684\u52a8\u4f5c\u5e38\u51fa\u73b0\u7269\u7406\u4e0d\u5408\u7406\u73b0\u8c61\uff08\u5982\u811a\u90e8\u6f02\u6d6e\uff09\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u7269\u7406\u5408\u7406\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faDistortion-aware Motion Calibrator (DMC)\uff0c\u8fd9\u662f\u4e00\u4e2a\u540e\u5904\u7406\u6a21\u5757\uff0c\u91c7\u7528\u81ea\u76d1\u7763\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u3002DMC\u5b66\u4e60\u5728\u7ed9\u5b9a\u6545\u610f\u626d\u66f2\u7684\u52a8\u4f5c\u548c\u539f\u59cb\u6587\u672c\u63cf\u8ff0\u7684\u60c5\u51b5\u4e0b\uff0c\u751f\u6210\u7269\u7406\u5408\u7406\u7684\u52a8\u4f5c\uff0c\u800c\u4e0d\u4f9d\u8d56\u590d\u6742\u7684\u7269\u7406\u5efa\u6a21\u3002", "result": "DMC\u663e\u8457\u63d0\u5347\u52a8\u4f5c\u8d28\u91cf\uff1a\u5728T2M\u4e0aFID\u5206\u6570\u964d\u4f4e42.74%\uff0c\u5728T2M-GPT\u4e0a\u964d\u4f4e13.20%\uff0c\u540c\u65f6\u8fbe\u5230\u6700\u9ad8\u7684R-Precision\u3002\u5e94\u7528\u4e8eMoMask\u7b49\u9ad8\u8d28\u91cf\u6a21\u578b\u65f6\uff0c\u7a7f\u900f\u7387\u964d\u4f4e33.0%\uff0c\u6f02\u6d6e\u4f2a\u5f71\u66f4\u63a5\u8fd1\u771f\u5b9e\u53c2\u8003\u3002DMC\u80fd\u4f5c\u4e3a\u901a\u7528\u540e\u5904\u7406\u6846\u67b6\u63d0\u5347\u5404\u79cd\u6587\u672c\u751f\u6210\u52a8\u4f5c\u6a21\u578b\u7684\u7269\u7406\u5408\u7406\u6027\u3002", "conclusion": "DMC\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684\u540e\u5904\u7406\u52a8\u4f5c\u4f18\u5316\u6846\u67b6\uff0c\u80fd\u591f\u4e3a\u4efb\u4f55\u6587\u672c\u751f\u6210\u52a8\u4f5c\u6a21\u578b\u540c\u65f6\u63d0\u5347\u6587\u672c\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u7269\u7406\u5408\u7406\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7269\u7406\u771f\u5b9e\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002"}}
{"id": "2602.17868", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17868", "abs": "https://arxiv.org/abs/2602.17868", "authors": ["Vasilii Feofanov", "Songkang Wen", "Jianfeng Zhang", "Lujia Pan", "Ievgen Redko"], "title": "MantisV2: Closing the Zero-Shot Gap in Time Series Classification with Synthetic Data and Test-Time Strategies", "comment": null, "summary": "Developing foundation models for time series classification is of high practical relevance, as such models can serve as universal feature extractors for diverse downstream tasks. Although early models such as Mantis have shown the promise of this approach, a substantial performance gap remained between frozen and fine-tuned encoders. In this work, we introduce methods that significantly strengthen zero-shot feature extraction for time series. First, we introduce Mantis+, a variant of Mantis pre-trained entirely on synthetic time series. Second, through controlled ablation studies, we refine the architecture and obtain MantisV2, an improved and more lightweight encoder. Third, we propose an enhanced test-time methodology that leverages intermediate-layer representations and refines output-token aggregation. In addition, we show that performance can be further improved via self-ensembling and cross-model embedding fusion. Extensive experiments on UCR, UEA, Human Activity Recognition (HAR) benchmarks, and EEG datasets show that MantisV2 and Mantis+ consistently outperform prior time series foundation models, achieving state-of-the-art zero-shot performance.", "AI": {"tldr": "MantisV2\u548cMantis+\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u57fa\u7840\u6a21\u578b\u7684\u96f6\u6837\u672c\u7279\u5f81\u63d0\u53d6\u80fd\u529b\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u9884\u8bad\u7ec3\u3001\u67b6\u6784\u4f18\u5316\u548c\u6d4b\u8bd5\u65f6\u65b9\u6cd5\u6539\u8fdb\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u5f00\u53d1\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u57fa\u7840\u6a21\u578b\u5177\u6709\u91cd\u8981\u5b9e\u8df5\u4ef7\u503c\uff0c\u53ef\u4f5c\u4e3a\u901a\u7528\u7279\u5f81\u63d0\u53d6\u5668\u7528\u4e8e\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002\u5c3d\u7ba1\u65e9\u671f\u6a21\u578b\u5982Mantis\u5df2\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u51bb\u7ed3\u7f16\u7801\u5668\u548c\u5fae\u8c03\u7f16\u7801\u5668\u4e4b\u95f4\u4ecd\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\u3002", "method": "1. \u5f15\u5165Mantis+\uff1a\u5b8c\u5168\u5728\u5408\u6210\u65f6\u95f4\u5e8f\u5217\u4e0a\u9884\u8bad\u7ec3\u7684Mantis\u53d8\u4f53\uff1b2. \u901a\u8fc7\u53d7\u63a7\u6d88\u878d\u7814\u7a76\u6539\u8fdb\u67b6\u6784\uff0c\u5f97\u5230\u66f4\u8f7b\u91cf\u5316\u7684MantisV2\u7f16\u7801\u5668\uff1b3. \u63d0\u51fa\u589e\u5f3a\u7684\u6d4b\u8bd5\u65f6\u65b9\u6cd5\uff0c\u5229\u7528\u4e2d\u95f4\u5c42\u8868\u793a\u5e76\u6539\u8fdb\u8f93\u51fa\u6807\u8bb0\u805a\u5408\uff1b4. \u901a\u8fc7\u81ea\u96c6\u6210\u548c\u8de8\u6a21\u578b\u5d4c\u5165\u878d\u5408\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "result": "\u5728UCR\u3001UEA\u3001\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u57fa\u51c6\u548cEEG\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMantisV2\u548cMantis+\u59cb\u7ec8\u4f18\u4e8e\u5148\u524d\u7684\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u663e\u8457\u52a0\u5f3a\u4e86\u65f6\u95f4\u5e8f\u5217\u7684\u96f6\u6837\u672c\u7279\u5f81\u63d0\u53d6\u80fd\u529b\uff0c\u901a\u8fc7\u5408\u6210\u6570\u636e\u9884\u8bad\u7ec3\u3001\u67b6\u6784\u4f18\u5316\u548c\u6d4b\u8bd5\u65f6\u65b9\u6cd5\u6539\u8fdb\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2602.17798", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17798", "abs": "https://arxiv.org/abs/2602.17798", "authors": ["Ibne Farabi Shihab", "Sanjeda Akter", "Anuj Sharma"], "title": "Grassmannian Mixture-of-Experts: Concentration-Controlled Routing on Subspace Manifolds", "comment": null, "summary": "Mixture-of-Experts models rely on learned routers to assign tokens to experts, yet standard softmax gating provides no principled mechanism to control the tradeoff between sparsity and utilization. We propose Grassmannian MoE (GrMoE), a routing framework that operates on the Grassmannian manifold of subspaces, where gating weights arise from the concentration parameters of Matrix Bingham distributions. This construction yields a single, interpretable knob -- the concentration matrix $\u039b$ -- that continuously controls routing entropy, replacing discrete top-$k$ selection with a smooth, geometrically principled sparsity mechanism. We further develop an amortized variational inference procedure for posterior routing distributions, enabling uncertainty-aware expert assignment that naturally resists expert collapse. We formally prove tight bounds relating the Bingham concentration spectrum to routing entropy, expected top-$k$ mass, and an exponential bound on expert collapse, establishing the first formal theory of concentration-controlled sparsity. On synthetic routing tasks, a 350M-parameter MoE language model with 8 experts, a 1.3B-parameter model with 16 experts, and a 2.7B-parameter model with 32 experts, GrMoE achieves 0\\% routing collapse across all seeds, comparable or better perplexity with 15--30\\% improved load balance, and a smooth monotonic relationship between concentration and effective sparsity that enables post-hoc sparsity tuning without retraining. Token-level analysis reveals that experts learn heterogeneous concentration values that correlate with linguistic specialization, providing interpretable routing behavior.", "AI": {"tldr": "GrMoE\u63d0\u51fa\u57fa\u4e8eGrassmann\u6d41\u5f62\u7684MoE\u8def\u7531\u6846\u67b6\uff0c\u7528\u77e9\u9635Bingham\u5206\u5e03\u7684\u6d53\u5ea6\u53c2\u6570\u63a7\u5236\u8def\u7531\u7a00\u758f\u6027\uff0c\u5b9e\u73b0\u8fde\u7eed\u53ef\u8c03\u7684\u7a00\u758f\u673a\u5236\uff0c\u907f\u514d\u4e13\u5bb6\u5d29\u6e83\u3002", "motivation": "\u4f20\u7edfMoE\u6a21\u578b\u4f7f\u7528softmax\u95e8\u63a7\u7f3a\u4e4f\u63a7\u5236\u7a00\u758f\u6027\u4e0e\u5229\u7528\u7387\u6743\u8861\u7684\u7406\u8bba\u673a\u5236\uff0c\u4e14\u5b58\u5728\u4e13\u5bb6\u5d29\u6e83\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u51e0\u4f55\u539f\u7406\u6e05\u6670\u3001\u53ef\u8fde\u7eed\u63a7\u5236\u7a00\u758f\u6027\u7684\u8def\u7531\u6846\u67b6\u3002", "method": "\u5728Grassmann\u6d41\u5f62\u5b50\u7a7a\u95f4\u4e0a\u6784\u5efa\u8def\u7531\u6846\u67b6\uff0c\u4f7f\u7528\u77e9\u9635Bingham\u5206\u5e03\u7684\u6d53\u5ea6\u53c2\u6570\u039b\u4f5c\u4e3a\u63a7\u5236\u7a00\u758f\u6027\u7684\u5355\u4e00\u53ef\u89e3\u91ca\u53c2\u6570\uff0c\u5f00\u53d1\u644a\u9500\u53d8\u5206\u63a8\u7406\u65b9\u6cd5\u4f30\u8ba1\u540e\u9a8c\u8def\u7531\u5206\u5e03\u3002", "result": "\u5728350M\u30011.3B\u30012.7B\u53c2\u6570\u7684MoE\u8bed\u8a00\u6a21\u578b\u4e0a\u5b9e\u73b00%\u8def\u7531\u5d29\u6e83\uff0c\u56f0\u60d1\u5ea6\u76f8\u5f53\u6216\u66f4\u597d\uff0c\u8d1f\u8f7d\u5747\u8861\u63d0\u534715-30%\uff0c\u6d53\u5ea6\u4e0e\u7a00\u758f\u6027\u5448\u5355\u8c03\u5173\u7cfb\uff0c\u652f\u6301\u8bad\u7ec3\u540e\u7a00\u758f\u6027\u8c03\u6574\u3002", "conclusion": "GrMoE\u63d0\u4f9b\u4e86\u57fa\u4e8e\u51e0\u4f55\u539f\u7406\u7684\u8def\u7531\u6846\u67b6\uff0c\u9996\u6b21\u5efa\u7acb\u4e86\u6d53\u5ea6\u63a7\u5236\u7a00\u758f\u6027\u7684\u5f62\u5f0f\u5316\u7406\u8bba\uff0c\u5b9e\u73b0\u4e86\u8fde\u7eed\u53ef\u8c03\u7684\u7a00\u758f\u673a\u5236\u548c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u8def\u7531\u5206\u914d\uff0c\u907f\u514d\u4e13\u5bb6\u5d29\u6e83\u3002"}}
{"id": "2602.18252", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18252", "abs": "https://arxiv.org/abs/2602.18252", "authors": ["Rishika Bhagwatkar", "Irina Rish", "Nicolas Flammarion", "Francesco Croce"], "title": "On the Adversarial Robustness of Discrete Image Tokenizers", "comment": null, "summary": "Discrete image tokenizers encode visual inputs as sequences of tokens from a finite vocabulary and are gaining popularity in multimodal systems, including encoder-only, encoder-decoder, and decoder-only models. However, unlike CLIP encoders, their vulnerability to adversarial attacks has not been explored. Ours being the first work studying this topic, we first formulate attacks that aim to perturb the features extracted by discrete tokenizers, and thus change the extracted tokens. These attacks are computationally efficient, application-agnostic, and effective across classification, multimodal retrieval, and captioning tasks. Second, to defend against this vulnerability, inspired by recent work on robust CLIP encoders, we fine-tune popular tokenizers with unsupervised adversarial training, keeping all other components frozen. While unsupervised and task-agnostic, our approach significantly improves robustness to both unsupervised and end-to-end supervised attacks and generalizes well to unseen tasks and data. Unlike supervised adversarial training, our approach can leverage unlabeled images, making it more versatile. Overall, our work highlights the critical role of tokenizer robustness in downstream tasks and presents an important step in the development of safe multimodal foundation models.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7814\u7a76\u4e86\u79bb\u6563\u56fe\u50cf\u5206\u8bcd\u5668\u5bf9\u6297\u653b\u51fb\u7684\u8106\u5f31\u6027\uff0c\u63d0\u51fa\u4e86\u9ad8\u6548\u7684\u5e94\u7528\u65e0\u5173\u653b\u51fb\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u65e0\u76d1\u7763\u5bf9\u6297\u8bad\u7ec3\u63d0\u5347\u5206\u8bcd\u5668\u9c81\u68d2\u6027\uff0c\u4e3a\u5b89\u5168\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u53d1\u5c55\u8fc8\u51fa\u91cd\u8981\u4e00\u6b65\u3002", "motivation": "\u79bb\u6563\u56fe\u50cf\u5206\u8bcd\u5668\u5728\u591a\u6a21\u6001\u7cfb\u7edf\u4e2d\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5176\u5bf9\u6297\u653b\u51fb\u8106\u5f31\u6027\u5c1a\u672a\u88ab\u63a2\u7d22\u3002\u4e0eCLIP\u7f16\u7801\u5668\u4e0d\u540c\uff0c\u79bb\u6563\u5206\u8bcd\u5668\u7684\u5b89\u5168\u6027\u7814\u7a76\u5b58\u5728\u7a7a\u767d\uff0c\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u4ee5\u5f00\u53d1\u66f4\u5b89\u5168\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u3002", "method": "1. \u9996\u5148\u5236\u5b9a\u653b\u51fb\u65b9\u6cd5\uff0c\u65e8\u5728\u6270\u52a8\u79bb\u6563\u5206\u8bcd\u5668\u63d0\u53d6\u7684\u7279\u5f81\u5e76\u6539\u53d8\u63d0\u53d6\u7684\u6807\u8bb0\uff0c\u8fd9\u4e9b\u653b\u51fb\u8ba1\u7b97\u9ad8\u6548\u3001\u5e94\u7528\u65e0\u5173\uff1b2. \u63d0\u51fa\u9632\u5fa1\u65b9\u6cd5\uff0c\u53d7\u9c81\u68d2CLIP\u7f16\u7801\u5668\u7814\u7a76\u542f\u53d1\uff0c\u901a\u8fc7\u65e0\u76d1\u7763\u5bf9\u6297\u8bad\u7ec3\u5fae\u8c03\u6d41\u884c\u5206\u8bcd\u5668\uff0c\u4fdd\u6301\u5176\u4ed6\u7ec4\u4ef6\u51bb\u7ed3\u3002", "result": "\u653b\u51fb\u65b9\u6cd5\u5728\u5206\u7c7b\u3001\u591a\u6a21\u6001\u68c0\u7d22\u548c\u5b57\u5e55\u751f\u6210\u4efb\u52a1\u4e2d\u5747\u6709\u6548\uff1b\u9632\u5fa1\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5bf9\u65e0\u76d1\u7763\u548c\u7aef\u5230\u7aef\u76d1\u7763\u653b\u51fb\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u80fd\u5f88\u597d\u5730\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u4efb\u52a1\u548c\u6570\u636e\uff0c\u4e14\u53ef\u5229\u7528\u672a\u6807\u8bb0\u56fe\u50cf\uff0c\u6bd4\u76d1\u7763\u5bf9\u6297\u8bad\u7ec3\u66f4\u901a\u7528\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u5206\u8bcd\u5668\u9c81\u68d2\u6027\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u4e3a\u5f00\u53d1\u5b89\u5168\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002\u65e0\u76d1\u7763\u5bf9\u6297\u8bad\u7ec3\u65b9\u6cd5\u4e3a\u63d0\u5347\u79bb\u6563\u5206\u8bcd\u5668\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2602.17809", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17809", "abs": "https://arxiv.org/abs/2602.17809", "authors": ["Ibne Farabi Shihab", "Sanjeda Akter", "Anuj Sharma"], "title": "Calibrated Adaptation: Bayesian Stiefel Manifold Priors for Reliable Parameter-Efficient Fine-Tuning", "comment": null, "summary": "Parameter-efficient fine-tuning methods such as LoRA enable practical adaptation of large language models but provide no principled uncertainty estimates, leading to poorly calibrated predictions and unreliable behavior under domain shift. We introduce Stiefel-Bayes Adapters (SBA), a Bayesian PEFT framework that places a Matrix Langevin prior over orthonormal adapter factors on the Stiefel manifold $\\St$ and performs approximate posterior inference via tangent space Laplace approximation with geodesic retraction. Unlike Gaussian priors in flat space projected onto orthogonality constraints, our prior on the manifold naturally encodes the inductive bias that adapter subspaces should be well conditioned and orthogonal, while the posterior provides calibrated predictive uncertainty without recalibration. We prove formally that the tangent space approximation strictly avoids the structural variance inflation inherent in projecting from ambient space, establishing a rigorous theoretical advantage for intrinsic manifold inference. Across GLUE and SuperGLUE benchmarks on RoBERTa-large, LLaMA-2-7B, LLaMA-2-13B, Mistral-7B, and Qwen2.5-7B, domain shift evaluations, selective prediction protocols, and an abstractive summarization task, SBA achieves task performance comparable to LoRA and DoRA while reducing Expected Calibration Error by 18 to 34\\% over deterministic baselines, improving selective prediction AUROC by 12 to 25\\% under domain shift, and outperforming deep ensembles of five LoRA models on OOD detection at a fraction of the parameter cost. Our results demonstrate that where you place uncertainty, on the right geometric structure, matters more than simply adding any Bayesian treatment to adapters.", "AI": {"tldr": "SBA\u662f\u4e00\u79cd\u8d1d\u53f6\u65af\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7Stiefel\u6d41\u5f62\u4e0a\u7684\u77e9\u9635Langevin\u5148\u9a8c\u548c\u5207\u7a7a\u95f4Laplace\u8fd1\u4f3c\uff0c\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u5728\u4fdd\u6301\u4efb\u52a1\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u6539\u5584\u6821\u51c6\u8bef\u5dee\u548c\u57df\u5916\u68c0\u6d4b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff08\u5982LoRA\uff09\u7f3a\u4e4f\u539f\u5219\u6027\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u5bfc\u81f4\u9884\u6d4b\u6821\u51c6\u4e0d\u4f73\u4e14\u5728\u9886\u57df\u504f\u79fb\u4e0b\u884c\u4e3a\u4e0d\u53ef\u9760\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u53c2\u6570\u6548\u7387\u53c8\u80fd\u63d0\u4f9b\u6821\u51c6\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u7684\u8d1d\u53f6\u65af\u6846\u67b6\u3002", "method": "\u63d0\u51faStiefel-Bayes Adapters (SBA)\uff1a\u5728Stiefel\u6d41\u5f62\u4e0a\u653e\u7f6e\u77e9\u9635Langevin\u5148\u9a8c\uff0c\u901a\u8fc7\u5207\u7a7a\u95f4Laplace\u8fd1\u4f3c\u4e0e\u6d4b\u5730\u7ebf\u56de\u7f29\u8fdb\u884c\u8fd1\u4f3c\u540e\u9a8c\u63a8\u65ad\u3002\u8be5\u65b9\u6cd5\u5728\u6d41\u5f62\u4e0a\u81ea\u7136\u7f16\u7801\u9002\u914d\u5668\u5b50\u7a7a\u95f4\u5e94\u826f\u597d\u6761\u4ef6\u4e14\u6b63\u4ea4\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u907f\u514d\u4ece\u73af\u5883\u7a7a\u95f4\u6295\u5f71\u5e26\u6765\u7684\u7ed3\u6784\u65b9\u5dee\u81a8\u80c0\u3002", "result": "\u5728RoBERTa-large\u3001LLaMA-2-7B/13B\u3001Mistral-7B\u3001Qwen2.5-7B\u7b49\u591a\u4e2a\u6a21\u578b\u4e0a\uff0cSBA\u5728GLUE\u548cSuperGLUE\u57fa\u51c6\u4e0a\u8fbe\u5230\u4e0eLoRA\u548cDoRA\u76f8\u5f53\u7684\u4efb\u52a1\u6027\u80fd\uff0c\u540c\u65f6\u5c06\u9884\u671f\u6821\u51c6\u8bef\u5dee\u964d\u4f4e18-34%\uff0c\u5728\u9886\u57df\u504f\u79fb\u4e0b\u5c06\u9009\u62e9\u6027\u9884\u6d4bAUROC\u63d0\u9ad812-25%\uff0c\u5e76\u4ee5\u8fdc\u4f4e\u4e8e\u4e94\u4e2aLoRA\u6a21\u578b\u96c6\u6210\u6210\u672c\u7684\u53c2\u6570\u5728OOD\u68c0\u6d4b\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u5728\u6b63\u786e\u7684\u51e0\u4f55\u7ed3\u6784\u4e0a\u653e\u7f6e\u4e0d\u786e\u5b9a\u6027\u6bd4\u7b80\u5355\u5730\u4e3a\u9002\u914d\u5668\u6dfb\u52a0\u4efb\u4f55\u8d1d\u53f6\u65af\u5904\u7406\u66f4\u91cd\u8981\u3002SBA\u8bc1\u660e\u4e86\u6d41\u5f62\u4e0a\u7684\u5185\u5728\u63a8\u65ad\u5177\u6709\u4e25\u683c\u7684\u7406\u8ad6\u4f18\u52bf\uff0c\u80fd\u591f\u63d0\u4f9b\u6821\u51c6\u7684\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u800c\u65e0\u9700\u91cd\u65b0\u6821\u51c6\u3002"}}
{"id": "2602.18282", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18282", "abs": "https://arxiv.org/abs/2602.18282", "authors": ["Shiyan Du", "Conghan Yue", "Xinyu Cheng", "Dongyu Zhang"], "title": "DEIG: Detail-Enhanced Instance Generation with Fine-Grained Semantic Control", "comment": "Accepted by AAAI 2026", "summary": "Multi-Instance Generation has advanced significantly in spatial placement and attribute binding. However, existing approaches still face challenges in fine-grained semantic understanding, particularly when dealing with complex textual descriptions. To overcome these limitations, we propose DEIG, a novel framework for fine-grained and controllable multi-instance generation. DEIG integrates an Instance Detail Extractor (IDE) that transforms text encoder embeddings into compact, instance-aware representations, and a Detail Fusion Module (DFM) that applies instance-based masked attention to prevent attribute leakage across instances. These components enable DEIG to generate visually coherent multi-instance scenes that precisely match rich, localized textual descriptions. To support fine-grained supervision, we construct a high-quality dataset with detailed, compositional instance captions generated by VLMs. We also introduce DEIG-Bench, a new benchmark with region-level annotations and multi-attribute prompts for both humans and objects. Experiments demonstrate that DEIG consistently outperforms existing approaches across multiple benchmarks in spatial consistency, semantic accuracy, and compositional generalization. Moreover, DEIG functions as a plug-and-play module, making it easily integrable into standard diffusion-based pipelines.", "AI": {"tldr": "DEIG\u662f\u4e00\u4e2a\u7528\u4e8e\u7ec6\u7c92\u5ea6\u53ef\u63a7\u591a\u5b9e\u4f8b\u751f\u6210\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u4f8b\u7ec6\u8282\u63d0\u53d6\u5668\u548c\u7ec6\u8282\u878d\u5408\u6a21\u5757\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u6587\u672c\u63cf\u8ff0\u4e0b\u7684\u8bed\u4e49\u7406\u89e3\u95ee\u9898\uff0c\u5728\u7a7a\u95f4\u4e00\u81f4\u6027\u3001\u8bed\u4e49\u51c6\u786e\u6027\u548c\u7ec4\u5408\u6cdb\u5316\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u591a\u5b9e\u4f8b\u751f\u6210\u65b9\u6cd5\u5728\u7a7a\u95f4\u5e03\u5c40\u548c\u5c5e\u6027\u7ed1\u5b9a\u65b9\u9762\u5df2\u6709\u8fdb\u5c55\uff0c\u4f46\u5728\u5904\u7406\u590d\u6742\u6587\u672c\u63cf\u8ff0\u65f6\u4ecd\u9762\u4e34\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7406\u89e3\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u9632\u6b62\u5b9e\u4f8b\u95f4\u5c5e\u6027\u6cc4\u6f0f\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faDEIG\u6846\u67b6\uff0c\u5305\u542b\u5b9e\u4f8b\u7ec6\u8282\u63d0\u53d6\u5668\uff08\u5c06\u6587\u672c\u7f16\u7801\u5668\u5d4c\u5165\u8f6c\u6362\u4e3a\u7d27\u51d1\u7684\u5b9e\u4f8b\u611f\u77e5\u8868\u793a\uff09\u548c\u7ec6\u8282\u878d\u5408\u6a21\u5757\uff08\u5e94\u7528\u57fa\u4e8e\u5b9e\u4f8b\u7684\u63a9\u7801\u6ce8\u610f\u529b\u9632\u6b62\u5c5e\u6027\u6cc4\u6f0f\uff09\uff0c\u5e76\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u548cDEIG-Bench\u57fa\u51c6\u3002", "result": "DEIG\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5728\u7a7a\u95f4\u4e00\u81f4\u6027\u3001\u8bed\u4e49\u51c6\u786e\u6027\u548c\u7ec4\u5408\u6cdb\u5316\u65b9\u9762\u4e00\u81f4\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u53ef\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\u8f7b\u677e\u96c6\u6210\u5230\u6807\u51c6\u6269\u6563\u7ba1\u9053\u4e2d\u3002", "conclusion": "DEIG\u901a\u8fc7\u7ec6\u7c92\u5ea6\u8bed\u4e49\u7406\u89e3\u548c\u5c5e\u6027\u6cc4\u6f0f\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u591a\u5b9e\u4f8b\u751f\u6210\uff0c\u4e3a\u53ef\u63a7\u573a\u666f\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.17888", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17888", "abs": "https://arxiv.org/abs/2602.17888", "authors": ["Sayeed Shafayet Chowdhury", "Karen D'Souza", "V. Siva Kakumani", "Snehasis Mukhopadhyay", "Shiaofen Fang", "Rodney J. Schlosser", "Daniel M. Beswick", "Jeremiah A. Alt", "Jess C. Mace", "Zachary M. Soler", "Timothy L. Smith", "Vijay R. Ramakrishnan"], "title": "Machine Learning Based Prediction of Surgical Outcomes in Chronic Rhinosinusitis from Clinical Data", "comment": null, "summary": "Artificial intelligence (AI) has increasingly transformed medical prognostics by enabling rapid and accurate analysis across imaging and pathology. However, the investigation of machine learning predictions applied to prospectively collected, standardized data from observational clinical intervention trials remains underexplored, despite its potential to reduce costs and improve patient outcomes. Chronic rhinosinusitis (CRS), a persistent inflammatory disease of the paranasal sinuses lasting more than three months, imposes a substantial burden on quality of life (QoL) and societal cost. Although many patients respond to medical therapy, others with refractory symptoms often pursue surgical intervention. Surgical decision-making in CRS is complex, as it must weigh known procedural risks against uncertain individualized outcomes. In this study, we evaluated supervised machine learning models for predicting surgical benefit in CRS, using the Sino-Nasal Outcome Test-22 (SNOT-22) as the primary patient-reported outcome. Our prospectively collected cohort from an observational intervention trial comprised patients who all underwent surgery; we investigated whether models trained only on preoperative data could identify patients who might not have been recommended surgery prior to the procedure. Across multiple algorithms, including an ensemble approach, our best model achieved approximately 85% classification accuracy, providing accurate and interpretable predictions of surgical candidacy. Moreover, on a held-out set of 30 cases spanning mixed difficulty, our model achieved 80% accuracy, exceeding the average prediction accuracy of expert clinicians (75.6%), demonstrating its potential to augment clinical decision-making and support personalized CRS care.", "AI": {"tldr": "\u8be5\u7814\u7a76\u4f7f\u7528\u76d1\u7763\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u6162\u6027\u9f3b\u7aa6\u708e\u60a3\u8005\u7684\u624b\u672f\u83b7\u76ca\uff0c\u57fa\u4e8e\u524d\u77bb\u6027\u6536\u96c6\u7684\u89c2\u5bdf\u6027\u4e34\u5e8a\u8bd5\u9a8c\u6570\u636e\uff0c\u6a21\u578b\u51c6\u786e\u7387\u8fbe85%\uff0c\u572830\u4e2a\u75c5\u4f8b\u6d4b\u8bd5\u4e2d\u51c6\u786e\u738780%\uff0c\u8d85\u8fc7\u4e34\u5e8a\u4e13\u5bb6\u5e73\u5747\u51c6\u786e\u738775.6%\u3002", "motivation": "\u5c3d\u7ba1\u4eba\u5de5\u667a\u80fd\u5728\u533b\u7597\u9884\u540e\u9886\u57df\u53d1\u5c55\u8fc5\u901f\uff0c\u4f46\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u5728\u89c2\u5bdf\u6027\u4e34\u5e8a\u8bd5\u9a8c\u524d\u77bb\u6027\u6807\u51c6\u5316\u6570\u636e\u4e2d\u7684\u5e94\u7528\u4ecd\u5f85\u63a2\u7d22\u3002\u6162\u6027\u9f3b\u7aa6\u708e\u4f5c\u4e3a\u4e00\u79cd\u6301\u7eed\u708e\u75c7\u6027\u75be\u75c5\uff0c\u5bf9\u60a3\u8005\u751f\u6d3b\u8d28\u91cf\u548c\u793e\u4f1a\u6210\u672c\u9020\u6210\u91cd\u5927\u8d1f\u62c5\uff0c\u800c\u624b\u672f\u51b3\u7b56\u590d\u6742\uff0c\u9700\u8981\u6743\u8861\u5df2\u77e5\u98ce\u9669\u4e0e\u4e0d\u786e\u5b9a\u7684\u4e2a\u4f53\u5316\u7ed3\u679c\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u76d1\u7763\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u6162\u6027\u9f3b\u7aa6\u708e\u60a3\u8005\u7684\u624b\u672f\u83b7\u76ca\uff0c\u4ee5Sino-Nasal Outcome Test-22\uff08SNOT-22\uff09\u4f5c\u4e3a\u4e3b\u8981\u60a3\u8005\u62a5\u544a\u7ed3\u5c40\u6307\u6807\u3002\u57fa\u4e8e\u524d\u77bb\u6027\u6536\u96c6\u7684\u89c2\u5bdf\u6027\u5e72\u9884\u8bd5\u9a8c\u961f\u5217\u6570\u636e\uff0c\u6240\u6709\u60a3\u8005\u5747\u63a5\u53d7\u624b\u672f\uff0c\u7814\u7a76\u63a2\u7d22\u4ec5\u4f7f\u7528\u672f\u524d\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u662f\u5426\u80fd\u8bc6\u522b\u90a3\u4e9b\u672f\u524d\u53ef\u80fd\u4e0d\u88ab\u63a8\u8350\u624b\u672f\u7684\u60a3\u8005\u3002\u91c7\u7528\u591a\u79cd\u7b97\u6cd5\u5305\u62ec\u96c6\u6210\u65b9\u6cd5\u3002", "result": "\u6700\u4f73\u6a21\u578b\u8fbe\u5230\u7ea685%\u7684\u5206\u7c7b\u51c6\u786e\u7387\uff0c\u80fd\u591f\u63d0\u4f9b\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u624b\u672f\u5019\u9009\u9884\u6d4b\u3002\u5728\u5305\u542b30\u4e2a\u6df7\u5408\u96be\u5ea6\u75c5\u4f8b\u7684\u4fdd\u7559\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u6a21\u578b\u51c6\u786e\u7387\u8fbe80%\uff0c\u8d85\u8fc7\u4e13\u5bb6\u4e34\u5e8a\u533b\u751f\u5e73\u5747\u9884\u6d4b\u51c6\u786e\u7387\uff0875.6%\uff09\u3002", "conclusion": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u9884\u6d4b\u6162\u6027\u9f3b\u7aa6\u708e\u624b\u672f\u83b7\u76ca\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u51c6\u786e\u7387\u8d85\u8fc7\u4e34\u5e8a\u4e13\u5bb6\uff0c\u5177\u6709\u589e\u5f3a\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u548c\u4fc3\u8fdb\u4e2a\u4f53\u5316\u6162\u6027\u9f3b\u7aa6\u708e\u62a4\u7406\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.18309", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18309", "abs": "https://arxiv.org/abs/2602.18309", "authors": ["Ziyue Liu", "Davide Talon", "Federico Girella", "Zanxi Ruan", "Mattia Mondo", "Loris Bazzani", "Yiming Wang", "Marco Cristani"], "title": "Multi-Level Conditioning by Pairing Localized Text and Sketch for Fashion Image Generation", "comment": "Project page: https://intelligolabs.github.io/lots/", "summary": "Sketches offer designers a concise yet expressive medium for early-stage fashion ideation by specifying structure, silhouette, and spatial relationships, while textual descriptions complement sketches to convey material, color, and stylistic details. Effectively combining textual and visual modalities requires adherence to the sketch visual structure when leveraging the guidance of localized attributes from text. We present LOcalized Text and Sketch with multi-level guidance (LOTS), a framework that enhances fashion image generation by combining global sketch guidance with multiple localized sketch-text pairs. LOTS employs a Multi-level Conditioning Stage to independently encode local features within a shared latent space while maintaining global structural coordination. Then, the Diffusion Pair Guidance stage integrates both local and global conditioning via attention-based guidance within the diffusion model's multi-step denoising process. To validate our method, we develop Sketchy, the first fashion dataset where multiple text-sketch pairs are provided per image. Sketchy provides high-quality, clean sketches with a professional look and consistent structure. To assess robustness beyond this setting, we also include an \"in the wild\" split with non-expert sketches, featuring higher variability and imperfections. Experiments demonstrate that our method strengthens global structural adherence while leveraging richer localized semantic guidance, achieving improvement over state-of-the-art. The dataset, platform, and code are publicly available.", "AI": {"tldr": "LOTS\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u5168\u5c40\u8349\u56fe\u5f15\u5bfc\u548c\u591a\u4e2a\u5c40\u90e8\u8349\u56fe-\u6587\u672c\u5bf9\u6765\u589e\u5f3a\u65f6\u5c1a\u56fe\u50cf\u751f\u6210\uff0c\u5728\u4fdd\u6301\u5168\u5c40\u7ed3\u6784\u4e00\u81f4\u6027\u7684\u540c\u65f6\u5229\u7528\u5c40\u90e8\u8bed\u4e49\u6307\u5bfc\u3002", "motivation": "\u8349\u56fe\u4e3a\u8bbe\u8ba1\u5e08\u63d0\u4f9b\u4e86\u65e9\u671f\u65f6\u5c1a\u6982\u5ff5\u5316\u7684\u7b80\u6d01\u8868\u8fbe\u5a92\u4ecb\uff0c\u800c\u6587\u672c\u63cf\u8ff0\u5219\u8865\u5145\u4e86\u6750\u6599\u3001\u989c\u8272\u548c\u98ce\u683c\u7ec6\u8282\u3002\u6709\u6548\u7ed3\u5408\u6587\u672c\u548c\u89c6\u89c9\u6a21\u6001\u9700\u8981\u5728\u5229\u7528\u6587\u672c\u5c40\u90e8\u5c5e\u6027\u6307\u5bfc\u65f6\u9075\u5b88\u8349\u56fe\u89c6\u89c9\u7ed3\u6784\u3002", "method": "\u63d0\u51faLOTS\u6846\u67b6\uff0c\u5305\u542b\u591a\u7ea7\u6761\u4ef6\u9636\u6bb5\u72ec\u7acb\u7f16\u7801\u5c40\u90e8\u7279\u5f81\u4e8e\u5171\u4eab\u6f5c\u5728\u7a7a\u95f4\uff0c\u4ee5\u53ca\u6269\u6563\u5bf9\u5f15\u5bfc\u9636\u6bb5\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u5728\u6269\u6563\u6a21\u578b\u53bb\u566a\u8fc7\u7a0b\u4e2d\u6574\u5408\u5c40\u90e8\u548c\u5168\u5c40\u6761\u4ef6\u3002\u8fd8\u521b\u5efa\u4e86Sketchy\u6570\u636e\u96c6\uff0c\u5305\u542b\u4e13\u4e1a\u8349\u56fe\u548c\u975e\u4e13\u4e1a\u8349\u56fe\u4e24\u4e2a\u7248\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u5168\u5c40\u7ed3\u6784\u4e00\u81f4\u6027\u7684\u540c\u65f6\u5229\u7528\u66f4\u4e30\u5bcc\u7684\u5c40\u90e8\u8bed\u4e49\u6307\u5bfc\uff0c\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u6709\u6240\u6539\u8fdb\u3002\u521b\u5efa\u4e86\u9996\u4e2a\u5305\u542b\u591a\u4e2a\u6587\u672c-\u8349\u56fe\u5bf9\u7684\u65f6\u5c1a\u6570\u636e\u96c6\u3002", "conclusion": "LOTS\u6846\u67b6\u901a\u8fc7\u591a\u7ea7\u8349\u56fe-\u6587\u672c\u5f15\u5bfc\u589e\u5f3a\u4e86\u65f6\u5c1a\u56fe\u50cf\u751f\u6210\uff0c\u5728\u4e13\u4e1a\u548c\u975e\u4e13\u4e1a\u8349\u56fe\u8bbe\u7f6e\u4e0b\u90fd\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u4e3a\u65f6\u5c1a\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u591a\u6a21\u6001\u751f\u6210\u5de5\u5177\u3002"}}
{"id": "2602.17930", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17930", "abs": "https://arxiv.org/abs/2602.17930", "authors": ["Narjes Nourzad", "Carlee Joe-Wong"], "title": "MIRA: Memory-Integrated Reinforcement Learning Agent with Limited LLM Guidance", "comment": "International Conference on Learning Representations (ICLR'26)", "summary": "Reinforcement learning (RL) agents often suffer from high sample complexity in sparse or delayed reward settings due to limited prior structure. Large language models (LLMs) can provide subgoal decompositions, plausible trajectories, and abstract priors that facilitate early learning. However, heavy reliance on LLM supervision introduces scalability constraints and dependence on potentially unreliable signals. We propose MIRA (Memory-Integrated Reinforcement Learning Agent), which incorporates a structured, evolving memory graph to guide early training. The graph stores decision-relevant information, including trajectory segments and subgoal structures, and is constructed from both the agent's high-return experiences and LLM outputs. This design amortizes LLM queries into a persistent memory rather than requiring continuous real-time supervision. From this memory graph, we derive a utility signal that softly adjusts advantage estimation to influence policy updates without modifying the underlying reward function. As training progresses, the agent's policy gradually surpasses the initial LLM-derived priors, and the utility term decays, preserving standard convergence guarantees. We provide theoretical analysis showing that utility-based shaping improves early-stage learning in sparse-reward environments. Empirically, MIRA outperforms RL baselines and achieves returns comparable to approaches that rely on frequent LLM supervision, while requiring substantially fewer online LLM queries. Project webpage: https://narjesno.github.io/MIRA/", "AI": {"tldr": "MIRA\u662f\u4e00\u79cd\u7ed3\u5408LLM\u5148\u9a8c\u77e5\u8bc6\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u8bb0\u5fc6\u56fe\u6765\u6307\u5bfc\u65e9\u671f\u8bad\u7ec3\uff0c\u51cf\u5c11\u5bf9\u5b9e\u65f6LLM\u76d1\u7763\u7684\u4f9d\u8d56\uff0c\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u7a00\u758f\u6216\u5ef6\u8fdf\u5956\u52b1\u73af\u5883\u4e2d\u6837\u672c\u6548\u7387\u4f4e\uff0c\u800cLLM\u867d\u7136\u80fd\u63d0\u4f9b\u5148\u9a8c\u77e5\u8bc6\uff08\u5982\u5b50\u76ee\u6807\u5206\u89e3\u3001\u8f68\u8ff9\u89c4\u5212\uff09\uff0c\u4f46\u8fc7\u5ea6\u4f9d\u8d56LLM\u76d1\u7763\u4f1a\u5bfc\u81f4\u53ef\u6269\u5c55\u6027\u95ee\u9898\u548c\u4e0d\u53ef\u9760\u4fe1\u53f7\u4f9d\u8d56\u3002", "method": "\u63d0\u51faMIRA\u6846\u67b6\uff0c\u6784\u5efa\u7ed3\u6784\u5316\u8bb0\u5fc6\u56fe\u5b58\u50a8\u51b3\u7b56\u76f8\u5173\u4fe1\u606f\uff08\u8f68\u8ff9\u7247\u6bb5\u3001\u5b50\u76ee\u6807\u7ed3\u6784\uff09\uff0c\u7ed3\u5408\u667a\u80fd\u4f53\u9ad8\u56de\u62a5\u7ecf\u9a8c\u548cLLM\u8f93\u51fa\u3002\u4ece\u8bb0\u5fc6\u56fe\u63a8\u5bfc\u6548\u7528\u4fe1\u53f7\uff0c\u8f6f\u8c03\u6574\u4f18\u52bf\u4f30\u8ba1\u4ee5\u5f71\u54cd\u7b56\u7565\u66f4\u65b0\uff0c\u800c\u4e0d\u6539\u53d8\u57fa\u7840\u5956\u52b1\u51fd\u6570\u3002\u968f\u7740\u8bad\u7ec3\u8fdb\u884c\uff0c\u6548\u7528\u9879\u8870\u51cf\uff0c\u4fdd\u7559\u6807\u51c6\u6536\u655b\u4fdd\u8bc1\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\u57fa\u4e8e\u6548\u7528\u7684\u5851\u5f62\u6539\u8fdb\u4e86\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u7684\u65e9\u671f\u5b66\u4e60\u3002\u5b9e\u8bc1\u663e\u793aMIRA\u4f18\u4e8eRL\u57fa\u7ebf\uff0c\u8fbe\u5230\u4e0e\u9891\u7e41LLM\u76d1\u7763\u65b9\u6cd5\u76f8\u5f53\u7684\u56de\u62a5\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u5728\u7ebfLLM\u67e5\u8be2\u6b21\u6570\u3002", "conclusion": "MIRA\u901a\u8fc7\u5c06LLM\u5148\u9a8c\u77e5\u8bc6\u644a\u9500\u5230\u6301\u4e45\u8bb0\u5fc6\u4e2d\uff0c\u51cf\u5c11\u4e86\u5b9e\u65f6\u76d1\u7763\u9700\u6c42\uff0c\u5728\u4fdd\u6301\u6536\u655b\u4fdd\u8bc1\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u7684\u5b66\u4e60\u6548\u7387\uff0c\u5e73\u8861\u4e86LLM\u6307\u5bfc\u4e0e\u81ea\u4e3b\u5b66\u4e60\u7684\u4f18\u52bf\u3002"}}
{"id": "2602.17829", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17829", "abs": "https://arxiv.org/abs/2602.17829", "authors": ["Preetom Biswas", "Giulia Pedrielli", "K. Sel\u00e7uk Candan"], "title": "Causality by Abstraction: Symbolic Rule Learning in Multivariate Timeseries with Large Language Models", "comment": null, "summary": "Inferring causal relations in timeseries data with delayed effects is a fundamental challenge, especially when the underlying system exhibits complex dynamics that cannot be captured by simple functional mappings. Traditional approaches often fail to produce generalized and interpretable explanations, as multiple distinct input trajectories may yield nearly indistinguishable outputs. In this work, we present ruleXplain, a framework that leverages Large Language Models (LLMs) to extract formal explanations for input-output relations in simulation-driven dynamical systems. Our method introduces a constrained symbolic rule language with temporal operators and delay semantics, enabling LLMs to generate verifiable causal rules through structured prompting. ruleXplain relies on the availability of a principled model (e.g., a simulator) that maps multivariate input time series to output time series. Within ruleXplain, the simulator is used to generate diverse counterfactual input trajectories that yield similar target output, serving as candidate explanations. Such counterfactual inputs are clustered and provided as context to the LLM, which is tasked with the generation of symbolic rules encoding the joint temporal trends responsible for the patterns observable in the output times series. A closed-loop refinement process ensures rule consistency and semantic validity. We validate the framework using the PySIRTEM epidemic simulator, mapping testing rate inputs to daily infection counts; and the EnergyPlus building energy simulator, observing temperature and solar irradiance inputs to electricity needs. For validation, we perform three classes of experiments: (1) the efficacy of the ruleset through input reconstruction; (2) ablation studies evaluating the causal encoding of the ruleset; and (3) generalization tests of the extracted rules across unseen output trends with varying phase dynamics.", "AI": {"tldr": "ruleXplain\u662f\u4e00\u4e2a\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u4eff\u771f\u9a71\u52a8\u52a8\u529b\u7cfb\u7edf\u4e2d\u63d0\u53d6\u5f62\u5f0f\u5316\u89e3\u91ca\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ea6\u675f\u7b26\u53f7\u89c4\u5219\u8bed\u8a00\u548c\u65f6\u6001\u7b97\u5b50\u751f\u6210\u53ef\u9a8c\u8bc1\u7684\u56e0\u679c\u89c4\u5219\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u5177\u6709\u5ef6\u8fdf\u6548\u5e94\u7684\u65f6\u5e8f\u6570\u636e\u56e0\u679c\u63a8\u65ad\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5f53\u591a\u4e2a\u4e0d\u540c\u7684\u8f93\u5165\u8f68\u8ff9\u4ea7\u751f\u51e0\u4e4e\u65e0\u6cd5\u533a\u5206\u7684\u8f93\u51fa\u65f6\uff0c\u96be\u4ee5\u63d0\u4f9b\u6cdb\u5316\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u91ca\u3002", "method": "\u63d0\u51faruleXplain\u6846\u67b6\uff0c\u5229\u7528LLMs\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u751f\u6210\u5305\u542b\u65f6\u6001\u7b97\u5b50\u548c\u5ef6\u8fdf\u8bed\u4e49\u7684\u7ea6\u675f\u7b26\u53f7\u89c4\u5219\u8bed\u8a00\u3002\u4f7f\u7528\u4eff\u771f\u5668\u751f\u6210\u591a\u6837\u5316\u7684\u53cd\u4e8b\u5b9e\u8f93\u5165\u8f68\u8ff9\uff0c\u805a\u7c7b\u540e\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u63d0\u4f9b\u7ed9LLM\uff0c\u901a\u8fc7\u95ed\u73af\u7cbe\u70bc\u8fc7\u7a0b\u786e\u4fdd\u89c4\u5219\u4e00\u81f4\u6027\u548c\u8bed\u4e49\u6709\u6548\u6027\u3002", "result": "\u5728PySIRTEM\u6d41\u884c\u75c5\u6a21\u62df\u5668\u548cEnergyPlus\u5efa\u7b51\u80fd\u6e90\u6a21\u62df\u5668\u4e0a\u9a8c\u8bc1\u4e86\u6846\u67b6\u6709\u6548\u6027\uff0c\u901a\u8fc7\u4e09\u7c7b\u5b9e\u9a8c\u8bc4\u4f30\uff1a\u89c4\u5219\u96c6\u7684\u8f93\u5165\u91cd\u6784\u6548\u679c\u3001\u56e0\u679c\u7f16\u7801\u6d88\u878d\u7814\u7a76\u4ee5\u53ca\u63d0\u53d6\u89c4\u5219\u5728\u4e0d\u540c\u76f8\u4f4d\u52a8\u6001\u7684\u672a\u89c1\u8f93\u51fa\u8d8b\u52bf\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "ruleXplain\u80fd\u591f\u4ece\u590d\u6742\u52a8\u529b\u7cfb\u7edf\u4e2d\u63d0\u53d6\u5f62\u5f0f\u5316\u3001\u53ef\u9a8c\u8bc1\u7684\u56e0\u679c\u89c4\u5219\uff0c\u4e3a\u5177\u6709\u5ef6\u8fdf\u6548\u5e94\u7684\u65f6\u5e8f\u6570\u636e\u63d0\u4f9b\u6cdb\u5316\u4e14\u53ef\u89e3\u91ca\u7684\u56e0\u679c\u63a8\u65ad\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.18314", "categories": ["cs.CV", "cs.GR", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18314", "abs": "https://arxiv.org/abs/2602.18314", "authors": ["Tianyi Song", "Danail Stoyanov", "Evangelos Mazomenos", "Francisco Vasconcelos"], "title": "Diff2DGS: Reliable Reconstruction of Occluded Surgical Scenes via 2D Gaussian Splatting", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Real-time reconstruction of deformable surgical scenes is vital for advancing robotic surgery, improving surgeon guidance, and enabling automation. Recent methods achieve dense reconstructions from da Vinci robotic surgery videos, with Gaussian Splatting (GS) offering real-time performance via graphics acceleration. However, reconstruction quality in occluded regions remains limited, and depth accuracy has not been fully assessed, as benchmarks like EndoNeRF and StereoMIS lack 3D ground truth. We propose Diff2DGS, a novel two-stage framework for reliable 3D reconstruction of occluded surgical scenes. In the first stage, a diffusion-based video module with temporal priors inpaints tissue occluded by instruments with high spatial-temporal consistency. In the second stage, we adapt 2D Gaussian Splatting (2DGS) with a Learnable Deformation Model (LDM) to capture dynamic tissue deformation and anatomical geometry. We also extend evaluation beyond prior image-quality metrics by performing quantitative depth accuracy analysis on the SCARED dataset. Diff2DGS outperforms state-of-the-art approaches in both appearance and geometry, reaching 38.02 dB PSNR on EndoNeRF and 34.40 dB on StereoMIS. Furthermore, our experiments demonstrate that optimizing for image quality alone does not necessarily translate into optimal 3D reconstruction accuracy. To address this, we further optimize the depth quality of the reconstructed 3D results, ensuring more faithful geometry in addition to high-fidelity appearance.", "AI": {"tldr": "Diff2DGS\uff1a\u4e00\u79cd\u7528\u4e8e\u624b\u672f\u573a\u666f\u91cd\u5efa\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u4fee\u590d\u88ab\u5668\u68b0\u906e\u6321\u7684\u7ec4\u7ec7\uff0c\u5e76\u4f7f\u7528\u53ef\u5b66\u4e60\u53d8\u5f62\u6a21\u578b\u76842D\u9ad8\u65af\u6e85\u5c04\u6765\u6355\u6349\u52a8\u6001\u7ec4\u7ec7\u53d8\u5f62\uff0c\u5728\u56fe\u50cf\u8d28\u91cf\u548c\u6df1\u5ea6\u7cbe\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5b9e\u65f6\u91cd\u5efa\u53ef\u53d8\u5f62\u624b\u672f\u573a\u666f\u5bf9\u4e8e\u63a8\u8fdb\u673a\u5668\u4eba\u624b\u672f\u3001\u6539\u5584\u5916\u79d1\u533b\u751f\u5f15\u5bfc\u548c\u5b9e\u73b0\u81ea\u52a8\u5316\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u906e\u6321\u533a\u57df\u7684\u91cd\u5efa\u8d28\u91cf\u6709\u9650\uff0c\u4e14\u7f3a\u4e4f\u6df1\u5ea6\u7cbe\u5ea6\u8bc4\u4f30\uff0c\u56e0\u4e3a\u73b0\u6709\u57fa\u51c6\u6570\u636e\u96c6\u7f3a\u4e4f3D\u771f\u5b9e\u6570\u636e\u3002", "method": "\u63d0\u51faDiff2DGS\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u57fa\u4e8e\u6269\u6563\u7684\u89c6\u9891\u6a21\u5757\uff0c\u5229\u7528\u65f6\u95f4\u5148\u9a8c\u4fee\u590d\u88ab\u624b\u672f\u5668\u68b0\u906e\u6321\u7684\u7ec4\u7ec7\uff0c\u4fdd\u6301\u65f6\u7a7a\u4e00\u81f4\u6027\uff1b\u7b2c\u4e8c\u9636\u6bb5\u91c7\u7528\u5e26\u6709\u53ef\u5b66\u4e60\u53d8\u5f62\u6a21\u578b(LDM)\u76842D\u9ad8\u65af\u6e85\u5c04(2DGS)\u6765\u6355\u6349\u52a8\u6001\u7ec4\u7ec7\u53d8\u5f62\u548c\u89e3\u5256\u51e0\u4f55\u7ed3\u6784\u3002", "result": "\u5728EndoNeRF\u6570\u636e\u96c6\u4e0a\u8fbe\u523038.02 dB PSNR\uff0c\u5728StereoMIS\u6570\u636e\u96c6\u4e0a\u8fbe\u523034.40 dB PSNR\uff0c\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5728SCARED\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6df1\u5ea6\u7cbe\u5ea6\u5b9a\u91cf\u5206\u6790\uff0c\u53d1\u73b0\u4ec5\u4f18\u5316\u56fe\u50cf\u8d28\u91cf\u4e0d\u4e00\u5b9a\u80fd\u83b7\u5f97\u6700\u4f733D\u91cd\u5efa\u7cbe\u5ea6\uff0c\u56e0\u6b64\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u6df1\u5ea6\u8d28\u91cf\u3002", "conclusion": "Diff2DGS\u5728\u624b\u672f\u573a\u666f\u91cd\u5efa\u4e2d\u5b9e\u73b0\u4e86\u5916\u89c2\u548c\u51e0\u4f55\u7ed3\u6784\u7684\u540c\u65f6\u4f18\u5316\uff0c\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u56fe\u50cf\u8d28\u91cf\uff0c\u8fd8\u786e\u4fdd\u4e86\u66f4\u51c6\u786e\u76843D\u51e0\u4f55\u91cd\u5efa\uff0c\u4e3a\u673a\u5668\u4eba\u624b\u672f\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u573a\u666f\u91cd\u5efa\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.17931", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17931", "abs": "https://arxiv.org/abs/2602.17931", "authors": ["Narjes Nourzad", "Carlee Joe-Wong"], "title": "Memory-Based Advantage Shaping for LLM-Guided Reinforcement Learning", "comment": "Association for the Advancement of Artificial Intelligence (AAAI)", "summary": "In environments with sparse or delayed rewards, reinforcement learning (RL) incurs high sample complexity due to the large number of interactions needed for learning. This limitation has motivated the use of large language models (LLMs) for subgoal discovery and trajectory guidance. While LLMs can support exploration, frequent reliance on LLM calls raises concerns about scalability and reliability. We address these challenges by constructing a memory graph that encodes subgoals and trajectories from both LLM guidance and the agent's own successful rollouts. From this graph, we derive a utility function that evaluates how closely the agent's trajectories align with prior successful strategies. This utility shapes the advantage function, providing the critic with additional guidance without altering the reward. Our method relies primarily on offline input and only occasional online queries, avoiding dependence on continuous LLM supervision. Preliminary experiments in benchmark environments show improved sample efficiency and faster early learning compared to baseline RL methods, with final returns comparable to methods that require frequent LLM interaction.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408LLM\u6307\u5bfc\u4e0e\u667a\u80fd\u4f53\u81ea\u8eab\u7ecf\u9a8c\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u8bb0\u5fc6\u56fe\u6765\u7f16\u7801\u5b50\u76ee\u6807\u548c\u8f68\u8ff9\uff0c\u4ece\u4e2d\u63a8\u5bfc\u6548\u7528\u51fd\u6570\u6765\u6307\u5bfc\u5f3a\u5316\u5b66\u4e60\uff0c\u51cf\u5c11\u5bf9\u9891\u7e41LLM\u8c03\u7528\u7684\u4f9d\u8d56\uff0c\u63d0\u9ad8\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u7684\u6837\u672c\u6548\u7387\u3002", "motivation": "\u5728\u7a00\u758f\u6216\u5ef6\u8fdf\u5956\u52b1\u73af\u5883\u4e2d\uff0c\u5f3a\u5316\u5b66\u4e60\u9700\u8981\u5927\u91cf\u4ea4\u4e92\u5bfc\u81f4\u6837\u672c\u590d\u6742\u5ea6\u9ad8\u3002\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u7528\u4e8e\u5b50\u76ee\u6807\u53d1\u73b0\u548c\u8f68\u8ff9\u6307\u5bfc\uff0c\u4f46\u9891\u7e41\u4f9d\u8d56LLM\u8c03\u7528\u5b58\u5728\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u95ee\u9898\u3002", "method": "\u6784\u5efa\u8bb0\u5fc6\u56fe\u7f16\u7801\u6765\u81eaLLM\u6307\u5bfc\u548c\u667a\u80fd\u4f53\u81ea\u8eab\u6210\u529f\u8f68\u8ff9\u7684\u5b50\u76ee\u6807\u4e0e\u8f68\u8ff9\uff0c\u4ece\u4e2d\u63a8\u5bfc\u6548\u7528\u51fd\u6570\u8bc4\u4f30\u667a\u80fd\u4f53\u8f68\u8ff9\u4e0e\u5148\u524d\u6210\u529f\u7b56\u7565\u7684\u5339\u914d\u7a0b\u5ea6\uff0c\u8be5\u6548\u7528\u51fd\u6570\u5851\u9020\u4f18\u52bf\u51fd\u6570\u4e3acritic\u63d0\u4f9b\u989d\u5916\u6307\u5bfc\u800c\u4e0d\u6539\u53d8\u5956\u52b1\uff0c\u4e3b\u8981\u4f9d\u8d56\u79bb\u7ebf\u8f93\u5165\u548c\u5076\u5c14\u5728\u7ebf\u67e5\u8be2\u3002", "result": "\u5728\u57fa\u51c6\u73af\u5883\u4e2d\u7684\u521d\u6b65\u5b9e\u9a8c\u663e\u793a\uff0c\u76f8\u6bd4\u57fa\u7ebfRL\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u5e76\u52a0\u901f\u4e86\u65e9\u671f\u5b66\u4e60\uff0c\u6700\u7ec8\u56de\u62a5\u4e0e\u9700\u8981\u9891\u7e41LLM\u4ea4\u4e92\u7684\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408LLM\u6307\u5bfc\u4e0e\u667a\u80fd\u4f53\u81ea\u8eab\u7ecf\u9a8c\uff0c\u6784\u5efa\u8bb0\u5fc6\u56fe\u5e76\u63a8\u5bfc\u6548\u7528\u51fd\u6570\uff0c\u6709\u6548\u51cf\u5c11\u5bf9\u9891\u7e41LLM\u8c03\u7528\u7684\u4f9d\u8d56\uff0c\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u63d0\u9ad8\u4e86\u5f3a\u5316\u5b66\u4e60\u7684\u6837\u672c\u6548\u7387\u3002"}}
{"id": "2602.17832", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.17832", "abs": "https://arxiv.org/abs/2602.17832", "authors": ["Hang Liu", "Sangli Teng", "Maani Ghaffari"], "title": "MePoly: Max Entropy Polynomial Policy Optimization", "comment": null, "summary": "Stochastic Optimal Control provides a unified mathematical framework for solving complex decision-making problems, encompassing paradigms such as maximum entropy reinforcement learning(RL) and imitation learning(IL). However, conventional parametric policies often struggle to represent the multi-modality of the solutions. Though diffusion-based policies are aimed at recovering the multi-modality, they lack an explicit probability density, which complicates policy-gradient optimization. To bridge this gap, we propose MePoly, a novel policy parameterization based on polynomial energy-based models. MePoly provides an explicit, tractable probability density, enabling exact entropy maximization. Theoretically, we ground our method in the classical moment problem, leveraging the universal approximation capabilities for arbitrary distributions. Empirically, we demonstrate that MePoly effectively captures complex non-convex manifolds and outperforms baselines in performance across diverse benchmarks.", "AI": {"tldr": "\u63d0\u51faMePoly\u65b9\u6cd5\uff0c\u57fa\u4e8e\u591a\u9879\u5f0f\u80fd\u91cf\u6a21\u578b\u89e3\u51b3\u968f\u673a\u6700\u4f18\u63a7\u5236\u4e2d\u591a\u6a21\u6001\u7b56\u7565\u8868\u793a\u95ee\u9898\uff0c\u63d0\u4f9b\u663e\u5f0f\u53ef\u5904\u7406\u7684\u6982\u7387\u5bc6\u5ea6\uff0c\u5b9e\u73b0\u7cbe\u786e\u71b5\u6700\u5927\u5316", "motivation": "\u4f20\u7edf\u53c2\u6570\u5316\u7b56\u7565\u96be\u4ee5\u8868\u793a\u591a\u6a21\u6001\u89e3\uff0c\u800c\u57fa\u4e8e\u6269\u6563\u7684\u7b56\u7565\u7f3a\u4e4f\u663e\u5f0f\u6982\u7387\u5bc6\u5ea6\uff0c\u4f7f\u7b56\u7565\u68af\u5ea6\u4f18\u5316\u590d\u6742\u5316", "method": "\u63d0\u51faMePoly\u7b56\u7565\u53c2\u6570\u5316\u65b9\u6cd5\uff0c\u57fa\u4e8e\u591a\u9879\u5f0f\u80fd\u91cf\u6a21\u578b\uff0c\u63d0\u4f9b\u663e\u5f0f\u53ef\u5904\u7406\u7684\u6982\u7387\u5bc6\u5ea6\uff0c\u5229\u7528\u7ecf\u5178\u77e9\u95ee\u9898\u7684\u7406\u8bba\u57fa\u7840", "result": "MePoly\u80fd\u6709\u6548\u6355\u6349\u590d\u6742\u975e\u51f8\u6d41\u5f62\uff0c\u5728\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "MePoly\u4e3a\u968f\u673a\u6700\u4f18\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u80fd\u8868\u793a\u591a\u6a21\u6001\u89e3\u4e14\u5177\u6709\u663e\u5f0f\u6982\u7387\u5bc6\u5ea6\u7684\u65b0\u7b56\u7565\u53c2\u6570\u5316\u65b9\u6cd5"}}
{"id": "2602.18322", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18322", "abs": "https://arxiv.org/abs/2602.18322", "authors": ["Ziteng Cui", "Shuhong Liu", "Xiaoyu Dong", "Xuangeng Chu", "Lin Gu", "Ming-Hsuan Yang", "Tatsuya Harada"], "title": "Unifying Color and Lightness Correction with View-Adaptive Curve Adjustment for Robust 3D Novel View Synthesis", "comment": "Journal extension version of CVPR 2025 paper: arXiv:2504.01503", "summary": "High-quality image acquisition in real-world environments remains challenging due to complex illumination variations and inherent limitations of camera imaging pipelines. These issues are exacerbated in multi-view capture, where differences in lighting, sensor responses, and image signal processor (ISP) configurations introduce photometric and chromatic inconsistencies that violate the assumptions of photometric consistency underlying modern 3D novel view synthesis (NVS) methods, including Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS), leading to degraded reconstruction and rendering quality. We propose Luminance-GS++, a 3DGS-based framework for robust NVS under diverse illumination conditions. Our method combines a globally view-adaptive lightness adjustment with a local pixel-wise residual refinement for precise color correction. We further design unsupervised objectives that jointly enforce lightness correction and multi-view geometric and photometric consistency. Extensive experiments demonstrate state-of-the-art performance across challenging scenarios, including low-light, overexposure, and complex luminance and chromatic variations. Unlike prior approaches that modify the underlying representation, our method preserves the explicit 3DGS formulation, improving reconstruction fidelity while maintaining real-time rendering efficiency.", "AI": {"tldr": "Luminance-GS++\uff1a\u57fa\u4e8e3D\u9ad8\u65af\u6cfc\u6e85\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u81ea\u9002\u5e94\u4eae\u5ea6\u8c03\u6574\u548c\u5c40\u90e8\u50cf\u7d20\u7ea7\u6b8b\u5dee\u7ec6\u5316\uff0c\u89e3\u51b3\u591a\u89c6\u89d2\u6355\u83b7\u4e2d\u5149\u7167\u53d8\u5316\u5bfc\u81f4\u7684\u8272\u5f69\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u63d0\u53473D\u91cd\u5efa\u548c\u6e32\u67d3\u8d28\u91cf\u3002", "motivation": "\u73b0\u5b9e\u73af\u5883\u4e2d\u590d\u6742\u7684\u5149\u7167\u53d8\u5316\u548c\u76f8\u673a\u6210\u50cf\u7ba1\u9053\u7684\u9650\u5236\u5bfc\u81f4\u9ad8\u8d28\u91cf\u56fe\u50cf\u91c7\u96c6\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u591a\u89c6\u89d2\u6355\u83b7\u4e2d\uff0c\u5149\u7167\u3001\u4f20\u611f\u5668\u54cd\u5e94\u548cISP\u914d\u7f6e\u7684\u5dee\u5f02\u4f1a\u5f15\u5165\u5149\u5ea6\u5b66\u548c\u8272\u5f69\u4e0d\u4e00\u81f4\u6027\uff0c\u8fd9\u8fdd\u53cd\u4e86\u73b0\u4ee33D\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u6cd5\uff08\u5982NeRF\u548c3DGS\uff09\u6240\u4f9d\u8d56\u7684\u5149\u5ea6\u4e00\u81f4\u6027\u5047\u8bbe\uff0c\u5bfc\u81f4\u91cd\u5efa\u548c\u6e32\u67d3\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u63d0\u51faLuminance-GS++\u6846\u67b6\uff0c\u7ed3\u5408\u5168\u5c40\u89c6\u56fe\u81ea\u9002\u5e94\u4eae\u5ea6\u8c03\u6574\u548c\u5c40\u90e8\u50cf\u7d20\u7ea7\u6b8b\u5dee\u7ec6\u5316\u8fdb\u884c\u7cbe\u786e\u8272\u5f69\u6821\u6b63\u3002\u8bbe\u8ba1\u65e0\u76d1\u7763\u76ee\u6807\uff0c\u8054\u5408\u5f3a\u5236\u6267\u884c\u4eae\u5ea6\u6821\u6b63\u4ee5\u53ca\u591a\u89c6\u89d2\u51e0\u4f55\u548c\u5149\u5ea6\u4e00\u81f4\u6027\u3002\u8be5\u65b9\u6cd5\u4fdd\u7559\u4e86\u663e\u5f0f\u76843DGS\u516c\u5f0f\uff0c\u4e0d\u4fee\u6539\u5e95\u5c42\u8868\u793a\u3002", "result": "\u5728\u4f4e\u5149\u7167\u3001\u8fc7\u66dd\u548c\u590d\u6742\u4eae\u5ea6\u53ca\u8272\u5f69\u53d8\u5316\u7b49\u6311\u6218\u6027\u573a\u666f\u4e2d\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u63d0\u9ad8\u4e86\u91cd\u5efa\u4fdd\u771f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5b9e\u65f6\u6e32\u67d3\u6548\u7387\u3002", "conclusion": "Luminance-GS++\u80fd\u591f\u6709\u6548\u5904\u7406\u591a\u89c6\u89d2\u6355\u83b7\u4e2d\u7684\u5149\u7167\u53d8\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u8272\u5f69\u6821\u6b63\u65b9\u6cd5\u5728\u4fdd\u63013DGS\u5b9e\u65f6\u6e32\u67d3\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u590d\u6742\u5149\u7167\u6761\u4ef6\u4e0b\u76843D\u91cd\u5efa\u548c\u6e32\u67d3\u8d28\u91cf\u3002"}}
{"id": "2602.18329", "categories": ["cs.CV", "math.AT"], "pdf": "https://arxiv.org/pdf/2602.18329", "abs": "https://arxiv.org/abs/2602.18329", "authors": ["Qingsong Wang", "Jiaxing He", "Bingzhe Hou", "Tieru Wu", "Yang Cao", "Cailing Yao"], "title": "G-LoG Bi-filtration for Medical Image Classification", "comment": null, "summary": "Building practical filtrations on objects to detect topological and geometric features is an important task in the field of Topological Data Analysis (TDA). In this paper, leveraging the ability of the Laplacian of Gaussian operator to enhance the boundaries of medical images, we define the G-LoG (Gaussian-Laplacian of Gaussian) bi-filtration to generate the features more suitable for multi-parameter persistence module. By modeling volumetric images as bounded functions, then we prove the interleaving distance on the persistence modules obtained from our bi-filtrations on the bounded functions is stable with respect to the maximum norm of the bounded functions. Finally, we conduct experiments on the MedMNIST dataset, comparing our bi-filtration against single-parameter filtration and the established deep learning baselines, including Google AutoML Vision, ResNet, AutoKeras and auto-sklearn. Experiments results demonstrate that our bi-filtration significantly outperforms single-parameter filtration. Notably, a simple Multi-Layer Perceptron (MLP) trained on the topological features generated by our bi-filtration achieves performance comparable to complex deep learning models trained on the original dataset.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af-\u62c9\u666e\u62c9\u65af\u7b97\u5b50(G-LoG)\u7684\u53cc\u53c2\u6570\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u62d3\u6251\u7279\u5f81\u63d0\u53d6\uff0c\u5728\u591a\u53c2\u6570\u6301\u4e45\u6027\u6a21\u5757\u4e2d\u8868\u73b0\u4f18\u4e8e\u5355\u53c2\u6570\u8fc7\u6ee4\uff0c\u4e14MLP\u6a21\u578b\u5728\u62d3\u6251\u7279\u5f81\u4e0a\u7684\u6027\u80fd\u53ef\u4e0e\u590d\u6742\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u76f8\u5ab2\u7f8e\u3002", "motivation": "\u5728\u62d3\u6251\u6570\u636e\u5206\u6790\u4e2d\uff0c\u6784\u5efa\u5b9e\u7528\u7684\u8fc7\u6ee4\u65b9\u6cd5\u6765\u68c0\u6d4b\u62d3\u6251\u548c\u51e0\u4f55\u7279\u5f81\u662f\u4e00\u4e2a\u91cd\u8981\u4efb\u52a1\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u62c9\u666e\u62c9\u65af\u9ad8\u65af\u7b97\u5b50\u589e\u5f3a\u533b\u5b66\u56fe\u50cf\u8fb9\u754c\u7684\u80fd\u529b\uff0c\u5b9a\u4e49\u66f4\u9002\u5408\u591a\u53c2\u6570\u6301\u4e45\u6027\u6a21\u5757\u7684\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u3002", "method": "1. \u63d0\u51faG-LoG\uff08\u9ad8\u65af-\u62c9\u666e\u62c9\u65af\u9ad8\u65af\uff09\u53cc\u53c2\u6570\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u5229\u7528\u62c9\u666e\u62c9\u65af\u9ad8\u65af\u7b97\u5b50\u589e\u5f3a\u533b\u5b66\u56fe\u50cf\u8fb9\u754c\n2. \u5c06\u4f53\u79ef\u56fe\u50cf\u5efa\u6a21\u4e3a\u6709\u754c\u51fd\u6570\n3. \u8bc1\u660e\u4ece\u6709\u754c\u51fd\u6570\u7684\u53cc\u53c2\u6570\u8fc7\u6ee4\u83b7\u5f97\u7684\u6301\u4e45\u6027\u6a21\u5757\u7684\u4ea4\u9519\u8ddd\u79bb\u76f8\u5bf9\u4e8e\u6709\u754c\u51fd\u6570\u7684\u6700\u5927\u8303\u6570\u662f\u7a33\u5b9a\u7684\n4. \u5728MedMNIST\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u4e0e\u5355\u53c2\u6570\u8fc7\u6ee4\u548c\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\uff08Google AutoML Vision\u3001ResNet\u3001AutoKeras\u3001auto-sklearn\uff09\u8fdb\u884c\u6bd4\u8f83", "result": "1. G-LoG\u53cc\u53c2\u6570\u8fc7\u6ee4\u663e\u8457\u4f18\u4e8e\u5355\u53c2\u6570\u8fc7\u6ee4\n2. \u7b80\u5355\u7684\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u5728\u7531\u53cc\u53c2\u6570\u8fc7\u6ee4\u751f\u6210\u7684\u62d3\u6251\u7279\u5f81\u4e0a\u8bad\u7ec3\uff0c\u5176\u6027\u80fd\u53ef\u4e0e\u5728\u539f\u59cb\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684\u590d\u6742\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u76f8\u5ab2\u7f8e\n3. \u8bc1\u660e\u4e86\u6301\u4e45\u6027\u6a21\u5757\u4ea4\u9519\u8ddd\u79bb\u7684\u7a33\u5b9a\u6027", "conclusion": "G-LoG\u53cc\u53c2\u6570\u8fc7\u6ee4\u65b9\u6cd5\u4e3a\u533b\u5b66\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u62d3\u6251\u7279\u5f81\u63d0\u53d6\u65b9\u6848\uff0c\u5728\u591a\u53c2\u6570\u6301\u4e45\u6027\u6a21\u5757\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e14\u57fa\u4e8e\u62d3\u6251\u7279\u5f81\u7684\u7b80\u5355MLP\u6a21\u578b\u80fd\u8fbe\u5230\u4e0e\u590d\u6742\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u62d3\u6251\u6570\u636e\u5206\u6790\u5728\u533b\u5b66\u56fe\u50cf\u5904\u7406\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.17846", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17846", "abs": "https://arxiv.org/abs/2602.17846", "authors": ["Nick Dodson", "Xinyu Gao", "Qingsong Wang", "Yusu Wang", "Zhengchao Wan"], "title": "Two Calm Ends and the Wild Middle: A Geometric Picture of Memorization in Diffusion Models", "comment": null, "summary": "Diffusion models generate high-quality samples but can also memorize training data, raising serious privacy concerns. Understanding the mechanisms governing when memorization versus generalization occurs remains an active area of research. In particular, it is unclear where along the noise schedule memorization is induced, how data geometry influences it, and how phenomena at different noise scales interact. We introduce a geometric framework that partitions the noise schedule into three regimes based on the coverage properties of training data by Gaussian shells and the concentration behavior of the posterior, which we argue are two fundamental objects governing memorization and generalization in diffusion models. This perspective reveals that memorization risk is highly non-uniform across noise levels. We further identify a danger zone at medium noise levels where memorization is most pronounced. In contrast, both the small and large noise regimes resist memorization, but through fundamentally different mechanisms: small noise avoids memorization due to limited training coverage, while large noise exhibits low posterior concentration and admits a provably near linear Gaussian denoising behavior. For the medium noise regime, we identify geometric conditions through which we propose a geometry-informed targeted intervention that mitigates memorization.", "AI": {"tldr": "\u6269\u6563\u6a21\u578b\u5b58\u5728\u8bad\u7ec3\u6570\u636e\u8bb0\u5fc6\u98ce\u9669\uff0c\u4f5c\u8005\u63d0\u51fa\u51e0\u4f55\u6846\u67b6\u5c06\u566a\u58f0\u8c03\u5ea6\u5206\u4e3a\u4e09\u4e2a\u533a\u57df\uff0c\u8bc6\u522b\u51fa\u4e2d\u7b49\u566a\u58f0\u533a\u57df\u662f\u8bb0\u5fc6\u98ce\u9669\u6700\u9ad8\u7684\"\u5371\u9669\u533a\"\uff0c\u5e76\u63d0\u51fa\u51e0\u4f55\u5e72\u9884\u65b9\u6cd5\u7f13\u89e3\u8bb0\u5fc6\u95ee\u9898\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u6837\u672c\u4f46\u4e5f\u53ef\u80fd\u8bb0\u5fc6\u8bad\u7ec3\u6570\u636e\uff0c\u5f15\u53d1\u9690\u79c1\u62c5\u5fe7\u3002\u76ee\u524d\u5bf9\u8bb0\u5fc6\u4e0e\u6cdb\u5316\u673a\u5236\u7684\u7406\u89e3\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u566a\u58f0\u8c03\u5ea6\u4e2d\u8bb0\u5fc6\u4f55\u65f6\u4ea7\u751f\u3001\u6570\u636e\u51e0\u4f55\u5982\u4f55\u5f71\u54cd\u8bb0\u5fc6\u3001\u4e0d\u540c\u566a\u58f0\u5c3a\u5ea6\u73b0\u8c61\u5982\u4f55\u76f8\u4e92\u4f5c\u7528\u7b49\u95ee\u9898\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u5f15\u5165\u51e0\u4f55\u6846\u67b6\uff0c\u57fa\u4e8e\u9ad8\u65af\u58f3\u5c42\u8986\u76d6\u8bad\u7ec3\u6570\u636e\u7684\u7279\u6027\u548c\u540e\u9a8c\u96c6\u4e2d\u884c\u4e3a\u8fd9\u4e24\u4e2a\u51b3\u5b9a\u6269\u6563\u6a21\u578b\u8bb0\u5fc6\u4e0e\u6cdb\u5316\u7684\u57fa\u672c\u5bf9\u8c61\uff0c\u5c06\u566a\u58f0\u8c03\u5ea6\u5212\u5206\u4e3a\u4e09\u4e2a\u533a\u57df\u3002\u8bc6\u522b\u51fa\u4e2d\u7b49\u566a\u58f0\u533a\u57df\u4e3a\u8bb0\u5fc6\u98ce\u9669\u6700\u9ad8\u7684\"\u5371\u9669\u533a\"\uff0c\u5e76\u63d0\u51fa\u51e0\u4f55\u611f\u77e5\u7684\u9488\u5bf9\u6027\u5e72\u9884\u63aa\u65bd\u6765\u7f13\u89e3\u8bb0\u5fc6\u95ee\u9898\u3002", "result": "\u53d1\u73b0\u8bb0\u5fc6\u98ce\u9669\u5728\u566a\u58f0\u6c34\u5e73\u4e0a\u9ad8\u5ea6\u4e0d\u5747\u5300\uff0c\u4e2d\u7b49\u566a\u58f0\u533a\u57df\u8bb0\u5fc6\u98ce\u9669\u6700\u663e\u8457\u3002\u5c0f\u566a\u58f0\u533a\u57df\u56e0\u8bad\u7ec3\u8986\u76d6\u6709\u9650\u800c\u907f\u514d\u8bb0\u5fc6\uff0c\u5927\u566a\u58f0\u533a\u57df\u540e\u9a8c\u96c6\u4e2d\u5ea6\u4f4e\u4e14\u53ef\u8bc1\u660e\u63a5\u8fd1\u7ebf\u6027\u9ad8\u65af\u53bb\u566a\u884c\u4e3a\u3002\u9488\u5bf9\u4e2d\u7b49\u566a\u58f0\u533a\u57df\uff0c\u901a\u8fc7\u51e0\u4f55\u6761\u4ef6\u63d0\u51fa\u4e86\u6709\u6548\u7684\u5e72\u9884\u65b9\u6cd5\u3002", "conclusion": "\u6269\u6563\u6a21\u578b\u7684\u8bb0\u5fc6\u98ce\u9669\u5728\u566a\u58f0\u8c03\u5ea6\u4e2d\u5206\u5e03\u4e0d\u5747\uff0c\u4e2d\u7b49\u566a\u58f0\u533a\u57df\u662f\u4e3b\u8981\u98ce\u9669\u533a\u3002\u901a\u8fc7\u51e0\u4f55\u6846\u67b6\u5206\u6790\uff0c\u53ef\u4ee5\u7406\u89e3\u4e0d\u540c\u566a\u58f0\u533a\u57df\u7684\u8bb0\u5fc6\u673a\u5236\uff0c\u5e76\u8bbe\u8ba1\u9488\u5bf9\u6027\u5e72\u9884\u63aa\u65bd\u6765\u7f13\u89e3\u9690\u79c1\u98ce\u9669\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2602.18422", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18422", "abs": "https://arxiv.org/abs/2602.18422", "authors": ["Linxi Xie", "Lisong C. Sun", "Ashley Neall", "Tong Wu", "Shengqu Cai", "Gordon Wetzstein"], "title": "Generated Reality: Human-centric World Simulation using Interactive Video Generation with Hand and Camera Control", "comment": "Project page here: https://codeysun.github.io/generated-reality", "summary": "Extended reality (XR) demands generative models that respond to users' tracked real-world motion, yet current video world models accept only coarse control signals such as text or keyboard input, limiting their utility for embodied interaction. We introduce a human-centric video world model that is conditioned on both tracked head pose and joint-level hand poses. For this purpose, we evaluate existing diffusion transformer conditioning strategies and propose an effective mechanism for 3D head and hand control, enabling dexterous hand--object interactions. We train a bidirectional video diffusion model teacher using this strategy and distill it into a causal, interactive system that generates egocentric virtual environments. We evaluate this generated reality system with human subjects and demonstrate improved task performance as well as a significantly higher level of perceived amount of control over the performed actions compared with relevant baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5934\u624b\u59ff\u6001\u63a7\u5236\u7684\u89c6\u9891\u4e16\u754c\u6a21\u578b\uff0c\u7528\u4e8e\u6269\u5c55\u73b0\u5b9e(XR)\u4e2d\u7684\u4ea4\u4e92\u5f0f\u865a\u62df\u73af\u5883\u751f\u6210", "motivation": "\u5f53\u524d\u89c6\u9891\u4e16\u754c\u6a21\u578b\u4ec5\u63a5\u53d7\u6587\u672c\u6216\u952e\u76d8\u7b49\u7c97\u7565\u63a7\u5236\u4fe1\u53f7\uff0c\u65e0\u6cd5\u54cd\u5e94XR\u4e2d\u7528\u6237\u771f\u5b9e\u8fd0\u52a8\u8ffd\u8e2a\uff0c\u9650\u5236\u4e86\u5177\u8eab\u4ea4\u4e92\u7684\u5b9e\u7528\u6027", "method": "\u63d0\u51fa\u4ee5\u8ffd\u8e2a\u7684\u5934\u59ff\u6001\u548c\u5173\u8282\u7ea7\u624b\u59ff\u6001\u4e3a\u6761\u4ef6\u7684\u4eba\u7c7b\u4e2d\u5fc3\u89c6\u9891\u4e16\u754c\u6a21\u578b\uff1b\u8bc4\u4f30\u73b0\u6709\u6269\u6563\u53d8\u6362\u5668\u6761\u4ef6\u7b56\u7565\uff0c\u63d0\u51fa\u6709\u6548\u76843D\u5934\u624b\u63a7\u5236\u673a\u5236\uff1b\u8bad\u7ec3\u53cc\u5411\u89c6\u9891\u6269\u6563\u6a21\u578b\u6559\u5e08\uff0c\u5e76\u84b8\u998f\u4e3a\u56e0\u679c\u4ea4\u4e92\u7cfb\u7edf", "result": "\u7cfb\u7edf\u80fd\u591f\u751f\u6210\u4ee5\u81ea\u6211\u4e3a\u4e2d\u5fc3\u7684\u865a\u62df\u73af\u5883\uff0c\u652f\u6301\u7075\u5de7\u7684\u624b-\u7269\u4f53\u4ea4\u4e92\uff1b\u4eba\u7c7b\u5b9e\u9a8c\u663e\u793a\u4efb\u52a1\u6027\u80fd\u63d0\u5347\uff0c\u611f\u77e5\u63a7\u5236\u6c34\u5e73\u663e\u8457\u9ad8\u4e8e\u57fa\u7ebf", "conclusion": "\u8be5\u6a21\u578b\u4e3aXR\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u81ea\u7136\u3001\u54cd\u5e94\u5f0f\u7684\u865a\u62df\u73af\u5883\u751f\u6210\u80fd\u529b\uff0c\u589e\u5f3a\u4e86\u7528\u6237\u7684\u63a7\u5236\u611f\u548c\u4ea4\u4e92\u4f53\u9a8c"}}
{"id": "2602.17978", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.17978", "abs": "https://arxiv.org/abs/2602.17978", "authors": ["Daqian Shao"], "title": "Learning Optimal and Sample-Efficient Decision Policies with Guarantees", "comment": "A thesis submitted for the degree of DPhil in Computer Science at Oxford", "summary": "The paradigm of decision-making has been revolutionised by reinforcement learning and deep learning. Although this has led to significant progress in domains such as robotics, healthcare, and finance, the use of RL in practice is challenging, particularly when learning decision policies in high-stakes applications that may require guarantees. Traditional RL algorithms rely on a large number of online interactions with the environment, which is problematic in scenarios where online interactions are costly, dangerous, or infeasible. However, learning from offline datasets is hindered by the presence of hidden confounders. Such confounders can cause spurious correlations in the dataset and can mislead the agent into taking suboptimal or adversarial actions. Firstly, we address the problem of learning from offline datasets in the presence of hidden confounders. We work with instrumental variables (IVs) to identify the causal effect, which is an instance of a conditional moment restrictions (CMR) problem. Inspired by double/debiased machine learning, we derive a sample-efficient algorithm for solving CMR problems with convergence and optimality guarantees, which outperforms state-of-the-art algorithms. Secondly, we relax the conditions on the hidden confounders in the setting of (offline) imitation learning, and adapt our CMR estimator to derive an algorithm that can learn effective imitator policies with convergence rate guarantees. Finally, we consider the problem of learning high-level objectives expressed in linear temporal logic (LTL) and develop a provably optimal learning algorithm that improves sample efficiency over existing methods. Through evaluation on reinforcement learning benchmarks and synthetic and semi-synthetic datasets, we demonstrate the usefulness of the methods developed in this thesis in real-world decision making.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5b58\u5728\u9690\u85cf\u6df7\u6742\u56e0\u7d20\u7684\u79bb\u7ebf\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u51b3\u7b56\u7b56\u7565\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u5de5\u5177\u53d8\u91cf\u89e3\u51b3\u56e0\u679c\u63a8\u65ad\u95ee\u9898\uff0c\u5e76\u5728\u6a21\u4eff\u5b66\u4e60\u548c\u65f6\u5e8f\u903b\u8f91\u76ee\u6807\u5b66\u4e60\u65b9\u9762\u8fdb\u884c\u4e86\u6269\u5c55\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u9700\u8981\u5927\u91cf\u5728\u7ebf\u73af\u5883\u4ea4\u4e92\uff0c\u8fd9\u5728\u6210\u672c\u9ad8\u3001\u5371\u9669\u6216\u4e0d\u53ef\u884c\u7684\u573a\u666f\u4e2d\u5b58\u5728\u95ee\u9898\u3002\u79bb\u7ebf\u5b66\u4e60\u9762\u4e34\u9690\u85cf\u6df7\u6742\u56e0\u7d20\u7684\u6311\u6218\uff0c\u8fd9\u4e9b\u6df7\u6742\u56e0\u7d20\u53ef\u80fd\u5bfc\u81f4\u865a\u5047\u76f8\u5173\u6027\u548c\u6b21\u4f18\u51b3\u7b56\u3002", "method": "1. \u4f7f\u7528\u5de5\u5177\u53d8\u91cf\u8bc6\u522b\u56e0\u679c\u6548\u5e94\uff0c\u57fa\u4e8e\u6761\u4ef6\u77e9\u9650\u5236\u95ee\u9898\uff1b2. \u53d7\u53cc\u91cd/\u53bb\u504f\u673a\u5668\u5b66\u4e60\u542f\u53d1\uff0c\u5f00\u53d1\u5177\u6709\u6536\u655b\u6027\u548c\u6700\u4f18\u6027\u4fdd\u8bc1\u7684\u6837\u672c\u9ad8\u6548\u7b97\u6cd5\uff1b3. \u5728\u6a21\u4eff\u5b66\u4e60\u4e2d\u653e\u5bbd\u9690\u85cf\u6df7\u6742\u56e0\u7d20\u6761\u4ef6\u9650\u5236\uff1b4. \u9488\u5bf9\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\u8868\u8fbe\u7684\u9ad8\u5c42\u76ee\u6807\u5f00\u53d1\u53ef\u8bc1\u660e\u6700\u4f18\u7684\u5b66\u4e60\u7b97\u6cd5\u3002", "result": "\u63d0\u51fa\u7684CMR\u6c42\u89e3\u7b97\u6cd5\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7b97\u6cd5\uff0c\u6a21\u4eff\u5b66\u4e60\u7b97\u6cd5\u5177\u6709\u6536\u655b\u7387\u4fdd\u8bc1\uff0c\u65f6\u5e8f\u903b\u8f91\u5b66\u4e60\u7b97\u6cd5\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u3002\u5728\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u3001\u5408\u6210\u548c\u534a\u5408\u6210\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u8bba\u6587\u5f00\u53d1\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u79bb\u7ebf\u5b66\u4e60\u4e2d\u7684\u9690\u85cf\u6df7\u6742\u56e0\u7d20\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u51b3\u7b56\u5236\u5b9a\u63d0\u4f9b\u4e86\u6709\u7528\u7684\u5de5\u5177\uff0c\u7279\u522b\u662f\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u9700\u8981\u4fdd\u8bc1\u7684\u573a\u666f\u3002"}}
{"id": "2602.17861", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17861", "abs": "https://arxiv.org/abs/2602.17861", "authors": ["Ryan McKenna", "Galen Andrew", "Borja Balle", "Vadym Doroshenko", "Arun Ganesh", "Weiwei Kong", "Alex Kurakin", "Brendan McMahan", "Mikhail Pravilov"], "title": "JAX-Privacy: A library for differentially private machine learning", "comment": null, "summary": "JAX-Privacy is a library designed to simplify the deployment of robust and performant mechanisms for differentially private machine learning. Guided by design principles of usability, flexibility, and efficiency, JAX-Privacy serves both researchers requiring deep customization and practitioners who want a more out-of-the-box experience. The library provides verified, modular primitives for critical components for all aspects of the mechanism design including batch selection, gradient clipping, noise addition, accounting, and auditing, and brings together a large body of recent research on differentially private ML.", "AI": {"tldr": "JAX-Privacy\u662f\u4e00\u4e2a\u7528\u4e8e\u7b80\u5316\u5dee\u5206\u9690\u79c1\u673a\u5668\u5b66\u4e60\u673a\u5236\u90e8\u7f72\u7684\u5e93\uff0c\u63d0\u4f9b\u6a21\u5757\u5316\u7ec4\u4ef6\u548c\u6700\u65b0\u7814\u7a76\u6210\u679c\u96c6\u6210\u3002", "motivation": "\u7b80\u5316\u5dee\u5206\u9690\u79c1\u673a\u5668\u5b66\u4e60\u673a\u5236\u7684\u90e8\u7f72\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u6613\u7528\u3001\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u57fa\u4e8eJAX\u6784\u5efa\uff0c\u63d0\u4f9b\u6a21\u5757\u5316\u539f\u8bed\uff0c\u5305\u62ec\u6279\u6b21\u9009\u62e9\u3001\u68af\u5ea6\u88c1\u526a\u3001\u566a\u58f0\u6dfb\u52a0\u3001\u4f1a\u8ba1\u548c\u5ba1\u8ba1\u7b49\u5173\u952e\u7ec4\u4ef6\u3002", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u65e2\u652f\u6301\u6df1\u5ea6\u5b9a\u5236\u53c8\u63d0\u4f9b\u5f00\u7bb1\u5373\u7528\u4f53\u9a8c\u7684\u5e93\uff0c\u6574\u5408\u4e86\u5927\u91cf\u5dee\u5206\u9690\u79c1\u673a\u5668\u5b66\u4e60\u7684\u6700\u65b0\u7814\u7a76\u6210\u679c\u3002", "conclusion": "JAX-Privacy\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u6613\u7528\u6027\u539f\u5219\uff0c\u4e3a\u5dee\u5206\u9690\u79c1\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5de5\u5177\u652f\u6301\u3002"}}
{"id": "2602.18424", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2602.18424", "abs": "https://arxiv.org/abs/2602.18424", "authors": ["Xia Su", "Ruiqi Chen", "Benlin Liu", "Jingwei Ma", "Zonglin Di", "Ranjay Krishna", "Jon Froehlich"], "title": "CapNav: Benchmarking Vision Language Models on Capability-conditioned Indoor Navigation", "comment": null, "summary": "Vision-Language Models (VLMs) have shown remarkable progress in Vision-Language Navigation (VLN), offering new possibilities for navigation decision-making that could benefit both robotic platforms and human users. However, real-world navigation is inherently conditioned by the agent's mobility constraints. For example, a sweeping robot cannot traverse stairs, while a quadruped can. We introduce Capability-Conditioned Navigation (CapNav), a benchmark designed to evaluate how well VLMs can navigate complex indoor spaces given an agent's specific physical and operational capabilities. CapNav defines five representative human and robot agents, each described with physical dimensions, mobility capabilities, and environmental interaction abilities. CapNav provides 45 real-world indoor scenes, 473 navigation tasks, and 2365 QA pairs to test if VLMs can traverse indoor environments based on agent capabilities. We evaluate 13 modern VLMs and find that current VLM's navigation performance drops sharply as mobility constraints tighten, and that even state-of-the-art models struggle with obstacle types that require reasoning on spatial dimensions. We conclude by discussing the implications for capability-aware navigation and the opportunities for advancing embodied spatial reasoning in future VLMs. The benchmark is available at https://github.com/makeabilitylab/CapNav", "AI": {"tldr": "CapNav\u662f\u4e00\u4e2a\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8003\u8651\u667a\u80fd\u4f53\u7269\u7406\u80fd\u529b\u7ea6\u675f\u4e0b\u8fdb\u884c\u5ba4\u5185\u5bfc\u822a\u7684\u65b0\u57fa\u51c6\uff0c\u5305\u542b5\u79cd\u4ee3\u8868\u6027\u667a\u80fd\u4f53\u300145\u4e2a\u573a\u666f\u3001473\u4e2a\u4efb\u52a1\uff0c\u53d1\u73b0\u5f53\u524dVLM\u5728\u4e25\u683c\u79fb\u52a8\u7ea6\u675f\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d", "motivation": "\u771f\u5b9e\u4e16\u754c\u5bfc\u822a\u53d7\u667a\u80fd\u4f53\u79fb\u52a8\u80fd\u529b\u7ea6\u675f\uff08\u5982\u626b\u5730\u673a\u5668\u4eba\u4e0d\u80fd\u722c\u697c\u68af\uff0c\u56db\u8db3\u673a\u5668\u4eba\u53ef\u4ee5\uff09\uff0c\u4f46\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u7814\u7a76\u7f3a\u4e4f\u5bf9\u667a\u80fd\u4f53\u5177\u4f53\u7269\u7406\u548c\u64cd\u4f5c\u80fd\u529b\u7684\u8003\u8651", "method": "\u63d0\u51fa\u80fd\u529b\u6761\u4ef6\u5bfc\u822a\u57fa\u51c6CapNav\uff0c\u5b9a\u4e495\u79cd\u4ee3\u8868\u6027\u4eba\u7c7b\u548c\u673a\u5668\u4eba\u667a\u80fd\u4f53\uff08\u5404\u6709\u7269\u7406\u5c3a\u5bf8\u3001\u79fb\u52a8\u80fd\u529b\u548c\u73af\u5883\u4ea4\u4e92\u80fd\u529b\u63cf\u8ff0\uff09\uff0c\u5305\u542b45\u4e2a\u771f\u5b9e\u5ba4\u5185\u573a\u666f\u3001473\u4e2a\u5bfc\u822a\u4efb\u52a1\u548c2365\u4e2a\u95ee\u7b54\u5bf9", "result": "\u8bc4\u4f3013\u4e2a\u73b0\u4ee3VLM\u53d1\u73b0\uff1a1\uff09\u5f53\u524dVLM\u5bfc\u822a\u6027\u80fd\u968f\u79fb\u52a8\u7ea6\u675f\u6536\u7d27\u800c\u6025\u5267\u4e0b\u964d\uff1b2\uff09\u5373\u4f7f\u6700\u5148\u8fdb\u6a21\u578b\u4e5f\u96be\u4ee5\u5904\u7406\u9700\u8981\u7a7a\u95f4\u7ef4\u5ea6\u63a8\u7406\u7684\u969c\u788d\u7c7b\u578b", "conclusion": "\u8ba8\u8bba\u4e86\u80fd\u529b\u611f\u77e5\u5bfc\u822a\u7684\u610f\u4e49\uff0c\u4ee5\u53ca\u672a\u6765VLM\u5728\u5177\u8eab\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u7684\u6539\u8fdb\u673a\u4f1a\uff0c\u4e3a\u5f00\u53d1\u66f4\u5b9e\u7528\u7684\u5bfc\u822a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u51c6"}}
{"id": "2602.18432", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18432", "abs": "https://arxiv.org/abs/2602.18432", "authors": ["Evonne Ng", "Siwei Zhang", "Zhang Chen", "Michael Zollhoefer", "Alexander Richard"], "title": "SARAH: Spatially Aware Real-time Agentic Humans", "comment": "Project page: https://evonneng.github.io/sarah/", "summary": "As embodied agents become central to VR, telepresence, and digital human applications, their motion must go beyond speech-aligned gestures: agents should turn toward users, respond to their movement, and maintain natural gaze. Current methods lack this spatial awareness. We close this gap with the first real-time, fully causal method for spatially-aware conversational motion, deployable on a streaming VR headset. Given a user's position and dyadic audio, our approach produces full-body motion that aligns gestures with speech while orienting the agent according to the user. Our architecture combines a causal transformer-based VAE with interleaved latent tokens for streaming inference and a flow matching model conditioned on user trajectory and audio. To support varying gaze preferences, we introduce a gaze scoring mechanism with classifier-free guidance to decouple learning from control: the model captures natural spatial alignment from data, while users can adjust eye contact intensity at inference time. On the Embody 3D dataset, our method achieves state-of-the-art motion quality at over 300 FPS -- 3x faster than non-causal baselines -- while capturing the subtle spatial dynamics of natural conversation. We validate our approach on a live VR system, bringing spatially-aware conversational agents to real-time deployment. Please see https://evonneng.github.io/sarah/ for details.", "AI": {"tldr": "\u9996\u4e2a\u5b9e\u65f6\u3001\u5b8c\u5168\u56e0\u679c\u7684\u7a7a\u95f4\u611f\u77e5\u5bf9\u8bdd\u52a8\u4f5c\u751f\u6210\u65b9\u6cd5\uff0c\u53ef\u5728\u6d41\u5f0fVR\u5934\u663e\u4e0a\u90e8\u7f72\uff0c\u7ed3\u5408\u7528\u6237\u4f4d\u7f6e\u548c\u97f3\u9891\u751f\u6210\u5168\u8eab\u52a8\u4f5c\uff0c\u5b9e\u73b0300+FPS\u7684\u5b9e\u65f6\u6027\u80fd", "motivation": "\u5f53\u524d\u65b9\u6cd5\u7f3a\u4e4f\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\uff0c\u800c\u5177\u8eab\u667a\u80fd\u4f53\u9700\u8981\u8f6c\u5411\u7528\u6237\u3001\u54cd\u5e94\u7528\u6237\u52a8\u4f5c\u5e76\u4fdd\u6301\u81ea\u7136\u6ce8\u89c6\uff0c\u8fd9\u5bf9VR\u3001\u8fdc\u7a0b\u5448\u73b0\u548c\u6570\u5b57\u4eba\u5e94\u7528\u81f3\u5173\u91cd\u8981", "method": "\u7ed3\u5408\u56e0\u679ctransformer-based VAE\u4e0e\u6d41\u5339\u914d\u6a21\u578b\uff0c\u4f7f\u7528\u4ea4\u9519\u6f5c\u5728token\u8fdb\u884c\u6d41\u5f0f\u63a8\u7406\uff0c\u5f15\u5165\u6ce8\u89c6\u8bc4\u5206\u673a\u5236\u548c\u5206\u7c7b\u5668\u65e0\u6307\u5bfc\u6765\u89e3\u8026\u5b66\u4e60\u4e0e\u63a7\u5236", "result": "\u5728Embody 3D\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u8fd0\u52a8\u8d28\u91cf\uff0c\u8d85\u8fc7300 FPS\uff08\u6bd4\u975e\u56e0\u679c\u57fa\u7ebf\u5feb3\u500d\uff09\uff0c\u80fd\u6355\u6349\u81ea\u7136\u5bf9\u8bdd\u7684\u5fae\u5999\u7a7a\u95f4\u52a8\u6001\uff0c\u5e76\u5728\u5b9e\u65f6VR\u7cfb\u7edf\u4e2d\u9a8c\u8bc1", "conclusion": "\u9996\u6b21\u5b9e\u73b0\u4e86\u5b9e\u65f6\u3001\u5b8c\u5168\u56e0\u679c\u7684\u7a7a\u95f4\u611f\u77e5\u5bf9\u8bdd\u52a8\u4f5c\u751f\u6210\uff0c\u4f7f\u7a7a\u95f4\u611f\u77e5\u5bf9\u8bdd\u667a\u80fd\u4f53\u80fd\u591f\u5b9e\u65f6\u90e8\u7f72\uff0c\u4e3aVR\u3001\u8fdc\u7a0b\u5448\u73b0\u548c\u6570\u5b57\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u6280\u672f\u7a81\u7834"}}
{"id": "2602.17998", "categories": ["cs.LG", "cs.AI", "cs.CE", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.17998", "abs": "https://arxiv.org/abs/2602.17998", "authors": ["Shubham Bhardwaj", "Chandrajit Bajaj"], "title": "PHAST: Port-Hamiltonian Architecture for Structured Temporal Dynamics Forecasting", "comment": "50 pages", "summary": "Real physical systems are dissipative -- a pendulum slows, a circuit loses charge to heat -- and forecasting their dynamics from partial observations is a central challenge in scientific machine learning. We address the \\emph{position-only} (q-only) problem: given only generalized positions~$q_t$ at discrete times (momenta~$p_t$ latent), learn a structured model that (a)~produces stable long-horizon forecasts and (b)~recovers physically meaningful parameters when sufficient structure is provided. The port-Hamiltonian framework makes the conservative-dissipative split explicit via $\\dot{x}=(J-R)\\nabla H(x)$, guaranteeing $dH/dt\\le 0$ when $R\\succeq 0$. We introduce \\textbf{PHAST} (Port-Hamiltonian Architecture for Structured Temporal dynamics), which decomposes the Hamiltonian into potential~$V(q)$, mass~$M(q)$, and damping~$D(q)$ across three knowledge regimes (KNOWN, PARTIAL, UNKNOWN), uses efficient low-rank PSD/SPD parameterizations, and advances dynamics with Strang splitting. Across thirteen q-only benchmarks spanning mechanical, electrical, molecular, thermal, gravitational, and ecological systems, PHAST achieves the best long-horizon forecasting among competitive baselines and enables physically meaningful parameter recovery when the regime provides sufficient anchors. We show that identification is fundamentally ill-posed without such anchors (gauge freedom), motivating a two-axis evaluation that separates forecasting stability from identifiability.", "AI": {"tldr": "PHAST\uff1a\u4e00\u79cd\u7528\u4e8e\u90e8\u5206\u89c2\u6d4b\u7269\u7406\u7cfb\u7edf\uff08\u4ec5\u4f4d\u7f6e\u4fe1\u606f\uff09\u7684\u7aef\u53e3\u54c8\u5bc6\u987f\u67b6\u6784\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5206\u89e3\u5b9e\u73b0\u7a33\u5b9a\u957f\u671f\u9884\u6d4b\u548c\u7269\u7406\u53c2\u6570\u6062\u590d", "motivation": "\u771f\u5b9e\u7269\u7406\u7cfb\u7edf\u90fd\u662f\u8017\u6563\u7684\uff0c\u4ece\u90e8\u5206\u89c2\u6d4b\uff08\u4ec5\u4f4d\u7f6e\u4fe1\u606f\uff09\u9884\u6d4b\u5176\u52a8\u529b\u5b66\u662f\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u6838\u5fc3\u6311\u6218\u3002\u9700\u8981\u89e3\u51b3\u4ec5\u4f4d\u7f6e\u4fe1\u606f\u95ee\u9898\uff1a\u7ed9\u5b9a\u79bb\u6563\u65f6\u95f4\u7684\u5e7f\u4e49\u4f4d\u7f6e\uff08\u52a8\u91cf\u4e3a\u9690\u53d8\u91cf\uff09\uff0c\u5b66\u4e60\u4e00\u4e2a\u7ed3\u6784\u5316\u6a21\u578b\uff0c\u65e2\u80fd\u4ea7\u751f\u7a33\u5b9a\u957f\u671f\u9884\u6d4b\uff0c\u53c8\u80fd\u5728\u63d0\u4f9b\u8db3\u591f\u7ed3\u6784\u65f6\u6062\u590d\u6709\u7269\u7406\u610f\u4e49\u7684\u53c2\u6570\u3002", "method": "\u63d0\u51faPHAST\uff08\u7aef\u53e3\u54c8\u5bc6\u987f\u7ed3\u6784\u5316\u65f6\u95f4\u52a8\u529b\u5b66\u67b6\u6784\uff09\uff0c\u57fa\u4e8e\u7aef\u53e3\u54c8\u5bc6\u987f\u6846\u67b6\u5c06\u4fdd\u5b88-\u8017\u6563\u5206\u89e3\u663e\u5f0f\u8868\u793a\u4e3a$\\dot{x}=(J-R)\\nabla H(x)$\u3002\u5c06\u54c8\u5bc6\u987f\u91cf\u5206\u89e3\u4e3a\u52bf\u80fd$V(q)$\u3001\u8d28\u91cf$M(q)$\u548c\u963b\u5c3c$D(q)$\uff0c\u8003\u8651\u4e09\u79cd\u77e5\u8bc6\u72b6\u6001\uff08\u5df2\u77e5\u3001\u90e8\u5206\u5df2\u77e5\u3001\u672a\u77e5\uff09\uff0c\u4f7f\u7528\u9ad8\u6548\u4f4e\u79e9PSD/SPD\u53c2\u6570\u5316\uff0c\u5e76\u901a\u8fc7Strang\u5206\u88c2\u63a8\u8fdb\u52a8\u529b\u5b66\u3002", "result": "\u572813\u4e2a\u4ec5\u4f4d\u7f6e\u4fe1\u606f\u7684\u57fa\u51c6\u6d4b\u8bd5\uff08\u6db5\u76d6\u673a\u68b0\u3001\u7535\u6c14\u3001\u5206\u5b50\u3001\u70ed\u529b\u5b66\u3001\u5f15\u529b\u548c\u751f\u6001\u7cfb\u7edf\uff09\u4e2d\uff0cPHAST\u5728\u7ade\u4e89\u57fa\u7ebf\u4e2d\u5b9e\u73b0\u4e86\u6700\u4f73\u957f\u671f\u9884\u6d4b\u6027\u80fd\uff0c\u5e76\u5728\u77e5\u8bc6\u72b6\u6001\u63d0\u4f9b\u8db3\u591f\u951a\u70b9\u65f6\u80fd\u591f\u6062\u590d\u6709\u7269\u7406\u610f\u4e49\u7684\u53c2\u6570\u3002\u7814\u7a76\u8868\u660e\uff0c\u6ca1\u6709\u8fd9\u79cd\u951a\u70b9\u65f6\u8bc6\u522b\u95ee\u9898\u672c\u8d28\u4e0a\u662f\u75c5\u6001\u7684\uff08\u89c4\u8303\u81ea\u7531\u5ea6\uff09\u3002", "conclusion": "PHAST\u901a\u8fc7\u7ed3\u6784\u5316\u7aef\u53e3\u54c8\u5bc6\u987f\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4ec5\u4f4d\u7f6e\u4fe1\u606f\u7684\u7269\u7406\u7cfb\u7edf\u9884\u6d4b\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7a33\u5b9a\u957f\u671f\u9884\u6d4b\u548c\u7269\u7406\u53c2\u6570\u6062\u590d\u3002\u63d0\u51fa\u4e86\u53cc\u8f74\u8bc4\u4f30\u6846\u67b6\uff0c\u5c06\u9884\u6d4b\u7a33\u5b9a\u6027\u4e0e\u53ef\u8bc6\u522b\u6027\u5206\u5f00\u8bc4\u4f30\uff0c\u4e3a\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u7ed3\u6784\u5316\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.17867", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.17867", "abs": "https://arxiv.org/abs/2602.17867", "authors": ["Jo\u00e3o N. Cardoso", "Arlindo L. Oliveira", "Bruno Martins"], "title": "ADAPT: Hybrid Prompt Optimization for LLM Feature Visualization", "comment": null, "summary": "Understanding what features are encoded by learned directions in LLM activation space requires identifying inputs that strongly activate them. Feature visualization, which optimizes inputs to maximally activate a target direction, offers an alternative to costly dataset search approaches, but remains underexplored for LLMs due to the discrete nature of text. Furthermore, existing prompt optimization techniques are poorly suited to this domain, which is highly prone to local minima. To overcome these limitations, we introduce ADAPT, a hybrid method combining beam search initialization with adaptive gradient-guided mutation, designed around these failure modes. We evaluate on Sparse Autoencoder latents from Gemma 2 2B, proposing metrics grounded in dataset activation statistics to enable rigorous comparison, and show that ADAPT consistently outperforms prior methods across layers and latent types. Our results establish that feature visualization for LLMs is tractable, but requires design assumptions tailored to the domain.", "AI": {"tldr": "ADAPT\u662f\u4e00\u79cd\u7ed3\u5408\u675f\u641c\u7d22\u521d\u59cb\u5316\u548c\u81ea\u9002\u5e94\u68af\u5ea6\u5f15\u5bfc\u53d8\u5f02\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316LLM\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u7684\u7279\u5f81\u53ef\u89c6\u5316\uff0c\u5728Gemma 2 2B\u6a21\u578b\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7406\u89e3LLM\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u5b66\u4e60\u5230\u7684\u65b9\u5411\u7f16\u7801\u4e86\u54ea\u4e9b\u7279\u5f81\u9700\u8981\u627e\u5230\u80fd\u5f3a\u70c8\u6fc0\u6d3b\u8fd9\u4e9b\u65b9\u5411\u7684\u8f93\u5165\u3002\u7279\u5f81\u53ef\u89c6\u5316\u901a\u8fc7\u4f18\u5316\u8f93\u5165\u6765\u6700\u5927\u5316\u6fc0\u6d3b\u76ee\u6807\u65b9\u5411\uff0c\u4e3a\u6602\u8d35\u7684\u6570\u636e\u5e93\u641c\u7d22\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u7531\u4e8e\u6587\u672c\u7684\u79bb\u6563\u6027\uff0c\u5728LLM\u4e2d\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u73b0\u6709\u7684\u63d0\u793a\u4f18\u5316\u6280\u672f\u4e5f\u4e0d\u9002\u5408\u8fd9\u4e2a\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u5c0f\u503c\u7684\u9886\u57df\u3002", "method": "ADAPT\u662f\u4e00\u79cd\u6df7\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u675f\u641c\u7d22\u521d\u59cb\u5316\u548c\u81ea\u9002\u5e94\u68af\u5ea6\u5f15\u5bfc\u53d8\u5f02\uff0c\u4e13\u95e8\u9488\u5bf9\u8fd9\u4e9b\u5931\u8d25\u6a21\u5f0f\u8bbe\u8ba1\u3002\u8be5\u65b9\u6cd5\u5728Gemma 2 2B\u7684\u7a00\u758f\u81ea\u7f16\u7801\u5668\u6f5c\u5728\u7a7a\u95f4\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u6570\u636e\u96c6\u6fc0\u6d3b\u7edf\u8ba1\u7684\u6307\u6807\u6765\u5b9e\u73b0\u4e25\u683c\u6bd4\u8f83\u3002", "result": "ADAPT\u5728\u4e0d\u540c\u5c42\u548c\u6f5c\u5728\u7c7b\u578b\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\u3002\u7ed3\u679c\u8868\u660eLLM\u7684\u7279\u5f81\u53ef\u89c6\u5316\u662f\u53ef\u884c\u7684\uff0c\u4f46\u9700\u8981\u9488\u5bf9\u8be5\u9886\u57df\u91cf\u8eab\u5b9a\u5236\u7684\u8bbe\u8ba1\u5047\u8bbe\u3002", "conclusion": "LLM\u7684\u7279\u5f81\u53ef\u89c6\u5316\u662f\u53ef\u884c\u7684\uff0c\u4f46\u9700\u8981\u4e13\u95e8\u9488\u5bf9\u8be5\u9886\u57df\u8bbe\u8ba1\u7684\u5047\u8bbe\u548c\u65b9\u6cd5\u3002ADAPT\u901a\u8fc7\u7ed3\u5408\u675f\u641c\u7d22\u521d\u59cb\u5316\u548c\u81ea\u9002\u5e94\u68af\u5ea6\u5f15\u5bfc\u53d8\u5f02\uff0c\u6210\u529f\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u7406\u89e3LLM\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u7684\u7279\u5f81\u7f16\u7801\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5de5\u5177\u3002"}}
{"id": "2602.18434", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.18434", "abs": "https://arxiv.org/abs/2602.18434", "authors": ["Vatsal Agarwal", "Saksham Suri", "Matthew Gwilliam", "Pulkit Kumar", "Abhinav Shrivastava"], "title": "Going Down Memory Lane: Scaling Tokens for Video Stream Understanding with Dynamic KV-Cache Memory", "comment": "Project page: see https://vatsalag99.github.io/memstream/", "summary": "Streaming video understanding requires models to robustly encode, store, and retrieve information from a continuous video stream to support accurate video question answering (VQA). Existing state-of-the-art approaches rely on key-value caching to accumulate frame-level information over time, but use a limited number of tokens per frame, leading to the loss of fine-grained visual details. In this work, we propose scaling the token budget to enable more granular spatiotemporal understanding and reasoning. First, we find that current methods are ill-equipped to handle dense streams: their feature encoding causes query-frame similarity scores to increase over time, biasing retrieval toward later frames. To address this, we introduce an adaptive selection strategy that reduces token redundancy while preserving local spatiotemporal information. We further propose a training-free retrieval mixture-of-experts that leverages external models to better identify relevant frames. Our method, MemStream, achieves +8.0% on CG-Bench, +8.5% on LVBench, and +2.4% on VideoMME (Long) over ReKV with Qwen2.5-VL-7B.", "AI": {"tldr": "MemStream\u901a\u8fc7\u589e\u52a0token\u9884\u7b97\u3001\u81ea\u9002\u5e94\u9009\u62e9\u7b56\u7565\u548c\u514d\u8bad\u7ec3\u68c0\u7d22\u4e13\u5bb6\u6df7\u5408\uff0c\u663e\u8457\u63d0\u5347\u6d41\u5f0f\u89c6\u9891\u95ee\u7b54\u6027\u80fd", "motivation": "\u73b0\u6709\u6d41\u5f0f\u89c6\u9891\u7406\u89e3\u65b9\u6cd5\u4f7f\u7528\u6709\u9650\u7684\u6bcf\u5e27token\u6570\u91cf\uff0c\u5bfc\u81f4\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7ec6\u8282\u4e22\u5931\uff0c\u4e14\u5728\u5904\u7406\u5bc6\u96c6\u89c6\u9891\u6d41\u65f6\u5b58\u5728\u67e5\u8be2-\u5e27\u76f8\u4f3c\u5ea6\u968f\u65f6\u95f4\u589e\u52a0\u7684\u504f\u5dee\u95ee\u9898", "method": "1) \u589e\u52a0token\u9884\u7b97\u4ee5\u5b9e\u73b0\u66f4\u7ec6\u7c92\u5ea6\u7684\u65f6\u7a7a\u7406\u89e3\uff1b2) \u5f15\u5165\u81ea\u9002\u5e94\u9009\u62e9\u7b56\u7565\u51cf\u5c11token\u5197\u4f59\u540c\u65f6\u4fdd\u7559\u5c40\u90e8\u65f6\u7a7a\u4fe1\u606f\uff1b3) \u63d0\u51fa\u514d\u8bad\u7ec3\u68c0\u7d22\u4e13\u5bb6\u6df7\u5408\uff0c\u5229\u7528\u5916\u90e8\u6a21\u578b\u66f4\u597d\u5730\u8bc6\u522b\u76f8\u5173\u5e27", "result": "\u5728CG-Bench\u4e0a\u63d0\u53478.0%\uff0cLVBench\u4e0a\u63d0\u53478.5%\uff0cVideoMME(Long)\u4e0a\u63d0\u53472.4%\uff08\u76f8\u6bd4ReKV with Qwen2.5-VL-7B\uff09", "conclusion": "MemStream\u901a\u8fc7\u6269\u5c55token\u9884\u7b97\u548c\u521b\u65b0\u7684\u81ea\u9002\u5e94\u9009\u62e9\u4e0e\u68c0\u7d22\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6d41\u5f0f\u89c6\u9891\u95ee\u7b54\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5bc6\u96c6\u89c6\u9891\u6d41\u65f6\u7684\u5c40\u9650\u6027"}}
{"id": "2602.17898", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17898", "abs": "https://arxiv.org/abs/2602.17898", "authors": ["Jingquan Yan", "Yuwei Miao", "Peiran Yu", "Junzhou Huang"], "title": "Breaking the Correlation Plateau: On the Optimization and Capacity Limits of Attention-Based Regressors", "comment": "Accepted by ICLR 2026", "summary": "Attention-based regression models are often trained by jointly optimizing Mean Squared Error (MSE) loss and Pearson correlation coefficient (PCC) loss, emphasizing the magnitude of errors and the order or shape of targets, respectively. A common but poorly understood phenomenon during training is the PCC plateau: PCC stops improving early in training, even as MSE continues to decrease. We provide the first rigorous theoretical analysis of this behavior, revealing fundamental limitations in both optimization dynamics and model capacity. First, in regard to the flattened PCC curve, we uncover a critical conflict where lowering MSE (magnitude matching) can paradoxically suppress the PCC gradient (shape matching). This issue is exacerbated by the softmax attention mechanism, particularly when the data to be aggregated is highly homogeneous. Second, we identify a limitation in the model capacity: we derived a PCC improvement limit for any convex aggregator (including the softmax attention), showing that the convex hull of the inputs strictly bounds the achievable PCC gain. We demonstrate that data homogeneity intensifies both limitations. Motivated by these insights, we propose the Extrapolative Correlation Attention (ECA), which incorporates novel, theoretically-motivated mechanisms to improve the PCC optimization and extrapolate beyond the convex hull. Across diverse benchmarks, including challenging homogeneous data setting, ECA consistently breaks the PCC plateau, achieving significant improvements in correlation without compromising MSE performance.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u4ece\u7406\u8bba\u548c\u4f18\u5316\u89d2\u5ea6\u5206\u6790\u4e86\u6ce8\u610f\u529b\u56de\u5f52\u6a21\u578b\u4e2dPCC\u505c\u6ede\u73b0\u8c61\uff0c\u63ed\u793a\u4e86MSE\u4f18\u5316\u4e0ePCC\u68af\u5ea6\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u4ee5\u53ca\u51f8\u805a\u5408\u5668\u7684PCC\u6539\u8fdb\u6781\u9650\uff0c\u5e76\u63d0\u51fa\u4e86\u521b\u65b0\u7684\u5916\u63a8\u76f8\u5173\u6ce8\u610f\u529b\u673a\u5236\u6765\u7a81\u7834\u8fd9\u4e00\u74f6\u9888\u3002", "motivation": "\u6ce8\u610f\u529b\u56de\u5f52\u6a21\u578b\u8bad\u7ec3\u4e2d\u666e\u904d\u5b58\u5728\u4f46\u672a\u88ab\u6df1\u5165\u7406\u89e3\u7684PCC\u505c\u6ede\u73b0\u8c61\uff1a\u5373\u4f7fMSE\u6301\u7eed\u4e0b\u964d\uff0cPCC\u5728\u8bad\u7ec3\u65e9\u671f\u5c31\u505c\u6b62\u6539\u5584\u3002\u4f5c\u8005\u65e8\u5728\u4ece\u7406\u8bba\u548c\u4f18\u5316\u89d2\u5ea6\u6df1\u5165\u5206\u6790\u8fd9\u4e00\u73b0\u8c61\u7684\u6839\u672c\u539f\u56e0\u3002", "method": "\u9996\u5148\u4ece\u4f18\u5316\u52a8\u529b\u5b66\u89d2\u5ea6\u5206\u6790MSE\u964d\u4f4e\u4e0ePCC\u68af\u5ea6\u6291\u5236\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u7279\u522b\u5173\u6ce8softmax\u6ce8\u610f\u529b\u673a\u5236\u5728\u6570\u636e\u9ad8\u5ea6\u540c\u8d28\u5316\u65f6\u7684\u8868\u73b0\u3002\u5176\u6b21\u4ece\u6a21\u578b\u80fd\u529b\u89d2\u5ea6\u63a8\u5bfc\u51fa\u4efb\u4f55\u51f8\u805a\u5408\u5668\uff08\u5305\u62ecsoftmax\u6ce8\u610f\u529b\uff09\u7684PCC\u6539\u8fdb\u6781\u9650\u3002\u57fa\u4e8e\u8fd9\u4e9b\u7406\u8bba\u6d1e\u5bdf\uff0c\u63d0\u51fa\u4e86\u5916\u63a8\u76f8\u5173\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5305\u542b\u65b0\u9896\u7684\u7406\u8bba\u9a71\u52a8\u673a\u5236\u6765\u6539\u5584PCC\u4f18\u5316\u5e76\u8d85\u8d8a\u51f8\u5305\u9650\u5236\u3002", "result": "\u7406\u8bba\u5206\u6790\u63ed\u793a\u4e86PCC\u505c\u6ede\u7684\u6839\u672c\u539f\u56e0\uff1aMSE\u4f18\u5316\u4f1a\u6291\u5236PCC\u68af\u5ea6\uff0c\u4e14\u51f8\u805a\u5408\u5668\u7684PCC\u6539\u8fdb\u53d7\u8f93\u5165\u51f8\u5305\u7684\u4e25\u683c\u9650\u5236\u3002\u63d0\u51fa\u7684ECA\u673a\u5236\u5728\u5305\u62ec\u5177\u6709\u6311\u6218\u6027\u7684\u540c\u8d28\u6570\u636e\u8bbe\u7f6e\u5728\u5185\u7684\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u4e00\u81f4\u5730\u6253\u7834\u4e86PCC\u505c\u6ede\uff0c\u5728\u4fdd\u6301MSE\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u76f8\u5173\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u4e3a\u6ce8\u610f\u529b\u56de\u5f52\u6a21\u578b\u4e2d\u7684PCC\u505c\u6ede\u73b0\u8c61\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u7406\u8bba\u5206\u6790\uff0c\u63ed\u793a\u4e86\u4f18\u5316\u52a8\u529b\u5b66\u548c\u6a21\u578b\u5bb9\u91cf\u7684\u53cc\u91cd\u9650\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002ECA\u673a\u5236\u901a\u8fc7\u7406\u8bba\u9a71\u52a8\u7684\u8bbe\u8ba1\u6210\u529f\u7a81\u7834\u4e86PCC\u6539\u8fdb\u7684\u74f6\u9888\uff0c\u4e3a\u76f8\u5173\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u91cd\u8981\u6d1e\u89c1\u3002"}}
{"id": "2602.18037", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18037", "abs": "https://arxiv.org/abs/2602.18037", "authors": ["Johannes Ackermann", "Michael Noukhovitch", "Takashi Ishida", "Masashi Sugiyama"], "title": "Gradient Regularization Prevents Reward Hacking in Reinforcement Learning from Human Feedback and Verifiable Rewards", "comment": "25 pages, 15 figures", "summary": "Reinforcement Learning from Human Feedback (RLHF) or Verifiable Rewards (RLVR) are two key steps in the post-training of modern Language Models (LMs). A common problem is reward hacking, where the policy may exploit inaccuracies of the reward and learn an unintended behavior. Most previous works address this by limiting the policy update with a Kullback-Leibler (KL) penalty towards a reference model. We propose a different framing: Train the LM in a way that biases policy updates towards regions in which the reward is more accurate. First, we derive a theoretical connection between the accuracy of a reward model and the flatness of an optimum at convergence. Gradient regularization (GR) can then be used to bias training to flatter regions and thereby maintain reward model accuracy. We confirm these results by showing that the gradient norm and reward accuracy are empirically correlated in RLHF. We then show that Reference Resets of the KL penalty implicitly use GR to find flatter regions with higher reward accuracy. We further improve on this by proposing to use explicit GR with an efficient finite-difference estimate. Empirically, GR performs better than a KL penalty across a diverse set of RL experiments with LMs. GR achieves a higher GPT-judged win-rate in RLHF, avoids overly focusing on the format in rule-based math rewards, and prevents hacking the judge in LLM-as-a-Judge math tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u68af\u5ea6\u6b63\u5219\u5316\uff08GR\uff09\u66ff\u4ee3\u4f20\u7edf\u7684KL\u60e9\u7f5a\u6765\u89e3\u51b3RLHF\u4e2d\u7684\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\uff0c\u901a\u8fc7\u5c06\u7b56\u7565\u66f4\u65b0\u504f\u5411\u5956\u52b1\u6a21\u578b\u66f4\u51c6\u786e\u7684\u5e73\u5766\u533a\u57df\u6765\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u5728\u8bed\u8a00\u6a21\u578b\u7684\u540e\u8bad\u7ec3\u4e2d\uff0cRLHF\u548cRLVR\u5b58\u5728\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\uff0c\u5373\u7b56\u7565\u53ef\u80fd\u5229\u7528\u5956\u52b1\u6a21\u578b\u7684\u4e0d\u51c6\u786e\u6027\u5b66\u4e60\u5230\u975e\u9884\u671f\u7684\u884c\u4e3a\u3002\u4f20\u7edf\u65b9\u6cd5\u4f7f\u7528KL\u60e9\u7f5a\u6765\u9650\u5236\u7b56\u7565\u66f4\u65b0\uff0c\u4f46\u4f5c\u8005\u8ba4\u4e3a\u5e94\u8be5\u5c06\u7b56\u7565\u66f4\u65b0\u504f\u5411\u5956\u52b1\u6a21\u578b\u66f4\u51c6\u786e\u7684\u533a\u57df\u3002", "method": "\u4f5c\u8005\u9996\u5148\u5efa\u7acb\u4e86\u5956\u52b1\u6a21\u578b\u51c6\u786e\u6027\u4e0e\u6536\u655b\u65f6\u6700\u4f18\u89e3\u5e73\u5766\u5ea6\u4e4b\u95f4\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u63d0\u51fa\u4f7f\u7528\u68af\u5ea6\u6b63\u5219\u5316\uff08GR\uff09\u5c06\u8bad\u7ec3\u504f\u5411\u5e73\u5766\u533a\u57df\u4ee5\u4fdd\u6301\u5956\u52b1\u6a21\u578b\u51c6\u786e\u6027\u3002\u4ed6\u4eec\u53d1\u73b0KL\u60e9\u7f5a\u7684\u53c2\u8003\u91cd\u7f6e\u9690\u5f0f\u4f7f\u7528\u4e86GR\uff0c\u5e76\u8fdb\u4e00\u6b65\u63d0\u51fa\u4f7f\u7528\u663e\u5f0fGR\u914d\u5408\u9ad8\u6548\u6709\u9650\u5dee\u5206\u4f30\u8ba1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u68af\u5ea6\u6b63\u5219\u5316\u5728\u591a\u79cd\u8bed\u8a00\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u5b9e\u9a8c\u4e2d\u4f18\u4e8eKL\u60e9\u7f5a\uff1a\u5728RLHF\u4e2d\u83b7\u5f97\u66f4\u9ad8\u7684GPT\u8bc4\u5224\u80dc\u7387\uff0c\u907f\u514d\u8fc7\u5ea6\u5173\u6ce8\u57fa\u4e8e\u89c4\u5219\u7684\u6570\u5b66\u5956\u52b1\u683c\u5f0f\uff0c\u5e76\u9632\u6b62\u5728LLM-as-a-Judge\u6570\u5b66\u4efb\u52a1\u4e2d\u9ed1\u5ba2\u8bc4\u5224\u8005\u3002", "conclusion": "\u68af\u5ea6\u6b63\u5219\u5316\u4e3a\u89e3\u51b3RLHF\u4e2d\u7684\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u6846\u67b6\u548c\u5b9e\u7528\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u7b56\u7565\u66f4\u65b0\u504f\u5411\u5956\u52b1\u6a21\u578b\u66f4\u51c6\u786e\u7684\u5e73\u5766\u533a\u57df\uff0c\u6bd4\u4f20\u7edfKL\u60e9\u7f5a\u65b9\u6cd5\u8868\u73b0\u66f4\u597d\u3002"}}
{"id": "2602.17918", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17918", "abs": "https://arxiv.org/abs/2602.17918", "authors": ["Jialin Yu", "Mo\u00efse Blanchard"], "title": "Distribution-Free Sequential Prediction with Abstentions", "comment": "38 pages, 2 figures. Submitted to COLT 2026. Extended version", "summary": "We study a sequential prediction problem in which an adversary is allowed to inject arbitrarily many adversarial instances in a stream of i.i.d.\\ instances, but at each round, the learner may also \\emph{abstain} from making a prediction without incurring any penalty if the instance was indeed corrupted. This semi-adversarial setting naturally sits between the classical stochastic case with i.i.d.\\ instances for which function classes with finite VC dimension are learnable; and the adversarial case with arbitrary instances, known to be significantly more restrictive. For this problem, Goel et al. (2023) showed that, if the learner knows the distribution $\u03bc$ of clean samples in advance, learning can be achieved for all VC classes without restrictions on adversary corruptions. This is, however, a strong assumption in both theory and practice: a natural question is whether similar learning guarantees can be achieved without prior distributional knowledge, as is standard in classical learning frameworks (e.g., PAC learning or asymptotic consistency) and other non-i.i.d.\\ models (e.g., smoothed online learning). We therefore focus on the distribution-free setting where $\u03bc$ is \\emph{unknown} and propose an algorithm \\textsc{AbstainBoost} based on a boosting procedure of weak learners, which guarantees sublinear error for general VC classes in \\emph{distribution-free} abstention learning for oblivious adversaries. These algorithms also enjoy similar guarantees for adaptive adversaries, for structured function classes including linear classifiers. These results are complemented with corresponding lower bounds, which reveal an interesting polynomial trade-off between misclassification error and number of erroneous abstentions.", "AI": {"tldr": "\u7814\u7a76\u5728\u5bf9\u6297\u6027\u6ce8\u5165\u6570\u636e\u6d41\u4e2d\uff0c\u5b66\u4e60\u8005\u53ef\u4ee5\u9009\u62e9\u5f03\u6743\u4ee5\u907f\u514d\u9519\u8bef\u9884\u6d4b\u7684\u5e8f\u5217\u9884\u6d4b\u95ee\u9898\u3002\u5728\u672a\u77e5\u5206\u5e03\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u51faAbstainBoost\u7b97\u6cd5\uff0c\u4e3aVC\u7c7b\u5b9e\u73b0\u5206\u5e03\u65e0\u5173\u7684\u5b66\u4e60\u4fdd\u8bc1\u3002", "motivation": "\u7814\u7a76\u534a\u5bf9\u6297\u6027\u8bbe\u7f6e\u4e0b\u7684\u5b66\u4e60\u95ee\u9898\uff0c\u5176\u4e2d\u5bf9\u6297\u8005\u53ef\u4ee5\u5728\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u6d41\u4e2d\u6ce8\u5165\u4efb\u610f\u591a\u5bf9\u6297\u6027\u5b9e\u4f8b\uff0c\u4f46\u5b66\u4e60\u8005\u53ef\u4ee5\u5728\u5b9e\u4f8b\u88ab\u6c61\u67d3\u65f6\u5f03\u6743\u800c\u4e0d\u53d7\u60e9\u7f5a\u3002\u73b0\u6709\u7814\u7a76\u5047\u8bbe\u5df2\u77e5\u5e72\u51c0\u6837\u672c\u5206\u5e03\uff0c\u8fd9\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e2d\u90fd\u662f\u5f3a\u5047\u8bbe\uff0c\u9700\u8981\u63a2\u7d22\u672a\u77e5\u5206\u5e03\u4e0b\u7684\u5b66\u4e60\u4fdd\u8bc1\u3002", "method": "\u63d0\u51faAbstainBoost\u7b97\u6cd5\uff0c\u57fa\u4e8e\u5f31\u5b66\u4e60\u5668\u7684\u63d0\u5347\u8fc7\u7a0b\uff0c\u4e3a\u4e00\u822cVC\u7c7b\u5728\u5206\u5e03\u65e0\u5173\u7684\u5f03\u6743\u5b66\u4e60\u4e2d\u4fdd\u8bc1\u4e9a\u7ebf\u6027\u9519\u8bef\u7387\u3002\u7b97\u6cd5\u5bf9\u9057\u5fd8\u578b\u5bf9\u6297\u8005\u6709\u6548\uff0c\u5bf9\u81ea\u9002\u5e94\u5bf9\u6297\u8005\u5219\u9002\u7528\u4e8e\u7ed3\u6784\u5316\u51fd\u6570\u7c7b\uff08\u5982\u7ebf\u6027\u5206\u7c7b\u5668\uff09\u3002", "result": "\u7b97\u6cd5\u5728\u672a\u77e5\u5206\u5e03\u60c5\u51b5\u4e0b\u4e3aVC\u7c7b\u5b9e\u73b0\u4e86\u5206\u5e03\u65e0\u5173\u7684\u5f03\u6743\u5b66\u4e60\u4fdd\u8bc1\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u9519\u8bef\u5206\u7c7b\u9519\u8bef\u4e0e\u9519\u8bef\u5f03\u6743\u6b21\u6570\u4e4b\u95f4\u7684\u591a\u9879\u5f0f\u6743\u8861\u5173\u7cfb\u3002", "conclusion": "\u5728\u672a\u77e5\u5206\u5e03\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u5f03\u6743\u673a\u5236\u53ef\u4ee5\u5728\u534a\u5bf9\u6297\u6027\u8bbe\u7f6e\u4e2d\u5b9e\u73b0VC\u7c7b\u7684\u5b66\u4e60\uff0c\u586b\u8865\u4e86\u7ecf\u5178\u968f\u673a\u8bbe\u7f6e\u548c\u5b8c\u5168\u5bf9\u6297\u8bbe\u7f6e\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u7406\u8bba\u4fdd\u8bc1\u3002"}}
{"id": "2602.18117", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18117", "abs": "https://arxiv.org/abs/2602.18117", "authors": ["Yongjae Shin", "Jongseong Chae", "Jongeui Park", "Youngchul Sung"], "title": "Flow Matching with Injected Noise for Offline-to-Online Reinforcement Learning", "comment": "ICLR 2026 camera-ready", "summary": "Generative models have recently demonstrated remarkable success across diverse domains, motivating their adoption as expressive policies in reinforcement learning (RL). While they have shown strong performance in offline RL, particularly where the target distribution is well defined, their extension to online fine-tuning has largely been treated as a direct continuation of offline pre-training, leaving key challenges unaddressed. In this paper, we propose Flow Matching with Injected Noise for Offline-to-Online RL (FINO), a novel method that leverages flow matching-based policies to enhance sample efficiency for offline-to-online RL. FINO facilitates effective exploration by injecting noise into policy training, thereby encouraging a broader range of actions beyond those observed in the offline dataset. In addition to exploration-enhanced flow policy training, we combine an entropy-guided sampling mechanism to balance exploration and exploitation, allowing the policy to adapt its behavior throughout online fine-tuning. Experiments across diverse, challenging tasks demonstrate that FINO consistently achieves superior performance under limited online budgets.", "AI": {"tldr": "FINO\u662f\u4e00\u79cd\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u7b56\u7565\u65b9\u6cd5\uff0c\u901a\u8fc7\u6ce8\u5165\u566a\u58f0\u589e\u5f3a\u79bb\u7ebf\u5230\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u6837\u672c\u6548\u7387\uff0c\u7ed3\u5408\u71b5\u5f15\u5bfc\u91c7\u6837\u673a\u5236\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u6269\u5c55\u5230\u5728\u7ebf\u5fae\u8c03\u65f6\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5c06\u5728\u7ebf\u5fae\u8c03\u89c6\u4e3a\u79bb\u7ebf\u9884\u8bad\u7ec3\u7684\u76f4\u63a5\u5ef6\u7eed\uff0c\u672a\u80fd\u89e3\u51b3\u5173\u952e\u95ee\u9898\uff0c\u7279\u522b\u662f\u63a2\u7d22\u6548\u7387\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faFINO\u65b9\u6cd5\uff1a1) \u5728\u7b56\u7565\u8bad\u7ec3\u4e2d\u6ce8\u5165\u566a\u58f0\uff0c\u9f13\u52b1\u8d85\u8d8a\u79bb\u7ebf\u6570\u636e\u96c6\u89c2\u5bdf\u5230\u7684\u52a8\u4f5c\u8303\u56f4\uff1b2) \u7ed3\u5408\u71b5\u5f15\u5bfc\u91c7\u6837\u673a\u5236\uff0c\u5728\u5728\u7ebf\u5fae\u8c03\u8fc7\u7a0b\u4e2d\u52a8\u6001\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u3002", "result": "\u5728\u591a\u6837\u5316\u7684\u6311\u6218\u6027\u4efb\u52a1\u5b9e\u9a8c\u4e2d\uff0cFINO\u5728\u6709\u9650\u7684\u5728\u7ebf\u9884\u7b97\u4e0b\u59cb\u7ec8\u5b9e\u73b0\u5353\u8d8a\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u6837\u672c\u6548\u7387\u4f18\u52bf\u3002", "conclusion": "FINO\u901a\u8fc7\u566a\u58f0\u6ce8\u5165\u548c\u71b5\u5f15\u5bfc\u91c7\u6837\u6709\u6548\u89e3\u51b3\u4e86\u79bb\u7ebf\u5230\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u7ebf\u5fae\u8c03\u7684\u6837\u672c\u6548\u7387\u3002"}}
{"id": "2602.17940", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17940", "abs": "https://arxiv.org/abs/2602.17940", "authors": ["Shogo Iwazaki"], "title": "Tighter Regret Lower Bound for Gaussian Process Bandits with Squared Exponential Kernel in Hypersphere", "comment": "27 pages, 2 figures", "summary": "We study an algorithm-independent, worst-case lower bound for the Gaussian process (GP) bandit problem in the frequentist setting, where the reward function is fixed and has a bounded norm in the known reproducing kernel Hilbert space (RKHS). Specifically, we focus on the squared exponential (SE) kernel, one of the most widely used kernel functions in GP bandits. One of the remaining open questions for this problem is the gap in the \\emph{dimension-dependent} logarithmic factors between upper and lower bounds. This paper partially resolves this open question under a hyperspherical input domain. We show that any algorithm suffers $\u03a9(\\sqrt{T (\\ln T)^{d} (\\ln \\ln T)^{-d}})$ cumulative regret, where $T$ and $d$ represent the total number of steps and the dimension of the hyperspherical domain, respectively. Regarding the simple regret, we show that any algorithm requires $\u03a9(\u03b5^{-2}(\\ln \\frac{1}\u03b5)^d (\\ln \\ln \\frac{1}\u03b5)^{-d})$ time steps to find an $\u03b5$-optimal point. We also provide the improved $O((\\ln T)^{d+1}(\\ln \\ln T)^{-d})$ upper bound on the maximum information gain for the SE kernel. Our results guarantee the optimality of the existing best algorithm up to \\emph{dimension-independent} logarithmic factors under a hyperspherical input domain.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u9ad8\u65af\u8fc7\u7a0b\uff08GP\uff09\u8d4c\u535a\u673a\u95ee\u9898\u5728\u9891\u7387\u4e3b\u4e49\u8bbe\u7f6e\u4e0b\u7684\u7b97\u6cd5\u65e0\u5173\u6700\u574f\u60c5\u51b5\u4e0b\u754c\uff0c\u7279\u522b\u5173\u6ce8\u5e73\u65b9\u6307\u6570\uff08SE\uff09\u6838\u51fd\u6570\u3002\u8bba\u6587\u90e8\u5206\u89e3\u51b3\u4e86\u7ef4\u5ea6\u4f9d\u8d56\u5bf9\u6570\u56e0\u5b50\u7684\u5f00\u653e\u6027\u95ee\u9898\uff0c\u7ed9\u51fa\u4e86\u7d2f\u79ef\u9057\u61be\u548c\u7b80\u5355\u9057\u61be\u7684\u4e0b\u754c\uff0c\u5e76\u6539\u8fdb\u4e86SE\u6838\u7684\u6700\u5927\u4fe1\u606f\u589e\u76ca\u4e0a\u754c\u3002", "motivation": "\u9ad8\u65af\u8fc7\u7a0b\u8d4c\u535a\u673a\u95ee\u9898\u4e2d\uff0c\u5bf9\u4e8e\u5e73\u65b9\u6307\u6570\u6838\u51fd\u6570\uff0c\u73b0\u6709\u4e0a\u4e0b\u754c\u5728\u7ef4\u5ea6\u4f9d\u8d56\u7684\u5bf9\u6570\u56e0\u5b50\u65b9\u9762\u5b58\u5728\u5dee\u8ddd\uff0c\u8fd9\u662f\u4e00\u4e2a\u5c1a\u672a\u89e3\u51b3\u7684\u5f00\u653e\u6027\u95ee\u9898\u3002\u8bba\u6587\u65e8\u5728\u90e8\u5206\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8d85\u7403\u5f62\u8f93\u5165\u57df\u4e0b\uff0c\u4e3a\u7b97\u6cd5\u6027\u80fd\u63d0\u4f9b\u66f4\u7cbe\u786e\u7684\u7406\u8bba\u754c\u9650\u3002", "method": "\u7814\u7a76\u91c7\u7528\u7406\u8bba\u5206\u6790\u65b9\u6cd5\uff0c\u5728\u9891\u7387\u4e3b\u4e49\u8bbe\u7f6e\u4e0b\u5206\u6790\u9ad8\u65af\u8fc7\u7a0b\u8d4c\u535a\u673a\u95ee\u9898\u3002\u901a\u8fc7\u6570\u5b66\u63a8\u5bfc\u548c\u8bc1\u660e\uff0c\u5efa\u7acb\u4e86\u7b97\u6cd5\u65e0\u5173\u7684\u6700\u574f\u60c5\u51b5\u7d2f\u79ef\u9057\u61be\u4e0b\u754c\u548c\u7b80\u5355\u9057\u61be\u4e0b\u754c\uff0c\u540c\u65f6\u6539\u8fdb\u4e86\u5e73\u65b9\u6307\u6570\u6838\u7684\u6700\u5927\u4fe1\u606f\u589e\u76ca\u4e0a\u754c\u3002", "result": "1. \u4efb\u4f55\u7b97\u6cd5\u5728\u8d85\u7403\u5f62\u8f93\u5165\u57df\u4e0b\u90fd\u906d\u53d7\u03a9(\u221a[T(ln T)^d(ln ln T)^{-d}])\u7684\u7d2f\u79ef\u9057\u61be\uff1b2. \u4efb\u4f55\u7b97\u6cd5\u9700\u8981\u03a9(\u03b5^{-2}(ln 1/\u03b5)^d(ln ln 1/\u03b5)^{-d})\u65f6\u95f4\u6b65\u627e\u5230\u03b5-\u6700\u4f18\u70b9\uff1b3. \u6539\u8fdb\u4e86SE\u6838\u7684\u6700\u5927\u4fe1\u606f\u589e\u76ca\u4e0a\u754c\u4e3aO((ln T)^{d+1}(ln ln T)^{-d})\u3002", "conclusion": "\u8be5\u7814\u7a76\u90e8\u5206\u89e3\u51b3\u4e86GP\u8d4c\u535a\u673a\u95ee\u9898\u4e2d\u7ef4\u5ea6\u4f9d\u8d56\u5bf9\u6570\u56e0\u5b50\u7684\u5f00\u653e\u6027\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5728\u8d85\u7403\u5f62\u8f93\u5165\u57df\u4e0b\uff0c\u73b0\u6709\u6700\u4f73\u7b97\u6cd5\u5728\u7ef4\u5ea6\u65e0\u5173\u5bf9\u6570\u56e0\u5b50\u610f\u4e49\u4e0b\u662f\u6700\u4f18\u7684\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u9ad8\u65af\u8fc7\u7a0b\u8d4c\u535a\u673a\u7b97\u6cd5\u7684\u7406\u8bba\u5206\u6790\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u754c\u9650\u3002"}}
{"id": "2602.18182", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18182", "abs": "https://arxiv.org/abs/2602.18182", "authors": ["Daniel Romero-Alvarado", "Fernando Mart\u00ednez-Plumed", "Lorenzo Pacchiardi", "Hugo Save", "Siddhesh Milind Pawar", "Behzad Mehrbakhsh", "Pablo Antonio Moreno Casares", "Ben Slater", "Paolo Bova", "Peter Romero", "Zachary R. Tyler", "Jonathan Prunty", "Luning Sun", "Jose Hernandez-Orallo"], "title": "Capabilities Ain't All You Need: Measuring Propensities in AI", "comment": null, "summary": "AI evaluation has primarily focused on measuring capabilities, with formal approaches inspired from Item Response Theory (IRT) being increasingly applied. Yet propensities - the tendencies of models to exhibit particular behaviours - play a central role in determining both performance and safety outcomes. However, traditional IRT describes a model's success on a task as a monotonic function of model capabilities and task demands, an approach unsuited to propensities, where both excess and deficiency can be problematic. Here, we introduce the first formal framework for measuring AI propensities by using a bilogistic formulation for model success, which attributes high success probability when the model's propensity is within an \"ideal band\". Further, we estimate the limits of the ideal band using LLMs equipped with newly developed task-agnostic rubrics. Applying our framework to six families of LLM models whose propensities are incited in either direction, we find that we can measure how much the propensity is shifted and what effect this has on the tasks. Critically, propensities estimated using one benchmark successfully predict behaviour on held-out tasks. Moreover, we obtain stronger predictive power when combining propensities and capabilities than either separately. More broadly, our framework showcases how rigorous propensity measurements can be conducted and how it yields gains over solely using capability evaluations to predict AI behaviour.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u6d4b\u91cfAI\u503e\u5411\u6027\u7684\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u4f7f\u7528\u53cc\u903b\u8f91\u516c\u5f0f\u6765\u63cf\u8ff0\u6a21\u578b\u5728\"\u7406\u60f3\u533a\u95f4\"\u5185\u7684\u9ad8\u6210\u529f\u7387\uff0c\u901a\u8fc7\u4efb\u52a1\u65e0\u5173\u7684\u8bc4\u4f30\u6807\u51c6\u4f30\u8ba1\u7406\u60f3\u533a\u95f4\u8fb9\u754c\uff0c\u53d1\u73b0\u503e\u5411\u6027\u4e0e\u80fd\u529b\u7ed3\u5408\u6bd4\u5355\u72ec\u4f7f\u7528\u4efb\u4e00\u9879\u80fd\u66f4\u597d\u5730\u9884\u6d4bAI\u884c\u4e3a\u3002", "motivation": "\u5f53\u524dAI\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u80fd\u529b\u6d4b\u91cf\uff0c\u4f46\u503e\u5411\u6027\uff08\u6a21\u578b\u5c55\u73b0\u7279\u5b9a\u884c\u4e3a\u7684\u8d8b\u52bf\uff09\u5bf9\u6027\u80fd\u548c\u5b89\u5168\u7ed3\u679c\u6709\u6838\u5fc3\u5f71\u54cd\u3002\u4f20\u7edf\u9879\u76ee\u53cd\u5e94\u7406\u8bba\uff08IRT\uff09\u5c06\u6a21\u578b\u5728\u4efb\u52a1\u4e0a\u7684\u6210\u529f\u63cf\u8ff0\u4e3a\u6a21\u578b\u80fd\u529b\u548c\u4efb\u52a1\u9700\u6c42\u7684\u5355\u8c03\u51fd\u6570\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8e\u503e\u5411\u6027\u8bc4\u4f30\uff0c\u56e0\u4e3a\u503e\u5411\u6027\u8fc7\u9ad8\u6216\u8fc7\u4f4e\u90fd\u53ef\u80fd\u5b58\u5728\u95ee\u9898\u3002", "method": "\u5f15\u5165\u9996\u4e2a\u6d4b\u91cfAI\u503e\u5411\u6027\u7684\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u903b\u8f91\u516c\u5f0f\u63cf\u8ff0\u6a21\u578b\u6210\u529f\u6982\u7387\uff0c\u5f53\u6a21\u578b\u503e\u5411\u6027\u5904\u4e8e\"\u7406\u60f3\u533a\u95f4\"\u5185\u65f6\u83b7\u5f97\u9ad8\u6210\u529f\u6982\u7387\u3002\u4f7f\u7528\u914d\u5907\u65b0\u5f00\u53d1\u7684\u4efb\u52a1\u65e0\u5173\u8bc4\u4f30\u6807\u51c6\u7684LLM\u6765\u4f30\u8ba1\u7406\u60f3\u533a\u95f4\u7684\u8fb9\u754c\u3002\u5c06\u8be5\u6846\u67b6\u5e94\u7528\u4e8e\u516d\u4e2aLLM\u6a21\u578b\u5bb6\u65cf\uff0c\u8fd9\u4e9b\u6a21\u578b\u7684\u503e\u5411\u6027\u5728\u4e0d\u540c\u65b9\u5411\u4e0a\u88ab\u6fc0\u53d1\u3002", "result": "\u80fd\u591f\u6d4b\u91cf\u503e\u5411\u6027\u504f\u79fb\u7a0b\u5ea6\u53ca\u5176\u5bf9\u4efb\u52a1\u7684\u5f71\u54cd\uff1b\u4f7f\u7528\u4e00\u4e2a\u57fa\u51c6\u4f30\u8ba1\u7684\u503e\u5411\u6027\u6210\u529f\u9884\u6d4b\u4e86\u4fdd\u7559\u4efb\u52a1\u4e0a\u7684\u884c\u4e3a\uff1b\u7ed3\u5408\u503e\u5411\u6027\u548c\u80fd\u529b\u6bd4\u5355\u72ec\u4f7f\u7528\u4efb\u4e00\u9879\u83b7\u5f97\u66f4\u5f3a\u7684\u9884\u6d4b\u80fd\u529b\uff1b\u5c55\u793a\u4e86\u5982\u4f55\u8fdb\u884c\u4e25\u683c\u7684\u503e\u5411\u6027\u6d4b\u91cf\u4ee5\u53ca\u5982\u4f55\u8d85\u8d8a\u4ec5\u4f7f\u7528\u80fd\u529b\u8bc4\u4f30\u6765\u9884\u6d4bAI\u884c\u4e3a\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u5982\u4f55\u8fdb\u884c\u4e25\u683c\u7684\u503e\u5411\u6027\u6d4b\u91cf\uff0c\u5e76\u8bc1\u660e\u4e86\u7ed3\u5408\u503e\u5411\u6027\u548c\u80fd\u529b\u8bc4\u4f30\u6bd4\u5355\u72ec\u4f7f\u7528\u80fd\u529b\u8bc4\u4f30\u80fd\u66f4\u597d\u5730\u9884\u6d4bAI\u884c\u4e3a\uff0c\u4e3aAI\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.18195", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18195", "abs": "https://arxiv.org/abs/2602.18195", "authors": ["Hairong Chen", "Yicheng Feng", "Ziyu Jia", "Samir Bhatt", "Hengguan Huang"], "title": "LERD: Latent Event-Relational Dynamics for Neurodegenerative Classification", "comment": null, "summary": "Alzheimer's disease (AD) alters brain electrophysiology and disrupts multichannel EEG dynamics, making accurate and clinically useful EEG-based diagnosis increasingly important for screening and disease monitoring. However, many existing approaches rely on black-box classifiers and do not explicitly model the underlying dynamics that generate observed signals. To address these limitations, we propose LERD, an end-to-end Bayesian electrophysiological neural dynamical system that infers latent neural events and their relational structure directly from multichannel EEG without event or interaction annotations. LERD combines a continuous-time event inference module with a stochastic event-generation process to capture flexible temporal patterns, while incorporating an electrophysiology-inspired dynamical prior to guide learning in a principled way. We further provide theoretical analysis that yields a tractable bound for training and stability guarantees for the inferred relational dynamics. Extensive experiments on synthetic benchmarks and two real-world AD EEG cohorts demonstrate that LERD consistently outperforms strong baselines and yields physiology-aligned latent summaries that help characterize group-level dynamical differences.", "AI": {"tldr": "LERD\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u8d1d\u53f6\u65af\u7535\u751f\u7406\u795e\u7ecf\u52a8\u529b\u5b66\u7cfb\u7edf\uff0c\u76f4\u63a5\u4ece\u591a\u901a\u9053EEG\u63a8\u65ad\u6f5c\u5728\u795e\u7ecf\u4e8b\u4ef6\u53ca\u5176\u5173\u7cfb\u7ed3\u6784\uff0c\u7528\u4e8e\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u8bca\u65ad\u548c\u8868\u5f81\u3002", "motivation": "\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u6539\u53d8\u4e86\u5927\u8111\u7535\u751f\u7406\u5e76\u7834\u574f\u4e86\u591a\u901a\u9053EEG\u52a8\u529b\u5b66\uff0c\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u4f9d\u8d56\u9ed1\u76d2\u5206\u7c7b\u5668\uff0c\u6ca1\u6709\u660e\u786e\u5efa\u6a21\u751f\u6210\u89c2\u5bdf\u4fe1\u53f7\u7684\u57fa\u7840\u52a8\u529b\u5b66\u3002", "method": "\u63d0\u51faLERD\u7cfb\u7edf\uff0c\u7ed3\u5408\u8fde\u7eed\u65f6\u95f4\u4e8b\u4ef6\u63a8\u65ad\u6a21\u5757\u548c\u968f\u673a\u4e8b\u4ef6\u751f\u6210\u8fc7\u7a0b\uff0c\u6355\u6349\u7075\u6d3b\u7684\u65f6\u95f4\u6a21\u5f0f\uff0c\u5e76\u878d\u5165\u7535\u751f\u7406\u542f\u53d1\u7684\u52a8\u529b\u5b66\u5148\u9a8c\u6765\u6307\u5bfc\u5b66\u4e60\u3002", "result": "\u5728\u5408\u6210\u57fa\u51c6\u548c\u4e24\u4e2a\u771f\u5b9e\u4e16\u754cAD EEG\u961f\u5217\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cLERD\u59cb\u7ec8\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u4ea7\u751f\u4e0e\u751f\u7406\u5bf9\u9f50\u7684\u6f5c\u5728\u6458\u8981\uff0c\u6709\u52a9\u4e8e\u8868\u5f81\u7fa4\u4f53\u6c34\u5e73\u7684\u52a8\u529b\u5b66\u5dee\u5f02\u3002", "conclusion": "LERD\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u3001\u57fa\u4e8e\u52a8\u529b\u5b66\u7684EEG\u5206\u6790\u65b9\u6cd5\uff0c\u80fd\u591f\u76f4\u63a5\u4ece\u591a\u901a\u9053EEG\u63a8\u65ad\u795e\u7ecf\u4e8b\u4ef6\u548c\u5173\u7cfb\u7ed3\u6784\uff0c\u4e3a\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u8bca\u65ad\u548c\u76d1\u6d4b\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2602.17947", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17947", "abs": "https://arxiv.org/abs/2602.17947", "authors": ["Yubo Zhou", "Jun Shu", "Junmin Liu", "Deyu Meng"], "title": "Understanding the Generalization of Bilevel Programming in Hyperparameter Optimization: A Tale of Bias-Variance Decomposition", "comment": null, "summary": "Gradient-based hyperparameter optimization (HPO) have emerged recently, leveraging bilevel programming techniques to optimize hyperparameter by estimating hypergradient w.r.t. validation loss. Nevertheless, previous theoretical works mainly focus on reducing the gap between the estimation and ground-truth (i.e., the bias), while ignoring the error due to data distribution (i.e., the variance), which degrades performance. To address this issue, we conduct a bias-variance decomposition for hypergradient estimation error and provide a supplemental detailed analysis of the variance term ignored by previous works. We also present a comprehensive analysis of the error bounds for hypergradient estimation. This facilitates an easy explanation of some phenomena commonly observed in practice, like overfitting to the validation set. Inspired by the derived theories, we propose an ensemble hypergradient strategy to reduce the variance in HPO algorithms effectively. Experimental results on tasks including regularization hyperparameter learning, data hyper-cleaning, and few-shot learning demonstrate that our variance reduction strategy improves hypergradient estimation. To explain the improved performance, we establish a connection between excess error and hypergradient estimation, offering some understanding of empirical observations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u57fa\u4e8e\u68af\u5ea6\u7684\u8d85\u53c2\u6570\u4f18\u5316\u4e2d\u8d85\u68af\u5ea6\u4f30\u8ba1\u7684\u504f\u5dee-\u65b9\u5dee\u5206\u89e3\uff0c\u91cd\u70b9\u5173\u6ce8\u4ee5\u5f80\u7814\u7a76\u5ffd\u7565\u7684\u65b9\u5dee\u9879\uff0c\u63d0\u51fa\u96c6\u6210\u8d85\u68af\u5ea6\u7b56\u7565\u6765\u964d\u4f4e\u65b9\u5dee\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u9a8c\u8bc1\u4e86\u6539\u8fdb\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u68af\u5ea6\u7684\u8d85\u53c2\u6570\u4f18\u5316\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8d85\u68af\u5ea6\u4f30\u8ba1\u7684\u504f\u5dee\uff0c\u800c\u5ffd\u7565\u4e86\u6570\u636e\u5206\u5e03\u5bfc\u81f4\u7684\u65b9\u5dee\u8bef\u5dee\uff0c\u8fd9\u4f1a\u5f71\u54cd\u7b97\u6cd5\u6027\u80fd\u3002\u4f5c\u8005\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7406\u8bba\u7a7a\u767d\uff0c\u5e76\u57fa\u4e8e\u5206\u6790\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "1. \u5bf9\u8d85\u68af\u5ea6\u4f30\u8ba1\u8bef\u5dee\u8fdb\u884c\u504f\u5dee-\u65b9\u5dee\u5206\u89e3\uff1b2. \u63d0\u4f9b\u8d85\u68af\u5ea6\u4f30\u8ba1\u8bef\u5dee\u754c\u7684\u5168\u9762\u5206\u6790\uff1b3. \u63d0\u51fa\u96c6\u6210\u8d85\u68af\u5ea6\u7b56\u7565\u6765\u6709\u6548\u964d\u4f4e\u65b9\u5dee\uff1b4. \u5728\u6b63\u5219\u5316\u8d85\u53c2\u6570\u5b66\u4e60\u3001\u6570\u636e\u8d85\u6e05\u6d17\u548c\u5c11\u6837\u672c\u5b66\u4e60\u7b49\u4efb\u52a1\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u65b9\u5dee\u964d\u4f4e\u7b56\u7565\u6539\u8fdb\u4e86\u8d85\u68af\u5ea6\u4f30\u8ba1\u3002\u4f5c\u8005\u8fd8\u5efa\u7acb\u4e86\u8d85\u68af\u5ea6\u4f30\u8ba1\u4e0e\u8d85\u989d\u8bef\u5dee\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u4e3a\u5b9e\u8bc1\u89c2\u5bdf\u63d0\u4f9b\u4e86\u7406\u8bba\u89e3\u91ca\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u8d85\u68af\u5ea6\u4f30\u8ba1\u4e2d\u65b9\u5dee\u9879\u7684\u91cd\u8981\u6027\uff0c\u63d0\u51fa\u7684\u96c6\u6210\u8d85\u68af\u5ea6\u7b56\u7565\u80fd\u6709\u6548\u964d\u4f4e\u65b9\u5dee\uff0c\u6539\u5584\u8d85\u53c2\u6570\u4f18\u5316\u6027\u80fd\uff0c\u5e76\u4e3a\u5b9e\u8df5\u4e2d\u89c2\u5bdf\u5230\u7684\u73b0\u8c61\uff08\u5982\u9a8c\u8bc1\u96c6\u8fc7\u62df\u5408\uff09\u63d0\u4f9b\u4e86\u7406\u8bba\u89e3\u91ca\u3002"}}
{"id": "2602.18230", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18230", "abs": "https://arxiv.org/abs/2602.18230", "authors": ["Jorge Carrasco Pollo", "Ioannis Kapetangeorgis", "Joshua Rosenthal", "John Hua Yao"], "title": "[Re] Benchmarking LLM Capabilities in Negotiation through Scoreable Games", "comment": "Accepted for publication at Transactions on Machine Learning Research (TMLR) and MLRC Journal Track, 2025. Code available at: https://github.com/joshrosie/FACT29", "summary": "Large Language Models (LLMs) demonstrate significant potential in multi-agent negotiation tasks, yet evaluation in this domain remains challenging due to a lack of robust and generalizable benchmarks. Abdelnabi et al. (2024) introduce a negotiation benchmark based on Scoreable Games, with the aim of developing a highly complex and realistic evaluation framework for LLMs. Our work investigates the reproducibility of claims in their benchmark, and provides a deeper understanding of its usability and generalizability. We replicate the original experiments on additional models, and introduce additional metrics to verify negotiation quality and evenness of evaluation. Our findings reveal that while the benchmark is indeed complex, model comparison is ambiguous, raising questions about its objectivity. Furthermore, we identify limitations in the experimental setup, particularly in information leakage detection and thoroughness of the ablation study. By examining and analyzing the behavior of a wider range of models on an extended version of the benchmark, we reveal insights that provide additional context to potential users. Our results highlight the importance of context in model-comparative evaluations.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5bf9Abdelnabi\u7b49\u4eba\uff082024\uff09\u63d0\u51fa\u7684\u57fa\u4e8e\u53ef\u8bc4\u5206\u6e38\u620f\u7684LLM\u591a\u667a\u80fd\u4f53\u8c08\u5224\u57fa\u51c6\u8fdb\u884c\u4e86\u53ef\u91cd\u590d\u6027\u5206\u6790\uff0c\u53d1\u73b0\u57fa\u51c6\u867d\u7136\u590d\u6742\u4f46\u6a21\u578b\u6bd4\u8f83\u5b58\u5728\u6a21\u7cca\u6027\uff0c\u5b9e\u9a8c\u8bbe\u7f6e\u5b58\u5728\u4fe1\u606f\u6cc4\u9732\u68c0\u6d4b\u548c\u6d88\u878d\u7814\u7a76\u4e0d\u8db3\u7b49\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524dLLM\u5728\u591a\u667a\u80fd\u4f53\u8c08\u5224\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u8be5\u9886\u57df\u7f3a\u4e4f\u7a33\u5065\u4e14\u53ef\u6cdb\u5316\u7684\u8bc4\u4f30\u57fa\u51c6\u3002Abdelnabi\u7b49\u4eba\uff082024\uff09\u63d0\u51fa\u4e86\u57fa\u4e8e\u53ef\u8bc4\u5206\u6e38\u620f\u7684\u8c08\u5224\u57fa\u51c6\uff0c\u672c\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u8be5\u57fa\u51c6\u58f0\u660e\u7684\u53ef\u91cd\u590d\u6027\uff0c\u5e76\u6df1\u5165\u7406\u89e3\u5176\u53ef\u7528\u6027\u548c\u6cdb\u5316\u6027\u3002", "method": "1. \u5728\u66f4\u591a\u6a21\u578b\u4e0a\u590d\u73b0\u539f\u59cb\u5b9e\u9a8c\uff1b2. \u5f15\u5165\u989d\u5916\u6307\u6807\u9a8c\u8bc1\u8c08\u5224\u8d28\u91cf\u548c\u8bc4\u4f30\u516c\u5e73\u6027\uff1b3. \u5728\u6269\u5c55\u7248\u672c\u7684\u57fa\u51c6\u4e0a\u5206\u6790\u66f4\u5e7f\u6cdb\u6a21\u578b\u7684\u884c\u4e3a\uff1b4. \u8bc6\u522b\u5b9e\u9a8c\u8bbe\u7f6e\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u4fe1\u606f\u6cc4\u9732\u68c0\u6d4b\u548c\u6d88\u878d\u7814\u7a76\u7684\u5b8c\u6574\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1. \u57fa\u51c6\u786e\u5b9e\u590d\u6742\uff0c\u4f46\u6a21\u578b\u6bd4\u8f83\u5b58\u5728\u6a21\u7cca\u6027\uff0c\u5bf9\u5176\u5ba2\u89c2\u6027\u63d0\u51fa\u8d28\u7591\uff1b2. \u5b9e\u9a8c\u8bbe\u7f6e\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5305\u62ec\u4fe1\u606f\u6cc4\u9732\u68c0\u6d4b\u4e0d\u8db3\u548c\u6d88\u878d\u7814\u7a76\u4e0d\u5f7b\u5e95\uff1b3. \u901a\u8fc7\u5206\u6790\u66f4\u5e7f\u6cdb\u6a21\u578b\u5728\u6269\u5c55\u57fa\u51c6\u4e0a\u7684\u884c\u4e3a\uff0c\u63ed\u793a\u4e86\u4e3a\u6f5c\u5728\u7528\u6237\u63d0\u4f9b\u989d\u5916\u80cc\u666f\u7684\u89c1\u89e3\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u4e0a\u4e0b\u6587\u5728\u6a21\u578b\u6bd4\u8f83\u8bc4\u4f30\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u6307\u51fa\u867d\u7136\u8be5\u8c08\u5224\u57fa\u51c6\u5177\u6709\u590d\u6742\u6027\uff0c\u4f46\u5728\u5ba2\u89c2\u6027\u548c\u5b9e\u9a8c\u4e25\u8c28\u6027\u65b9\u9762\u5b58\u5728\u6539\u8fdb\u7a7a\u95f4\uff0c\u4e3a\u672a\u6765\u57fa\u51c6\u5f00\u53d1\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2602.17948", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17948", "abs": "https://arxiv.org/abs/2602.17948", "authors": ["Yu Bai", "Zhe Wang", "Jiarui Zhang", "Dong-Xiao Zhang", "Yinjun Gao", "Jun-Jie Zhang"], "title": "A Geometric Probe of the Accuracy-Robustness Trade-off: Sharp Boundaries in Symmetry-Breaking Dimensional Expansion", "comment": "22 pages, 3 figures", "summary": "The trade-off between clean accuracy and adversarial robustness is a pervasive phenomenon in deep learning, yet its geometric origin remains elusive. In this work, we utilize Symmetry-Breaking Dimensional Expansion (SBDE) as a controlled probe to investigate the mechanism underlying this trade-off. SBDE expands input images by inserting constant-valued pixels, which breaks translational symmetry and consistently improves clean accuracy (e.g., from $90.47\\%$ to $95.63\\%$ on CIFAR-10 with ResNet-18) by reducing parameter degeneracy. However, this accuracy gain comes at the cost of reduced robustness against iterative white-box attacks. By employing a test-time \\emph{mask projection} that resets the inserted auxiliary pixels to their training values, we demonstrate that the vulnerability stems almost entirely from the inserted dimensions. The projection effectively neutralizes the attacks and restores robustness, revealing that the model achieves high accuracy by creating \\emph{sharp boundaries} (steep loss gradients) specifically along the auxiliary axes. Our findings provide a concrete geometric explanation for the accuracy-robustness paradox: the optimization landscape deepens the basin of attraction to improve accuracy but inevitably erects steep walls along the auxiliary degrees of freedom, creating a fragile sensitivity to off-manifold perturbations.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5bf9\u79f0\u7834\u7f3a\u7ef4\u5ea6\u6269\u5c55(SBDE)\u6280\u672f\u63a2\u7a76\u4e86\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u51c6\u786e\u7387\u4e0e\u5bf9\u6297\u9c81\u68d2\u6027\u6743\u8861\u95ee\u9898\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u8fc7\u63d2\u5165\u5e38\u6570\u503c\u50cf\u7d20\u6269\u5c55\u8f93\u5165\u7ef4\u5ea6\u53ef\u4ee5\u63d0\u5347\u5e72\u51c0\u51c6\u786e\u7387\uff0c\u4f46\u540c\u65f6\u4f1a\u964d\u4f4e\u5bf9\u6297\u9c81\u68d2\u6027\uff0c\u8fd9\u79cd\u8106\u5f31\u6027\u4e3b\u8981\u6e90\u4e8e\u65b0\u589e\u7ef4\u5ea6\u4e0a\u7684\u9661\u5ced\u8fb9\u754c\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5e72\u51c0\u51c6\u786e\u7387\u548c\u5bf9\u6297\u9c81\u68d2\u6027\u4e4b\u95f4\u5b58\u5728\u666e\u904d\u6743\u8861\uff0c\u4f46\u5176\u51e0\u4f55\u6839\u6e90\u5c1a\u4e0d\u660e\u786e\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u53d7\u63a7\u5b9e\u9a8c\u63a2\u7a76\u8fd9\u4e00\u6743\u8861\u80cc\u540e\u7684\u673a\u5236\u3002", "method": "\u91c7\u7528\u5bf9\u79f0\u7834\u7f3a\u7ef4\u5ea6\u6269\u5c55(SBDE)\u6280\u672f\uff1a\u901a\u8fc7\u5728\u8f93\u5165\u56fe\u50cf\u4e2d\u63d2\u5165\u5e38\u6570\u503c\u50cf\u7d20\u6765\u6269\u5c55\u8f93\u5165\u7ef4\u5ea6\uff0c\u6253\u7834\u5e73\u79fb\u5bf9\u79f0\u6027\u3002\u540c\u65f6\u4f7f\u7528\u6d4b\u8bd5\u65f6\u7684\u63a9\u7801\u6295\u5f71\u6280\u672f\uff0c\u5c06\u63d2\u5165\u7684\u8f85\u52a9\u50cf\u7d20\u91cd\u7f6e\u4e3a\u8bad\u7ec3\u65f6\u7684\u503c\uff0c\u4ee5\u5206\u6790\u8106\u5f31\u6027\u7684\u6765\u6e90\u3002", "result": "SBDE\u663e\u8457\u63d0\u5347\u4e86\u5e72\u51c0\u51c6\u786e\u7387\uff08\u5982CIFAR-10\u4e0aResNet-18\u4ece90.47%\u63d0\u5347\u523095.63%\uff09\uff0c\u4f46\u964d\u4f4e\u4e86\u5bf9\u6297\u8fed\u4ee3\u767d\u76d2\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002\u63a9\u7801\u6295\u5f71\u5b9e\u9a8c\u8868\u660e\u8106\u5f31\u6027\u51e0\u4e4e\u5b8c\u5168\u6e90\u4e8e\u63d2\u5165\u7684\u7ef4\u5ea6\uff0c\u6295\u5f71\u80fd\u6709\u6548\u4e2d\u548c\u653b\u51fb\u5e76\u6062\u590d\u9c81\u68d2\u6027\u3002\u6a21\u578b\u901a\u8fc7\u6cbf\u8f85\u52a9\u8f74\u521b\u5efa\u9661\u5ced\u8fb9\u754c\uff08\u9661\u5ced\u635f\u5931\u68af\u5ea6\uff09\u6765\u5b9e\u73b0\u9ad8\u51c6\u786e\u7387\u3002", "conclusion": "\u51c6\u786e\u7387-\u9c81\u68d2\u6027\u6096\u8bba\u7684\u51e0\u4f55\u89e3\u91ca\u662f\uff1a\u4f18\u5316\u8fc7\u7a0b\u901a\u8fc7\u52a0\u6df1\u5438\u5f15\u76c6\u6765\u63d0\u5347\u51c6\u786e\u7387\uff0c\u4f46\u4e0d\u53ef\u907f\u514d\u5730\u4f1a\u5728\u8f85\u52a9\u81ea\u7531\u5ea6\u4e0a\u5efa\u7acb\u9661\u5ced\u8fb9\u754c\uff0c\u4ece\u800c\u5bf9\u6d41\u5f62\u5916\u6270\u52a8\u4ea7\u751f\u8106\u5f31\u654f\u611f\u6027\u3002"}}
{"id": "2602.17958", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.17958", "abs": "https://arxiv.org/abs/2602.17958", "authors": ["Aida Afshar", "Yuke Zhang", "Aldo Pacchiano"], "title": "Bayesian Online Model Selection", "comment": null, "summary": "Online model selection in Bayesian bandits raises a fundamental exploration challenge: When an environment instance is sampled from a prior distribution, how can we design an adaptive strategy that explores multiple bandit learners and competes with the best one in hindsight? We address this problem by introducing a new Bayesian algorithm for online model selection in stochastic bandits. We prove an oracle-style guarantee of $O\\left( d^* M \\sqrt{T} + \\sqrt{(MT)} \\right)$ on the Bayesian regret, where $M$ is the number of base learners, $d^*$ is the regret coefficient of the optimal base learner, and $T$ is the time horizon. We also validate our method empirically across a range of stochastic bandit settings, demonstrating performance that is competitive with the best base learner. Additionally, we study the effect of sharing data among base learners and its role in mitigating prior mis-specification.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8d1d\u53f6\u65af\u5728\u7ebf\u6a21\u578b\u9009\u62e9\u7b97\u6cd5\uff0c\u7528\u4e8e\u968f\u673a\u591a\u81c2\u8d4c\u535a\u673a\u95ee\u9898\uff0c\u80fd\u591f\u5728\u591a\u4e2a\u57fa\u7840\u5b66\u4e60\u5668\u4e2d\u9009\u62e9\u6700\u4f18\u8005\uff0c\u5e76\u83b7\u5f97\u7406\u8bba\u4fdd\u8bc1\u3002", "motivation": "\u89e3\u51b3\u8d1d\u53f6\u65af\u8d4c\u535a\u673a\u4e2d\u7684\u5728\u7ebf\u6a21\u578b\u9009\u62e9\u95ee\u9898\uff1a\u5f53\u73af\u5883\u5b9e\u4f8b\u4ece\u5148\u9a8c\u5206\u5e03\u4e2d\u91c7\u6837\u65f6\uff0c\u5982\u4f55\u8bbe\u8ba1\u81ea\u9002\u5e94\u7b56\u7565\u6765\u63a2\u7d22\u591a\u4e2a\u8d4c\u535a\u673a\u5b66\u4e60\u5668\uff0c\u5e76\u4e0e\u4e8b\u540e\u6700\u4f18\u5b66\u4e60\u5668\u7ade\u4e89\uff1f", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8d1d\u53f6\u65af\u7b97\u6cd5\u7528\u4e8e\u968f\u673a\u8d4c\u535a\u673a\u4e2d\u7684\u5728\u7ebf\u6a21\u578b\u9009\u62e9\uff0c\u8be5\u7b97\u6cd5\u80fd\u591f\u81ea\u9002\u5e94\u5730\u5728\u591a\u4e2a\u57fa\u7840\u5b66\u4e60\u5668\u4e4b\u95f4\u8fdb\u884c\u9009\u62e9\u548c\u5207\u6362\u3002", "result": "\u8bc1\u660e\u4e86\u8d1d\u53f6\u65af\u9057\u61be\u7684oracle\u5f0f\u4fdd\u8bc1\u4e3aO(d*M\u221aT + \u221a(MT))\uff0c\u5176\u4e2dM\u662f\u57fa\u7840\u5b66\u4e60\u5668\u6570\u91cf\uff0cd*\u662f\u6700\u4f18\u57fa\u7840\u5b66\u4e60\u5668\u7684\u9057\u61be\u7cfb\u6570\uff0cT\u662f\u65f6\u95f4\u8303\u56f4\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u968f\u673a\u8d4c\u535a\u673a\u8bbe\u7f6e\u4e0b\u7684\u7ade\u4e89\u529b\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u8d1d\u53f6\u65af\u5728\u7ebf\u6a21\u578b\u9009\u62e9\u95ee\u9898\uff0c\u6027\u80fd\u53ef\u4e0e\u6700\u4f18\u57fa\u7840\u5b66\u4e60\u5668\u7ade\u4e89\uff0c\u540c\u65f6\u7814\u7a76\u4e86\u6570\u636e\u5171\u4eab\u5bf9\u7f13\u89e3\u5148\u9a8c\u8bbe\u5b9a\u9519\u8bef\u7684\u5f71\u54cd\u3002"}}
{"id": "2602.18292", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18292", "abs": "https://arxiv.org/abs/2602.18292", "authors": ["Xiaotong Ji", "Rasul Tutunov", "Matthieu Zimmer", "Haitham Bou-Ammar"], "title": "Decoding as Optimisation on the Probability Simplex: From Top-K to Top-P (Nucleus) to Best-of-K Samplers", "comment": null, "summary": "Decoding sits between a language model and everything we do with it, yet it is still treated as a heuristic knob-tuning exercise. We argue decoding should be understood as a principled optimisation layer: at each token, we solve a regularised problem over the probability simplex that trades off model score against structural preferences and constraints. This single template recovers greedy decoding, Softmax sampling, Top-K, Top-P, and Sparsemax-style sparsity as special cases, and explains their common structure through optimality conditions. More importantly, the framework makes it easy to invent new decoders without folklore. We demonstrate this by designing Best-of-K (BoK), a KL-anchored coverage objective aimed at multi-sample pipelines (self-consistency, reranking, verifier selection). BoK targets the probability of covering good alternatives within a fixed K-sample budget and improves empirical performance. We show that such samples can improve accuracy by, for example, +18.6% for Qwen2.5-Math-7B on MATH500 at high sampling temperatures.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u89e3\u7801\u89c6\u4e3a\u4e00\u4e2a\u539f\u5219\u5316\u7684\u4f18\u5316\u5c42\uff0c\u901a\u8fc7\u6b63\u5219\u5316\u95ee\u9898\u7edf\u4e00\u73b0\u6709\u89e3\u7801\u65b9\u6cd5\uff0c\u5e76\u57fa\u4e8e\u6b64\u6846\u67b6\u8bbe\u8ba1\u4e86\u65b0\u7684Best-of-K\u89e3\u7801\u5668\u6765\u63d0\u5347\u591a\u6837\u672c\u7ba1\u9053\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u8bed\u8a00\u6a21\u578b\u7684\u89e3\u7801\u8fc7\u7a0b\u4ecd\u88ab\u89c6\u4e3a\u542f\u53d1\u5f0f\u7684\u53c2\u6570\u8c03\u6574\u8fc7\u7a0b\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u3002\u4f5c\u8005\u8ba4\u4e3a\u89e3\u7801\u5e94\u8be5\u88ab\u7406\u89e3\u4e3a\u4e00\u4e2a\u539f\u5219\u5316\u7684\u4f18\u5316\u5c42\uff0c\u80fd\u591f\u7cfb\u7edf\u5730\u89e3\u91ca\u73b0\u6709\u65b9\u6cd5\u5e76\u65b9\u4fbf\u5730\u8bbe\u8ba1\u65b0\u89e3\u7801\u5668\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7edf\u4e00\u7684\u89e3\u7801\u6846\u67b6\uff1a\u5728\u6bcf\u4e2atoken\u4f4d\u7f6e\uff0c\u89e3\u51b3\u4e00\u4e2a\u5728\u6982\u7387\u5355\u7eaf\u5f62\u4e0a\u7684\u6b63\u5219\u5316\u4f18\u5316\u95ee\u9898\uff0c\u5e73\u8861\u6a21\u578b\u5f97\u5206\u4e0e\u7ed3\u6784\u504f\u597d\u548c\u7ea6\u675f\u3002\u8be5\u6846\u67b6\u5c06\u8d2a\u5a6a\u89e3\u7801\u3001Softmax\u91c7\u6837\u3001Top-K\u3001Top-P\u548cSparsemax\u7b49\u73b0\u6709\u65b9\u6cd5\u7edf\u4e00\u4e3a\u7279\u4f8b\u3002\u57fa\u4e8e\u6b64\u6846\u67b6\u8bbe\u8ba1\u4e86Best-of-K\u89e3\u7801\u5668\uff0c\u4f7f\u7528KL\u6563\u5ea6\u951a\u5b9a\u7684\u8986\u76d6\u76ee\u6807\u6765\u4f18\u5316\u591a\u6837\u672c\u7ba1\u9053\u3002", "result": "Best-of-K\u89e3\u7801\u5668\u663e\u8457\u63d0\u5347\u4e86\u591a\u6837\u672c\u7ba1\u9053\u7684\u6027\u80fd\uff0c\u4f8b\u5982\u5728Qwen2.5-Math-7B\u6a21\u578b\u4e0a\uff0cMATH500\u6570\u636e\u96c6\u5728\u9ad8\u91c7\u6837\u6e29\u5ea6\u4e0b\u51c6\u786e\u7387\u63d0\u5347\u4e86+18.6%\u3002\u8be5\u6846\u67b6\u6210\u529f\u89e3\u91ca\u4e86\u73b0\u6709\u89e3\u7801\u65b9\u6cd5\u7684\u5171\u540c\u7ed3\u6784\uff0c\u5e76\u4e3a\u8bbe\u8ba1\u65b0\u89e3\u7801\u5668\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u65b9\u6cd5\u3002", "conclusion": "\u89e3\u7801\u5e94\u8be5\u88ab\u89c6\u4e3a\u8bed\u8a00\u6a21\u578b\u7684\u539f\u5219\u5316\u4f18\u5316\u5c42\uff0c\u800c\u4e0d\u662f\u542f\u53d1\u5f0f\u53c2\u6570\u8c03\u6574\u3002\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u4e0d\u4ec5\u89e3\u91ca\u4e86\u73b0\u6709\u89e3\u7801\u65b9\u6cd5\uff0c\u8fd8\u4f7f\u8bbe\u8ba1\u65b0\u89e3\u7801\u5668\u53d8\u5f97\u7b80\u5355\uff0cBest-of-K\u89e3\u7801\u5668\u7684\u6210\u529f\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2602.18297", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.IT"], "pdf": "https://arxiv.org/pdf/2602.18297", "abs": "https://arxiv.org/abs/2602.18297", "authors": ["Usman Anwar", "Tim Bakker", "Dana Kianfar", "Cristina Pinneri", "Christos Louizos"], "title": "Analyzing and Improving Chain-of-Thought Monitorability Through Information Theory", "comment": "First two authors contributed equally", "summary": "Chain-of-thought (CoT) monitors are LLM-based systems that analyze reasoning traces to detect when outputs may exhibit attributes of interest, such as test-hacking behavior during code generation. In this paper, we use information-theoretic analysis to show that non-zero mutual information between CoT and output is a necessary but not sufficient condition for CoT monitorability. We identify two sources of approximation error that may undermine the performance of CoT monitors in practice: information gap, which measures the extent to which the monitor can extract the information available in CoT, and elicitation error, which measures the extent to which the monitor approximates the optimal monitoring function. We further demonstrate that CoT monitorability can be systematically improved through targeted training objectives. To this end, we propose two complementary approaches: (a) an oracle-based method that directly rewards the monitored model for producing CoTs that maximize monitor accuracy, and (b) a more practical, label-free approach that maximizes conditional mutual information between outputs and CoTs. Across multiple different environments, we show both methods significantly improve monitor accuracy while preventing CoT degeneration even when training against a monitor, thereby mitigating reward hacking when the task reward is imperfectly specified.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u4fe1\u606f\u8bba\u5206\u6790CoT\u76d1\u63a7\u673a\u5236\uff0c\u63d0\u51fa\u4e24\u79cd\u8bad\u7ec3\u65b9\u6cd5\u63d0\u5347\u76d1\u63a7\u51c6\u786e\u6027\uff0c\u9632\u6b62CoT\u9000\u5316", "motivation": "\u7814\u7a76CoT\u76d1\u63a7\u5668\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u9645\u6027\u80fd\u9650\u5236\uff0c\u89e3\u51b3\u76d1\u63a7\u5668\u5728\u5b9e\u8df5\u4e2d\u53ef\u80fd\u5931\u6548\u7684\u95ee\u9898\uff0c\u9632\u6b62\u5956\u52b1\u9ed1\u5ba2\u884c\u4e3a", "method": "1) \u4fe1\u606f\u8bba\u5206\u6790CoT\u4e0e\u8f93\u51fa\u95f4\u4e92\u4fe1\u606f\u6761\u4ef6\uff1b2) \u8bc6\u522b\u4fe1\u606f\u5dee\u8ddd\u548c\u542f\u53d1\u8bef\u5dee\u4e24\u79cd\u8fd1\u4f3c\u8bef\u5dee\u6e90\uff1b3) \u63d0\u51fa\u4e24\u79cd\u8bad\u7ec3\u65b9\u6cd5\uff1a\u57fa\u4e8eoracle\u7684\u76f4\u63a5\u5956\u52b1\u6cd5\u548c\u65e0\u6807\u7b7e\u7684\u6761\u4ef6\u4e92\u4fe1\u606f\u6700\u5927\u5316\u6cd5", "result": "\u4e24\u79cd\u65b9\u6cd5\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u663e\u8457\u63d0\u5347\u76d1\u63a7\u51c6\u786e\u6027\uff0c\u9632\u6b62CoT\u9000\u5316\uff0c\u6709\u6548\u7f13\u89e3\u4efb\u52a1\u5956\u52b1\u4e0d\u5b8c\u5584\u65f6\u7684\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898", "conclusion": "CoT\u76d1\u63a7\u6027\u53ef\u901a\u8fc7\u9488\u5bf9\u6027\u8bad\u7ec3\u76ee\u6807\u7cfb\u7edf\u6539\u8fdb\uff0c\u63d0\u51fa\u7684\u4e24\u79cd\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u5347\u76d1\u63a7\u6027\u80fd\u5e76\u9632\u6b62\u9000\u5316\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u7684CoT\u76d1\u63a7\u7cfb\u7edf\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u65b9\u6848"}}
{"id": "2602.18308", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18308", "abs": "https://arxiv.org/abs/2602.18308", "authors": ["Biswa Sengupta", "Jinhua Wang", "Leo Brunswic"], "title": "JPmHC Dynamical Isometry via Orthogonal Hyper-Connections", "comment": null, "summary": "Recent advances in deep learning, exemplified by Hyper-Connections (HC), have expanded the residual connection paradigm by introducing wider residual streams and diverse connectivity patterns. While these innovations yield significant performance gains, they compromise the identity mapping property of residual connections, leading to training instability, limited scalability, and increased memory overhead. To address these challenges, we propose JPmHC (Jacobian-spectrum Preserving manifold-constrained Hyper-Connections), a framework that replaces identity skips with a trainable linear mixer acting on n parallel streams while explicitly controlling gradient conditioning. By constraining the mixer M on operator-norm-bounded manifolds (e.g., bistochastic, Stiefel, Grassmann), JPmHC prevents gradient pathologies and enhances stability. JPmHC introduces three key contributions: (i) a free-probability analysis that predicts Jacobian spectra for structured skips, providing actionable design rules for mixer selection; (ii) memory-efficient implicit differentiation for fixed-point projections, reducing activation memory and synchronization overhead; and (iii) a Stiefel-constrained mixer via Cayley transforms, ensuring orthogonality without post-hoc normalization. Empirical evaluations on ARC-AGI demonstrate that JPmHC achieves faster convergence, higher accuracy, and lower computational cost compared to bistochastic baselines. As a flexible and scalable extension of HC, JPmHC advances spectrum-aware, stable, and efficient deep learning, offering insights into topological architecture design and foundational model evolution.", "AI": {"tldr": "JPmHC\u6846\u67b6\u901a\u8fc7\u7ea6\u675f\u7ebf\u6027\u6df7\u5408\u5668\u5728\u7b97\u5b50\u8303\u6570\u6709\u754c\u6d41\u5f62\u4e0a\uff0c\u89e3\u51b3\u4e86\u8d85\u8fde\u63a5\u67b6\u6784\u4e2d\u68af\u5ea6\u75c5\u7406\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u3001\u9ad8\u6548\u7684\u8bad\u7ec3\u3002", "motivation": "\u8d85\u8fde\u63a5\u7b49\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6269\u5c55\u4e86\u6b8b\u5dee\u8fde\u63a5\u8303\u5f0f\uff0c\u4f46\u7834\u574f\u4e86\u6052\u7b49\u6620\u5c04\u7279\u6027\uff0c\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u3001\u53ef\u6269\u5c55\u6027\u53d7\u9650\u548c\u5185\u5b58\u5f00\u9500\u589e\u52a0\u3002", "method": "\u63d0\u51faJPmHC\u6846\u67b6\uff0c\u7528\u53ef\u8bad\u7ec3\u7684\u7ebf\u6027\u6df7\u5408\u5668\u66ff\u4ee3\u6052\u7b49\u8df3\u8dc3\u8fde\u63a5\uff0c\u901a\u8fc7\u7ea6\u675f\u6df7\u5408\u5668\u5728\u7b97\u5b50\u8303\u6570\u6709\u754c\u6d41\u5f62\u4e0a\u63a7\u5236\u68af\u5ea6\u6761\u4ef6\uff0c\u5305\u62ec\u81ea\u7531\u6982\u7387\u5206\u6790\u3001\u5185\u5b58\u9ad8\u6548\u7684\u9690\u5f0f\u5fae\u5206\u548cCayley\u53d8\u6362\u5b9e\u73b0\u7684Stiefel\u7ea6\u675f\u6df7\u5408\u5668\u3002", "result": "\u5728ARC-AGI\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cJPmHC\u76f8\u6bd4\u53cc\u968f\u673a\u57fa\u7ebf\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3001\u66f4\u9ad8\u7684\u51c6\u786e\u7387\u548c\u66f4\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "JPmHC\u4f5c\u4e3a\u8d85\u8fde\u63a5\u7684\u7075\u6d3b\u53ef\u6269\u5c55\u6269\u5c55\uff0c\u63a8\u8fdb\u4e86\u8c31\u611f\u77e5\u3001\u7a33\u5b9a\u4e14\u9ad8\u6548\u7684\u6df1\u5ea6\u5b66\u4e60\uff0c\u4e3a\u62d3\u6251\u67b6\u6784\u8bbe\u8ba1\u548c\u57fa\u7840\u6a21\u578b\u6f14\u8fdb\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2602.18384", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.18384", "abs": "https://arxiv.org/abs/2602.18384", "authors": ["Fotios Zantalis", "Evangelos Zervas", "Grigorios Koulouras"], "title": "FedZMG: Efficient Client-Side Optimization in Federated Learning", "comment": null, "summary": "Federated Learning (FL) enables distributed model training on edge devices while preserving data privacy. However, clients tend to have non-Independent and Identically Distributed (non-IID) data, which often leads to client-drift, and therefore diminishing convergence speed and model performance. While adaptive optimizers have been proposed to mitigate these effects, they frequently introduce computational complexity or communication overhead unsuitable for resource-constrained IoT environments. This paper introduces Federated Zero Mean Gradients (FedZMG), a novel, parameter-free, client-side optimization algorithm designed to tackle client-drift by structurally regularizing the optimization space. Advancing the idea of Gradient Centralization, FedZMG projects local gradients onto a zero-mean hyperplane, effectively neutralizing the \"intensity\" or \"bias\" shifts inherent in heterogeneous data distributions without requiring additional communication or hyperparameter tuning. A theoretical analysis is provided, proving that FedZMG reduces the effective gradient variance and guarantees tighter convergence bounds compared to standard FedAvg. Extensive empirical evaluations on EMNIST, CIFAR100, and Shakespeare datasets demonstrate that FedZMG achieves better convergence speed and final validation accuracy compared to the baseline FedAvg and the adaptive optimizer FedAdam, particularly in highly non-IID settings.", "AI": {"tldr": "FedZMG\u662f\u4e00\u79cd\u65e0\u9700\u989d\u5916\u53c2\u6570\u3001\u5ba2\u6237\u7aef\u4fa7\u7684\u8054\u90a6\u5b66\u4e60\u4f18\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c06\u5c40\u90e8\u68af\u5ea6\u6295\u5f71\u5230\u96f6\u5747\u503c\u8d85\u5e73\u9762\u6765\u7f13\u89e3\u975eIID\u6570\u636e\u5bfc\u81f4\u7684\u5ba2\u6237\u7aef\u6f02\u79fb\u95ee\u9898\uff0c\u65e0\u9700\u989d\u5916\u901a\u4fe1\u6216\u8d85\u53c2\u6570\u8c03\u4f18\uff0c\u5728\u975eIID\u8bbe\u7f6e\u4e0b\u4f18\u4e8eFedAvg\u548cFedAdam\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u5ba2\u6237\u7aef\u6570\u636e\u901a\u5e38\u662f\u975e\u72ec\u7acb\u540c\u5206\u5e03\u7684\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u5ba2\u6237\u7aef\u6f02\u79fb\u95ee\u9898\uff0c\u964d\u4f4e\u6536\u655b\u901f\u5ea6\u548c\u6a21\u578b\u6027\u80fd\u3002\u73b0\u6709\u7684\u81ea\u9002\u5e94\u4f18\u5316\u5668\u867d\u7136\u80fd\u7f13\u89e3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u901a\u5e38\u8ba1\u7b97\u590d\u6742\u6216\u901a\u4fe1\u5f00\u9500\u5927\uff0c\u4e0d\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u7269\u8054\u7f51\u73af\u5883\u3002", "method": "\u63d0\u51faFedZMG\u7b97\u6cd5\uff0c\u57fa\u4e8e\u68af\u5ea6\u4e2d\u5fc3\u5316\u7684\u601d\u60f3\uff0c\u5c06\u5c40\u90e8\u68af\u5ea6\u6295\u5f71\u5230\u96f6\u5747\u503c\u8d85\u5e73\u9762\u4e0a\uff0c\u6709\u6548\u4e2d\u548c\u5f02\u6784\u6570\u636e\u5206\u5e03\u4e2d\u56fa\u6709\u7684\"\u5f3a\u5ea6\"\u6216\"\u504f\u5dee\"\u504f\u79fb\u3002\u8be5\u7b97\u6cd5\u65e0\u9700\u989d\u5916\u53c2\u6570\u3001\u65e0\u9700\u989d\u5916\u901a\u4fe1\u3001\u65e0\u9700\u8d85\u53c2\u6570\u8c03\u4f18\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660eFedZMG\u80fd\u964d\u4f4e\u6709\u6548\u68af\u5ea6\u65b9\u5dee\u5e76\u4fdd\u8bc1\u6bd4\u6807\u51c6FedAvg\u66f4\u7d27\u7684\u6536\u655b\u754c\u3002\u5728EMNIST\u3001CIFAR100\u548cShakespeare\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFedZMG\u5728\u9ad8\u5ea6\u975eIID\u8bbe\u7f6e\u4e0b\u6bd4FedAvg\u548cFedAdam\u5177\u6709\u66f4\u597d\u7684\u6536\u655b\u901f\u5ea6\u548c\u6700\u7ec8\u9a8c\u8bc1\u51c6\u786e\u7387\u3002", "conclusion": "FedZMG\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u8f7b\u91cf\u7ea7\u7684\u8054\u90a6\u5b66\u4e60\u4f18\u5316\u7b97\u6cd5\uff0c\u80fd\u6709\u6548\u89e3\u51b3\u975eIID\u6570\u636e\u5bfc\u81f4\u7684\u5ba2\u6237\u7aef\u6f02\u79fb\u95ee\u9898\uff0c\u7279\u522b\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u7269\u8054\u7f51\u73af\u5883\uff0c\u5728\u4fdd\u6301\u9690\u79c1\u7684\u540c\u65f6\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2602.18409", "categories": ["cs.LG", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2602.18409", "abs": "https://arxiv.org/abs/2602.18409", "authors": ["Huan Luo", "Jonni Virtema"], "title": "Unifying approach to uniform expressivity of graph neural networks", "comment": null, "summary": "The expressive power of Graph Neural Networks (GNNs) is often analysed via correspondence to the Weisfeiler-Leman (WL) algorithm and fragments of first-order logic. Standard GNNs are limited to performing aggregation over immediate neighbourhoods or over global read-outs. To increase their expressivity, recent attempts have been made to incorporate substructural information (e.g. cycle counts and subgraph properties). In this paper, we formalize this architectural trend by introducing Template GNNs (T-GNNs), a generalized framework where node features are updated by aggregating over valid template embeddings from a specified set of graph templates. We propose a corresponding logic, Graded template modal logic (GML(T)), and generalized notions of template-based bisimulation and WL algorithm. We establish an equivalence between the expressive power of T-GNNs and GML(T), and provide a unifying approach for analysing GNN expressivity: we show how standard AC-GNNs and its recent variants can be interpreted as instantiations of T-GNNs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u6a21\u677fGNN\uff08T-GNNs\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u805a\u5408\u6307\u5b9a\u56fe\u6a21\u677f\u7684\u5d4c\u5165\u6765\u66f4\u65b0\u8282\u70b9\u7279\u5f81\uff0c\u5e76\u5efa\u7acb\u4e86\u5bf9\u5e94\u7684\u903b\u8f91\u7cfb\u7edfGML(T)\uff0c\u7edf\u4e00\u5206\u6790\u4e86GNN\u7684\u8868\u8fbe\u80fd\u529b\u3002", "motivation": "\u6807\u51c6GNN\u53ea\u80fd\u805a\u5408\u76f4\u63a5\u90bb\u5c45\u6216\u5168\u5c40\u4fe1\u606f\uff0c\u8868\u8fbe\u80fd\u529b\u6709\u9650\u3002\u4e3a\u4e86\u589e\u5f3a\u8868\u8fbe\u80fd\u529b\uff0c\u6700\u8fd1\u7684\u7814\u7a76\u5c1d\u8bd5\u7eb3\u5165\u5b50\u7ed3\u6784\u4fe1\u606f\uff08\u5982\u73af\u8ba1\u6570\u548c\u5b50\u56fe\u5c5e\u6027\uff09\u3002\u672c\u6587\u65e8\u5728\u5f62\u5f0f\u5316\u8fd9\u4e00\u67b6\u6784\u8d8b\u52bf\u3002", "method": "\u5f15\u5165\u6a21\u677fGNN\uff08T-GNNs\uff09\u6846\u67b6\uff0c\u8282\u70b9\u7279\u5f81\u901a\u8fc7\u805a\u5408\u6307\u5b9a\u56fe\u6a21\u677f\u96c6\u5408\u4e2d\u7684\u6709\u6548\u6a21\u677f\u5d4c\u5165\u6765\u66f4\u65b0\u3002\u63d0\u51fa\u5bf9\u5e94\u7684\u5206\u7ea7\u6a21\u677f\u6a21\u6001\u903b\u8f91GML(T)\uff0c\u4ee5\u53ca\u6a21\u677f\u57fa\u4e8c\u5206\u548cWL\u7b97\u6cd5\u7684\u5e7f\u4e49\u6982\u5ff5\u3002", "result": "\u5efa\u7acb\u4e86T-GNNs\u4e0eGML(T)\u8868\u8fbe\u80fd\u529b\u7684\u7b49\u4ef7\u6027\uff0c\u63d0\u4f9b\u4e86\u5206\u6790GNN\u8868\u8fbe\u80fd\u529b\u7684\u7edf\u4e00\u65b9\u6cd5\uff1a\u5c55\u793a\u4e86\u6807\u51c6AC-GNNs\u53ca\u5176\u53d8\u4f53\u53ef\u4ee5\u4f5c\u4e3aT-GNNs\u7684\u5b9e\u4f8b\u5316\u3002", "conclusion": "T-GNNs\u6846\u67b6\u4e3a\u5206\u6790\u548c\u7406\u89e3GNN\u8868\u8fbe\u80fd\u529b\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u57fa\u7840\uff0c\u80fd\u591f\u6db5\u76d6\u73b0\u6709GNN\u53d8\u4f53\u5e76\u5f62\u5f0f\u5316\u5b50\u7ed3\u6784\u4fe1\u606f\u6574\u5408\u7684\u8d8b\u52bf\u3002"}}
{"id": "2602.18002", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18002", "abs": "https://arxiv.org/abs/2602.18002", "authors": ["Junfei Sun", "Dixi Yao", "Xuchen Gong", "Tahseen Rabbani", "Manzil Zaheer", "Tian Li"], "title": "Asynchronous Heavy-Tailed Optimization", "comment": "8-page main body, 25-page appendix, 5 figures", "summary": "Heavy-tailed stochastic gradient noise, commonly observed in transformer models, can destabilize the optimization process. Recent works mainly focus on developing and understanding approaches to address heavy-tailed noise in the centralized or distributed, synchronous setting, leaving the interactions between such noise and asynchronous optimization underexplored. In this work, we investigate two communication schemes that handle stragglers with asynchronous updates in the presence of heavy-tailed gradient noise. We propose and theoretically analyze algorithmic modifications based on delay-aware learning rate scheduling and delay compensation to enhance the performance of asynchronous algorithms. Our convergence guarantees under heavy-tailed noise match the rate of the synchronous counterparts and improve delay tolerance compared with existing asynchronous approaches. Empirically, our approaches outperform prior synchronous and asynchronous methods in terms of accuracy/runtime trade-offs and are more robust to hyperparameters in both image and language tasks.", "AI": {"tldr": "\u7814\u7a76\u9488\u5bf9\u5f02\u6b65\u4f18\u5316\u4e2d\u91cd\u5c3e\u68af\u5ea6\u566a\u58f0\u7684\u7b97\u6cd5\u6539\u8fdb\uff0c\u901a\u8fc7\u5ef6\u8fdf\u611f\u77e5\u5b66\u4e60\u7387\u8c03\u5ea6\u548c\u5ef6\u8fdf\u8865\u507f\u63d0\u5347\u6027\u80fd\uff0c\u5728\u56fe\u50cf\u548c\u8bed\u8a00\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u91cd\u5c3e\u968f\u673a\u68af\u5ea6\u566a\u58f0\u5728Transformer\u6a21\u578b\u4e2d\u5e38\u89c1\uff0c\u4f1a\u7834\u574f\u4f18\u5316\u7a33\u5b9a\u6027\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u96c6\u4e2d\u5f0f\u6216\u5206\u5e03\u5f0f\u540c\u6b65\u8bbe\u7f6e\u4e2d\u7684\u91cd\u5c3e\u566a\u58f0\u5904\u7406\uff0c\u800c\u91cd\u5c3e\u566a\u58f0\u4e0e\u5f02\u6b65\u4f18\u5316\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u5904\u7406\u5f02\u6b65\u66f4\u65b0\u4e2d\u6162\u8282\u70b9\uff08stragglers\uff09\u7684\u901a\u4fe1\u65b9\u6848\uff0c\u57fa\u4e8e\u5ef6\u8fdf\u611f\u77e5\u5b66\u4e60\u7387\u8c03\u5ea6\u548c\u5ef6\u8fdf\u8865\u507f\u8fdb\u884c\u7b97\u6cd5\u4fee\u6539\uff0c\u4ee5\u589e\u5f3a\u5f02\u6b65\u7b97\u6cd5\u5728\u91cd\u5c3e\u68af\u5ea6\u566a\u58f0\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u5728\u91cd\u5c3e\u566a\u58f0\u4e0b\u7684\u6536\u655b\u4fdd\u8bc1\u4e0e\u540c\u6b65\u5bf9\u5e94\u65b9\u6cd5\u7684\u901f\u7387\u5339\u914d\uff0c\u76f8\u6bd4\u73b0\u6709\u5f02\u6b65\u65b9\u6cd5\u63d0\u9ad8\u4e86\u5ef6\u8fdf\u5bb9\u5fcd\u5ea6\u3002\u5728\u56fe\u50cf\u548c\u8bed\u8a00\u4efb\u52a1\u4e2d\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u51c6\u786e\u7387/\u8fd0\u884c\u65f6\u95f4\u6743\u8861\u65b9\u9762\u4f18\u4e8e\u5148\u524d\u7684\u540c\u6b65\u548c\u5f02\u6b65\u65b9\u6cd5\uff0c\u5e76\u4e14\u5bf9\u8d85\u53c2\u6570\u66f4\u52a0\u9c81\u68d2\u3002", "conclusion": "\u901a\u8fc7\u5ef6\u8fdf\u611f\u77e5\u5b66\u4e60\u7387\u8c03\u5ea6\u548c\u5ef6\u8fdf\u8865\u507f\u7684\u7b97\u6cd5\u4fee\u6539\uff0c\u53ef\u4ee5\u6709\u6548\u5904\u7406\u5f02\u6b65\u4f18\u5316\u4e2d\u7684\u91cd\u5c3e\u68af\u5ea6\u566a\u58f0\u95ee\u9898\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u8bc1\u4e0a\u90fd\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u7ed3\u679c\u3002"}}
{"id": "2602.18055", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18055", "abs": "https://arxiv.org/abs/2602.18055", "authors": ["Jingyang Qiao", "Zhizhong Zhang", "Xin Tan", "Jingyu Gong", "Yanyun Qu", "Yuan Xie"], "title": "Continual-NExT: A Unified Comprehension And Generation Continual Learning Framework", "comment": null, "summary": "Dual-to-Dual MLLMs refer to Multimodal Large Language Models, which can enable unified multimodal comprehension and generation through text and image modalities. Although exhibiting strong instantaneous learning and generalization capabilities, Dual-to-Dual MLLMs still remain deficient in lifelong evolution, significantly affecting continual adaptation to dynamic real-world scenarios. One of the challenges is that learning new tasks inevitably destroys the learned knowledge. Beyond traditional catastrophic forgetting, Dual-to-Dual MLLMs face other challenges, including hallucination, instruction unfollowing, and failures in cross-modal knowledge transfer. However, no standardized continual learning framework for Dual-to-Dual MLLMs has been established yet, leaving these challenges unexplored. Thus, in this paper, we establish Continual-NExT, a continual learning framework for Dual-to-Dual MLLMs with deliberately-architected evaluation metrics. To improve the continual learning capability of Dual-to-Dual MLLMs, we propose an efficient MAGE (Mixture and Aggregation of General LoRA and Expert LoRA) method to further facilitate knowledge transfer across modalities and mitigate forgetting. Extensive experiments demonstrate that MAGE outperforms other continual learning methods and achieves state-of-the-art performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Continual-NExT\u6846\u67b6\u548cMAGE\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u53cc\u6a21\u6001\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u9057\u5fd8\u3001\u5e7b\u89c9\u548c\u8de8\u6a21\u6001\u77e5\u8bc6\u8fc1\u79fb\u95ee\u9898", "motivation": "\u53cc\u6a21\u6001\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5177\u6709\u5f3a\u5927\u7684\u5373\u65f6\u5b66\u4e60\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5728\u6301\u7eed\u5b66\u4e60\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u73b0\u5b9e\u573a\u666f\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u4e14\u6a21\u578b\u9762\u4e34\u707e\u96be\u6027\u9057\u5fd8\u3001\u5e7b\u89c9\u3001\u6307\u4ee4\u4e0d\u9075\u5faa\u548c\u8de8\u6a21\u6001\u77e5\u8bc6\u8fc1\u79fb\u5931\u8d25\u7b49\u6311\u6218", "method": "\u63d0\u51fa\u4e86Continual-NExT\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u8bc4\u4f30\u6307\u6807\u3002\u540c\u65f6\u63d0\u51fa\u4e86MAGE\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df7\u5408\u548c\u805a\u5408\u901a\u7528LoRA\u4e0e\u4e13\u5bb6LoRA\u6765\u4fc3\u8fdb\u8de8\u6a21\u6001\u77e5\u8bc6\u8fc1\u79fb\u5e76\u51cf\u8f7b\u9057\u5fd8", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cMAGE\u65b9\u6cd5\u4f18\u4e8e\u5176\u4ed6\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd", "conclusion": "\u672c\u6587\u4e3a\u53cc\u6a21\u6001\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5efa\u7acb\u4e86\u9996\u4e2a\u6807\u51c6\u5316\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\uff0c\u5e76\u901a\u8fc7MAGE\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9002\u5e94\u80fd\u529b"}}
{"id": "2602.18084", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18084", "abs": "https://arxiv.org/abs/2602.18084", "authors": ["Benjamin Honor\u00e9", "Alba Carballo-Castro", "Yiming Qin", "Pascal Frossard"], "title": "Balancing Symmetry and Efficiency in Graph Flow Matching", "comment": "15 pages, 11 figures", "summary": "Equivariance is central to graph generative models, as it ensures the model respects the permutation symmetry of graphs. However, strict equivariance can increase computational cost due to added architectural constraints, and can slow down convergence because the model must be consistent across a large space of possible node permutations. We study this trade-off for graph generative models. Specifically, we start from an equivariant discrete flow-matching model, and relax its equivariance during training via a controllable symmetry modulation scheme based on sinusoidal positional encodings and node permutations. Experiments first show that symmetry-breaking can accelerate early training by providing an easier learning signal, but at the expense of encouraging shortcut solutions that can cause overfitting, where the model repeatedly generates graphs that are duplicates of the training set. On the contrary, properly modulating the symmetry signal can delay overfitting while accelerating convergence, allowing the model to reach stronger performance with $19\\%$ of the baseline training epochs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u56fe\u751f\u6210\u6a21\u578b\u4e2d\u4e25\u683c\u7b49\u53d8\u6027\u7684\u6743\u8861\u95ee\u9898\uff0c\u63d0\u51fa\u901a\u8fc7\u53ef\u63a7\u5bf9\u79f0\u6027\u8c03\u5236\u65b9\u6848\u653e\u677e\u7b49\u53d8\u6027\u7ea6\u675f\uff0c\u4ee5\u52a0\u901f\u8bad\u7ec3\u6536\u655b\u5e76\u907f\u514d\u8fc7\u62df\u5408\u3002", "motivation": "\u4e25\u683c\u7b49\u53d8\u6027\u867d\u7136\u80fd\u786e\u4fdd\u6a21\u578b\u5c0a\u91cd\u56fe\u7684\u7f6e\u6362\u5bf9\u79f0\u6027\uff0c\u4f46\u4f1a\u589e\u52a0\u8ba1\u7b97\u6210\u672c\u5e76\u51cf\u7f13\u6536\u655b\u901f\u5ea6\uff0c\u56e0\u4e3a\u6a21\u578b\u5fc5\u987b\u5728\u5927\u91cf\u53ef\u80fd\u7684\u8282\u70b9\u7f6e\u6362\u4e2d\u4fdd\u6301\u4e00\u81f4\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u8fd9\u79cd\u6743\u8861\u5173\u7cfb\u3002", "method": "\u4ece\u7b49\u53d8\u79bb\u6563\u6d41\u5339\u914d\u6a21\u578b\u51fa\u53d1\uff0c\u901a\u8fc7\u57fa\u4e8e\u6b63\u5f26\u4f4d\u7f6e\u7f16\u7801\u548c\u8282\u70b9\u7f6e\u6362\u7684\u53ef\u63a7\u5bf9\u79f0\u6027\u8c03\u5236\u65b9\u6848\uff0c\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u653e\u677e\u7b49\u53d8\u6027\u7ea6\u675f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a\u5bf9\u79f0\u6027\u7834\u574f\u80fd\u901a\u8fc7\u63d0\u4f9b\u66f4\u7b80\u5355\u7684\u5b66\u4e60\u4fe1\u53f7\u52a0\u901f\u65e9\u671f\u8bad\u7ec3\uff0c\u4f46\u4f1a\u5bfc\u81f4\u6a21\u578b\u8fc7\u62df\u5408\uff0c\u91cd\u590d\u751f\u6210\u8bad\u7ec3\u96c6\u4e2d\u7684\u56fe\uff1b\u800c\u9002\u5f53\u8c03\u5236\u5bf9\u79f0\u6027\u4fe1\u53f7\u53ef\u4ee5\u5ef6\u8fdf\u8fc7\u62df\u5408\u540c\u65f6\u52a0\u901f\u6536\u655b\uff0c\u4f7f\u6a21\u578b\u4ec5\u7528\u57fa\u7ebf\u8bad\u7ec3\u8f6e\u6570\u768419%\u5c31\u8fbe\u5230\u66f4\u5f3a\u6027\u80fd\u3002", "conclusion": "\u5728\u56fe\u751f\u6210\u6a21\u578b\u4e2d\uff0c\u9002\u5f53\u653e\u677e\u7b49\u53d8\u6027\u7ea6\u675f\u5e76\u901a\u8fc7\u53ef\u63a7\u5bf9\u79f0\u6027\u8c03\u5236\u65b9\u6848\u8fdb\u884c\u7ba1\u7406\uff0c\u53ef\u4ee5\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u52a0\u901f\u8bad\u7ec3\u6536\u655b\uff0c\u907f\u514d\u8fc7\u62df\u5408\u95ee\u9898\u3002"}}
{"id": "2602.18109", "categories": ["cs.LG", "cs.OS", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.18109", "abs": "https://arxiv.org/abs/2602.18109", "authors": ["Rong Fu", "Yibo Meng", "Guangzhen Yao", "Jiaxuan Lu", "Zeyu Zhang", "Zhaolu Kang", "Ziming Guo", "Jia Yee Tan", "Xiaojing Du", "Simon James Fong"], "title": "TempoNet: Slack-Quantized Transformer-Guided Reinforcement Scheduler for Adaptive Deadline-Centric Real-Time Dispatchs", "comment": "43 pages, 12 figures", "summary": "Real-time schedulers must reason about tight deadlines under strict compute budgets. We present TempoNet, a reinforcement learning scheduler that pairs a permutation-invariant Transformer with a deep Q-approximation. An Urgency Tokenizer discretizes temporal slack into learnable embeddings, stabilizing value learning and capturing deadline proximity. A latency-aware sparse attention stack with blockwise top-k selection and locality-sensitive chunking enables global reasoning over unordered task sets with near-linear scaling and sub-millisecond inference. A multicore mapping layer converts contextualized Q-scores into processor assignments through masked-greedy selection or differentiable matching. Extensive evaluations on industrial mixed-criticality traces and large multiprocessor settings show consistent gains in deadline fulfillment over analytic schedulers and neural baselines, together with improved optimization stability. Diagnostics include sensitivity analyses for slack quantization, attention-driven policy interpretation, hardware-in-the-loop and kernel micro-benchmarks, and robustness under stress with simple runtime mitigations; we also report sample-efficiency benefits from behavioral-cloning pretraining and compatibility with an actor-critic variant without altering the inference pipeline. These results establish a practical framework for Transformer-based decision making in high-throughput real-time scheduling.", "AI": {"tldr": "TempoNet\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5b9e\u65f6\u8c03\u5ea6\u5668\uff0c\u4f7f\u7528Transformer\u548c\u6df1\u5ea6Q\u5b66\u4e60\uff0c\u901a\u8fc7Urgency Tokenizer\u5904\u7406\u65f6\u95f4\u677e\u5f1b\u5ea6\uff0c\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\uff0c\u5728\u591a\u6838\u73af\u5883\u4e0b\u4f18\u4e8e\u4f20\u7edf\u8c03\u5ea6\u5668\u3002", "motivation": "\u5b9e\u65f6\u8c03\u5ea6\u5668\u9700\u8981\u5728\u4e25\u683c\u7684\u8ba1\u7b97\u9884\u7b97\u4e0b\u5904\u7406\u7d27\u8feb\u7684\u622a\u6b62\u65f6\u95f4\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5728\u590d\u6742\u591a\u6838\u73af\u5883\u4e2d\u5b9e\u73b0\u6700\u4f18\u8c03\u5ea6\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "1. \u4f7f\u7528\u7f6e\u6362\u4e0d\u53d8Transformer\u548c\u6df1\u5ea6Q\u8fd1\u4f3c\uff1b2. Urgency Tokenizer\u5c06\u65f6\u95f4\u677e\u5f1b\u5ea6\u79bb\u6563\u5316\u4e3a\u53ef\u5b66\u4e60\u5d4c\u5165\uff1b3. \u5ef6\u8fdf\u611f\u77e5\u7a00\u758f\u6ce8\u610f\u529b\u5806\u6808\uff08\u5757\u72b6top-k\u9009\u62e9\u548c\u5c40\u90e8\u654f\u611f\u5206\u5757\uff09\uff1b4. \u591a\u6838\u6620\u5c04\u5c42\u5c06Q\u5206\u6570\u8f6c\u6362\u4e3a\u5904\u7406\u5668\u5206\u914d\u3002", "result": "\u5728\u5de5\u4e1a\u6df7\u5408\u5173\u952e\u6027\u8ffd\u8e2a\u548c\u5927\u89c4\u6a21\u591a\u5904\u7406\u5668\u8bbe\u7f6e\u4e2d\uff0c\u76f8\u6bd4\u5206\u6790\u8c03\u5ea6\u5668\u548c\u795e\u7ecf\u57fa\u7ebf\uff0cTempoNet\u5728\u622a\u6b62\u65f6\u95f4\u6ee1\u8db3\u7387\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u5177\u6709\u6539\u8fdb\u7684\u4f18\u5316\u7a33\u5b9a\u6027\u548c\u4e9a\u6beb\u79d2\u7ea7\u63a8\u7406\u901f\u5ea6\u3002", "conclusion": "TempoNet\u4e3a\u57fa\u4e8eTransformer\u7684\u9ad8\u541e\u5410\u91cf\u5b9e\u65f6\u8c03\u5ea6\u51b3\u7b56\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u590d\u6742\u8c03\u5ea6\u95ee\u9898\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.18114", "categories": ["cs.LG", "cs.DS", "math.OC"], "pdf": "https://arxiv.org/pdf/2602.18114", "abs": "https://arxiv.org/abs/2602.18114", "authors": ["Yiding Feng", "Jiashuo Jiang", "Yige Wang"], "title": "Non-Stationary Online Resource Allocation: Learning from a Single Sample", "comment": null, "summary": "We study online resource allocation under non-stationary demand with a minimum offline data requirement. In this problem, a decision-maker must allocate multiple types of resources to sequentially arriving queries over a finite horizon. Each query belongs to a finite set of types with fixed resource consumption and a stochastic reward drawn from an unknown, type-specific distribution. Critically, the environment exhibits arbitrary non-stationarity -- arrival distributions may shift unpredictably-while the algorithm requires only one historical sample per period to operate effectively. We distinguish two settings based on sample informativeness: (i) reward-observed samples containing both query type and reward realization, and (ii) the more challenging type-only samples revealing only query type information.\n  We propose a novel type-dependent quantile-based meta-policy that decouples the problem into modular components: reward distribution estimation, optimization of target service probabilities via fluid relaxation, and real-time decisions through dynamic acceptance thresholds. For reward-observed samples, our static threshold policy achieves $\\tilde{O}(\\sqrt{T})$ regret. For type-only samples, we first establish that sublinear regret is impossible without additional structure; under a mild minimum-arrival-probability assumption, we design both a partially adaptive policy attaining the same $\\tilde{O}({T})$ bound and, more significantly, a fully adaptive resolving policy with careful rounding that achieves the first poly-logarithmic regret guarantee of $O((\\log T)^3)$ for non-stationary multi-resource allocation. Our framework advances prior work by operating with minimal offline data (one sample per period), handling arbitrary non-stationarity without variation-budget assumptions, and supporting multiple resource constraints.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u4f4d\u6570\u7684\u5143\u7b56\u7565\uff0c\u7528\u4e8e\u5904\u7406\u975e\u5e73\u7a33\u9700\u6c42\u4e0b\u7684\u5728\u7ebf\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u4ec5\u9700\u6bcf\u4e2a\u5468\u671f\u4e00\u4e2a\u5386\u53f2\u6837\u672c\uff0c\u5e76\u5728\u4e24\u79cd\u6837\u672c\u4fe1\u606f\u8bbe\u7f6e\u4e0b\u5206\u522b\u5b9e\u73b0\u4e86\u221aT\u548c\u5bf9\u6570\u7ea7\u9057\u61be\u3002", "motivation": "\u89e3\u51b3\u5728\u7ebf\u8d44\u6e90\u5206\u914d\u4e2d\u975e\u5e73\u7a33\u9700\u6c42\u5e26\u6765\u7684\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u5386\u53f2\u6570\u636e\u6216\u5047\u8bbe\u73af\u5883\u5e73\u7a33\uff0c\u800c\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u6c42\u5206\u5e03\u53ef\u80fd\u4efb\u610f\u53d8\u5316\u4e14\u5386\u53f2\u6570\u636e\u6709\u9650\u3002", "method": "\u63d0\u51fa\u7c7b\u578b\u4f9d\u8d56\u7684\u5206\u4f4d\u6570\u5143\u7b56\u7565\uff0c\u5c06\u95ee\u9898\u89e3\u8026\u4e3a\u4e09\u4e2a\u6a21\u5757\uff1a\u5956\u52b1\u5206\u5e03\u4f30\u8ba1\u3001\u901a\u8fc7\u6d41\u4f53\u677e\u5f1b\u4f18\u5316\u76ee\u6807\u670d\u52a1\u6982\u7387\u3001\u901a\u8fc7\u52a8\u6001\u63a5\u53d7\u9608\u503c\u8fdb\u884c\u5b9e\u65f6\u51b3\u7b56\u3002", "result": "\u5bf9\u4e8e\u5956\u52b1\u53ef\u89c2\u6d4b\u6837\u672c\uff0c\u9759\u6001\u9608\u503c\u7b56\u7565\u5b9e\u73b0\u4e86O\u0303(\u221aT)\u9057\u61be\uff1b\u5bf9\u4e8e\u4ec5\u7c7b\u578b\u6837\u672c\uff0c\u5728\u6700\u5c0f\u5230\u8fbe\u6982\u7387\u5047\u8bbe\u4e0b\uff0c\u90e8\u5206\u81ea\u9002\u5e94\u7b56\u7565\u8fbe\u5230O\u0303(\u221aT)\uff0c\u5b8c\u5168\u81ea\u9002\u5e94\u89e3\u6790\u7b56\u7565\u5b9e\u73b0\u4e86O((log T)\u00b3)\u7684\u591a\u5bf9\u6570\u9057\u61be\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u4ec5\u9700\u6bcf\u4e2a\u5468\u671f\u4e00\u4e2a\u79bb\u7ebf\u6837\u672c\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u5904\u7406\u4efb\u610f\u975e\u5e73\u7a33\u6027\uff0c\u652f\u6301\u591a\u8d44\u6e90\u7ea6\u675f\uff0c\u4e3a\u975e\u5e73\u7a33\u591a\u8d44\u6e90\u5728\u7ebf\u5206\u914d\u63d0\u4f9b\u4e86\u9996\u4e2a\u591a\u5bf9\u6570\u9057\u61be\u4fdd\u8bc1\u3002"}}
{"id": "2602.18196", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18196", "abs": "https://arxiv.org/abs/2602.18196", "authors": ["Xiuying Wei", "Caglar Gulcehre"], "title": "RAT+: Train Dense, Infer Sparse -- Recurrence Augmented Attention for Dilated Inference", "comment": null, "summary": "Structured dilated attention has an appealing inference-time efficiency knob: it reduces the FLOPs of the attention and the KV cache size by a factor of the dilation size D, while preserving long-range connectivity. However, we find a persistent failure mode of them -- sparsifying a pretrained attention model to a dilated pattern leads to severe accuracy degradation. We introduce RAT+, a dense-pretraining architecture that augments attention with full-sequence recurrence and active recurrence learning. A single RAT+ model is pretrained densely once, then flexibly switched at inference time to dilated attention (optionally with local windows) or hybrid layer/head compositions, requiring only a short 1B-token resolution adaptation rather than retraining separate sparse models. At 1.5B parameters trained on 100B tokens, RAT+ closely matches dense accuracy at 16 and drops by about 2-3 points at 64 on commonsense reasoning and LongBench tasks, respectively. Moreover, RAT+ outperforms attention when sparsifying to the top-k block attention. We further scale to 2.6B parameters and 200B tokens and observe the same trend.", "AI": {"tldr": "RAT+\u662f\u4e00\u79cd\u5bc6\u96c6\u9884\u8bad\u7ec3\u67b6\u6784\uff0c\u901a\u8fc7\u5168\u5e8f\u5217\u5faa\u73af\u548c\u4e3b\u52a8\u5faa\u73af\u5b66\u4e60\u589e\u5f3a\u6ce8\u610f\u529b\u673a\u5236\uff0c\u53ef\u5728\u63a8\u7406\u65f6\u7075\u6d3b\u5207\u6362\u5230\u6269\u5f20\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "motivation": "\u7ed3\u6784\u5316\u6269\u5f20\u6ce8\u610f\u529b\u5728\u63a8\u7406\u65f6\u5177\u6709\u6548\u7387\u4f18\u52bf\uff0c\u4f46\u5c06\u9884\u8bad\u7ec3\u6ce8\u610f\u529b\u6a21\u578b\u7a00\u758f\u5316\u4e3a\u6269\u5f20\u6a21\u5f0f\u4f1a\u5bfc\u81f4\u4e25\u91cd\u7684\u51c6\u786e\u6027\u4e0b\u964d\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u957f\u8ddd\u79bb\u8fde\u63a5\uff0c\u53c8\u80fd\u5728\u63a8\u7406\u65f6\u7075\u6d3b\u8c03\u6574\u8ba1\u7b97\u5f00\u9500\u7684\u65b9\u6cd5\u3002", "method": "RAT+\u67b6\u6784\u5728\u5bc6\u96c6\u9884\u8bad\u7ec3\u65f6\u901a\u8fc7\u5168\u5e8f\u5217\u5faa\u73af\u548c\u4e3b\u52a8\u5faa\u73af\u5b66\u4e60\u589e\u5f3a\u6ce8\u610f\u529b\u673a\u5236\u3002\u5355\u4e00\u6a21\u578b\u7ecf\u8fc7\u5bc6\u96c6\u9884\u8bad\u7ec3\u540e\uff0c\u53ef\u5728\u63a8\u7406\u65f6\u7075\u6d3b\u5207\u6362\u5230\u6269\u5f20\u6ce8\u610f\u529b\u6a21\u5f0f\uff08\u53ef\u9009\u5c40\u90e8\u7a97\u53e3\uff09\u6216\u6df7\u5408\u5c42/\u5934\u7ec4\u5408\uff0c\u4ec5\u9700\u5c11\u91cf\u5206\u8fa8\u7387\u9002\u5e94\u800c\u975e\u91cd\u65b0\u8bad\u7ec3\u7a00\u758f\u6a21\u578b\u3002", "result": "\u57281.5B\u53c2\u6570\u3001100B token\u8bad\u7ec3\u4e0b\uff0cRAT+\u572816\u500d\u6269\u5f20\u65f6\u63a5\u8fd1\u5bc6\u96c6\u6ce8\u610f\u529b\u51c6\u786e\u6027\uff0c64\u500d\u6269\u5f20\u65f6\u5728\u5e38\u8bc6\u63a8\u7406\u548cLongBench\u4efb\u52a1\u4e0a\u4ec5\u4e0b\u964d2-3\u4e2a\u70b9\u3002\u6b64\u5916\uff0cRAT+\u5728\u7a00\u758f\u5316\u4e3atop-k\u5757\u6ce8\u610f\u529b\u65f6\u4f18\u4e8e\u539f\u59cb\u6ce8\u610f\u529b\u3002\u57282.6B\u53c2\u6570\u3001200B token\u89c4\u6a21\u4e0b\u89c2\u5bdf\u5230\u76f8\u540c\u8d8b\u52bf\u3002", "conclusion": "RAT+\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff1a\u901a\u8fc7\u5355\u4e00\u5bc6\u96c6\u9884\u8bad\u7ec3\u6a21\u578b\u652f\u6301\u591a\u79cd\u63a8\u7406\u65f6\u7a00\u758f\u5316\u6a21\u5f0f\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\uff0c\u89e3\u51b3\u4e86\u7ed3\u6784\u5316\u6269\u5f20\u6ce8\u610f\u529b\u7684\u51c6\u786e\u6027\u4e0b\u964d\u95ee\u9898\u3002"}}
{"id": "2602.18250", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18250", "abs": "https://arxiv.org/abs/2602.18250", "authors": ["Yves Ruffenach"], "title": "Variational Distributional Neuron", "comment": "29 pages, 7 figures. Code available at GitHub (link in paper)", "summary": "We propose a proof of concept for a variational distributional neuron: a compute unit formulated as a VAE brick, explicitly carrying a prior, an amortized posterior and a local ELBO. The unit is no longer a deterministic scalar but a distribution: computing is no longer about propagating values, but about contracting a continuous space of possibilities under constraints. Each neuron parameterizes a posterior, propagates a reparameterized sample and is regularized by the KL term of a local ELBO - hence, the activation is distributional. This \"contraction\" becomes testable through local constraints and can be monitored via internal measures. The amount of contextual information carried by the unit, as well as the temporal persistence of this information, are locally tuned by distinct constraints. This proposal addresses a structural tension: in sequential generation, causality is predominantly organized in the symbolic space and, even when latents exist, they often remain auxiliary, while the effective dynamics are carried by a largely deterministic decoder. In parallel, probabilistic latent models capture factors of variation and uncertainty, but that uncertainty typically remains borne by global or parametric mechanisms, while units continue to propagate scalars - hence the pivot question: if uncertainty is intrinsic to computation, why does the compute unit not carry it explicitly? We therefore draw two axes: (i) the composition of probabilistic constraints, which must be made stable, interpretable and controllable; and (ii) granularity: if inference is a negotiation of distributions under constraints, should the primitive unit remain deterministic or become distributional? We analyze \"collapse\" modes and the conditions for a \"living neuron\", then extend the contribution over time via autoregressive priors over the latent, per unit.", "AI": {"tldr": "\u63d0\u51fa\u53d8\u5206\u5206\u5e03\u795e\u7ecf\u5143\u7684\u8bc1\u660e\u6982\u5ff5\uff1a\u5c06\u795e\u7ecf\u5143\u6784\u5efa\u4e3aVAE\u6a21\u5757\uff0c\u5305\u542b\u5148\u9a8c\u3001\u644a\u9500\u540e\u9a8c\u548c\u5c40\u90e8ELBO\uff0c\u4f7f\u795e\u7ecf\u5143\u4ece\u786e\u5b9a\u6027\u6807\u91cf\u53d8\u4e3a\u5206\u5e03\uff0c\u8ba1\u7b97\u4ece\u4f20\u64ad\u503c\u53d8\u4e3a\u5728\u7ea6\u675f\u4e0b\u6536\u7f29\u53ef\u80fd\u6027\u7a7a\u95f4\u3002", "motivation": "\u89e3\u51b3\u7ed3\u6784\u4e0a\u7684\u5f20\u529b\uff1a\u5728\u5e8f\u5217\u751f\u6210\u4e2d\uff0c\u56e0\u679c\u6027\u4e3b\u8981\u5728\u7b26\u53f7\u7a7a\u95f4\u7ec4\u7ec7\uff0c\u5373\u4f7f\u5b58\u5728\u6f5c\u5728\u53d8\u91cf\u4e5f\u5e38\u4e3a\u8f85\u52a9\u4f5c\u7528\uff0c\u800c\u6709\u6548\u52a8\u6001\u7531\u786e\u5b9a\u6027\u89e3\u7801\u5668\u627f\u8f7d\uff1b\u540c\u65f6\u6982\u7387\u6f5c\u5728\u6a21\u578b\u6355\u83b7\u53d8\u5316\u56e0\u7d20\u548c\u4e0d\u786e\u5b9a\u6027\uff0c\u4f46\u8fd9\u4e9b\u4e0d\u786e\u5b9a\u6027\u901a\u5e38\u7531\u5168\u5c40\u6216\u53c2\u6570\u673a\u5236\u627f\u62c5\uff0c\u800c\u8ba1\u7b97\u5355\u5143\u4ecd\u4f20\u64ad\u6807\u91cf\u3002\u6838\u5fc3\u95ee\u9898\uff1a\u5982\u679c\u4e0d\u786e\u5b9a\u6027\u662f\u8ba1\u7b97\u7684\u5185\u5728\u5c5e\u6027\uff0c\u4e3a\u4ec0\u4e48\u8ba1\u7b97\u5355\u5143\u4e0d\u663e\u5f0f\u5730\u627f\u8f7d\u5b83\uff1f", "method": "\u63d0\u51fa\u53d8\u5206\u5206\u5e03\u795e\u7ecf\u5143\uff1a\u6bcf\u4e2a\u795e\u7ecf\u5143\u53c2\u6570\u5316\u540e\u9a8c\u5206\u5e03\uff0c\u4f20\u64ad\u91cd\u53c2\u6570\u5316\u6837\u672c\uff0c\u5e76\u901a\u8fc7\u5c40\u90e8ELBO\u7684KL\u9879\u8fdb\u884c\u6b63\u5219\u5316\u3002\u5206\u6790\"\u5d29\u6e83\"\u6a21\u5f0f\u548c\"\u6d3b\u795e\u7ecf\u5143\"\u6761\u4ef6\uff0c\u5e76\u901a\u8fc7\u81ea\u56de\u5f52\u5148\u9a8c\u5728\u65f6\u95f4\u4e0a\u6269\u5c55\u8d21\u732e\u3002", "result": "\u63d0\u51fa\u6982\u5ff5\u8bc1\u660e\uff0c\u5c55\u793a\u5982\u4f55\u901a\u8fc7\u5c40\u90e8\u7ea6\u675f\u6d4b\u8bd5\"\u6536\u7f29\"\u8fc7\u7a0b\uff0c\u5e76\u901a\u8fc7\u5185\u90e8\u5ea6\u91cf\u76d1\u63a7\u3002\u795e\u7ecf\u5143\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u91cf\u548c\u65f6\u95f4\u6301\u4e45\u6027\u53ef\u901a\u8fc7\u4e0d\u540c\u7ea6\u675f\u8fdb\u884c\u5c40\u90e8\u8c03\u8282\u3002", "conclusion": "\u63a2\u7d22\u4e24\u4e2a\u8f74\u7ebf\uff1a(i)\u6982\u7387\u7ea6\u675f\u7684\u7ec4\u5408\u5fc5\u987b\u7a33\u5b9a\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u63a7\uff1b(ii)\u7c92\u5ea6\u95ee\u9898\uff1a\u5982\u679c\u63a8\u7406\u662f\u5728\u7ea6\u675f\u4e0b\u5206\u5e03\u534f\u5546\u7684\u8fc7\u7a0b\uff0c\u539f\u59cb\u5355\u5143\u5e94\u4fdd\u6301\u786e\u5b9a\u6027\u8fd8\u662f\u53d8\u4e3a\u5206\u5e03\u6027\uff1f\u4e3a\u5206\u5e03\u8ba1\u7b97\u5355\u5143\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.18253", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18253", "abs": "https://arxiv.org/abs/2602.18253", "authors": ["Xabier de Zuazo", "Vincenzo Verbeni", "Eva Navas", "Ibon Saratxaga", "Mathieu Bourguignon", "Nicola Molinaro"], "title": "MEG-to-MEG Transfer Learning and Cross-Task Speech/Silence Detection with Limited Data", "comment": "6 pages, 3 figures, 3 tables, submitted to Interspeech 2026", "summary": "Data-efficient neural decoding is a central challenge for speech brain-computer interfaces. We present the first demonstration of transfer learning and cross-task decoding for MEG-based speech models spanning perception and production. We pre-train a Conformer-based model on 50 hours of single-subject listening data and fine-tune on just 5 minutes per subject across 18 participants. Transfer learning yields consistent improvements, with in-task accuracy gains of 1-4% and larger cross-task gains of up to 5-6%. Not only does pre-training improve performance within each task, but it also enables reliable cross-task decoding between perception and production. Critically, models trained on speech production decode passive listening above chance, confirming that learned representations reflect shared neural processes rather than task-specific motor activity.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u5c55\u793a\u4e86MEG\u8bed\u97f3\u6a21\u578b\u5728\u611f\u77e5\u4e0e\u4ea7\u751f\u4efb\u52a1\u95f4\u7684\u8fc1\u79fb\u5b66\u4e60\u548c\u8de8\u4efb\u52a1\u89e3\u7801\uff0c\u4f7f\u7528Conformer\u6a21\u578b\u572850\u5c0f\u65f6\u5355\u88ab\u8bd5\u542c\u97f3\u6570\u636e\u4e0a\u9884\u8bad\u7ec3\uff0c\u4ec5\u75285\u5206\u949f\u6570\u636e\u5fae\u8c03\u5373\u53ef\u572818\u540d\u88ab\u8bd5\u4e0a\u83b7\u5f97\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u8bed\u97f3\u8111\u673a\u63a5\u53e3\u4e2d\u7684\u6570\u636e\u6548\u7387\u95ee\u9898\uff0c\u63a2\u7d22\u8bed\u97f3\u611f\u77e5\u4e0e\u4ea7\u751f\u4efb\u52a1\u95f4\u7684\u5171\u4eab\u795e\u7ecf\u8868\u5f81\uff0c\u9a8c\u8bc1\u9884\u8bad\u7ec3\u6a21\u578b\u80fd\u5426\u6355\u6349\u8de8\u4efb\u52a1\u7684\u5171\u540c\u795e\u7ecf\u8fc7\u7a0b\u800c\u975e\u4efb\u52a1\u7279\u5f02\u6027\u6d3b\u52a8\u3002", "method": "\u4f7f\u7528Conformer\u67b6\u6784\u6a21\u578b\uff0c\u5728\u5355\u4e2a\u88ab\u8bd5\u768450\u5c0f\u65f6\u542c\u97f3\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u572818\u540d\u88ab\u8bd5\u4e0a\u4ec5\u7528\u6bcf\u4eba5\u5206\u949f\u7684\u6570\u636e\u8fdb\u884c\u5fae\u8c03\uff0c\u8bc4\u4f30\u6a21\u578b\u5728\u8bed\u97f3\u611f\u77e5\u548c\u4ea7\u751f\u4efb\u52a1\u5185\u7684\u6027\u80fd\u4ee5\u53ca\u8de8\u4efb\u52a1\u89e3\u7801\u80fd\u529b\u3002", "result": "\u8fc1\u79fb\u5b66\u4e60\u5e26\u6765\u4e00\u81f4\u6027\u80fd\u63d0\u5347\uff1a\u4efb\u52a1\u5185\u51c6\u786e\u7387\u63d0\u9ad81-4%\uff0c\u8de8\u4efb\u52a1\u51c6\u786e\u7387\u63d0\u5347\u8fbe5-6%\u3002\u9884\u8bad\u7ec3\u4e0d\u4ec5\u6539\u5584\u5404\u4efb\u52a1\u6027\u80fd\uff0c\u8fd8\u5b9e\u73b0\u4e86\u611f\u77e5\u4e0e\u4ea7\u751f\u4efb\u52a1\u95f4\u7684\u53ef\u9760\u8de8\u4efb\u52a1\u89e3\u7801\u3002\u5173\u952e\u53d1\u73b0\uff1a\u5728\u8bed\u97f3\u4ea7\u751f\u4efb\u52a1\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u80fd\u591f\u89e3\u7801\u88ab\u52a8\u542c\u97f3\u4efb\u52a1\uff0c\u8868\u660e\u5b66\u4e60\u5230\u7684\u8868\u5f81\u53cd\u6620\u4e86\u5171\u4eab\u795e\u7ecf\u8fc7\u7a0b\u800c\u975e\u4efb\u52a1\u7279\u5f02\u6027\u8fd0\u52a8\u6d3b\u52a8\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u8fc1\u79fb\u5b66\u4e60\u5728MEG\u8bed\u97f3\u89e3\u7801\u4e2d\u7684\u6709\u6548\u6027\uff0c\u63ed\u793a\u4e86\u8bed\u97f3\u611f\u77e5\u4e0e\u4ea7\u751f\u4efb\u52a1\u95f4\u5b58\u5728\u5171\u4eab\u795e\u7ecf\u8868\u5f81\uff0c\u4e3a\u6570\u636e\u9ad8\u6548\u7684\u8bed\u97f3\u8111\u673a\u63a5\u53e3\u5f00\u53d1\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u5e76\u786e\u8ba4\u4e86\u5b66\u4e60\u5230\u7684\u8868\u5f81\u6355\u6349\u7684\u662f\u8de8\u4efb\u52a1\u7684\u5171\u540c\u795e\u7ecf\u8fc7\u7a0b\u3002"}}
{"id": "2602.18301", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18301", "abs": "https://arxiv.org/abs/2602.18301", "authors": ["Ivan Bondarenko", "Egor Palkin", "Fedor Tikunov"], "title": "On the Semantic and Syntactic Information Encoded in Proto-Tokens for One-Step Text Reconstruction", "comment": null, "summary": "Autoregressive large language models (LLMs) generate text token-by-token, requiring n forward passes to produce a sequence of length n. Recent work, Exploring the Latent Capacity of LLMs for One-Step Text Reconstruction (Mezentsev and Oseledets), shows that frozen LLMs can reconstruct hundreds of tokens from only two learned proto-tokens in a single forward pass, suggesting a path beyond the autoregressive paradigm. In this paper, we study what information these proto-tokens encode and how they behave under reconstruction and controlled constraints. We perform a series of experiments aimed at disentangling semantic and syntactic content in the two proto-tokens, analyzing stability properties of the e-token, and visualizing attention patterns to the e-token during reconstruction. Finally, we test two regularization schemes for \"imposing\" semantic structure on the e-token using teacher embeddings, including an anchor-based loss and a relational distillation objective. Our results indicate that the m-token tends to capture semantic information more strongly than the e-token under standard optimization; anchor-based constraints trade off sharply with reconstruction accuracy; and relational distillation can transfer batch-level semantic relations into the proto-token space without sacrificing reconstruction quality, supporting the feasibility of future non-autoregressive seq2seq systems that predict proto-tokens as an intermediate representation.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u51bb\u7ed3\u5927\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u4e24\u4e2a\u5b66\u4e60\u5230\u7684\u539f\u578b\u6807\u8bb0\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u91cd\u5efa\u6570\u767e\u4e2a\u6807\u8bb0\uff0c\u63a2\u7d22\u4e86\u539f\u578b\u6807\u8bb0\u7f16\u7801\u7684\u4fe1\u606f\u53ca\u5176\u5728\u91cd\u5efa\u548c\u7ea6\u675f\u4e0b\u7684\u884c\u4e3a\uff0c\u4e3a\u8d85\u8d8a\u81ea\u56de\u5f52\u8303\u5f0f\u63d0\u4f9b\u4e86\u8def\u5f84\u3002", "motivation": "\u81ea\u56de\u5f52\u5927\u8bed\u8a00\u6a21\u578b\u9700\u8981n\u6b21\u524d\u5411\u4f20\u64ad\u6765\u751f\u6210\u957f\u5ea6\u4e3an\u7684\u5e8f\u5217\uff0c\u6548\u7387\u8f83\u4f4e\u3002\u6700\u8fd1\u7814\u7a76\u8868\u660e\u51bb\u7ed3LLMs\u53ef\u4ee5\u4ece\u4e24\u4e2a\u5b66\u4e60\u5230\u7684\u539f\u578b\u6807\u8bb0\u4e2d\u91cd\u5efa\u6570\u767e\u4e2a\u6807\u8bb0\uff0c\u8fd9\u4e3a\u8d85\u8d8a\u81ea\u56de\u5f52\u8303\u5f0f\u63d0\u4f9b\u4e86\u53ef\u80fd\uff0c\u56e0\u6b64\u9700\u8981\u6df1\u5165\u7814\u7a76\u8fd9\u4e9b\u539f\u578b\u6807\u8bb0\u7f16\u7801\u7684\u4fe1\u606f\u53ca\u5176\u884c\u4e3a\u7279\u6027\u3002", "method": "\u901a\u8fc7\u4e00\u7cfb\u5217\u5b9e\u9a8c\u5206\u6790\u4e24\u4e2a\u539f\u578b\u6807\u8bb0\u4e2d\u7684\u8bed\u4e49\u548c\u53e5\u6cd5\u5185\u5bb9\uff0c\u7814\u7a76e-token\u7684\u7a33\u5b9a\u6027\u7279\u6027\uff0c\u53ef\u89c6\u5316\u91cd\u5efa\u8fc7\u7a0b\u4e2d\u5bf9e-token\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u5e76\u6d4b\u8bd5\u4e24\u79cd\u4f7f\u7528\u6559\u5e08\u5d4c\u5165\u5728e-token\u4e0a\"\u65bd\u52a0\"\u8bed\u4e49\u7ed3\u6784\u7684\u6b63\u5219\u5316\u65b9\u6848\uff1a\u57fa\u4e8e\u951a\u70b9\u7684\u635f\u5931\u548c\u5173\u7cfb\u84b8\u998f\u76ee\u6807\u3002", "result": "\u6807\u51c6\u4f18\u5316\u4e0bm-token\u6bd4e-token\u66f4\u5f3a\u70c8\u5730\u6355\u83b7\u8bed\u4e49\u4fe1\u606f\uff1b\u57fa\u4e8e\u951a\u70b9\u7684\u7ea6\u675f\u4e0e\u91cd\u5efa\u7cbe\u5ea6\u4e4b\u95f4\u5b58\u5728\u660e\u663e\u6743\u8861\uff1b\u5173\u7cfb\u84b8\u998f\u53ef\u4ee5\u5728\u4e0d\u727a\u7272\u91cd\u5efa\u8d28\u91cf\u7684\u60c5\u51b5\u4e0b\u5c06\u6279\u6b21\u7ea7\u8bed\u4e49\u5173\u7cfb\u8f6c\u79fb\u5230\u539f\u578b\u6807\u8bb0\u7a7a\u95f4\u3002", "conclusion": "\u7814\u7a76\u652f\u6301\u4e86\u672a\u6765\u975e\u81ea\u56de\u5f52\u5e8f\u5217\u5230\u5e8f\u5217\u7cfb\u7edf\u7684\u53ef\u884c\u6027\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u53ef\u4ee5\u9884\u6d4b\u539f\u578b\u6807\u8bb0\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u4e3a\u8d85\u8d8a\u4f20\u7edf\u81ea\u56de\u5f52\u751f\u6210\u8303\u5f0f\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2602.18333", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2602.18333", "abs": "https://arxiv.org/abs/2602.18333", "authors": ["M. Reza Ebrahimi", "Micha\u00ebl Defferrard", "Sunny Panchal", "Roland Memisevic"], "title": "On the \"Induction Bias\" in Sequence Models", "comment": null, "summary": "Despite the remarkable practical success of transformer-based language models, recent work has raised concerns about their ability to perform state tracking. In particular, a growing body of literature has shown this limitation primarily through failures in out-of-distribution (OOD) generalization, such as length extrapolation. In this work, we shift attention to the in-distribution implications of these limitations. We conduct a large-scale experimental study of the data efficiency of transformers and recurrent neural networks (RNNs) across multiple supervision regimes. We find that the amount of training data required by transformers grows much more rapidly with state-space size and sequence length than for RNNs. Furthermore, we analyze the extent to which learned state-tracking mechanisms are shared across different sequence lengths. We show that transformers exhibit negligible or even detrimental weight sharing across lengths, indicating that they learn length-specific solutions in isolation. In contrast, recurrent models exhibit effective amortized learning by sharing weights across lengths, allowing data from one sequence length to improve performance on others. Together, these results demonstrate that state tracking remains a fundamental challenge for transformers, even when training and evaluation distributions match.", "AI": {"tldr": "Transformer\u6a21\u578b\u5728\u72b6\u6001\u8ddf\u8e2a\u4efb\u52a1\u4e0a\u5b58\u5728\u6839\u672c\u6027\u6311\u6218\uff0c\u5373\u4f7f\u5728\u8bad\u7ec3\u548c\u8bc4\u4f30\u5206\u5e03\u5339\u914d\u7684\u60c5\u51b5\u4e0b\uff0c\u5176\u6570\u636e\u6548\u7387\u8fdc\u4f4e\u4e8eRNN\uff0c\u4e14\u7f3a\u4e4f\u8de8\u5e8f\u5217\u957f\u5ea6\u7684\u6743\u91cd\u5171\u4eab\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1Transformer\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u8fd1\u671f\u7814\u7a76\u8868\u660e\u5b83\u4eec\u5728\u72b6\u6001\u8ddf\u8e2a\u80fd\u529b\u4e0a\u5b58\u5728\u5c40\u9650\uff0c\u7279\u522b\u662f\u5728\u5206\u5e03\u5916\u6cdb\u5316\u65b9\u9762\u3002\u672c\u7814\u7a76\u5173\u6ce8\u8fd9\u4e9b\u5c40\u9650\u5728\u5206\u5e03\u5185\u7684\u5f71\u54cd\uff0c\u63a2\u7d22Transformer\u548cRNN\u5728\u4e0d\u540c\u76d1\u7763\u673a\u5236\u4e0b\u7684\u6570\u636e\u6548\u7387\u5dee\u5f02\u3002", "method": "\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u9a8c\u7814\u7a76\uff0c\u6bd4\u8f83Transformer\u548cRNN\u5728\u591a\u79cd\u76d1\u7763\u673a\u5236\u4e0b\u7684\u6570\u636e\u6548\u7387\u3002\u5206\u6790\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u968f\u72b6\u6001\u7a7a\u95f4\u5927\u5c0f\u548c\u5e8f\u5217\u957f\u5ea6\u7684\u589e\u957f\u60c5\u51b5\uff0c\u5e76\u7814\u7a76\u5b66\u4e60\u5230\u7684\u72b6\u6001\u8ddf\u8e2a\u673a\u5236\u5728\u4e0d\u540c\u5e8f\u5217\u957f\u5ea6\u95f4\u7684\u5171\u4eab\u7a0b\u5ea6\u3002", "result": "Transformer\u7684\u8bad\u7ec3\u6570\u636e\u9700\u6c42\u968f\u72b6\u6001\u7a7a\u95f4\u5927\u5c0f\u548c\u5e8f\u5217\u957f\u5ea6\u589e\u957f\u7684\u901f\u5ea6\u8fdc\u5feb\u4e8eRNN\u3002Transformer\u5728\u4e0d\u540c\u5e8f\u5217\u957f\u5ea6\u95f4\u8868\u73b0\u51fa\u53ef\u5ffd\u7565\u751a\u81f3\u6709\u5bb3\u7684\u6743\u91cd\u5171\u4eab\uff0c\u8868\u660e\u5b83\u4eec\u5b66\u4e60\u7684\u662f\u957f\u5ea6\u7279\u5b9a\u7684\u5b64\u7acb\u89e3\u51b3\u65b9\u6848\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5faa\u73af\u6a21\u578b\u901a\u8fc7\u8de8\u957f\u5ea6\u5171\u4eab\u6743\u91cd\u5b9e\u73b0\u6709\u6548\u7684\u644a\u9500\u5b66\u4e60\uff0c\u5141\u8bb8\u6765\u81ea\u4e00\u4e2a\u5e8f\u5217\u957f\u5ea6\u7684\u6570\u636e\u6539\u8fdb\u5176\u4ed6\u957f\u5ea6\u7684\u6027\u80fd\u3002", "conclusion": "\u72b6\u6001\u8ddf\u8e2a\u4ecd\u7136\u662fTransformer\u9762\u4e34\u7684\u6839\u672c\u6027\u6311\u6218\uff0c\u5373\u4f7f\u5728\u8bad\u7ec3\u548c\u8bc4\u4f30\u5206\u5e03\u5339\u914d\u7684\u60c5\u51b5\u4e0b\u3002Transformer\u7f3a\u4e4fRNN\u6240\u5177\u6709\u7684\u8de8\u5e8f\u5217\u957f\u5ea6\u7684\u6709\u6548\u6743\u91cd\u5171\u4eab\u80fd\u529b\uff0c\u5bfc\u81f4\u6570\u636e\u6548\u7387\u4f4e\u4e0b\u548c\u957f\u5ea6\u7279\u5b9a\u7684\u5b66\u4e60\u6a21\u5f0f\u3002"}}
{"id": "2602.18348", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18348", "abs": "https://arxiv.org/abs/2602.18348", "authors": ["Matheus Camilo da Silva", "Leonardo Arrighi", "Ana Carolina Lorena", "Sylvio Barbon Junior"], "title": "Explaining AutoClustering: Uncovering Meta-Feature Contribution in AutoML for Clustering", "comment": null, "summary": "AutoClustering methods aim to automate unsupervised learning tasks, including algorithm selection (AS), hyperparameter optimization (HPO), and pipeline synthesis (PS), by often leveraging meta-learning over dataset meta-features. While these systems often achieve strong performance, their recommendations are often difficult to justify: the influence of dataset meta-features on algorithm and hyperparameter choices is typically not exposed, limiting reliability, bias diagnostics, and efficient meta-feature engineering. This limits reliability and diagnostic insight for further improvements. In this work, we investigate the explainability of the meta-models in AutoClustering. We first review 22 existing methods and organize their meta-features into a structured taxonomy. We then apply a global explainability technique (i.e., Decision Predicate Graphs) to assess feature importance within meta-models from selected frameworks. Finally, we use local explainability tools such as SHAP (SHapley Additive exPlanations) to analyse specific clustering decisions. Our findings highlight consistent patterns in meta-feature relevance, identify structural weaknesses in current meta-learning strategies that can distort recommendations, and provide actionable guidance for more interpretable Automated Machine Learning (AutoML) design. This study therefore offers a practical foundation for increasing decision transparency in unsupervised learning automation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86AutoClustering\u5143\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u901a\u8fc7\u5206\u679022\u79cd\u73b0\u6709\u65b9\u6cd5\u7684\u5143\u7279\u5f81\uff0c\u5e94\u7528\u5168\u5c40\u548c\u5c40\u90e8\u89e3\u91ca\u6280\u672f\uff0c\u63ed\u793a\u4e86\u5143\u7279\u5f81\u91cd\u8981\u6027\u6a21\u5f0f\uff0c\u8bc6\u522b\u4e86\u5f53\u524d\u5143\u5b66\u4e60\u7b56\u7565\u7684\u7ed3\u6784\u6027\u5f31\u70b9\uff0c\u4e3a\u66f4\u53ef\u89e3\u91ca\u7684AutoML\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "motivation": "AutoClustering\u65b9\u6cd5\u867d\u7136\u6027\u80fd\u5f3a\u5927\uff0c\u4f46\u5176\u63a8\u8350\u7ed3\u679c\u96be\u4ee5\u89e3\u91ca\uff1a\u6570\u636e\u96c6\u5143\u7279\u5f81\u5bf9\u7b97\u6cd5\u548c\u8d85\u53c2\u6570\u9009\u62e9\u7684\u5f71\u54cd\u901a\u5e38\u4e0d\u900f\u660e\uff0c\u9650\u5236\u4e86\u53ef\u9760\u6027\u3001\u504f\u5dee\u8bca\u65ad\u548c\u9ad8\u6548\u7684\u5143\u7279\u5f81\u5de5\u7a0b\u3002\u8fd9\u5f71\u54cd\u4e86\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u8fdb\u4e00\u6b65\u6539\u8fdb\u7684\u8bca\u65ad\u6d1e\u5bdf\u3002", "method": "1. \u56de\u987e22\u79cd\u73b0\u6709\u65b9\u6cd5\uff0c\u5c06\u5176\u5143\u7279\u5f81\u7ec4\u7ec7\u6210\u7ed3\u6784\u5316\u5206\u7c7b\u6cd5\uff1b2. \u5e94\u7528\u5168\u5c40\u53ef\u89e3\u91ca\u6027\u6280\u672f\uff08\u51b3\u7b56\u8c13\u8bcd\u56fe\uff09\u8bc4\u4f30\u9009\u5b9a\u6846\u67b6\u4e2d\u5143\u6a21\u578b\u7684\u7279\u5f81\u91cd\u8981\u6027\uff1b3. \u4f7f\u7528SHAP\u7b49\u5c40\u90e8\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u5206\u6790\u5177\u4f53\u7684\u805a\u7c7b\u51b3\u7b56\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1. \u63ed\u793a\u4e86\u5143\u7279\u5f81\u76f8\u5173\u6027\u7684\u7a33\u5b9a\u6a21\u5f0f\uff1b2. \u8bc6\u522b\u4e86\u5f53\u524d\u5143\u5b66\u4e60\u7b56\u7565\u4e2d\u53ef\u80fd\u626d\u66f2\u63a8\u8350\u7ed3\u679c\u7684\u7ed3\u6784\u6027\u5f31\u70b9\uff1b3. \u4e3a\u66f4\u53ef\u89e3\u91ca\u7684AutoML\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u589e\u52a0\u65e0\u76d1\u7763\u5b66\u4e60\u81ea\u52a8\u5316\u4e2d\u7684\u51b3\u7b56\u900f\u660e\u5ea6\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u7840\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8AutoClustering\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2602.18435", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2602.18435", "abs": "https://arxiv.org/abs/2602.18435", "authors": ["Aggelos Semoglou", "John Pavlopoulos"], "title": "Assigning Confidence: K-partition Ensembles", "comment": "31 pages including appendix", "summary": "Clustering is widely used for unsupervised structure discovery, yet it offers limited insight into how reliable each individual assignment is. Diagnostics, such as convergence behavior or objective values, may reflect global quality, but they do not indicate whether particular instances are assigned confidently, especially for initialization-sensitive algorithms like k-means. This assignment-level instability can undermine both accuracy and robustness. Ensemble approaches improve global consistency by aggregating multiple runs, but they typically lack tools for quantifying pointwise confidence in a way that combines cross-run agreement with geometric support from the learned cluster structure. We introduce CAKE (Confidence in Assignments via K-partition Ensembles), a framework that evaluates each point using two complementary statistics computed over a clustering ensemble: assignment stability and consistency of local geometric fit. These are combined into a single, interpretable score in [0,1]. Our theoretical analysis shows that CAKE remains effective under noise and separates stable from unstable points. Experiments on synthetic and real-world datasets indicate that CAKE effectively highlights ambiguous points and stable core members, providing a confidence ranking that can guide filtering or prioritization to improve clustering quality.", "AI": {"tldr": "CAKE\u6846\u67b6\u901a\u8fc7\u805a\u7c7b\u96c6\u6210\u8ba1\u7b97\u6bcf\u4e2a\u6570\u636e\u70b9\u7684\u5206\u914d\u7a33\u5b9a\u6027\u548c\u5c40\u90e8\u51e0\u4f55\u62df\u5408\u4e00\u81f4\u6027\uff0c\u4e3a\u805a\u7c7b\u5206\u914d\u63d0\u4f9b\u7f6e\u4fe1\u5ea6\u8bc4\u5206\uff0c\u8bc6\u522b\u7a33\u5b9a\u6838\u5fc3\u70b9\u548c\u6a21\u7cca\u70b9\u3002", "motivation": "\u4f20\u7edf\u805a\u7c7b\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u5355\u4e2a\u5206\u914d\u53ef\u9760\u6027\u7684\u8bc4\u4f30\uff0c\u8bca\u65ad\u6307\u6807\u4ec5\u53cd\u6620\u5168\u5c40\u8d28\u91cf\uff0c\u65e0\u6cd5\u91cf\u5316\u7279\u5b9a\u5b9e\u4f8b\u7684\u5206\u914d\u7f6e\u4fe1\u5ea6\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u521d\u59cb\u5316\u654f\u611f\u7684\u7b97\u6cd5\u5982k-means\u3002\u5206\u914d\u5c42\u9762\u7684\u4e0d\u7a33\u5b9a\u6027\u4f1a\u5f71\u54cd\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faCAKE\u6846\u67b6\uff0c\u901a\u8fc7\u805a\u7c7b\u96c6\u6210\u8ba1\u7b97\u4e24\u4e2a\u4e92\u8865\u7edf\u8ba1\u91cf\uff1a\u5206\u914d\u7a33\u5b9a\u6027\uff08\u8de8\u8fd0\u884c\u4e00\u81f4\u6027\uff09\u548c\u5c40\u90e8\u51e0\u4f55\u62df\u5408\u4e00\u81f4\u6027\uff08\u5b66\u4e60\u5230\u7684\u805a\u7c7b\u7ed3\u6784\u652f\u6301\uff09\u3002\u5c06\u8fd9\u4e9b\u7edf\u8ba1\u91cf\u7ec4\u5408\u6210[0,1]\u8303\u56f4\u5185\u7684\u53ef\u89e3\u91ca\u7f6e\u4fe1\u5ea6\u5206\u6570\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660eCAKE\u5728\u566a\u58f0\u4e0b\u4fdd\u6301\u6709\u6548\uff0c\u5e76\u80fd\u533a\u5206\u7a33\u5b9a\u70b9\u548c\u4e0d\u7a33\u5b9a\u70b9\u3002\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCAKE\u80fd\u6709\u6548\u8bc6\u522b\u6a21\u7cca\u70b9\u548c\u7a33\u5b9a\u6838\u5fc3\u6210\u5458\uff0c\u63d0\u4f9b\u53ef\u7528\u4e8e\u6307\u5bfc\u8fc7\u6ee4\u6216\u4f18\u5148\u7ea7\u6392\u5e8f\u7684\u7f6e\u4fe1\u5ea6\u6392\u540d\u3002", "conclusion": "CAKE\u6846\u67b6\u4e3a\u805a\u7c7b\u5206\u914d\u63d0\u4f9b\u4e86\u70b9\u7ea7\u522b\u7684\u7f6e\u4fe1\u5ea6\u91cf\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u8de8\u8fd0\u884c\u4e00\u81f4\u6027\u548c\u51e0\u4f55\u652f\u6301\uff0c\u80fd\u591f\u63d0\u9ad8\u805a\u7c7b\u8d28\u91cf\u5e76\u6307\u5bfc\u540e\u7eed\u5206\u6790\u51b3\u7b56\u3002"}}
