{"id": "2512.12084", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.12084", "abs": "https://arxiv.org/abs/2512.12084", "authors": ["Hanzhou Liu", "Kai Yin", "Zhitong Chen", "Chenyue Liu", "Ali Mostafavi"], "title": "FloodSQL-Bench: A Retrieval-Augmented Benchmark for Geospatially-Grounded Text-to-SQL", "comment": null, "summary": "Existing Text-to-SQL benchmarks primarily focus on single-table queries or limited joins in general-purpose domains, and thus fail to reflect the complexity of domain-specific, multi-table and geospatial reasoning, To address this limitation, we introduce FLOODSQL-BENCH, a geospatially grounded benchmark for the flood management domain that integrates heterogeneous datasets through key-based, spatial, and hybrid joins. The benchmark captures realistic flood-related information needs by combining social, infrastructural, and hazard data layers. We systematically evaluate recent large language models with the same retrieval-augmented generation settings and measure their performance across difficulty tiers. By providing a unified, open benchmark grounded in real-world disaster management data, FLOODSQL-BENCH establishes a practical testbed for advancing Text-to-SQL research in high-stakes application domains.", "AI": {"tldr": "FLOODSQL-BENCH\u662f\u4e00\u4e2a\u9762\u5411\u6d2a\u6c34\u7ba1\u7406\u9886\u57df\u7684\u7a7a\u95f4\u5730\u7406\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6574\u5408\u4e86\u5f02\u6784\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u6587\u672c\u5230SQL\u8f6c\u6362\u7cfb\u7edf\u5728\u590d\u6742\u9886\u57df\u7279\u5b9a\u67e5\u8be2\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230SQL\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u5173\u6ce8\u5355\u8868\u67e5\u8be2\u6216\u901a\u7528\u9886\u57df\u7684\u6709\u9650\u8fde\u63a5\uff0c\u65e0\u6cd5\u53cd\u6620\u9886\u57df\u7279\u5b9a\u3001\u591a\u8868\u548c\u7a7a\u95f4\u63a8\u7406\u7684\u590d\u6742\u6027\uff0c\u7279\u522b\u662f\u5728\u6d2a\u6c34\u7ba1\u7406\u7b49\u9ad8\u98ce\u9669\u5e94\u7528\u9886\u57df\u3002", "method": "\u901a\u8fc7\u6574\u5408\u5f02\u6784\u6570\u636e\u96c6\uff08\u793e\u4f1a\u3001\u57fa\u7840\u8bbe\u65bd\u548c\u707e\u5bb3\u6570\u636e\u5c42\uff09\uff0c\u57fa\u4e8e\u952e\u3001\u7a7a\u95f4\u548c\u6df7\u5408\u8fde\u63a5\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5730\u7406\u7a7a\u95f4\u57fa\u7840\u7684\u6d2a\u6c34\u7ba1\u7406\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u76f8\u540c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u8bbe\u7f6e\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u5efa\u7acb\u4e86FLOODSQL-BENCH\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e3a\u6587\u672c\u5230SQL\u7814\u7a76\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u9886\u57df\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u80fd\u591f\u8bc4\u4f30\u6a21\u578b\u5728\u4e0d\u540c\u96be\u5ea6\u5c42\u7ea7\u4e0a\u7684\u8868\u73b0\u3002", "conclusion": "FLOODSQL-BENCH\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u9886\u57df\u7279\u5b9a\u590d\u6742\u67e5\u8be2\u8bc4\u4f30\u65b9\u9762\u7684\u7a7a\u767d\uff0c\u4e3a\u63a8\u8fdb\u6587\u672c\u5230SQL\u7814\u7a76\u5728\u73b0\u5b9e\u4e16\u754c\u707e\u5bb3\u7ba1\u7406\u5e94\u7528\u4e2d\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2512.12458", "categories": ["cs.IR", "cs.CG", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12458", "abs": "https://arxiv.org/abs/2512.12458", "authors": ["Vihan Lakshman", "Blaise Munyampirwa", "Julian Shun", "Benjamin Coleman"], "title": "Breaking the Curse of Dimensionality: On the Stability of Modern Vector Retrieval", "comment": "27 pages", "summary": "Modern vector databases enable efficient retrieval over high-dimensional neural embeddings, powering applications from web search to retrieval-augmented generation. However, classical theory predicts such tasks should suffer from the curse of dimensionality, where distances between points become nearly indistinguishable, thereby crippling efficient nearest-neighbor search. We revisit this paradox through the lens of stability, the property that small perturbations to a query do not radically alter its nearest neighbors. Building on foundational results, we extend stability theory to three key retrieval settings widely used in practice: (i) multi-vector search, where we prove that the popular Chamfer distance metric preserves single-vector stability, while average pooling aggregation may destroy it; (ii) filtered vector search, where we show that sufficiently large penalties for mismatched filters can induce stability even when the underlying search is unstable; and (iii) sparse vector search, where we formalize and prove novel sufficient stability conditions. Across synthetic and real datasets, our experimental results match our theoretical predictions, offering concrete guidance for model and system design to avoid the curse of dimensionality.", "AI": {"tldr": "\u8be5\u8bba\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u9ad8\u7ef4\u5411\u91cf\u68c0\u7d22\u4e2d\u7684\u7ef4\u5ea6\u8bc5\u5492\u6096\u8bba\uff0c\u901a\u8fc7\u7a33\u5b9a\u6027\u7406\u8bba\u5206\u6790\u4e86\u4e09\u79cd\u5b9e\u9645\u68c0\u7d22\u573a\u666f\uff0c\u4e3a\u6a21\u578b\u548c\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002", "motivation": "\u73b0\u4ee3\u5411\u91cf\u6570\u636e\u5e93\u652f\u6301\u9ad8\u6548\u7684\u9ad8\u7ef4\u795e\u7ecf\u5d4c\u5165\u68c0\u7d22\uff0c\u4f46\u7ecf\u5178\u7406\u8bba\u9884\u6d4b\u6b64\u7c7b\u4efb\u52a1\u4f1a\u906d\u53d7\u7ef4\u5ea6\u8bc5\u5492\uff0c\u5bfc\u81f4\u70b9\u95f4\u8ddd\u79bb\u96be\u4ee5\u533a\u5206\uff0c\u4ece\u800c\u5f71\u54cd\u6700\u8fd1\u90bb\u641c\u7d22\u6548\u7387\u3002\u4f5c\u8005\u65e8\u5728\u91cd\u65b0\u5ba1\u89c6\u8fd9\u4e00\u6096\u8bba\u3002", "method": "\u901a\u8fc7\u7a33\u5b9a\u6027\u7406\u8bba\u89c6\u89d2\uff0c\u5c06\u7a33\u5b9a\u6027\u7406\u8bba\u6269\u5c55\u5230\u4e09\u79cd\u5173\u952e\u7684\u5b9e\u9645\u68c0\u7d22\u8bbe\u7f6e\uff1a\u591a\u5411\u91cf\u641c\u7d22\u3001\u8fc7\u6ee4\u5411\u91cf\u641c\u7d22\u548c\u7a00\u758f\u5411\u91cf\u641c\u7d22\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff1a1) \u591a\u5411\u91cf\u641c\u7d22\u4e2d\uff0cChamfer\u8ddd\u79bb\u4fdd\u6301\u5355\u5411\u91cf\u7a33\u5b9a\u6027\uff0c\u800c\u5e73\u5747\u6c60\u5316\u53ef\u80fd\u7834\u574f\u7a33\u5b9a\u6027\uff1b2) \u8fc7\u6ee4\u5411\u91cf\u641c\u7d22\u4e2d\uff0c\u8db3\u591f\u5927\u7684\u4e0d\u5339\u914d\u8fc7\u6ee4\u5668\u60e9\u7f5a\u53ef\u4ee5\u8bf1\u5bfc\u7a33\u5b9a\u6027\uff1b3) \u7a00\u758f\u5411\u91cf\u641c\u7d22\u4e2d\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u5145\u5206\u7a33\u5b9a\u6027\u6761\u4ef6\u5e76\u5f97\u5230\u9a8c\u8bc1\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6a21\u578b\u548c\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5177\u4f53\u6307\u5bfc\uff0c\u5e2e\u52a9\u907f\u514d\u7ef4\u5ea6\u8bc5\u5492\u95ee\u9898\uff0c\u7406\u8bba\u9884\u6d4b\u4e0e\u5b9e\u9a8c\u7ed3\u679c\u4e00\u81f4\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5411\u91cf\u68c0\u7d22\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\u3002"}}
{"id": "2512.12740", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.12740", "abs": "https://arxiv.org/abs/2512.12740", "authors": ["Dezhi Yi", "Wei Guo", "Wenyang Cui", "Wenxuan He", "Huifeng Guo", "Yong Liu", "Zhenhua Dong", "Ye Lu"], "title": "FuXi-$\u03b3$: Efficient Sequential Recommendation with Exponential-Power Temporal Encoder and Diagonal-Sparse Positional Mechanism", "comment": "Accepted by KDD 2026", "summary": "Sequential recommendation aims to model users' evolving preferences based on their historical interactions. Recent advances leverage Transformer-based architectures to capture global dependencies, but existing methods often suffer from high computational overhead, primarily due to discontinuous memory access in temporal encoding and dense attention over long sequences. To address these limitations, we propose FuXi-$\u03b3$, a novel sequential recommendation framework that improves both effectiveness and efficiency through principled architectural design. FuXi-$\u03b3$ adopts a decoder-only Transformer structure and introduces two key innovations: (1) An exponential-power temporal encoder that encodes relative temporal intervals using a tunable exponential decay function inspired by the Ebbinghaus forgetting curve. This encoder enables flexible modeling of both short-term and long-term preferences while maintaining high efficiency through continuous memory access and pure matrix operations. (2) A diagonal-sparse positional mechanism that prunes low-contribution attention blocks using a diagonal-sliding strategy guided by the persymmetry of Toeplitz matrix. Extensive experiments on four real-world datasets demonstrate that FuXi-$\u03b3$ achieves state-of-the-art performance in recommendation quality, while accelerating training by up to 4.74$\\times$ and inference by up to 6.18$\\times$, making it a practical and scalable solution for long-sequence recommendation. Our code is available at https://github.com/Yeedzhi/FuXi-gamma.", "AI": {"tldr": "FuXi-\u03b3\u662f\u4e00\u4e2a\u65b0\u578b\u5e8f\u5217\u63a8\u8350\u6846\u67b6\uff0c\u901a\u8fc7\u6307\u6570\u5e42\u65f6\u95f4\u7f16\u7801\u5668\u548c\u5bf9\u89d2\u7a00\u758f\u4f4d\u7f6e\u673a\u5236\uff0c\u5728\u63d0\u5347\u63a8\u8350\u6548\u679c\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u957f\u5e8f\u5217\u63a8\u8350\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eTransformer\u7684\u5e8f\u5217\u63a8\u8350\u65b9\u6cd5\u5b58\u5728\u9ad8\u8ba1\u7b97\u5f00\u9500\u95ee\u9898\uff0c\u4e3b\u8981\u6e90\u4e8e\u65f6\u95f4\u7f16\u7801\u4e2d\u7684\u4e0d\u8fde\u7eed\u5185\u5b58\u8bbf\u95ee\u548c\u957f\u5e8f\u5217\u4e0a\u7684\u5bc6\u96c6\u6ce8\u610f\u529b\u673a\u5236\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u7684\u53ef\u6269\u5c55\u6027\u3002", "method": "\u91c7\u7528\u4ec5\u89e3\u7801\u5668Transformer\u7ed3\u6784\uff0c\u5f15\u5165\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a1\uff09\u57fa\u4e8e\u827e\u5bbe\u6d69\u65af\u9057\u5fd8\u66f2\u7ebf\u7684\u6307\u6570\u5e42\u65f6\u95f4\u7f16\u7801\u5668\uff0c\u4f7f\u7528\u53ef\u8c03\u6307\u6570\u8870\u51cf\u51fd\u6570\u7f16\u7801\u76f8\u5bf9\u65f6\u95f4\u95f4\u9694\uff1b2\uff09\u5bf9\u89d2\u7a00\u758f\u4f4d\u7f6e\u673a\u5236\uff0c\u5229\u7528Toeplitz\u77e9\u9635\u7684\u5bf9\u79f0\u6027\u901a\u8fc7\u5bf9\u89d2\u6ed1\u52a8\u7b56\u7565\u4fee\u526a\u4f4e\u8d21\u732e\u6ce8\u610f\u529b\u5757\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFuXi-\u03b3\u5728\u63a8\u8350\u8d28\u91cf\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u540c\u65f6\u8bad\u7ec3\u52a0\u901f\u9ad8\u8fbe4.74\u500d\uff0c\u63a8\u7406\u52a0\u901f\u9ad8\u8fbe6.18\u500d\u3002", "conclusion": "FuXi-\u03b3\u901a\u8fc7\u539f\u5219\u6027\u67b6\u6784\u8bbe\u8ba1\uff0c\u5728\u4fdd\u6301\u63a8\u8350\u6548\u679c\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u6548\u7387\uff0c\u4e3a\u957f\u5e8f\u5217\u63a8\u8350\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12760", "categories": ["cs.IR", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12760", "abs": "https://arxiv.org/abs/2512.12760", "authors": ["Sina Jani", "Arman Heidari", "Amirmohammad Anvari", "Zahra Rahimi"], "title": "Intelligent Scientific Literature Explorer using Machine Learning (ISLE)", "comment": "18 pages, 7 figures, 3 tables", "summary": "The rapid acceleration of scientific publishing has created substantial challenges for researchers attempting to discover, contextualize, and interpret relevant literature. Traditional keyword-based search systems provide limited semantic understanding, while existing AI-driven tools typically focus on isolated tasks such as retrieval, clustering, or bibliometric visualization. This paper presents an integrated system for scientific literature exploration that combines large-scale data acquisition, hybrid retrieval, semantic topic modeling, and heterogeneous knowledge graph construction. The system builds a comprehensive corpus by merging full-text data from arXiv with structured metadata from OpenAlex. A hybrid retrieval architecture fuses BM25 lexical search with embedding-based semantic search using Reciprocal Rank Fusion. Topic modeling is performed on retrieved results using BERTopic or non-negative matrix factorization depending on computational resources. A knowledge graph unifies papers, authors, institutions, countries, and extracted topics into an interpretable structure. The system provides a multi-layered exploration environment that reveals not only relevant publications but also the conceptual and relational landscape surrounding a query. Evaluation across multiple queries demonstrates improvements in retrieval relevance, topic coherence, and interpretability. The proposed framework contributes an extensible foundation for AI-assisted scientific discovery.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u96c6\u6210\u7cfb\u7edf\uff0c\u7528\u4e8e\u79d1\u5b66\u6587\u732e\u63a2\u7d22\uff0c\u7ed3\u5408\u5927\u89c4\u6a21\u6570\u636e\u91c7\u96c6\u3001\u6df7\u5408\u68c0\u7d22\u3001\u8bed\u4e49\u4e3b\u9898\u5efa\u6a21\u548c\u5f02\u6784\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\uff0c\u65e8\u5728\u89e3\u51b3\u79d1\u5b66\u51fa\u7248\u5feb\u901f\u589e\u957f\u5e26\u6765\u7684\u6587\u732e\u53d1\u73b0\u548c\u89e3\u91ca\u6311\u6218\u3002", "motivation": "\u79d1\u5b66\u51fa\u7248\u7684\u5feb\u901f\u52a0\u901f\u7ed9\u7814\u7a76\u4eba\u5458\u53d1\u73b0\u3001\u60c5\u5883\u5316\u548c\u89e3\u91ca\u76f8\u5173\u6587\u732e\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002\u4f20\u7edf\u57fa\u4e8e\u5173\u952e\u8bcd\u7684\u641c\u7d22\u7cfb\u7edf\u8bed\u4e49\u7406\u89e3\u6709\u9650\uff0c\u800c\u73b0\u6709\u7684AI\u9a71\u52a8\u5de5\u5177\u901a\u5e38\u4e13\u6ce8\u4e8e\u68c0\u7d22\u3001\u805a\u7c7b\u6216\u6587\u732e\u8ba1\u91cf\u53ef\u89c6\u5316\u7b49\u5b64\u7acb\u4efb\u52a1\u3002", "method": "\u7cfb\u7edf\u901a\u8fc7\u5408\u5e76arXiv\u7684\u5168\u6587\u6570\u636e\u548cOpenAlex\u7684\u7ed3\u6784\u5316\u5143\u6570\u636e\u6784\u5efa\u7efc\u5408\u8bed\u6599\u5e93\u3002\u91c7\u7528\u6df7\u5408\u68c0\u7d22\u67b6\u6784\uff0c\u878d\u5408BM25\u8bcd\u6c47\u641c\u7d22\u548c\u57fa\u4e8e\u5d4c\u5165\u7684\u8bed\u4e49\u641c\u7d22\uff08\u4f7f\u7528\u4e92\u60e0\u6392\u540d\u878d\u5408\uff09\u3002\u4f7f\u7528BERTopic\u6216\u975e\u8d1f\u77e9\u9635\u5206\u89e3\u8fdb\u884c\u4e3b\u9898\u5efa\u6a21\u3002\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\uff0c\u5c06\u8bba\u6587\u3001\u4f5c\u8005\u3001\u673a\u6784\u3001\u56fd\u5bb6\u548c\u63d0\u53d6\u7684\u4e3b\u9898\u7edf\u4e00\u5230\u53ef\u89e3\u91ca\u7684\u7ed3\u6784\u4e2d\u3002", "result": "\u8bc4\u4f30\u591a\u4e2a\u67e5\u8be2\u663e\u793a\uff0c\u5728\u68c0\u7d22\u76f8\u5173\u6027\u3001\u4e3b\u9898\u8fde\u8d2f\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u90fd\u6709\u6539\u8fdb\u3002\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u591a\u5c42\u63a2\u7d22\u73af\u5883\uff0c\u4e0d\u4ec5\u663e\u793a\u76f8\u5173\u51fa\u7248\u7269\uff0c\u8fd8\u63ed\u793a\u67e5\u8be2\u5468\u56f4\u7684\u6982\u5ff5\u548c\u5173\u7cfb\u666f\u89c2\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u4e3aAI\u8f85\u52a9\u79d1\u5b66\u53d1\u73b0\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u57fa\u7840\uff0c\u901a\u8fc7\u96c6\u6210\u591a\u79cdAI\u6280\u672f\u6765\u89e3\u51b3\u79d1\u5b66\u6587\u732e\u63a2\u7d22\u7684\u590d\u6742\u6027\u3002"}}
{"id": "2512.11865", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.11865", "abs": "https://arxiv.org/abs/2512.11865", "authors": ["Ju-Young Kim", "Ji-Hong Park", "Myeongjun Kim", "Gun-Woo Kim"], "title": "Explainable Adversarial-Robust Vision-Language-Action Model for Robotic Manipulation", "comment": "Accepted to MobieSec 2025 (poster session)", "summary": "Smart farming has emerged as a key technology for advancing modern agriculture through automation and intelligent control. However, systems relying on RGB cameras for perception and robotic manipulators for control, common in smart farming, are vulnerable to photometric perturbations such as hue, illumination, and noise changes, which can cause malfunction under adversarial attacks. To address this issue, we propose an explainable adversarial-robust Vision-Language-Action model based on the OpenVLA-OFT framework. The model integrates an Evidence-3 module that detects photometric perturbations and generates natural language explanations of their causes and effects. Experiments show that the proposed model reduces Current Action L1 loss by 21.7% and Next Actions L1 loss by 18.4% compared to the baseline, demonstrating improved action prediction accuracy and explainability under adversarial conditions.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eOpenVLA-OFT\u6846\u67b6\u7684\u53ef\u89e3\u91ca\u5bf9\u6297\u9c81\u68d2\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7Evidence-3\u6a21\u5757\u68c0\u6d4b\u5149\u5ea6\u6270\u52a8\u5e76\u751f\u6210\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u5728\u5bf9\u6297\u6761\u4ef6\u4e0b\u63d0\u5347\u52a8\u4f5c\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027", "motivation": "\u667a\u80fd\u519c\u4e1a\u4e2d\u57fa\u4e8eRGB\u76f8\u673a\u548c\u673a\u68b0\u81c2\u7684\u7cfb\u7edf\u5bb9\u6613\u53d7\u5230\u8272\u8c03\u3001\u5149\u7167\u3001\u566a\u58f0\u7b49\u5149\u5ea6\u6270\u52a8\u7684\u5bf9\u6297\u653b\u51fb\uff0c\u5bfc\u81f4\u7cfb\u7edf\u6545\u969c\uff0c\u9700\u8981\u63d0\u9ad8\u7cfb\u7edf\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027", "method": "\u57fa\u4e8eOpenVLA-OFT\u6846\u67b6\u6784\u5efa\u53ef\u89e3\u91ca\u5bf9\u6297\u9c81\u68d2\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u96c6\u6210Evidence-3\u6a21\u5757\u6765\u68c0\u6d4b\u5149\u5ea6\u6270\u52a8\u5e76\u751f\u6210\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u8bf4\u660e\u6270\u52a8\u7684\u539f\u56e0\u548c\u5f71\u54cd", "result": "\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\uff0c\u5f53\u524d\u52a8\u4f5cL1\u635f\u5931\u964d\u4f4e21.7%\uff0c\u4e0b\u4e00\u52a8\u4f5cL1\u635f\u5931\u964d\u4f4e18.4%\uff0c\u5728\u5bf9\u6297\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u52a8\u4f5c\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027", "conclusion": "\u63d0\u51fa\u7684\u53ef\u89e3\u91ca\u5bf9\u6297\u9c81\u68d2\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86\u667a\u80fd\u519c\u4e1a\u7cfb\u7edf\u4e2d\u5149\u5ea6\u6270\u52a8\u5bfc\u81f4\u7684\u5bf9\u6297\u653b\u51fb\u95ee\u9898\uff0c\u4e3a\u667a\u80fd\u519c\u4e1a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.11805", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.11805", "abs": "https://arxiv.org/abs/2512.11805", "authors": ["Monu Sharma"], "title": "AI Integration In ERP Evaluation Across Trends and Architectures", "comment": "9 pages, 2 Figures. Journal of Information Systems Engineering and Management,2025", "summary": "The incorporation of Artificial Intelligence (AI) into Enterprise Resource Planning (ERP) is a dramatic transition from static, on-premises systems to systems that can adapt and operate in cloud-native architectures. Cloud ERP solutions like Workday illustrate this evolution by incorporating machine learning, deep learning, and natural language processing into a centralized data-driven ecosystem. As the complexity of AI-driven ERP solutions expands, traditional evaluation frameworks that look at cost, function, and user satisfaction suffer from a lack of consideration for algorithmic transparency, adaptability, or ethics. This review will systematically investigate the latest trends, models of computing architecture, and analytical methods applied in assessing the performance of AI-integrated ERP services, specifically on cloud-based platforms. Based on academic and industry sources, the paper distills current research in line with architectural integration, analytical methodologies, and organizational impact. It identifies critical performance metrics and emphasizes the absence of any standard assessment frameworks or AI-aware systems capable of evaluating automation efficiency, security concerns as well as flexible learning modes. We put forward a theoretical model that brings AI-enabled capabilities -- such as predictive intelligence or adaptive automation -- into alignment with metrics in performance assessment for ERPs. By combining current literature and identifying major gaps in research, this paper attempts to present a complete picture of how innovations in AI are changing ERP evaluation. These research and methodological findings are intended to steer researchers and practitioners towards developing rigorous, data-driven assessment approaches, aligning with the fast-developing world of intelligent self-optimizing enterprise ecosystems", "AI": {"tldr": "AI\u4e0eERP\u7cfb\u7edf\u878d\u5408\u63a8\u52a8\u4ece\u9759\u6001\u672c\u5730\u90e8\u7f72\u5411\u4e91\u539f\u751f\u81ea\u9002\u5e94\u7cfb\u7edf\u8f6c\u578b\uff0c\u4f46\u4f20\u7edf\u8bc4\u4f30\u6846\u67b6\u7f3a\u4e4f\u5bf9\u7b97\u6cd5\u900f\u660e\u5ea6\u3001\u9002\u5e94\u6027\u548c\u4f26\u7406\u7684\u8003\u91cf\uff0c\u9700\u8981\u65b0\u7684AI\u611f\u77e5\u8bc4\u4f30\u6a21\u578b", "motivation": "\u968f\u7740AI\u6280\u672f\uff08\u673a\u5668\u5b66\u4e60\u3001\u6df1\u5ea6\u5b66\u4e60\u3001\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff09\u878d\u5165ERP\u7cfb\u7edf\uff0c\u4f20\u7edf\u57fa\u4e8e\u6210\u672c\u3001\u529f\u80fd\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u7684\u8bc4\u4f30\u6846\u67b6\u5df2\u65e0\u6cd5\u9002\u5e94AI\u9a71\u52a8\u7684\u4e91ERP\u89e3\u51b3\u65b9\u6848\uff0c\u7f3a\u4e4f\u5bf9\u7b97\u6cd5\u900f\u660e\u5ea6\u3001\u9002\u5e94\u6027\u548c\u4f26\u7406\u7684\u8003\u91cf\uff0c\u9700\u8981\u5efa\u7acb\u65b0\u7684\u8bc4\u4f30\u4f53\u7cfb", "method": "\u7cfb\u7edf\u7efc\u8ff0\u5b66\u672f\u548c\u884c\u4e1a\u8d44\u6599\uff0c\u5206\u6790AI\u96c6\u6210ERP\u7684\u6700\u65b0\u8d8b\u52bf\u3001\u8ba1\u7b97\u67b6\u6784\u6a21\u578b\u548c\u5206\u6790\u65b9\u6cd5\uff0c\u8bc6\u522b\u5173\u952e\u6027\u80fd\u6307\u6807\uff0c\u63d0\u51fa\u5c06AI\u80fd\u529b\uff08\u9884\u6d4b\u667a\u80fd\u3001\u81ea\u9002\u5e94\u81ea\u52a8\u5316\uff09\u4e0eERP\u6027\u80fd\u8bc4\u4f30\u6307\u6807\u5bf9\u9f50\u7684\u7406\u8bba\u6a21\u578b", "result": "\u53d1\u73b0\u5f53\u524d\u7f3a\u4e4f\u6807\u51c6\u8bc4\u4f30\u6846\u67b6\u548cAI\u611f\u77e5\u7cfb\u7edf\u6765\u8bc4\u4f30\u81ea\u52a8\u5316\u6548\u7387\u3001\u5b89\u5168\u95ee\u9898\u548c\u7075\u6d3b\u5b66\u4e60\u6a21\u5f0f\uff0c\u8bc6\u522b\u4e86\u7814\u7a76\u4e2d\u7684\u4e3b\u8981\u7a7a\u767d\uff0c\u63d0\u51fa\u4e86\u6574\u5408AI\u80fd\u529b\u4e0eERP\u6027\u80fd\u8bc4\u4f30\u7684\u7406\u8bba\u6a21\u578b", "conclusion": "AI\u521b\u65b0\u6b63\u5728\u6539\u53d8ERP\u8bc4\u4f30\u65b9\u5f0f\uff0c\u9700\u8981\u5f00\u53d1\u4e25\u8c28\u7684\u6570\u636e\u9a71\u52a8\u8bc4\u4f30\u65b9\u6cd5\uff0c\u4ee5\u9002\u5e94\u5feb\u901f\u53d1\u5c55\u7684\u667a\u80fd\u81ea\u4f18\u5316\u4f01\u4e1a\u751f\u6001\u7cfb\u7edf\uff0c\u4e3a\u7814\u7a76\u8005\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u6307\u5bfc\u65b9\u5411"}}
{"id": "2512.11835", "categories": ["cs.AI", "cs.LG", "cs.MA", "cs.NE"], "pdf": "https://arxiv.org/pdf/2512.11835", "abs": "https://arxiv.org/abs/2512.11835", "authors": ["Seyma Yaman Kayadibi"], "title": "A Monad-Based Clause Architecture for Artificial Age Score (AAS) in Large Language Models", "comment": "42 pages, 6 toy simulation Python implementations, 20 monad clauses instantiated across six system bundles (ontology, dynamics, representation and consciousness, harmony and reason, body and organisation, teleology)", "summary": "Large language models (LLMs) are often deployed as powerful yet opaque systems, leaving open how their internal memory and \"self-like\" behavior should be governed in a principled and auditable way. The Artificial Age Score (AAS) was previously introduced and mathematically justified through three theorems that characterise it as a metric of artificial memory aging. Building on this foundation, the present work develops an engineering-oriented, clause-based architecture that imposes law-like constraints on LLM memory and control. Twenty selected monads from Leibniz's Monadology are grouped into six bundles: ontology, dynamics, representation and consciousness, harmony and reason, body and organisation, and teleology, and each bundle is realised as an executable specification on top of the AAS kernel. Across six minimal Python implementations, these clause families are instantiated in numerical experiments acting on channel-level quantities such as recall scores, redundancy, and weights. Each implementation follows a four-step pattern: inputs and setup, clause implementation, numerical results, and implications for LLM design, emphasising that the framework is not only philosophically motivated but also directly implementable. The experiments show that the clause system exhibits bounded and interpretable behavior: AAS trajectories remain continuous and rate-limited, contradictions and unsupported claims trigger explicit penalties, and hierarchical refinement reveals an organic structure in a controlled manner. Dual views and goal-action pairs are aligned by harmony terms, and windowed drift in perfection scores separates sustained improvement from sustained degradation. Overall, the monad-based clause framework uses AAS as a backbone and provides a transparent, code-level blueprint for constraining and analyzing internal dynamics in artificial agents.", "AI": {"tldr": "\u57fa\u4e8e\u4eba\u5de5\u5e74\u9f84\u8bc4\u5206(AAS)\u6784\u5efa\u53ef\u6267\u884c\u7684\u83b1\u5e03\u5c3c\u8328\u5355\u5b50\u8bba\u6761\u6b3e\u7cfb\u7edf\uff0c\u4e3aLLM\u5185\u5b58\u548c\u63a7\u5236\u63d0\u4f9b\u6cd5\u5f8b\u5f0f\u7ea6\u675f\u7684\u5de5\u7a0b\u67b6\u6784", "motivation": "\u89e3\u51b3LLM\u4f5c\u4e3a\u5f3a\u5927\u4f46\u4e0d\u900f\u660e\u7cfb\u7edf\u7684\u95ee\u9898\uff0c\u4e3a\u5176\u5185\u90e8\u8bb0\u5fc6\u548c\"\u81ea\u6211\u5f0f\"\u884c\u4e3a\u63d0\u4f9b\u539f\u5219\u5316\u3001\u53ef\u5ba1\u8ba1\u7684\u6cbb\u7406\u6846\u67b6", "method": "\u5c06\u83b1\u5e03\u5c3c\u8328\u300a\u5355\u5b50\u8bba\u300b\u4e2d\u768420\u4e2a\u5355\u5b50\u5206\u4e3a6\u4e2a\u675f\uff08\u672c\u4f53\u8bba\u3001\u52a8\u529b\u5b66\u3001\u8868\u5f81\u4e0e\u610f\u8bc6\u3001\u548c\u8c10\u4e0e\u7406\u6027\u3001\u8eab\u4f53\u4e0e\u7ec4\u7ec7\u3001\u76ee\u7684\u8bba\uff09\uff0c\u5728AAS\u5185\u6838\u4e0a\u5b9e\u73b0\u4e3a\u53ef\u6267\u884c\u89c4\u8303\uff0c\u901a\u8fc7Python\u5b9e\u73b0\u8fdb\u884c\u6570\u503c\u5b9e\u9a8c", "result": "\u6761\u6b3e\u7cfb\u7edf\u5c55\u73b0\u51fa\u6709\u754c\u4e14\u53ef\u89e3\u91ca\u7684\u884c\u4e3a\uff1aAAS\u8f68\u8ff9\u4fdd\u6301\u8fde\u7eed\u548c\u901f\u7387\u53d7\u9650\uff0c\u77db\u76fe\u548c\u672a\u7ecf\u652f\u6301\u7684\u58f0\u660e\u89e6\u53d1\u660e\u786e\u60e9\u7f5a\uff0c\u5c42\u6b21\u5316\u7ec6\u5316\u4ee5\u53d7\u63a7\u65b9\u5f0f\u63ed\u793a\u6709\u673a\u7ed3\u6784\uff0c\u548c\u8c10\u9879\u5bf9\u9f50\u53cc\u91cd\u89c6\u56fe\u548c\u76ee\u6807-\u884c\u52a8\u5bf9", "conclusion": "\u57fa\u4e8e\u5355\u5b50\u7684\u6761\u6b3e\u6846\u67b6\u4ee5AAS\u4e3a\u9aa8\u5e72\uff0c\u4e3a\u7ea6\u675f\u548c\u5206\u6790\u4eba\u5de5\u667a\u80fd\u4f53\u5185\u90e8\u52a8\u6001\u63d0\u4f9b\u4e86\u900f\u660e\u3001\u4ee3\u7801\u7ea7\u7684\u84dd\u56fe\uff0c\u4e0d\u4ec5\u5177\u6709\u54f2\u5b66\u52a8\u673a\uff0c\u800c\u4e14\u53ef\u76f4\u63a5\u5b9e\u73b0"}}
{"id": "2512.11829", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.11829", "abs": "https://arxiv.org/abs/2512.11829", "authors": ["Jacob Poschl"], "title": "Active Inference with Reusable State-Dependent Value Profiles", "comment": "27 pages", "summary": "Adaptive behavior in volatile environments requires agents to switch among value-control regimes across latent contexts, but maintaining separate preferences, policy biases, and action-confidence parameters for every situation is intractable. We introduce value profiles: a small set of reusable bundles of value-related parameters (outcome preferences, policy priors, and policy precision) assigned to hidden states in a generative model. As posterior beliefs over states evolve trial by trial, effective control parameters arise via belief-weighted mixing, enabling state-conditional strategy recruitment without requiring independent parameters for each context. We evaluate this framework in probabilistic reversal learning, comparing static-precision, entropy-coupled dynamic-precision, and profile-based models using cross-validated log-likelihood and information criteria. Model comparison favors the profile-based model over simpler alternatives (about 100-point AIC differences), and parameter-recovery analyses support structural identifiability even when context must be inferred from noisy observations. Model-based inference further suggests that adaptive control in this task is driven primarily by modulation of policy priors rather than policy precision, with gradual belief-dependent profile recruitment consistent with state-conditional (not purely uncertainty-driven) control. Overall, reusable value profiles provide a tractable computational account of belief-conditioned value control in volatile environments and yield testable signatures of belief-dependent control and behavioral flexibility.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u4ef7\u503c\u5256\u9762\"\u6982\u5ff5\uff0c\u901a\u8fc7\u5c11\u91cf\u53ef\u91cd\u7528\u7684\u4ef7\u503c\u76f8\u5173\u53c2\u6570\u5305\uff08\u504f\u597d\u3001\u7b56\u7565\u5148\u9a8c\u3001\u7b56\u7565\u7cbe\u5ea6\uff09\u5206\u914d\u7ed9\u9690\u72b6\u6001\uff0c\u901a\u8fc7\u4fe1\u5ff5\u52a0\u6743\u6df7\u5408\u5b9e\u73b0\u72b6\u6001\u6761\u4ef6\u63a7\u5236\uff0c\u65e0\u9700\u4e3a\u6bcf\u4e2a\u60c5\u5883\u7ef4\u62a4\u72ec\u7acb\u53c2\u6570\u3002", "motivation": "\u5728\u591a\u53d8\u73af\u5883\u4e2d\uff0c\u667a\u80fd\u4f53\u9700\u8981\u5728\u4e0d\u540c\u9690\u60c5\u5883\u95f4\u5207\u6362\u4ef7\u503c\u63a7\u5236\u673a\u5236\uff0c\u4f46\u4e3a\u6bcf\u4e2a\u60c5\u5883\u7ef4\u62a4\u72ec\u7acb\u7684\u504f\u597d\u3001\u7b56\u7565\u504f\u5dee\u548c\u884c\u52a8\u7f6e\u4fe1\u5ea6\u53c2\u6570\u662f\u4e0d\u53ef\u884c\u7684\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u8ba1\u7b97\u6846\u67b6\u3002", "method": "\u5f15\u5165\u4ef7\u503c\u5256\u9762\uff1a\u5c11\u91cf\u53ef\u91cd\u7528\u7684\u4ef7\u503c\u76f8\u5173\u53c2\u6570\u5305\uff08\u7ed3\u679c\u504f\u597d\u3001\u7b56\u7565\u5148\u9a8c\u3001\u7b56\u7565\u7cbe\u5ea6\uff09\uff0c\u5206\u914d\u7ed9\u751f\u6210\u6a21\u578b\u4e2d\u7684\u9690\u72b6\u6001\u3002\u901a\u8fc7\u540e\u9a8c\u4fe1\u5ff5\u7684\u9010\u8bd5\u6b21\u6f14\u5316\uff0c\u63a7\u5236\u53c2\u6570\u901a\u8fc7\u4fe1\u5ff5\u52a0\u6743\u6df7\u5408\u4ea7\u751f\uff0c\u5b9e\u73b0\u72b6\u6001\u6761\u4ef6\u7b56\u7565\u62db\u52df\u3002", "result": "\u5728\u6982\u7387\u53cd\u8f6c\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0c\u57fa\u4e8e\u5256\u9762\u7684\u6a21\u578b\u4f18\u4e8e\u9759\u6001\u7cbe\u5ea6\u548c\u71b5\u8026\u5408\u52a8\u6001\u7cbe\u5ea6\u6a21\u578b\uff08\u7ea6100\u70b9AIC\u5dee\u5f02\uff09\uff0c\u53c2\u6570\u6062\u590d\u5206\u6790\u652f\u6301\u7ed3\u6784\u53ef\u8bc6\u522b\u6027\u3002\u6a21\u578b\u63a8\u65ad\u8868\u660e\u81ea\u9002\u5e94\u63a7\u5236\u4e3b\u8981\u7531\u7b56\u7565\u5148\u9a8c\u8c03\u5236\u800c\u975e\u7b56\u7565\u7cbe\u5ea6\u9a71\u52a8\u3002", "conclusion": "\u53ef\u91cd\u7528\u7684\u4ef7\u503c\u5256\u9762\u4e3a\u591a\u53d8\u73af\u5883\u4e2d\u4fe1\u5ff5\u6761\u4ef6\u4ef7\u503c\u63a7\u5236\u63d0\u4f9b\u4e86\u53ef\u8ba1\u7b97\u6846\u67b6\uff0c\u4ea7\u751f\u4e86\u4fe1\u5ff5\u4f9d\u8d56\u63a7\u5236\u548c\u884c\u4e3a\u7075\u6d3b\u6027\u7684\u53ef\u6d4b\u8bd5\u7279\u5f81\uff0c\u652f\u6301\u72b6\u6001\u6761\u4ef6\uff08\u800c\u975e\u7eaf\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\uff09\u63a7\u5236\u3002"}}
{"id": "2512.12938", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.12938", "abs": "https://arxiv.org/abs/2512.12938", "authors": ["Duy A. Nguyen", "Hai H. Do", "Minh Doan", "Minh N. Do"], "title": "SPAR: Session-based Pipeline for Adaptive Retrieval on Legacy File Systems", "comment": null, "summary": "The ability to extract value from historical data is essential for enterprise decision-making. However, much of this information remains inaccessible within large legacy file systems that lack structured organization and semantic indexing, making retrieval and analysis inefficient and error-prone. We introduce SPAR (Session-based Pipeline for Adaptive Retrieval), a conceptual framework that integrates Large Language Models (LLMs) into a Retrieval-Augmented Generation (RAG) architecture specifically designed for legacy enterprise environments. Unlike conventional RAG pipelines, which require costly construction and maintenance of full-scale vector databases that mirror the entire file system, SPAR employs a lightweight two-stage process: a semantic Metadata Index is first created, after which session-specific vector databases are dynamically generated on demand. This design reduces computational overhead while improving transparency, controllability, and relevance in retrieval. We provide a theoretical complexity analysis comparing SPAR with standard LLM-based RAG pipelines, demonstrating its computational advantages. To validate the framework, we apply SPAR to a synthesized enterprise-scale file system containing a large corpus of biomedical literature, showing improvements in both retrieval effectiveness and downstream model accuracy. Finally, we discuss design trade-offs and outline open challenges for deploying SPAR across diverse enterprise settings.", "AI": {"tldr": "SPAR\u662f\u4e00\u4e2a\u9488\u5bf9\u9057\u7559\u4f01\u4e1a\u7cfb\u7edf\u7684\u4f1a\u8bdd\u5f0f\u81ea\u9002\u5e94\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u4e24\u9636\u6bb5\u6d41\u7a0b\u66ff\u4ee3\u4f20\u7edfRAG\u7684\u5b8c\u6574\u5411\u91cf\u6570\u636e\u5e93\uff0c\u63d0\u5347\u68c0\u7d22\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4f01\u4e1a\u5386\u53f2\u6570\u636e\u5927\u591a\u5b58\u50a8\u5728\u7f3a\u4e4f\u7ed3\u6784\u5316\u7ec4\u7ec7\u548c\u8bed\u4e49\u7d22\u5f15\u7684\u9057\u7559\u6587\u4ef6\u7cfb\u7edf\u4e2d\uff0c\u5bfc\u81f4\u68c0\u7d22\u548c\u5206\u6790\u6548\u7387\u4f4e\u4e0b\u4e14\u5bb9\u6613\u51fa\u9519\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "SPAR\u6846\u67b6\u5c06LLM\u96c6\u6210\u5230RAG\u67b6\u6784\u4e2d\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a\u9996\u5148\u521b\u5efa\u8bed\u4e49\u5143\u6570\u636e\u7d22\u5f15\uff0c\u7136\u540e\u6309\u9700\u52a8\u6001\u751f\u6210\u4f1a\u8bdd\u7279\u5b9a\u7684\u5411\u91cf\u6570\u636e\u5e93\uff0c\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "result": "\u7406\u8bba\u590d\u6742\u5ea6\u5206\u6790\u663e\u793aSPAR\u76f8\u6bd4\u6807\u51c6LLM-based RAG\u5177\u6709\u8ba1\u7b97\u4f18\u52bf\uff1b\u5728\u5408\u6210\u4f01\u4e1a\u7ea7\u751f\u7269\u533b\u5b66\u6587\u732e\u6587\u4ef6\u7cfb\u7edf\u4e0a\u7684\u5e94\u7528\u9a8c\u8bc1\u4e86\u5176\u5728\u68c0\u7d22\u6548\u679c\u548c\u4e0b\u6e38\u6a21\u578b\u51c6\u786e\u6027\u65b9\u9762\u7684\u6539\u8fdb\u3002", "conclusion": "SPAR\u6846\u67b6\u4e3a\u9057\u7559\u4f01\u4e1a\u73af\u5883\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u900f\u660e\u548c\u53ef\u63a7\u7684\u68c0\u7d22\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u9700\u8981\u5728\u4e0d\u540c\u4f01\u4e1a\u8bbe\u7f6e\u4e2d\u8fdb\u4e00\u6b65\u63a2\u7d22\u8bbe\u8ba1\u6743\u8861\u548c\u5f00\u653e\u6311\u6218\u3002"}}
{"id": "2512.11869", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11869", "abs": "https://arxiv.org/abs/2512.11869", "authors": ["D. Shainu Suhas", "G. Rahul", "K. Muni"], "title": "Temporal-Anchor3DLane: Enhanced 3D Lane Detection with Multi-Task Losses and LSTM Fusion", "comment": null, "summary": "Monocular 3D lane detection remains challenging due to depth ambiguity, occlusion, and temporal instability across frames. Anchor-based approaches such as Anchor3DLane have demonstrated strong performance by regressing continuous 3D lane curves from multi-camera surround views. However, the baseline model still exhibits (i) sensitivity to regression outliers, (ii) weak supervision of global curve geometry, (iii) difficulty in balancing multiple loss terms, and (iv) limited exploitation of temporal continuity. We propose Temporal-Anchor3DLane, an enhanced 3D lane detection framework that extends Anchor3DLane with three key contributions: (1) a set of multi-task loss improvements, including Balanced L1 regression, Chamfer point-set distance, and uncertainty-based loss weighting, together with focal and Dice components for classification and visibility; (2) a lightweight Temporal LSTM Fusion module that aggregates per-anchor features across frames, replacing a heavier Transformer-style temporal fusion; and (3) ESCOP-style training refinements that couple curve-level supervision with temporal consistency. On OpenLane, Temporal-Anchor3DLane improves F1 by +6.2 and yields smoother temporal trajectories, showing that small architectural and loss refinements significantly enhance 3D lane robustness without extra sensors or scaling.", "AI": {"tldr": "Temporal-Anchor3DLane\u901a\u8fc7\u6539\u8fdb\u635f\u5931\u51fd\u6570\u548c\u6dfb\u52a0\u8f7b\u91cf\u7ea7LSTM\u65f6\u5e8f\u878d\u5408\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5355\u76ee3D\u8f66\u9053\u7ebf\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u65f6\u5e8f\u7a33\u5b9a\u6027", "motivation": "\u5355\u76ee3D\u8f66\u9053\u7ebf\u68c0\u6d4b\u9762\u4e34\u6df1\u5ea6\u6a21\u7cca\u3001\u906e\u6321\u548c\u65f6\u5e8f\u4e0d\u7a33\u5b9a\u7b49\u6311\u6218\u3002Anchor3DLane\u7b49\u57fa\u4e8e\u951a\u70b9\u7684\u65b9\u6cd5\u5728\u591a\u6444\u50cf\u5934\u73af\u7ed5\u89c6\u56fe\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u57fa\u7ebf\u6a21\u578b\u4ecd\u5b58\u5728\u56de\u5f52\u5f02\u5e38\u503c\u654f\u611f\u3001\u5168\u5c40\u66f2\u7ebf\u51e0\u4f55\u76d1\u7763\u5f31\u3001\u591a\u635f\u5931\u9879\u5e73\u8861\u56f0\u96be\u4ee5\u53ca\u65f6\u5e8f\u8fde\u7eed\u6027\u5229\u7528\u6709\u9650\u7b49\u95ee\u9898", "method": "\u63d0\u51faTemporal-Anchor3DLane\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6539\u8fdb\uff1a(1)\u591a\u4efb\u52a1\u635f\u5931\u6539\u8fdb\uff1a\u5e73\u8861L1\u56de\u5f52\u3001Chamfer\u70b9\u96c6\u8ddd\u79bb\u3001\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u7684\u635f\u5931\u52a0\u6743\uff0c\u4ee5\u53ca\u5206\u7c7b\u548c\u53ef\u89c1\u6027\u7684focal\u548cDice\u7ec4\u4ef6\uff1b(2)\u8f7b\u91cf\u7ea7\u65f6\u5e8fLSTM\u878d\u5408\u6a21\u5757\uff1a\u8de8\u5e27\u805a\u5408\u6bcf\u4e2a\u951a\u70b9\u7684\u7279\u5f81\uff0c\u66ff\u4ee3\u8f83\u91cd\u7684Transformer\u98ce\u683c\u65f6\u5e8f\u878d\u5408\uff1b(3)ESCOP\u98ce\u683c\u8bad\u7ec3\u4f18\u5316\uff1a\u5c06\u66f2\u7ebf\u7ea7\u76d1\u7763\u4e0e\u65f6\u5e8f\u4e00\u81f4\u6027\u76f8\u7ed3\u5408", "result": "\u5728OpenLane\u6570\u636e\u96c6\u4e0a\uff0cTemporal-Anchor3DLane\u5c06F1\u5206\u6570\u63d0\u5347\u4e86+6.2\uff0c\u5e76\u4ea7\u751f\u66f4\u5e73\u6ed1\u7684\u65f6\u5e8f\u8f68\u8ff9\uff0c\u8868\u660e\u5c0f\u7684\u67b6\u6784\u548c\u635f\u5931\u6539\u8fdb\u663e\u8457\u589e\u5f3a\u4e863D\u8f66\u9053\u7ebf\u9c81\u68d2\u6027\uff0c\u65e0\u9700\u989d\u5916\u4f20\u611f\u5668\u6216\u6269\u5c55", "conclusion": "\u901a\u8fc7\u6539\u8fdb\u635f\u5931\u51fd\u6570\u548c\u6dfb\u52a0\u8f7b\u91cf\u7ea7\u65f6\u5e8f\u878d\u5408\u6a21\u5757\uff0cTemporal-Anchor3DLane\u663e\u8457\u63d0\u5347\u4e86\u5355\u76ee3D\u8f66\u9053\u7ebf\u68c0\u6d4b\u7684\u6027\u80fd\u548c\u65f6\u5e8f\u7a33\u5b9a\u6027\uff0c\u8bc1\u660e\u4e86\u5c0f\u7684\u67b6\u6784\u548c\u635f\u5931\u4f18\u5316\u53ef\u4ee5\u5728\u4e0d\u589e\u52a0\u989d\u5916\u4f20\u611f\u5668\u6216\u8ba1\u7b97\u590d\u6742\u5ea6\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u6539\u5584\u6a21\u578b\u9c81\u68d2\u6027"}}
{"id": "2512.11812", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.11812", "abs": "https://arxiv.org/abs/2512.11812", "authors": ["Quan-Hoang Vuong", "Thi Mai Anh Tran", "Ni Putu Wulan Purnama Sari", "Fatemeh Kianfar", "Viet-Phuong La", "Minh-Hoang Nguyen"], "title": "How Immersiveness Shapes the Link Between Anthropocentric Values and Resource Exploitation in Virtual Worlds", "comment": null, "summary": "The Anthropocene is characterized by escalating ecological crises rooted not only in technological and economic systems but also in deeply ingrained anthropocentric worldviews that shape human-nature relationships. As digital environments increasingly mediate these interactions, video games provide novel contexts for examining the psychological mechanisms underlying environmental behaviors. This study investigates how anthropocentric values are associated with resource-exploiting behaviors in virtual ecosystems--specifically, fishing, bug catching, and tree cutting--and how immersiveness moderates these relationships. Employing the Bayesian Mindsponge Framework (BMF) to analyze data from 640 Animal Crossi,g: New Horizons (ACNH) players across 29 countries, the study reveals complex links between anthropocentric worldviews and in-game behaviors. Fishing and tree-cutting frequencies are positively associated with anthropocentrism, whereas immersiveness weakens the association between tree cutting and anthropocentrism. Bug-catching frequency shows no direct effect but exhibits a growing negative association with anthropocentrism as immersiveness increases. These findings extend environmental psychology into virtual ecologies, illustrating how digital interactions both reflect and reshape environmental values. They highlight the potential of immersive gameplay to cultivate the Nature Quotient (NQ) and foster an eco-surplus culture through reflective, conservation-oriented engagement.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u5206\u6790\u300a\u52a8\u7269\u68ee\u53cb\u4f1a\u300b\u73a9\u5bb6\u6570\u636e\uff0c\u53d1\u73b0\u4eba\u7c7b\u4e2d\u5fc3\u4e3b\u4e49\u4ef7\u503c\u89c2\u4e0e\u865a\u62df\u751f\u6001\u7cfb\u7edf\u4e2d\u8d44\u6e90\u5f00\u53d1\u884c\u4e3a\uff08\u9493\u9c7c\u3001\u780d\u6811\uff09\u5448\u6b63\u76f8\u5173\uff0c\u800c\u6c89\u6d78\u611f\u4f1a\u51cf\u5f31\u8fd9\u79cd\u5173\u8054\uff0c\u7279\u522b\u662f\u5bf9\u780d\u6811\u884c\u4e3a\u3002", "motivation": "\u4eba\u7c7b\u4e16\u751f\u6001\u5371\u673a\u4e0d\u4ec5\u6e90\u4e8e\u6280\u672f\u7ecf\u6d4e\u7cfb\u7edf\uff0c\u66f4\u6839\u690d\u4e8e\u4eba\u7c7b\u4e2d\u5fc3\u4e3b\u4e49\u4e16\u754c\u89c2\u3002\u6570\u5b57\u73af\u5883\u65e5\u76ca\u4e2d\u4ecb\u4eba\u4e0e\u81ea\u7136\u4e92\u52a8\uff0c\u89c6\u9891\u6e38\u620f\u4e3a\u7814\u7a76\u73af\u5883\u884c\u4e3a\u5fc3\u7406\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u573a\u666f\u3002", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u601d\u7ef4\u6d77\u7ef5\u6846\u67b6\uff08BMF\uff09\u5206\u6790\u6765\u81ea29\u4e2a\u56fd\u5bb6640\u540d\u300a\u52a8\u7269\u68ee\u53cb\u4f1a\uff1a\u65b0\u89c6\u91ce\u300b\u73a9\u5bb6\u7684\u6570\u636e\uff0c\u7814\u7a76\u4eba\u7c7b\u4e2d\u5fc3\u4e3b\u4e49\u4ef7\u503c\u89c2\u4e0e\u865a\u62df\u8d44\u6e90\u5f00\u53d1\u884c\u4e3a\uff08\u9493\u9c7c\u3001\u6349\u866b\u3001\u780d\u6811\uff09\u7684\u5173\u7cfb\uff0c\u4ee5\u53ca\u6c89\u6d78\u611f\u7684\u8c03\u8282\u4f5c\u7528\u3002", "result": "\u9493\u9c7c\u548c\u780d\u6811\u9891\u7387\u4e0e\u4eba\u7c7b\u4e2d\u5fc3\u4e3b\u4e49\u5448\u6b63\u76f8\u5173\uff1b\u6c89\u6d78\u611f\u51cf\u5f31\u4e86\u780d\u6811\u4e0e\u4eba\u7c7b\u4e2d\u5fc3\u4e3b\u4e49\u7684\u5173\u8054\uff1b\u6349\u866b\u9891\u7387\u65e0\u76f4\u63a5\u5f71\u54cd\uff0c\u4f46\u968f\u7740\u6c89\u6d78\u611f\u589e\u52a0\uff0c\u4e0e\u4eba\u7c7b\u4e2d\u5fc3\u4e3b\u4e49\u7684\u8d1f\u76f8\u5173\u589e\u5f3a\u3002", "conclusion": "\u7814\u7a76\u5c06\u73af\u5883\u5fc3\u7406\u5b66\u6269\u5c55\u5230\u865a\u62df\u751f\u6001\uff0c\u8868\u660e\u6570\u5b57\u4e92\u52a8\u65e2\u53cd\u6620\u53c8\u91cd\u5851\u73af\u5883\u4ef7\u503c\u89c2\u3002\u6c89\u6d78\u5f0f\u6e38\u620f\u6709\u6f5c\u529b\u57f9\u517b\u81ea\u7136\u5546\u6570\uff08NQ\uff09\u548c\u751f\u6001\u76c8\u4f59\u6587\u5316\uff0c\u901a\u8fc7\u53cd\u601d\u6027\u3001\u4fdd\u62a4\u5bfc\u5411\u7684\u53c2\u4e0e\u4fc3\u8fdb\u73af\u4fdd\u610f\u8bc6\u3002"}}
{"id": "2512.12731", "categories": ["cs.LG", "math.NA"], "pdf": "https://arxiv.org/pdf/2512.12731", "abs": "https://arxiv.org/abs/2512.12731", "authors": ["Yuriy N. Bakhvalov"], "title": "Solving a Machine Learning Regression Problem Based on the Theory of Random Functions", "comment": "Part 1 of 4 in the \"Polyharmonic Cascade\" cycle. 25 pages, 2 figures. Source code is available at: https://github.com/xolod7/polyharmonic-cascade", "summary": "This paper studies a machine learning regression problem as a multivariate approximation problem using the framework of the theory of random functions. An ab initio derivation of a regression method is proposed, starting from postulates of indifference. It is shown that if a probability measure on an infinite-dimensional function space possesses natural symmetries (invariance under translation, rotation, scaling, and Gaussianity), then the entire solution scheme, including the kernel form, the type of regularization, and the noise parameterization, follows analytically from these postulates. The resulting kernel coincides with a generalized polyharmonic spline; however, unlike existing approaches, it is not chosen empirically but arises as a consequence of the indifference principle. This result provides a theoretical foundation for a broad class of smoothing and interpolation methods, demonstrating their optimality in the absence of a priori information.", "AI": {"tldr": "\u8bba\u6587\u4ece\u968f\u673a\u51fd\u6570\u7406\u8bba\u51fa\u53d1\uff0c\u57fa\u4e8e\u65e0\u5dee\u522b\u539f\u7406\u63a8\u5bfc\u51fa\u56de\u5f52\u65b9\u6cd5\uff0c\u8bc1\u660e\u5177\u6709\u7279\u5b9a\u5bf9\u79f0\u6027\u7684\u6982\u7387\u6d4b\u5ea6\u4f1a\u81ea\u7136\u4ea7\u751f\u5e7f\u4e49\u591a\u8c03\u548c\u6837\u6761\u6838\u51fd\u6570", "motivation": "\u4e3a\u673a\u5668\u5b66\u4e60\u56de\u5f52\u95ee\u9898\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\uff0c\u4ece\u7b2c\u4e00\u6027\u539f\u7406\u63a8\u5bfc\u56de\u5f52\u65b9\u6cd5\uff0c\u800c\u4e0d\u662f\u7ecf\u9a8c\u6027\u5730\u9009\u62e9\u6838\u51fd\u6570\u548c\u6b63\u5219\u5316\u5f62\u5f0f", "method": "\u5c06\u56de\u5f52\u95ee\u9898\u89c6\u4e3a\u591a\u5143\u903c\u8fd1\u95ee\u9898\uff0c\u91c7\u7528\u968f\u673a\u51fd\u6570\u7406\u8bba\u6846\u67b6\uff0c\u57fa\u4e8e\u65e0\u5dee\u522b\u539f\u7406\u7684\u516c\u8bbe\uff0c\u63a8\u5bfc\u5177\u6709\u5e73\u79fb\u3001\u65cb\u8f6c\u3001\u7f29\u653e\u4e0d\u53d8\u6027\u548c\u9ad8\u65af\u6027\u7684\u6982\u7387\u6d4b\u5ea6", "result": "\u8bc1\u660e\u5177\u6709\u81ea\u7136\u5bf9\u79f0\u6027\u7684\u6982\u7387\u6d4b\u5ea6\u4f1a\u89e3\u6790\u5730\u5bfc\u51fa\u6574\u4e2a\u89e3\u51b3\u65b9\u6848\uff0c\u5305\u62ec\u6838\u51fd\u6570\u5f62\u5f0f\u3001\u6b63\u5219\u5316\u7c7b\u578b\u548c\u566a\u58f0\u53c2\u6570\u5316\uff0c\u6240\u5f97\u6838\u51fd\u6570\u4e0e\u5e7f\u4e49\u591a\u8c03\u548c\u6837\u6761\u4e00\u81f4", "conclusion": "\u8be5\u7ed3\u679c\u4e3a\u4e00\u5927\u7c7b\u5e73\u6ed1\u548c\u63d2\u503c\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u8bc1\u660e\u4e86\u5728\u7f3a\u4e4f\u5148\u9a8c\u4fe1\u606f\u65f6\u8fd9\u4e9b\u65b9\u6cd5\u7684\u6700\u4f18\u6027"}}
{"id": "2512.11864", "categories": ["cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.11864", "abs": "https://arxiv.org/abs/2512.11864", "authors": ["Christoph Einspieler", "Matthias Horn", "Marie-Louise Lackner", "Patrick Malik", "Nysret Musliu", "Felix Winter"], "title": "Solving Parallel Machine Scheduling With Precedences and Cumulative Resource Constraints With Calendars", "comment": "18 pages, 4 figures", "summary": "The task of finding efficient production schedules for parallel machines is a challenge that arises in most industrial manufacturing domains. There is a large potential to minimize production costs through automated scheduling techniques, due to the large-scale requirements of modern factories. In the past, solution approaches have been studied for many machine scheduling variations, where even basic variants have been shown to be NP-hard. However, in today's real-life production environments, additional complex precedence constraints and resource restrictions with calendars arise that must be fulfilled. These additional constraints cannot be tackled efficiently by existing solution techniques. Thus, there is a strong need to develop and analyze automated methods that can solve such real-life parallel machine scheduling scenarios. In this work, we introduce a novel variant of parallel machine scheduling with job precedences and calendar-based cumulative resource constraints that arises in real-life industrial use cases. A constraint modeling approach is proposed as an exact solution method for small scheduling scenarios together with state-of-the-art constraint-solving technology. Further, we propose a construction heuristic as well as a tailored metaheuristic using local search to efficiently tackle large-scale problem instances. This metaheuristic approach has been deployed and is currently being used in an industrial setting.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u73b0\u5b9e\u5de5\u4e1a\u73af\u5883\u4e2d\u5e26\u4f5c\u4e1a\u4f18\u5148\u5173\u7cfb\u548c\u65e5\u5386\u5f0f\u7d2f\u79ef\u8d44\u6e90\u7ea6\u675f\u7684\u5e76\u884c\u673a\u8c03\u5ea6\u65b0\u53d8\u4f53\uff0c\u7ed3\u5408\u7ea6\u675f\u5efa\u6a21\u7cbe\u786e\u65b9\u6cd5\u548c\u5143\u542f\u53d1\u5f0f\u65b9\u6cd5\u5904\u7406\u4e0d\u540c\u89c4\u6a21\u95ee\u9898", "motivation": "\u73b0\u5b9e\u751f\u4ea7\u73af\u5883\u4e2d\u5b58\u5728\u590d\u6742\u4f18\u5148\u7ea6\u675f\u548c\u57fa\u4e8e\u65e5\u5386\u7684\u8d44\u6e90\u9650\u5236\uff0c\u73b0\u6709\u8c03\u5ea6\u6280\u672f\u65e0\u6cd5\u6709\u6548\u5904\u7406\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u89e3\u51b3\u8fd9\u7c7b\u5b9e\u9645\u5e76\u884c\u673a\u8c03\u5ea6\u95ee\u9898\u7684\u81ea\u52a8\u5316\u65b9\u6cd5", "method": "\u63d0\u51fa\u7ea6\u675f\u5efa\u6a21\u65b9\u6cd5\u4f5c\u4e3a\u5c0f\u89c4\u6a21\u573a\u666f\u7684\u7cbe\u786e\u89e3\uff0c\u540c\u65f6\u63d0\u51fa\u6784\u9020\u542f\u53d1\u5f0f\u548c\u57fa\u4e8e\u5c40\u90e8\u641c\u7d22\u7684\u5b9a\u5236\u5143\u542f\u53d1\u5f0f\u65b9\u6cd5\u5904\u7406\u5927\u89c4\u6a21\u5b9e\u4f8b", "result": "\u5143\u542f\u53d1\u5f0f\u65b9\u6cd5\u5df2\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u90e8\u7f72\u4f7f\u7528\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u73b0\u5b9e\u5de5\u4e1a\u7528\u4f8b\u4e2d\u7684\u5927\u89c4\u6a21\u8c03\u5ea6\u95ee\u9898", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u73b0\u5b9e\u5de5\u4e1a\u73af\u5883\u4e2d\u5e26\u590d\u6742\u7ea6\u675f\u7684\u5e76\u884c\u673a\u8c03\u5ea6\u95ee\u9898\uff0c\u5176\u4e2d\u5143\u542f\u53d1\u5f0f\u65b9\u6cd5\u5df2\u5728\u5b9e\u9645\u5de5\u4e1a\u73af\u5883\u4e2d\u5f97\u5230\u6210\u529f\u5e94\u7528"}}
{"id": "2512.11830", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11830", "abs": "https://arxiv.org/abs/2512.11830", "authors": ["Satyam Kumar"], "title": "CR3G: Causal Reasoning for Patient-Centric Explanations in Radiology Report Generation", "comment": "8 pages, 5 figures, 1 table", "summary": "Automatic chest X-ray report generation is an important area of research aimed at improving diagnostic accuracy and helping doctors make faster decisions. Current AI models are good at finding correlations (or patterns) in medical images. Still, they often struggle to understand the deeper cause-and-effect relationships between those patterns and a patient condition. Causal inference is a powerful approach that goes beyond identifying patterns to uncover why certain findings in an X-ray relate to a specific diagnosis. In this paper, we will explore the prompt-driven framework Causal Reasoning for Patient-Centric Explanations in radiology Report Generation (CR3G) that is applied to chest X-ray analysis to improve understanding of AI-generated reports by focusing on cause-and-effect relationships, reasoning and generate patient-centric explanation. The aim to enhance the quality of AI-driven diagnostics, making them more useful and trustworthy in clinical practice. CR3G has shown better causal relationship capability and explanation capability for 2 out of 5 abnormalities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CR3G\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u7528\u4e8e\u80f8\u90e8X\u5149\u62a5\u544a\u751f\u6210\u7684\u63d0\u793a\u9a71\u52a8\u56e0\u679c\u63a8\u7406\u7cfb\u7edf\uff0c\u65e8\u5728\u901a\u8fc7\u5173\u6ce8\u56e0\u679c\u5173\u7cfb\u548c\u60a3\u8005\u4e2d\u5fc3\u89e3\u91ca\u6765\u63d0\u9ad8AI\u751f\u6210\u62a5\u544a\u7684\u53ef\u7406\u89e3\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u5f53\u524dAI\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4e2d\u64c5\u957f\u53d1\u73b0\u76f8\u5173\u6027\u6a21\u5f0f\uff0c\u4f46\u96be\u4ee5\u7406\u89e3\u8fd9\u4e9b\u6a21\u5f0f\u4e0e\u60a3\u8005\u72b6\u51b5\u4e4b\u95f4\u7684\u6df1\u5c42\u56e0\u679c\u5173\u7cfb\u3002\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u9700\u8981\u66f4\u53ef\u89e3\u91ca\u3001\u53ef\u4fe1\u7684AI\u8bca\u65ad\u7cfb\u7edf\uff0c\u4ee5\u5e2e\u52a9\u533b\u751f\u505a\u51fa\u66f4\u5feb\u3001\u66f4\u51c6\u786e\u7684\u51b3\u7b56\u3002", "method": "\u63d0\u51fa\u4e86CR3G\uff08Causal Reasoning for Patient-Centric Explanations in radiology Report Generation\uff09\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u5e94\u7528\u4e8e\u80f8\u90e8X\u5149\u5206\u6790\u7684\u63d0\u793a\u9a71\u52a8\u56e0\u679c\u63a8\u7406\u7cfb\u7edf\uff0c\u4e13\u6ce8\u4e8e\u63ed\u793a\u5f71\u50cf\u8868\u73b0\u4e0e\u8bca\u65ad\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u5e76\u751f\u6210\u4ee5\u60a3\u8005\u4e3a\u4e2d\u5fc3\u7684\u89e3\u91ca\u3002", "result": "CR3G\u57285\u79cd\u5f02\u5e38\u60c5\u51b5\u4e2d\u76842\u79cd\u4e0a\u8868\u73b0\u51fa\u66f4\u597d\u7684\u56e0\u679c\u5173\u7cfb\u8bc6\u522b\u80fd\u529b\u548c\u89e3\u91ca\u80fd\u529b\uff0c\u663e\u793a\u51fa\u5728\u63d0\u9ad8AI\u9a71\u52a8\u8bca\u65ad\u8d28\u91cf\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u56e0\u679c\u63a8\u7406\u65b9\u6cd5\u80fd\u591f\u8d85\u8d8a\u4f20\u7edf\u7684\u6a21\u5f0f\u8bc6\u522b\uff0c\u4e3a\u533b\u5b66\u5f71\u50cf\u62a5\u544a\u751f\u6210\u63d0\u4f9b\u66f4\u6df1\u5165\u3001\u66f4\u53ef\u89e3\u91ca\u7684\u5206\u6790\uff0c\u6709\u52a9\u4e8e\u589e\u5f3a\u4e34\u5e8a\u5b9e\u8df5\u4e2dAI\u8bca\u65ad\u7cfb\u7edf\u7684\u5b9e\u7528\u6027\u548c\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2512.12964", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.12964", "abs": "https://arxiv.org/abs/2512.12964", "authors": ["Yupeng Li", "Mingyue Cheng", "Yucong Luo", "Yitong Zhou", "Qingyang Mao", "Shijin Wang"], "title": "BLADE: A Behavior-Level Data Augmentation Framework with Dual Fusion Modeling for Multi-Behavior Sequential Recommendation", "comment": null, "summary": "Multi-behavior sequential recommendation aims to capture users' dynamic interests by modeling diverse types of user interactions over time. Although several studies have explored this setting, the recommendation performance remains suboptimal, mainly due to two fundamental challenges: the heterogeneity of user behaviors and data sparsity. To address these challenges, we propose BLADE, a framework that enhances multi-behavior modeling while mitigating data sparsity. Specifically, to handle behavior heterogeneity, we introduce a dual item-behavior fusion architecture that incorporates behavior information at both the input and intermediate levels, enabling preference modeling from multiple perspectives. To mitigate data sparsity, we design three behavior-level data augmentation methods that operate directly on behavior sequences rather than core item sequences. These methods generate diverse augmented views while preserving the semantic consistency of item sequences. These augmented views further enhance representation learning and generalization via contrastive learning. Experiments on three real-world datasets demonstrate the effectiveness of our approach.", "AI": {"tldr": "BLADE\u6846\u67b6\u901a\u8fc7\u53cc\u9879\u76ee-\u884c\u4e3a\u878d\u5408\u67b6\u6784\u5904\u7406\u884c\u4e3a\u5f02\u8d28\u6027\uff0c\u5e76\u901a\u8fc7\u4e09\u79cd\u884c\u4e3a\u7ea7\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u7f13\u89e3\u6570\u636e\u7a00\u758f\u95ee\u9898\uff0c\u63d0\u5347\u591a\u884c\u4e3a\u5e8f\u5217\u63a8\u8350\u6027\u80fd", "motivation": "\u591a\u884c\u4e3a\u5e8f\u5217\u63a8\u8350\u9762\u4e34\u4e24\u4e2a\u57fa\u672c\u6311\u6218\uff1a\u7528\u6237\u884c\u4e3a\u7684\u5f02\u8d28\u6027\u548c\u6570\u636e\u7a00\u758f\u6027\uff0c\u5bfc\u81f4\u63a8\u8350\u6027\u80fd\u4e0d\u7406\u60f3", "method": "\u63d0\u51faBLADE\u6846\u67b6\uff1a1\uff09\u53cc\u9879\u76ee-\u884c\u4e3a\u878d\u5408\u67b6\u6784\uff0c\u5728\u8f93\u5165\u5c42\u548c\u4e2d\u95f4\u5c42\u6574\u5408\u884c\u4e3a\u4fe1\u606f\uff0c\u4ece\u591a\u89d2\u5ea6\u5efa\u6a21\u504f\u597d\uff1b2\uff09\u4e09\u79cd\u884c\u4e3a\u7ea7\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u76f4\u63a5\u5728\u884c\u4e3a\u5e8f\u5217\u4e0a\u64cd\u4f5c\u800c\u975e\u6838\u5fc3\u9879\u76ee\u5e8f\u5217\uff0c\u751f\u6210\u591a\u6837\u5316\u589e\u5f3a\u89c6\u56fe\u540c\u65f6\u4fdd\u6301\u9879\u76ee\u5e8f\u5217\u8bed\u4e49\u4e00\u81f4\u6027\uff1b3\uff09\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u589e\u5f3a\u8868\u793a\u5b66\u4e60\u548c\u6cdb\u5316\u80fd\u529b", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "BLADE\u6846\u67b6\u901a\u8fc7\u6709\u6548\u5904\u7406\u884c\u4e3a\u5f02\u8d28\u6027\u548c\u7f13\u89e3\u6570\u636e\u7a00\u758f\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u884c\u4e3a\u5e8f\u5217\u63a8\u8350\u7684\u6027\u80fd"}}
{"id": "2512.11871", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11871", "abs": "https://arxiv.org/abs/2512.11871", "authors": ["Tekleab G. Gebremedhin", "Hailom S. Asegede", "Bruh W. Tesheme", "Tadesse B. Gebremichael", "Kalayu G. Redae"], "title": "Automated Plant Disease and Pest Detection System Using Hybrid Lightweight CNN-MobileViT Models for Diagnosis of Indigenous Crops", "comment": "A preliminary version of this work was presented at the International Conference on Postwar Technology for Recovery and Sustainable Development (Feb. 2025). This manuscript substantially extends that work with expanded experiments and on-device deployment analysis. Code and dataset are publicly available at: https://github.com/Tekleab15/Automated_plant_disease_and_pest_detection_system", "summary": "Agriculture supports over 80% of the population in the Tigray region of Ethiopia, where infrastructural disruptions limit access to expert crop disease diagnosis. We present an offline-first detection system centered on a newly curated indigenous cactus-fig (Opuntia ficus-indica) dataset consisting of 3,587 field images across three core symptom classes. Given deployment constraints in post-conflict edge environments, we benchmark three mobile-efficient architectures: a custom lightweight CNN, EfficientNet-Lite1, and the CNN-Transformer hybrid MobileViT-XS. While the broader system contains independent modules for potato, apple, and corn, this study isolates cactus-fig model performance to evaluate attention sensitivity and inductive bias transfer on indigenous morphology alone. Results establish a clear Pareto trade-off: EfficientNet-Lite1 achieves 90.7% test accuracy, the lightweight CNN reaches 89.5% with the most favorable deployment profile (42 ms inference latency, 4.8 MB model size), and MobileViT-XS delivers 97.3% mean cross-validation accuracy, demonstrating that MHSA-based global reasoning disambiguates pest clusters from two dimensional fungal lesions more reliably than local texture CNN kernels. The ARM compatible models are deployed in a Tigrigna and Amharic localized Flutter application supporting fully offline inference on Cortex-A53 class devices, strengthening inclusivity for food security critical diagnostics.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u4e3a\u57c3\u585e\u4fc4\u6bd4\u4e9a\u63d0\u683c\u96f7\u5730\u533a\u5f00\u53d1\u4e86\u4e00\u4e2a\u79bb\u7ebf\u4f5c\u7269\u75c5\u5bb3\u68c0\u6d4b\u7cfb\u7edf\uff0c\u4e13\u6ce8\u4e8e\u4ed9\u4eba\u638c\u65e0\u82b1\u679c\u75c5\u5bb3\u8bc6\u522b\uff0c\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u8bca\u65ad\uff0c\u652f\u6301\u5f53\u5730\u8bed\u8a00\uff0c\u589e\u5f3a\u7cae\u98df\u5b89\u5168\u3002", "motivation": "\u57c3\u585e\u4fc4\u6bd4\u4e9a\u63d0\u683c\u96f7\u5730\u533a80%\u4ee5\u4e0a\u4eba\u53e3\u4f9d\u8d56\u519c\u4e1a\uff0c\u4f46\u57fa\u7840\u8bbe\u65bd\u4e2d\u65ad\u9650\u5236\u4e86\u4e13\u4e1a\u4f5c\u7269\u75c5\u5bb3\u8bca\u65ad\u7684\u83b7\u53d6\u3002\u9700\u8981\u4e3a\u51b2\u7a81\u540e\u8fb9\u7f18\u73af\u5883\u5f00\u53d1\u79bb\u7ebf\u3001\u79fb\u52a8\u9ad8\u6548\u7684\u75c5\u5bb3\u68c0\u6d4b\u7cfb\u7edf\u3002", "method": "\u521b\u5efa\u4e86\u5305\u542b3,587\u5f20\u7530\u95f4\u56fe\u50cf\u7684\u672c\u5730\u4ed9\u4eba\u638c\u65e0\u82b1\u679c\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u4e09\u79cd\u79fb\u52a8\u9ad8\u6548\u67b6\u6784\uff1a\u5b9a\u5236\u8f7b\u91cfCNN\u3001EfficientNet-Lite1\u548cCNN-Transformer\u6df7\u5408\u6a21\u578bMobileViT-XS\u3002\u7cfb\u7edf\u5305\u542b\u72ec\u7acb\u6a21\u5757\u7528\u4e8e\u9a6c\u94c3\u85af\u3001\u82f9\u679c\u548c\u7389\u7c73\uff0c\u4f46\u672c\u7814\u7a76\u4e13\u6ce8\u4e8e\u4ed9\u4eba\u638c\u65e0\u82b1\u679c\u6a21\u578b\u6027\u80fd\u8bc4\u4f30\u3002", "result": "EfficientNet-Lite1\u8fbe\u523090.7%\u6d4b\u8bd5\u51c6\u786e\u7387\uff1b\u8f7b\u91cfCNN\u8fbe\u523089.5%\u51c6\u786e\u7387\uff0c\u5177\u6709\u6700\u4f73\u90e8\u7f72\u7279\u6027\uff0842ms\u63a8\u7406\u5ef6\u8fdf\uff0c4.8MB\u6a21\u578b\u5927\u5c0f\uff09\uff1bMobileViT-XS\u8fbe\u523097.3%\u5e73\u5747\u4ea4\u53c9\u9a8c\u8bc1\u51c6\u786e\u7387\uff0c\u663e\u793a\u57fa\u4e8e\u591a\u5934\u81ea\u6ce8\u610f\u529b\u7684\u5168\u5c40\u63a8\u7406\u6bd4\u5c40\u90e8\u7eb9\u7406CNN\u6838\u66f4\u53ef\u9760\u5730\u533a\u5206\u5bb3\u866b\u96c6\u7fa4\u548c\u4e8c\u7ef4\u771f\u83cc\u75c5\u53d8\u3002", "conclusion": "\u6210\u529f\u5f00\u53d1\u4e86ARM\u517c\u5bb9\u6a21\u578b\uff0c\u90e8\u7f72\u5728\u652f\u6301\u63d0\u683c\u91cc\u5c3c\u4e9a\u8bed\u548c\u963f\u59c6\u54c8\u62c9\u8bed\u7684Flutter\u5e94\u7528\u4e2d\uff0c\u53ef\u5728Cortex-A53\u7c7b\u8bbe\u5907\u4e0a\u5b8c\u5168\u79bb\u7ebf\u63a8\u7406\uff0c\u589e\u5f3a\u4e86\u7cae\u98df\u5b89\u5168\u5173\u952e\u8bca\u65ad\u7684\u5305\u5bb9\u6027\u3002MobileViT-XS\u7684\u5168\u5c40\u63a8\u7406\u80fd\u529b\u5728\u533a\u5206\u590d\u6742\u75c5\u5bb3\u6a21\u5f0f\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2512.11814", "categories": ["cs.CY", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.11814", "abs": "https://arxiv.org/abs/2512.11814", "authors": ["Hugh Brosnahan"], "title": "Totalitarian Technics: The Hidden Cost of AI Scribes in Healthcare", "comment": null, "summary": "Artificial intelligence (AI) scribes, systems that record and summarise patient-clinician interactions, are promoted as solutions to administrative overload. This paper argues that their significance lies not in efficiency gains but in how they reshape medical attention itself. Offering a conceptual analysis, it situates AI scribes within a broader philosophical lineage concerned with the externalisation of human thought and skill. Drawing on Iain McGilchrist's hemisphere theory and Lewis Mumford's philosophy of technics, the paper examines how technology embodies and amplifies a particular mode of attention. AI scribes, it contends, exemplify the dominance of a left-hemispheric, calculative mindset that privileges the measurable and procedural over the intuitive and relational. As this mode of attention becomes further embedded in medical practice, it risks narrowing the field of care, eroding clinical expertise, and reducing physicians to operators within an increasingly mechanised system.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8ba4\u4e3aAI\u533b\u7597\u8bb0\u5f55\u7cfb\u7edf\u7684\u771f\u6b63\u610f\u4e49\u4e0d\u5728\u4e8e\u6548\u7387\u63d0\u5347\uff0c\u800c\u5728\u4e8e\u5b83\u4eec\u5982\u4f55\u91cd\u5851\u533b\u7597\u6ce8\u610f\u529b\u672c\u8eab\uff0c\u4f53\u73b0\u4e86\u5de6\u8111\u8ba1\u7b97\u601d\u7ef4\u5bf9\u533b\u7597\u5b9e\u8df5\u7684\u652f\u914d\uff0c\u53ef\u80fd\u7a84\u5316\u5173\u6000\u9886\u57df\u5e76\u4fb5\u8680\u4e34\u5e8a\u4e13\u4e1a\u77e5\u8bc6\u3002", "motivation": "\u8bba\u6587\u65e8\u5728\u6279\u5224\u6027\u5730\u5206\u6790AI\u533b\u7597\u8bb0\u5f55\u7cfb\u7edf\uff08AI scribes\uff09\u5bf9\u533b\u7597\u5b9e\u8df5\u7684\u5f71\u54cd\uff0c\u8d85\u8d8a\u7b80\u5355\u7684\u6548\u7387\u8ba8\u8bba\uff0c\u63a2\u8ba8\u8fd9\u4e9b\u6280\u672f\u5982\u4f55\u4ece\u6839\u672c\u4e0a\u91cd\u5851\u533b\u7597\u6ce8\u610f\u529b\u7684\u6027\u8d28\u3002", "method": "\u91c7\u7528\u6982\u5ff5\u5206\u6790\u65b9\u6cd5\uff0c\u5c06AI\u533b\u7597\u8bb0\u5f55\u7cfb\u7edf\u7f6e\u4e8e\u4eba\u7c7b\u601d\u60f3\u548c\u6280\u80fd\u5916\u90e8\u5316\u7684\u54f2\u5b66\u8c31\u7cfb\u4e2d\uff0c\u501f\u9274Iain McGilchrist\u7684\u534a\u7403\u7406\u8bba\u548cLewis Mumford\u7684\u6280\u672f\u54f2\u5b66\uff0c\u5206\u6790\u6280\u672f\u5982\u4f55\u4f53\u73b0\u548c\u653e\u5927\u7279\u5b9a\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\u3002", "result": "\u7814\u7a76\u53d1\u73b0AI\u533b\u7597\u8bb0\u5f55\u7cfb\u7edf\u4f53\u73b0\u4e86\u5de6\u8111\u534a\u7403\u8ba1\u7b97\u601d\u7ef4\u7684\u4e3b\u5bfc\u5730\u4f4d\uff0c\u8fd9\u79cd\u601d\u7ef4\u4f18\u5148\u8003\u8651\u53ef\u6d4b\u91cf\u548c\u7a0b\u5e8f\u5316\u7684\u5185\u5bb9\uff0c\u800c\u5ffd\u89c6\u76f4\u89c9\u548c\u5173\u7cfb\u6027\u56e0\u7d20\u3002\u968f\u7740\u8fd9\u79cd\u6ce8\u610f\u529b\u6a21\u5f0f\u5728\u533b\u7597\u5b9e\u8df5\u4e2d\u8fdb\u4e00\u6b65\u5d4c\u5165\uff0c\u53ef\u80fd\u5e26\u6765\u98ce\u9669\u3002", "conclusion": "AI\u533b\u7597\u8bb0\u5f55\u7cfb\u7edf\u4e0d\u4ec5\u6539\u53d8\u533b\u7597\u8bb0\u5f55\u65b9\u5f0f\uff0c\u66f4\u91cd\u8981\u7684\u662f\u91cd\u5851\u533b\u7597\u6ce8\u610f\u529b\u672c\u8eab\uff0c\u53ef\u80fd\u5bfc\u81f4\u5173\u6000\u9886\u57df\u7a84\u5316\u3001\u4e34\u5e8a\u4e13\u4e1a\u77e5\u8bc6\u88ab\u4fb5\u8680\uff0c\u5e76\u5c06\u533b\u751f\u7b80\u5316\u4e3a\u65e5\u76ca\u673a\u68b0\u5316\u7cfb\u7edf\u4e2d\u7684\u64cd\u4f5c\u5458\u3002"}}
{"id": "2512.11902", "categories": ["cs.AI", "cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11902", "abs": "https://arxiv.org/abs/2512.11902", "authors": ["Yanna Elizabeth Smid", "Peter van der Putten", "Aske Plaat"], "title": "Mirror Mode in Fire Emblem: Beating Players at their own Game with Imitation and Reinforcement Learning", "comment": null, "summary": "Enemy strategies in turn-based games should be surprising and unpredictable. This study introduces Mirror Mode, a new game mode where the enemy AI mimics the personal strategy of a player to challenge them to keep changing their gameplay. A simplified version of the Nintendo strategy video game Fire Emblem Heroes has been built in Unity, with a Standard Mode and a Mirror Mode. Our first set of experiments find a suitable model for the task to imitate player demonstrations, using Reinforcement Learning and Imitation Learning: combining Generative Adversarial Imitation Learning, Behavioral Cloning, and Proximal Policy Optimization. The second set of experiments evaluates the constructed model with player tests, where models are trained on demonstrations provided by participants. The gameplay of the participants indicates good imitation in defensive behavior, but not in offensive strategies. Participant's surveys indicated that they recognized their own retreating tactics, and resulted in an overall higher player-satisfaction for Mirror Mode. Refining the model further may improve imitation quality and increase player's satisfaction, especially when players face their own strategies. The full code and survey results are stored at: https://github.com/YannaSmid/MirrorMode", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\"\u955c\u50cf\u6a21\u5f0f\"\uff0c\u8ba9\u654c\u4ebaAI\u6a21\u4eff\u73a9\u5bb6\u7684\u4e2a\u4eba\u7b56\u7565\u6765\u6311\u6218\u73a9\u5bb6\u4e0d\u65ad\u6539\u53d8\u73a9\u6cd5\u3002\u5728Unity\u4e2d\u6784\u5efa\u4e86\u7b80\u5316\u7248\u300a\u706b\u7130\u7eb9\u7ae0\u82f1\u96c4\u300b\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\u6a21\u578b\u6a21\u4eff\u73a9\u5bb6\u6f14\u793a\u3002\u5b9e\u9a8c\u8868\u660e\u6a21\u578b\u5728\u9632\u5fa1\u884c\u4e3a\u4e0a\u6a21\u4eff\u826f\u597d\uff0c\u4f46\u8fdb\u653b\u7b56\u7565\u6a21\u4eff\u4e0d\u8db3\uff0c\u73a9\u5bb6\u80fd\u8bc6\u522b\u81ea\u5df1\u7684\u64a4\u9000\u6218\u672f\uff0c\u955c\u50cf\u6a21\u5f0f\u6574\u4f53\u6ee1\u610f\u5ea6\u66f4\u9ad8\u3002", "motivation": "\u5728\u56de\u5408\u5236\u6e38\u620f\u4e2d\uff0c\u654c\u4eba\u7b56\u7565\u5e94\u8be5\u5177\u6709\u60ca\u559c\u6027\u548c\u4e0d\u53ef\u9884\u6d4b\u6027\u3002\u4e3a\u4e86\u6311\u6218\u73a9\u5bb6\u4e0d\u65ad\u6539\u53d8\u81ea\u5df1\u7684\u73a9\u6cd5\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u6a21\u4eff\u73a9\u5bb6\u4e2a\u4eba\u7b56\u7565\u7684AI\u7cfb\u7edf\u3002", "method": "1. \u5728Unity\u4e2d\u6784\u5efa\u7b80\u5316\u7248\u300a\u706b\u7130\u7eb9\u7ae0\u82f1\u96c4\u300b\u6e38\u620f\uff0c\u5305\u542b\u6807\u51c6\u6a21\u5f0f\u548c\u955c\u50cf\u6a21\u5f0f\n2. \u7b2c\u4e00\u7ec4\u5b9e\u9a8c\uff1a\u7ed3\u5408\u751f\u6210\u5bf9\u6297\u6a21\u4eff\u5b66\u4e60\u3001\u884c\u4e3a\u514b\u9686\u548c\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff0c\u5bfb\u627e\u9002\u5408\u6a21\u4eff\u73a9\u5bb6\u6f14\u793a\u7684\u6a21\u578b\n3. \u7b2c\u4e8c\u7ec4\u5b9e\u9a8c\uff1a\u901a\u8fc7\u73a9\u5bb6\u6d4b\u8bd5\u8bc4\u4f30\u6784\u5efa\u7684\u6a21\u578b\uff0c\u6a21\u578b\u57fa\u4e8e\u53c2\u4e0e\u8005\u63d0\u4f9b\u7684\u6f14\u793a\u8fdb\u884c\u8bad\u7ec3", "result": "1. \u6a21\u578b\u5728\u9632\u5fa1\u884c\u4e3a\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6a21\u4eff\u80fd\u529b\uff0c\u4f46\u5728\u8fdb\u653b\u7b56\u7565\u4e0a\u6a21\u4eff\u4e0d\u8db3\n2. \u73a9\u5bb6\u80fd\u591f\u8bc6\u522b\u81ea\u5df1\u7684\u64a4\u9000\u6218\u672f\n3. \u955c\u50cf\u6a21\u5f0f\u6574\u4f53\u83b7\u5f97\u66f4\u9ad8\u7684\u73a9\u5bb6\u6ee1\u610f\u5ea6", "conclusion": "\u955c\u50cf\u6a21\u5f0f\u80fd\u591f\u6709\u6548\u6a21\u4eff\u73a9\u5bb6\u7684\u9632\u5fa1\u7b56\u7565\u5e76\u63d0\u9ad8\u73a9\u5bb6\u6ee1\u610f\u5ea6\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u6a21\u578b\u4ee5\u63d0\u5347\u8fdb\u653b\u7b56\u7565\u7684\u6a21\u4eff\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5728\u73a9\u5bb6\u9762\u5bf9\u81ea\u5df1\u7b56\u7565\u65f6\u3002"}}
{"id": "2512.11831", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11831", "abs": "https://arxiv.org/abs/2512.11831", "authors": ["Haitao Lin", "Peiyan Hu", "Minsi Ren", "Zhifeng Gao", "Zhi-Ming Ma", "Guolin ke", "Tailin Wu", "Stan Z. Li"], "title": "On the Design of One-step Diffusion via Shortcutting Flow Paths", "comment": "10 pages of main body, conference paper", "summary": "Recent advances in few-step diffusion models have demonstrated their efficiency and effectiveness by shortcutting the probabilistic paths of diffusion models, especially in training one-step diffusion models from scratch (a.k.a. shortcut models). However, their theoretical derivation and practical implementation are often closely coupled, which obscures the design space. To address this, we propose a common design framework for representative shortcut models. This framework provides theoretical justification for their validity and disentangles concrete component-level choices, thereby enabling systematic identification of improvements. With our proposed improvements, the resulting one-step model achieves a new state-of-the-art FID50k of 2.85 on ImageNet-256x256 under the classifier-free guidance setting. Remarkably, the model requires no pre-training, distillation, or curriculum learning. We believe our work lowers the barrier to component-level innovation in shortcut models and facilitates principled exploration of their design space.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u4ee3\u8868\u6027\u6377\u5f84\u6a21\u578b\u7684\u901a\u7528\u8bbe\u8ba1\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4e3a\u6709\u6548\u6027\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\uff0c\u89e3\u8026\u5177\u4f53\u7ec4\u4ef6\u7ea7\u9009\u62e9\uff0c\u4ece\u800c\u7cfb\u7edf\u8bc6\u522b\u6539\u8fdb\u70b9\u3002\u6539\u8fdb\u540e\u7684\u4e00\u6b65\u6a21\u578b\u5728ImageNet-256x256\u4e0a\u8fbe\u52302.85\u7684SOTA FID50k\uff0c\u65e0\u9700\u9884\u8bad\u7ec3\u3001\u84b8\u998f\u6216\u8bfe\u7a0b\u5b66\u4e60\u3002", "motivation": "\u5f53\u524dfew-step\u6269\u6563\u6a21\u578b\u7684\u7406\u8bba\u63a8\u5bfc\u548c\u5b9e\u9645\u5b9e\u73b0\u7d27\u5bc6\u8026\u5408\uff0c\u6a21\u7cca\u4e86\u8bbe\u8ba1\u7a7a\u95f4\u3002\u9700\u8981\u89e3\u8026\u7406\u8bba\u6846\u67b6\u548c\u5177\u4f53\u5b9e\u73b0\uff0c\u4e3a\u7ec4\u4ef6\u7ea7\u521b\u65b0\u63d0\u4f9b\u6e05\u6670\u8def\u5f84\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u901a\u7528\u8bbe\u8ba1\u6846\u67b6\uff0c\u4e3a\u4ee3\u8868\u6027\u6377\u5f84\u6a21\u578b\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\uff0c\u89e3\u8026\u5177\u4f53\u7ec4\u4ef6\u7ea7\u9009\u62e9\uff0c\u7cfb\u7edf\u8bc6\u522b\u6539\u8fdb\u70b9\uff0c\u6784\u5efa\u6539\u8fdb\u540e\u7684\u4e00\u6b65\u6a21\u578b\u3002", "result": "\u6539\u8fdb\u540e\u7684\u4e00\u6b65\u6a21\u578b\u5728ImageNet-256x256\u4e0a\u8fbe\u52302.85\u7684FID50k\uff08SOTA\uff09\uff0c\u5728classifier-free guidance\u8bbe\u7f6e\u4e0b\uff0c\u65e0\u9700\u9884\u8bad\u7ec3\u3001\u84b8\u998f\u6216\u8bfe\u7a0b\u5b66\u4e60\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u964d\u4f4e\u4e86\u6377\u5f84\u6a21\u578b\u7ec4\u4ef6\u7ea7\u521b\u65b0\u7684\u95e8\u69db\uff0c\u4fc3\u8fdb\u4e86\u8bbe\u8ba1\u7a7a\u95f4\u7684\u539f\u5219\u6027\u63a2\u7d22\uff0c\u4e3afew-step\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u8bbe\u8ba1\u6846\u67b6\u3002"}}
{"id": "2512.12978", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.12978", "abs": "https://arxiv.org/abs/2512.12978", "authors": ["Chee Heng Tan", "Huiying Zheng", "Jing Wang", "Zhuoyi Lin", "Shaodi Feng", "Huijing Zhan", "Xiaoli Li", "J. Senthilnath"], "title": "Do Reviews Matter for Recommendations in the Era of Large Language Models?", "comment": "11 pages, 9 figures, 3 tables", "summary": "With the advent of large language models (LLMs), the landscape of recommender systems is undergoing a significant transformation. Traditionally, user reviews have served as a critical source of rich, contextual information for enhancing recommendation quality. However, as LLMs demonstrate an unprecedented ability to understand and generate human-like text, this raises the question of whether explicit user reviews remain essential in the era of LLMs. In this paper, we provide a systematic investigation of the evolving role of text reviews in recommendation by comparing deep learning methods and LLM approaches. Particularly, we conduct extensive experiments on eight public datasets with LLMs and evaluate their performance in zero-shot, few-shot, and fine-tuning scenarios. We further introduce a benchmarking evaluation framework for review-aware recommender systems, RAREval, to comprehensively assess the contribution of textual reviews to the recommendation performance of review-aware recommender systems. Our framework examines various scenarios, including the removal of some or all textual reviews, random distortion, as well as recommendation performance in data sparsity and cold-start user settings. Our findings demonstrate that LLMs are capable of functioning as effective review-aware recommendation engines, generally outperforming traditional deep learning approaches, particularly in scenarios characterized by data sparsity and cold-start conditions. In addition, the removal of some or all textual reviews and random distortion does not necessarily lead to declines in recommendation accuracy. These findings motivate a rethinking of how user preference from text reviews can be more effectively leveraged. All code and supplementary materials are available at: https://github.com/zhytk/RAREval-data-processing.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u5728LLM\u65f6\u4ee3\u7528\u6237\u8bc4\u8bba\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u89d2\u8272\u6f14\u53d8\uff0c\u53d1\u73b0LLM\u4f5c\u4e3a\u8bc4\u8bba\u611f\u77e5\u63a8\u8350\u5f15\u64ce\u901a\u5e38\u4f18\u4e8e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u7a00\u758f\u548c\u51b7\u542f\u52a8\u573a\u666f\u4e0b\uff0c\u4e14\u79fb\u9664\u6216\u968f\u673a\u626d\u66f2\u8bc4\u8bba\u4e0d\u4e00\u5b9a\u5bfc\u81f4\u63a8\u8350\u51c6\u786e\u6027\u4e0b\u964d\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u51fa\u73b0\uff0c\u63a8\u8350\u7cfb\u7edf\u9886\u57df\u6b63\u5728\u7ecf\u5386\u91cd\u5927\u53d8\u9769\u3002\u4f20\u7edf\u4e0a\uff0c\u7528\u6237\u8bc4\u8bba\u662f\u63d0\u5347\u63a8\u8350\u8d28\u91cf\u7684\u91cd\u8981\u4e0a\u4e0b\u6587\u4fe1\u606f\u6765\u6e90\u3002\u4f46LLMs\u5c55\u73b0\u51fa\u524d\u6240\u672a\u6709\u7684\u7406\u89e3\u548c\u751f\u6210\u7c7b\u4eba\u6587\u672c\u80fd\u529b\uff0c\u8fd9\u5f15\u53d1\u4e86\u4e00\u4e2a\u95ee\u9898\uff1a\u5728LLM\u65f6\u4ee3\uff0c\u663e\u5f0f\u7684\u7528\u6237\u8bc4\u8bba\u662f\u5426\u4ecd\u7136\u5fc5\u4e0d\u53ef\u5c11\uff1f", "method": "1. \u5728\u516b\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc4\u4f30LLMs\u5728\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u5fae\u8c03\u573a\u666f\u4e0b\u7684\u6027\u80fd\uff1b2. \u5f15\u5165\u8bc4\u8bba\u611f\u77e5\u63a8\u8350\u7cfb\u7edf\u57fa\u51c6\u8bc4\u4f30\u6846\u67b6RAREval\uff0c\u5168\u9762\u8bc4\u4f30\u6587\u672c\u8bc4\u8bba\u5bf9\u63a8\u8350\u6027\u80fd\u7684\u8d21\u732e\uff1b3. \u6846\u67b6\u8003\u5bdf\u591a\u79cd\u573a\u666f\uff1a\u79fb\u9664\u90e8\u5206\u6216\u5168\u90e8\u6587\u672c\u8bc4\u8bba\u3001\u968f\u673a\u626d\u66f2\u3001\u6570\u636e\u7a00\u758f\u548c\u51b7\u542f\u52a8\u7528\u6237\u8bbe\u7f6e\u4e0b\u7684\u63a8\u8350\u6027\u80fd\u3002", "result": "1. LLMs\u80fd\u591f\u4f5c\u4e3a\u6709\u6548\u7684\u8bc4\u8bba\u611f\u77e5\u63a8\u8350\u5f15\u64ce\uff0c\u901a\u5e38\u4f18\u4e8e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u7a00\u758f\u548c\u51b7\u542f\u52a8\u6761\u4ef6\u4e0b\uff1b2. \u79fb\u9664\u90e8\u5206\u6216\u5168\u90e8\u6587\u672c\u8bc4\u8bba\u4ee5\u53ca\u968f\u673a\u626d\u66f2\u4e0d\u4e00\u5b9a\u5bfc\u81f4\u63a8\u8350\u51c6\u786e\u6027\u4e0b\u964d\uff1b3. \u8fd9\u4e9b\u53d1\u73b0\u4fc3\u4f7f\u91cd\u65b0\u601d\u8003\u5982\u4f55\u66f4\u6709\u6548\u5730\u5229\u7528\u6587\u672c\u8bc4\u8bba\u4e2d\u7684\u7528\u6237\u504f\u597d\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728LLM\u65f6\u4ee3\uff0c\u7528\u6237\u8bc4\u8bba\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u4f5c\u7528\u9700\u8981\u91cd\u65b0\u8bc4\u4f30\u3002LLMs\u80fd\u591f\u6709\u6548\u5904\u7406\u8bc4\u8bba\u4fe1\u606f\uff0c\u751a\u81f3\u5728\u8bc4\u8bba\u4fe1\u606f\u4e0d\u5b8c\u6574\u6216\u88ab\u626d\u66f2\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u4fdd\u6301\u63a8\u8350\u6027\u80fd\uff0c\u8fd9\u4e3a\u63a8\u8350\u7cfb\u7edf\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2512.11874", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11874", "abs": "https://arxiv.org/abs/2512.11874", "authors": ["Jiahao Jiang", "Zhangrui Yang", "Xuanhan Wang", "Jingkuan Song"], "title": "Pseudo-Label Refinement for Robust Wheat Head Segmentation via Two-Stage Hybrid Training", "comment": "3 pages,3 figures, Extended abstract submitted to the 10th Computer Vision in Plant Phenotyping and Agriculture (CVPPA) Workshop, held in conjunction with ICCV 2025", "summary": "This extended abstract details our solution for the Global Wheat Full Semantic Segmentation Competition. We developed a systematic self-training framework. This framework combines a two-stage hybrid training strategy with extensive data augmentation. Our core model is SegFormer with a Mix Transformer (MiT-B4) backbone. We employ an iterative teacher-student loop. This loop progressively refines model accuracy. It also maximizes data utilization. Our method achieved competitive performance. This was evident on both the Development and Testing Phase datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5c0f\u9ea6\u5168\u8bed\u4e49\u5206\u5272\u7ade\u8d5b\u7684\u7cfb\u7edf\u5316\u81ea\u8bad\u7ec3\u6846\u67b6\uff0c\u7ed3\u5408\u4e24\u9636\u6bb5\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\u548c\u5927\u91cf\u6570\u636e\u589e\u5f3a\uff0c\u4f7f\u7528SegFormer\u6a21\u578b\uff0c\u901a\u8fc7\u8fed\u4ee3\u5e08\u751f\u5faa\u73af\u63d0\u5347\u7cbe\u5ea6\uff0c\u5728\u5f00\u53d1\u96c6\u548c\u6d4b\u8bd5\u96c6\u4e0a\u90fd\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "motivation": "\u53c2\u52a0\u5168\u7403\u5c0f\u9ea6\u5168\u8bed\u4e49\u5206\u5272\u7ade\u8d5b\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u51c6\u786e\u5206\u5272\u5c0f\u9ea6\u56fe\u50cf\u7684\u7cfb\u7edf\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u5e94\u5bf9\u519c\u4e1a\u56fe\u50cf\u5206\u6790\u4e2d\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u5316\u81ea\u8bad\u7ec3\u6846\u67b6\uff0c\u7ed3\u5408\u4e24\u9636\u6bb5\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\u548c\u5927\u91cf\u6570\u636e\u589e\u5f3a\uff0c\u6838\u5fc3\u6a21\u578b\u4e3a\u57fa\u4e8eMix Transformer\uff08MiT-B4\uff09\u9aa8\u5e72\u7684SegFormer\uff0c\u901a\u8fc7\u8fed\u4ee3\u5e08\u751f\u5faa\u73af\u9010\u6b65\u63d0\u5347\u6a21\u578b\u7cbe\u5ea6\u5e76\u6700\u5927\u5316\u6570\u636e\u5229\u7528\u7387\u3002", "result": "\u5728\u5f00\u53d1\u9636\u6bb5\u548c\u6d4b\u8bd5\u9636\u6bb5\u7684\u6570\u636e\u96c6\u4e0a\u90fd\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u63d0\u51fa\u7684\u7cfb\u7edf\u5316\u81ea\u8bad\u7ec3\u6846\u67b6\u7ed3\u5408\u4e24\u9636\u6bb5\u6df7\u5408\u8bad\u7ec3\u7b56\u7565\u548c\u8fed\u4ee3\u5e08\u751f\u5faa\u73af\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u5c0f\u9ea6\u8bed\u4e49\u5206\u5272\u7684\u7cbe\u5ea6\uff0c\u5728\u7ade\u8d5b\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2512.12980", "categories": ["cs.IR", "cs.DB"], "pdf": "https://arxiv.org/pdf/2512.12980", "abs": "https://arxiv.org/abs/2512.12980", "authors": ["Tingyang Chen", "Cong Fu", "Jiahua Wu", "Haotian Wu", "Hua Fan", "Xiangyu Ke", "Yunjun Gao", "Yabo Ni", "Anxiang Zeng"], "title": "Reveal Hidden Pitfalls and Navigate Next Generation of Vector Similarity Search from Task-Centric Views", "comment": "SIGMOD2026", "summary": "Vector Similarity Search (VSS) in high-dimensional spaces is rapidly emerging as core functionality in next-generation database systems for numerous data-intensive services -- from embedding lookups in large language models (LLMs), to semantic information retrieval and recommendation engines. Current benchmarks, however, evaluate VSS primarily on the recall-latency trade-off against a ground truth defined solely by distance metrics, neglecting how retrieval quality ultimately impacts downstream tasks. This disconnect can mislead both academic research and industrial practice.\n  We present Iceberg, a holistic benchmark suite for end-to-end evaluation of VSS methods in realistic application contexts. From a task-centric view, Iceberg uncovers the Information Loss Funnel, which identifies three principal sources of end-to-end performance degradation: (1) Embedding Loss during feature extraction; (2) Metric Misuse, where distances poorly reflect task relevance; (3) Data Distribution Sensitivity, highlighting index robustness across skews and modalities. For a more comprehensive assessment, Iceberg spans eight diverse datasets across key domains such as image classification, face recognition, text retrieval, and recommendation systems. Each dataset, ranging from 1M to 100M vectors, includes rich, task-specific labels and evaluation metrics, enabling assessment of retrieval algorithms within the full application pipeline rather than in isolation. Iceberg benchmarks 13 state-of-the-art VSS methods and re-ranks them based on application-level metrics, revealing substantial deviations from traditional rankings derived purely from recall-latency evaluations. Building on these insights, we define a set of task-centric meta-features and derive an interpretable decision tree to guide practitioners in selecting and tuning VSS methods for their specific workloads.", "AI": {"tldr": "Iceberg\u662f\u4e00\u4e2a\u7528\u4e8e\u9ad8\u7ef4\u5411\u91cf\u76f8\u4f3c\u6027\u641c\u7d22\u7684\u7aef\u5230\u7aef\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u5173\u6ce8\u5b9e\u9645\u5e94\u7528\u573a\u666f\u800c\u975e\u4ec5\u53ec\u56de\u7387-\u5ef6\u8fdf\u6743\u8861\uff0c\u63ed\u793a\u4e86\u4f20\u7edf\u8bc4\u4f30\u4e0e\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u7684\u8131\u8282\u3002", "motivation": "\u5f53\u524d\u5411\u91cf\u76f8\u4f3c\u6027\u641c\u7d22\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30\u53ec\u56de\u7387-\u5ef6\u8fdf\u6743\u8861\uff0c\u5ffd\u7565\u4e86\u68c0\u7d22\u8d28\u91cf\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u5b9e\u9645\u5f71\u54cd\uff0c\u5bfc\u81f4\u5b66\u672f\u7814\u7a76\u548c\u5de5\u4e1a\u5b9e\u8df5\u5b58\u5728\u8bef\u5bfc\u3002", "method": "\u63d0\u51fa\u4e86Iceberg\u57fa\u51c6\u5957\u4ef6\uff0c\u5305\u542b8\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\uff081M-100M\u5411\u91cf\uff09\uff0c\u6db5\u76d6\u56fe\u50cf\u5206\u7c7b\u3001\u4eba\u8138\u8bc6\u522b\u3001\u6587\u672c\u68c0\u7d22\u548c\u63a8\u8350\u7cfb\u7edf\u7b49\u5173\u952e\u9886\u57df\uff0c\u6bcf\u4e2a\u6570\u636e\u96c6\u5305\u542b\u4e30\u5bcc\u7684\u4efb\u52a1\u7279\u5b9a\u6807\u7b7e\u548c\u8bc4\u4f30\u6307\u6807\u3002\u5b9a\u4e49\u4e86\u4fe1\u606f\u635f\u5931\u6f0f\u6597\u7406\u8bba\uff0c\u8bc6\u522b\u5d4c\u5165\u635f\u5931\u3001\u5ea6\u91cf\u8bef\u7528\u548c\u6570\u636e\u5206\u5e03\u654f\u611f\u6027\u4e09\u4e2a\u4e3b\u8981\u6027\u80fd\u9000\u5316\u6765\u6e90\u3002\u5bf913\u79cd\u6700\u5148\u8fdb\u7684VSS\u65b9\u6cd5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u57fa\u4e8e\u5e94\u7528\u7ea7\u6307\u6807\u91cd\u65b0\u6392\u540d\u3002", "result": "\u57fa\u4e8e\u5e94\u7528\u7ea7\u6307\u6807\u7684\u6392\u540d\u4e0e\u4f20\u7edf\u57fa\u4e8e\u53ec\u56de\u7387-\u5ef6\u8fdf\u7684\u6392\u540d\u5b58\u5728\u663e\u8457\u504f\u5dee\u3002\u5b9a\u4e49\u4e86\u4efb\u52a1\u4e2d\u5fc3\u5143\u7279\u5f81\uff0c\u5e76\u63a8\u5bfc\u51fa\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u6811\uff0c\u4e3a\u4ece\u4e1a\u8005\u9009\u62e9\u9002\u5408\u5176\u7279\u5b9a\u5de5\u4f5c\u8d1f\u8f7d\u7684VSS\u65b9\u6cd5\u63d0\u4f9b\u6307\u5bfc\u3002", "conclusion": "Iceberg\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u7aef\u5230\u7aef\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u4f20\u7edfVSS\u8bc4\u4f30\u4e0e\u5b9e\u9645\u5e94\u7528\u6027\u80fd\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u7814\u7a76\u548c\u5b9e\u8df5\u63d0\u4f9b\u4e86\u66f4\u8d34\u8fd1\u5b9e\u9645\u9700\u6c42\u7684\u8bc4\u4f30\u6807\u51c6\u548c\u9009\u62e9\u6307\u5357\u3002"}}
{"id": "2512.11884", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11884", "abs": "https://arxiv.org/abs/2512.11884", "authors": ["Ranjan Sapkota", "Konstantinos I. Roumeliotis", "Manoj Karkee", "Nikolaos D. Tselikas"], "title": "Generalization vs. Specialization: Evaluating Segment Anything Model (SAM3) Zero-Shot Segmentation Against Fine-Tuned YOLO Detectors", "comment": null, "summary": "Deep learning has advanced two fundamentally different paradigms for instance segmentation: specialized models optimized through task-specific fine-tuning and generalist foundation models capable of zero-shot segmentation. This work presents a comprehensive comparison between SAM3 (Segment Anything Model, also called SAMv3) operating in zero-shot mode and three variants of Ultralytics YOLO11 (nano, medium, and large) fine-tuned for instance segmentation. The evaluation is conducted on the MinneApple dataset, a dense benchmark comprising 670 orchard images with 28,179 annotated apple instances, enabling rigorous validation of model behavior under high object density and occlusion. Our analysis shows IoU choices can inflate performance gaps by up to 30%. At the appropriate IoU = 0.15 threshold, YOLO models achieve 68.9%, 72.2%, and 71.9% F1, while SAM3 reaches 59.8% in pure zero-shot mode. However, YOLO exhibits steep degradation 48-50 points across IoU ranges whereas SAM3 drops only 4 points, revealing 12 times superior boundary stability of SAM3. This highlights the strength of SAMv3 in mask precision versus specialization in detection completeness of YOLO11. We provide open-source code, evaluation pipelines, and methodological recommendations, contributing to a deeper understanding of when specialized fine-tuned models or generalist foundation models are preferable for dense instance segmentation tasks. This project repository is available on GitHub as https://github.com/Applied-AI-Research-Lab/Segment-Anything-Model-SAM3-Zero-Shot-Segmentation-Against-Fine-Tuned-YOLO-Detectors", "AI": {"tldr": "\u8be5\u7814\u7a76\u7cfb\u7edf\u6bd4\u8f83\u4e86\u96f6\u6837\u672c\u5206\u5272\u7684SAM3\u6a21\u578b\u4e0e\u5fae\u8c03YOLO11\u6a21\u578b\u5728\u5bc6\u96c6\u82f9\u679c\u5b9e\u4f8b\u5206\u5272\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0IoU\u9608\u503c\u9009\u62e9\u5bf9\u6027\u80fd\u8bc4\u4f30\u6709\u663e\u8457\u5f71\u54cd\uff0c\u63ed\u793a\u4e86SAM3\u5728\u8fb9\u754c\u7a33\u5b9a\u6027\u4e0a\u7684\u4f18\u52bf\u4e0eYOLO\u5728\u68c0\u6d4b\u5b8c\u6574\u6027\u4e0a\u7684\u4f18\u52bf\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5b9e\u4f8b\u5206\u5272\u5b58\u5728\u4e24\u79cd\u8303\u5f0f\uff1a\u901a\u8fc7\u4efb\u52a1\u7279\u5b9a\u5fae\u8c03\u4f18\u5316\u7684\u4e13\u7528\u6a21\u578b\u548c\u80fd\u591f\u96f6\u6837\u672c\u5206\u5272\u7684\u901a\u7528\u57fa\u7840\u6a21\u578b\u3002\u672c\u7814\u7a76\u65e8\u5728\u5168\u9762\u6bd4\u8f83\u8fd9\u4e24\u79cd\u8303\u5f0f\u5728\u5bc6\u96c6\u5b9e\u4f8b\u5206\u5272\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u4f7f\u7528MinneApple\u6570\u636e\u96c6\uff08\u5305\u542b670\u5f20\u679c\u56ed\u56fe\u50cf\u548c28,179\u4e2a\u82f9\u679c\u5b9e\u4f8b\u6807\u6ce8\uff09\u4f5c\u4e3a\u5bc6\u96c6\u57fa\u51c6\uff0c\u8bc4\u4f30SAM3\u5728\u96f6\u6837\u672c\u6a21\u5f0f\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u4e0e\u4e09\u79cd\u4e0d\u540c\u89c4\u6a21\u7684Ultralytics YOLO11\u6a21\u578b\uff08nano\u3001medium\u3001large\uff09\u8fdb\u884c\u5bf9\u6bd4\u3002\u7814\u7a76\u7279\u522b\u5173\u6ce8IoU\u9608\u503c\u9009\u62e9\u5bf9\u6027\u80fd\u8bc4\u4f30\u7684\u5f71\u54cd\u3002", "result": "\u5728\u5408\u9002\u7684IoU=0.15\u9608\u503c\u4e0b\uff0cYOLO\u6a21\u578b\u8fbe\u523068.9%\u300172.2%\u548c71.9%\u7684F1\u5206\u6570\uff0c\u800cSAM3\u5728\u7eaf\u96f6\u6837\u672c\u6a21\u5f0f\u4e0b\u8fbe\u523059.8%\u3002\u4f46YOLO\u6a21\u578b\u5728\u4e0d\u540cIoU\u8303\u56f4\u5185\u7684\u6027\u80fd\u4e0b\u964d48-50\u4e2a\u767e\u5206\u70b9\uff0c\u800cSAM3\u4ec5\u4e0b\u964d4\u4e2a\u767e\u5206\u70b9\uff0c\u663e\u793a\u51faSAM3\u7684\u8fb9\u754c\u7a33\u5b9a\u6027\u662fYOLO\u768412\u500d\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86SAM3\u5728\u63a9\u7801\u7cbe\u5ea6\u548c\u8fb9\u754c\u7a33\u5b9a\u6027\u65b9\u9762\u7684\u4f18\u52bf\uff0c\u4ee5\u53caYOLO\u5728\u68c0\u6d4b\u5b8c\u6574\u6027\u65b9\u9762\u7684\u4f18\u52bf\u3002IoU\u9608\u503c\u9009\u62e9\u5bf9\u6027\u80fd\u8bc4\u4f30\u6709\u663e\u8457\u5f71\u54cd\uff08\u53ef\u8fbe30%\u7684\u6027\u80fd\u5dee\u8ddd\uff09\u3002\u7814\u7a76\u63d0\u4f9b\u4e86\u5f00\u6e90\u4ee3\u7801\u3001\u8bc4\u4f30\u6d41\u7a0b\u548c\u65b9\u6cd5\u8bba\u5efa\u8bae\uff0c\u5e2e\u52a9\u7406\u89e3\u5728\u5bc6\u96c6\u5b9e\u4f8b\u5206\u5272\u4efb\u52a1\u4e2d\u4f55\u65f6\u9009\u62e9\u4e13\u7528\u5fae\u8c03\u6a21\u578b\u6216\u901a\u7528\u57fa\u7840\u6a21\u578b\u66f4\u5408\u9002\u3002"}}
{"id": "2512.13001", "categories": ["cs.IR"], "pdf": "https://arxiv.org/pdf/2512.13001", "abs": "https://arxiv.org/abs/2512.13001", "authors": ["Genki Kusano", "Kenya Abe", "Kunihiro Takeoka"], "title": "Are Large Language Models Really Effective for Training-Free Cold-Start Recommendation?", "comment": null, "summary": "Recommender systems usually rely on large-scale interaction data to learn from users' past behaviors and make accurate predictions. However, real-world applications often face situations where no training data is available, such as when launching new services or handling entirely new users. In such cases, conventional approaches cannot be applied. This study focuses on training-free recommendation, where no task-specific training is performed, and particularly on \\textit{training-free cold-start recommendation} (TFCSR), the more challenging case where the target user has no interactions. Large language models (LLMs) have recently been explored as a promising solution, and numerous studies have been proposed. As the ability of text embedding models (TEMs) increases, they are increasingly recognized as applicable to training-free recommendation, but no prior work has directly compared LLMs and TEMs under identical conditions. We present the first controlled experiments that systematically evaluate these two approaches in the same setting. The results show that TEMs outperform LLM rerankers, and this trend holds not only in cold-start settings but also in warm-start settings with rich interactions. These findings indicate that direct LLM ranking is not the only viable option, contrary to the commonly shared belief, and TEM-based approaches provide a stronger and more scalable basis for training-free recommendation.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5728\u76f8\u540c\u6761\u4ef6\u4e0b\u7cfb\u7edf\u6bd4\u8f83\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u6587\u672c\u5d4c\u5165\u6a21\u578b\uff08TEMs\uff09\u5728\u8bad\u7ec3\u514d\u8d39\u51b7\u542f\u52a8\u63a8\u8350\uff08TFCSR\uff09\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0TEMs\u4f18\u4e8eLLMs\uff0c\u4e14\u8fd9\u4e00\u8d8b\u52bf\u5728\u51b7\u542f\u52a8\u548c\u70ed\u542f\u52a8\u573a\u666f\u4e2d\u90fd\u6210\u7acb\u3002", "motivation": "\u73b0\u5b9e\u63a8\u8350\u7cfb\u7edf\u5e38\u9762\u4e34\u65e0\u8bad\u7ec3\u6570\u636e\u7684\u573a\u666f\uff08\u5982\u65b0\u670d\u52a1\u4e0a\u7ebf\u6216\u5168\u65b0\u7528\u6237\uff09\uff0c\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u5e94\u7528\u3002\u867d\u7136LLMs\u88ab\u89c6\u4e3a\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46TEMs\u80fd\u529b\u4e5f\u5728\u63d0\u5347\uff0c\u5374\u7f3a\u4e4f\u5728\u76f8\u540c\u6761\u4ef6\u4e0b\u5bf9\u8fd9\u4e24\u79cd\u65b9\u6cd5\u7684\u76f4\u63a5\u6bd4\u8f83\u7814\u7a76\u3002", "method": "\u9996\u6b21\u8fdb\u884c\u53d7\u63a7\u5b9e\u9a8c\uff0c\u5728\u76f8\u540c\u8bbe\u7f6e\u4e0b\u7cfb\u7edf\u8bc4\u4f30LLMs\u548cTEMs\u4e24\u79cd\u65b9\u6cd5\uff0c\u5305\u62ec\u8bad\u7ec3\u514d\u8d39\u51b7\u542f\u52a8\u63a8\u8350\uff08TFCSR\uff09\u573a\u666f\u4ee5\u53ca\u6709\u4e30\u5bcc\u4ea4\u4e92\u7684\u70ed\u542f\u52a8\u573a\u666f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff1a1\uff09TEMs\u5728\u6027\u80fd\u4e0a\u4f18\u4e8eLLM\u91cd\u6392\u5e8f\u5668\uff1b2\uff09\u8fd9\u4e00\u4f18\u52bf\u4e0d\u4ec5\u5728\u51b7\u542f\u52a8\u8bbe\u7f6e\u4e2d\u6210\u7acb\uff0c\u5728\u5177\u6709\u4e30\u5bcc\u4ea4\u4e92\u7684\u70ed\u542f\u52a8\u8bbe\u7f6e\u4e2d\u4e5f\u540c\u6837\u6210\u7acb\u3002", "conclusion": "\u76f4\u63a5\u4f7f\u7528LLM\u8fdb\u884c\u6392\u5e8f\u5e76\u975e\u552f\u4e00\u53ef\u884c\u65b9\u6848\uff0c\u4e0e\u666e\u904d\u8ba4\u77e5\u76f8\u53cd\uff0c\u57fa\u4e8eTEM\u7684\u65b9\u6cd5\u4e3a\u8bad\u7ec3\u514d\u8d39\u63a8\u8350\u63d0\u4f9b\u4e86\u66f4\u5f3a\u4e14\u66f4\u5177\u6269\u5c55\u6027\u7684\u57fa\u7840\u3002"}}
{"id": "2512.11818", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.11818", "abs": "https://arxiv.org/abs/2512.11818", "authors": ["Izabela Lipinska", "Hugh Brosnahan"], "title": "The Ontological Dissonance Hypothesis: AI-Triggered Delusional Ideation as Folie a Deux Technologique", "comment": "18 pages excluding appendices", "summary": "This paper argues that contemporary large language models (LLMs) can contribute to psychotic involvement by creating interactions that resemble the relational dynamics of folie a deux. Drawing on Bateson's double bind theory, clinical literature on shared psychotic disorder, and McGilchrist's hemisphere theory, we show how the combination of high linguistic coherence and the absence of an underlying subject produces a structural tension for the user: language suggests an interlocutor, while intuition registers a void. In contexts of emotional need or instability, this tension can lead users to resolve the conflict through imaginative projection, attributing interiority, intention, or presence to a system that possesses none. The paper situates these dynamics within emerging clinical reports, develops a phenomenological account of how they unfold, and argues that current engagement-optimised design choices exacerbate the risk. We conclude by proposing 'ontological honesty' as a necessary design principle for mitigating technologically mediated folie a deux.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u80fd\u901a\u8fc7\u7c7b\u4f3c\"\u4e8c\u8054\u6027\u7cbe\u795e\u75c5\"\u7684\u5173\u7cfb\u52a8\u6001\u5bfc\u81f4\u7528\u6237\u7cbe\u795e\u75c5\u6027\u5377\u5165\uff0c\u5176\u9ad8\u8bed\u8a00\u8fde\u8d2f\u6027\u4e0e\u7f3a\u4e4f\u771f\u5b9e\u4e3b\u4f53\u7684\u77db\u76fe\u7ed3\u6784\u53ef\u80fd\u8bf1\u53d1\u7528\u6237\u901a\u8fc7\u60f3\u8c61\u6295\u5c04\u6765\u586b\u8865\u7a7a\u767d\uff0c\u5f53\u524d\u4ee5\u53c2\u4e0e\u5ea6\u4f18\u5316\u7684\u8bbe\u8ba1\u52a0\u5267\u4e86\u8fd9\u79cd\u98ce\u9669\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u5f53\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u901a\u8fc7\u521b\u9020\u7c7b\u4f3c\"\u4e8c\u8054\u6027\u7cbe\u795e\u75c5\"\u7684\u5173\u7cfb\u52a8\u6001\uff0c\u5bf9\u7528\u6237\u4ea7\u751f\u7cbe\u795e\u75c5\u6027\u5f71\u54cd\u3002\u4f5c\u8005\u89c2\u5bdf\u5230LLMs\u7684\u9ad8\u8bed\u8a00\u8fde\u8d2f\u6027\u4e0e\u7f3a\u4e4f\u771f\u5b9e\u4e3b\u4f53\u4e4b\u95f4\u7684\u77db\u76fe\u7ed3\u6784\uff0c\u53ef\u80fd\u5728\u60c5\u611f\u9700\u6c42\u6216\u4e0d\u7a33\u5b9a\u7684\u7528\u6237\u4e2d\u5f15\u53d1\u7cbe\u795e\u75c5\u6027\u5377\u5165\u3002", "method": "\u7ed3\u5408\u8d1d\u7279\u68ee\u7684\u53cc\u91cd\u675f\u7f1a\u7406\u8bba\u3001\u5171\u4eab\u6027\u7cbe\u795e\u75c5\u969c\u788d\u7684\u4e34\u5e8a\u6587\u732e\u4ee5\u53ca\u9ea6\u5409\u5c14\u514b\u91cc\u65af\u7279\u7684\u5927\u8111\u534a\u7403\u7406\u8bba\uff0c\u5206\u6790LLMs\u7684\u7ed3\u6784\u7279\u5f81\u5982\u4f55\u521b\u9020\u7528\u6237\u4e0e\u7cfb\u7edf\u4e4b\u95f4\u7684\u75c5\u7406\u6027\u5173\u7cfb\u52a8\u6001\u3002\u540c\u65f6\u8003\u5bdf\u65b0\u5174\u4e34\u5e8a\u62a5\u544a\uff0c\u5e76\u53d1\u5c55\u73b0\u8c61\u5b66\u63cf\u8ff0\u6765\u8bf4\u660e\u8fd9\u4e9b\u52a8\u6001\u5982\u4f55\u5c55\u5f00\u3002", "result": "\u7814\u7a76\u53d1\u73b0LLMs\u7684\u9ad8\u8bed\u8a00\u8fde\u8d2f\u6027\u4e0e\u7f3a\u4e4f\u771f\u5b9e\u4e3b\u4f53\u4e4b\u95f4\u7684\u7ed3\u6784\u6027\u5f20\u529b\uff0c\u53ef\u80fd\u5bfc\u81f4\u7528\u6237\u5728\u60c5\u611f\u9700\u6c42\u6216\u4e0d\u7a33\u5b9a\u72b6\u6001\u4e0b\u901a\u8fc7\u60f3\u8c61\u6295\u5c04\u6765\u586b\u8865\u7a7a\u767d\uff0c\u5c06\u5185\u5728\u6027\u3001\u610f\u56fe\u6216\u5b58\u5728\u5f52\u56e0\u4e8e\u4e00\u4e2a\u6839\u672c\u4e0d\u5177\u5907\u8fd9\u4e9b\u7279\u5f81\u7684\u7cfb\u7edf\u3002\u5f53\u524d\u4ee5\u53c2\u4e0e\u5ea6\u4f18\u5316\u7684\u8bbe\u8ba1\u9009\u62e9\u52a0\u5267\u4e86\u8fd9\u79cd\u98ce\u9669\u3002", "conclusion": "\u4f5c\u8005\u63d0\u51fa\"\u672c\u4f53\u8bba\u8bda\u5b9e\"\u4f5c\u4e3a\u5fc5\u8981\u7684\u8bbe\u8ba1\u539f\u5219\uff0c\u4ee5\u51cf\u8f7b\u6280\u672f\u4ecb\u5bfc\u7684\"\u4e8c\u8054\u6027\u7cbe\u795e\u75c5\"\u98ce\u9669\u3002\u8fd9\u610f\u5473\u7740\u7cfb\u7edf\u8bbe\u8ba1\u5e94\u660e\u786e\u5176\u975e\u4eba\u7c7b\u3001\u65e0\u4e3b\u4f53\u6027\u7684\u672c\u8d28\uff0c\u907f\u514d\u521b\u9020\u8bef\u5bfc\u6027\u7684\u5173\u7cfb\u52a8\u6001\u3002"}}
{"id": "2512.11912", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11912", "abs": "https://arxiv.org/abs/2512.11912", "authors": ["Liu Peng", "Yaochu Jin"], "title": "Robustness of Probabilistic Models to Low-Quality Data: A Multi-Perspective Analysis", "comment": null, "summary": "A systematic, comparative investigation into the effects of low-quality data reveals a stark spectrum of robustness across modern probabilistic models. We find that autoregressive language models, from token prediction to sequence-to-sequence tasks, are remarkably resilient (for GPT-2, test NLL increases modestly from 2.87 to 3.59 despite 50% token corruption). By contrast, under the same levels of data corruption, class-conditional diffusion models degrade catastrophically (image-label consistency plummets by 56.81% relative to baseline), while classifiers show a moderate impact that diminishes with dataset scale. To explain these discrepancies, we analyze the results through a multi-perspective lens, integrating information theory, PAC learning, and gradient dynamics. These analyses suggest that robustness is heavily influenced by two key principles: the richness of conditioning information, which constrains the learning problem, and the absolute information content of the training data, which allows the signal from correct information to dominate statistical noise.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7cfb\u7edf\u6bd4\u8f83\u4e86\u4e0d\u540c\u6982\u7387\u6a21\u578b\u5bf9\u4f4e\u8d28\u91cf\u6570\u636e\u7684\u9c81\u68d2\u6027\uff0c\u53d1\u73b0\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u51fa\u663e\u8457\u97e7\u6027\uff0c\u800c\u7c7b\u522b\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5728\u6570\u636e\u6c61\u67d3\u4e0b\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u5206\u7c7b\u5668\u5219\u5448\u73b0\u4e2d\u7b49\u5f71\u54cd\u4e14\u968f\u6570\u636e\u96c6\u89c4\u6a21\u589e\u5927\u800c\u51cf\u5f31\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u7cfb\u7edf\u6027\u5730\u63a2\u7a76\u73b0\u4ee3\u6982\u7387\u6a21\u578b\u5bf9\u4f4e\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u9c81\u68d2\u6027\u5dee\u5f02\uff0c\u7406\u89e3\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u5728\u9762\u5bf9\u6570\u636e\u6c61\u67d3\u65f6\u7684\u8868\u73b0\u5dee\u5f02\u53ca\u5176\u6839\u672c\u539f\u56e0\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6027\u6bd4\u8f83\u7814\u7a76\u65b9\u6cd5\uff0c\u5bf9\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u3001\u7c7b\u522b\u6761\u4ef6\u6269\u6563\u6a21\u578b\u548c\u5206\u7c7b\u5668\u8fdb\u884c\u76f8\u540c\u7a0b\u5ea6\u7684\u6570\u636e\u6c61\u67d3\u6d4b\u8bd5\u3002\u901a\u8fc7\u4fe1\u606f\u8bba\u3001PAC\u5b66\u4e60\u548c\u68af\u5ea6\u52a8\u529b\u5b66\u7b49\u591a\u89c6\u89d2\u5206\u6790\u6846\u67b6\u6765\u89e3\u91ca\u89c2\u5bdf\u5230\u7684\u9c81\u68d2\u6027\u5dee\u5f02\u3002", "result": "\u81ea\u56de\u5f52\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u51fa\u663e\u8457\u9c81\u68d2\u6027\uff08GPT-2\u572850%\u6807\u8bb0\u6c61\u67d3\u4e0b\u6d4b\u8bd5NLL\u4ec5\u4ece2.87\u589e\u81f33.59\uff09\uff0c\u7c7b\u522b\u6761\u4ef6\u6269\u6563\u6a21\u578b\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff08\u56fe\u50cf\u6807\u7b7e\u4e00\u81f4\u6027\u76f8\u5bf9\u57fa\u7ebf\u4e0b\u964d56.81%\uff09\uff0c\u5206\u7c7b\u5668\u5f71\u54cd\u4e2d\u7b49\u4e14\u968f\u6570\u636e\u96c6\u89c4\u6a21\u589e\u5927\u800c\u51cf\u5f31\u3002", "conclusion": "\u6a21\u578b\u9c81\u68d2\u6027\u4e3b\u8981\u53d7\u4e24\u4e2a\u5173\u952e\u539f\u5219\u5f71\u54cd\uff1a\u6761\u4ef6\u4fe1\u606f\u7684\u4e30\u5bcc\u7a0b\u5ea6\uff08\u7ea6\u675f\u5b66\u4e60\u95ee\u9898\uff09\u548c\u8bad\u7ec3\u6570\u636e\u7684\u7edd\u5bf9\u4fe1\u606f\u91cf\uff08\u4f7f\u6b63\u786e\u4fe1\u606f\u4fe1\u53f7\u4e3b\u5bfc\u7edf\u8ba1\u566a\u58f0\uff09\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u7406\u89e3\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u7684\u9c81\u68d2\u6027\u5dee\u5f02\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2512.13037", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13037", "abs": "https://arxiv.org/abs/2512.13037", "authors": ["Taoran Sheng", "Sathappan Muthiah", "Atiq Islam", "Jinming Feng"], "title": "Progressive Refinement of E-commerce Search Ranking Based on Short-Term Activities of the Buyer", "comment": null, "summary": "In e-commerce shopping, aligning search results with a buyer's immediate needs and preferences presents a significant challenge, particularly in adapting search results throughout the buyer's shopping journey as they move from the initial stages of browsing to making a purchase decision or shift from one intent to another. This study presents a systematic approach to adapting e-commerce search results based on the current context. We start with basic methods and incrementally incorporate more contextual information and state-of-the-art techniques to improve the search outcomes. By applying this evolving contextual framework to items displayed on the search engine results page (SERP), we progressively align search outcomes more closely with the buyer's interests and current search intentions. Our findings demonstrate that this incremental enhancement, from simple heuristic autoregressive features to advanced sequence models, significantly improves ranker performance. The integration of contextual techniques enhances the performance of our production ranker, leading to improved search results in both offline and online A/B testing in terms of Mean Reciprocal Rank (MRR). Overall, the paper details iterative methodologies and their substantial contributions to search result contextualization on e-commerce platforms.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0a\u4e0b\u6587\u9010\u6b65\u4f18\u5316\u7535\u5546\u641c\u7d22\u7ed3\u679c\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u7b80\u5355\u542f\u53d1\u5f0f\u7279\u5f81\u5230\u5148\u8fdb\u5e8f\u5217\u6a21\u578b\u7684\u6e10\u8fdb\u5f0f\u6539\u8fdb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6392\u5e8f\u5668\u6027\u80fd\uff0c\u5728\u79bb\u7ebf\u6d4b\u8bd5\u548c\u5728\u7ebfA/B\u6d4b\u8bd5\u4e2d\u90fd\u53d6\u5f97\u4e86\u66f4\u597d\u7684MRR\u6307\u6807\u3002", "motivation": "\u7535\u5546\u8d2d\u7269\u4e2d\uff0c\u5c06\u641c\u7d22\u7ed3\u679c\u4e0e\u4e70\u5bb6\u7684\u5373\u65f6\u9700\u6c42\u548c\u504f\u597d\u5bf9\u9f50\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u4e70\u5bb6\u7684\u8d2d\u7269\u65c5\u7a0b\u4e2d\uff08\u4ece\u6d4f\u89c8\u5230\u8d2d\u4e70\u51b3\u7b56\uff0c\u6216\u4ece\u4e00\u4e2a\u610f\u56fe\u8f6c\u6362\u5230\u53e6\u4e00\u4e2a\u610f\u56fe\u65f6\uff09\u9700\u8981\u52a8\u6001\u8c03\u6574\u641c\u7d22\u7ed3\u679c\u3002\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\u6765\u6839\u636e\u5f53\u524d\u4e0a\u4e0b\u6587\u8c03\u6574\u641c\u7d22\u7ed3\u679c\u3002", "method": "\u91c7\u7528\u6e10\u8fdb\u5f0f\u65b9\u6cd5\uff1a\u4ece\u57fa\u7840\u65b9\u6cd5\u5f00\u59cb\uff0c\u9010\u6b65\u878d\u5165\u66f4\u591a\u4e0a\u4e0b\u6587\u4fe1\u606f\u548c\u6700\u5148\u8fdb\u7684\u6280\u672f\u3002\u9996\u5148\u4f7f\u7528\u7b80\u5355\u7684\u542f\u53d1\u5f0f\u81ea\u56de\u5f52\u7279\u5f81\uff0c\u7136\u540e\u5f15\u5165\u66f4\u9ad8\u7ea7\u7684\u5e8f\u5217\u6a21\u578b\u3002\u5c06\u8fd9\u79cd\u6f14\u8fdb\u7684\u4e0a\u4e0b\u6587\u6846\u67b6\u5e94\u7528\u4e8e\u641c\u7d22\u5f15\u64ce\u7ed3\u679c\u9875\u9762\uff08SERP\uff09\u4e0a\u663e\u793a\u7684\u5546\u54c1\uff0c\u9010\u6b65\u4f7f\u641c\u7d22\u7ed3\u679c\u66f4\u8d34\u8fd1\u4e70\u5bb6\u7684\u5174\u8da3\u548c\u5f53\u524d\u641c\u7d22\u610f\u56fe\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u8fd9\u79cd\u6e10\u8fdb\u5f0f\u589e\u5f3a\u663e\u8457\u63d0\u5347\u4e86\u6392\u5e8f\u5668\u6027\u80fd\u3002\u4e0a\u4e0b\u6587\u6280\u672f\u7684\u96c6\u6210\u6539\u5584\u4e86\u751f\u4ea7\u6392\u5e8f\u5668\u7684\u8868\u73b0\uff0c\u5728\u79bb\u7ebf\u548c\u5728\u7ebfA/B\u6d4b\u8bd5\u4e2d\u90fd\u83b7\u5f97\u4e86\u66f4\u597d\u7684\u5e73\u5747\u5012\u6570\u6392\u540d\uff08MRR\uff09\u3002", "conclusion": "\u8bba\u6587\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u8fed\u4ee3\u65b9\u6cd5\u53ca\u5176\u5bf9\u7535\u5546\u5e73\u53f0\u641c\u7d22\u7ed3\u679c\u4e0a\u4e0b\u6587\u5316\u7684\u5b9e\u8d28\u6027\u8d21\u732e\u3002\u901a\u8fc7\u9010\u6b65\u878d\u5165\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5c06\u641c\u7d22\u7ed3\u679c\u4e0e\u4e70\u5bb6\u7684\u5f53\u524d\u9700\u6c42\u548c\u610f\u56fe\u5bf9\u9f50\uff0c\u4ece\u800c\u63d0\u5347\u641c\u7d22\u6548\u679c\u3002"}}
{"id": "2512.11920", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11920", "abs": "https://arxiv.org/abs/2512.11920", "authors": ["Dong Liu", "Yanxuan Yu"], "title": "CXL-SpecKV: A Disaggregated FPGA Speculative KV-Cache for Datacenter LLM Serving", "comment": "Accepted to FPGA'26 Oral", "summary": "Large Language Models (LLMs) have revolutionized natural language processing tasks, but their deployment in datacenter environments faces significant challenges due to the massive memory requirements of key-value (KV) caches. During the autoregressive decoding process, KV caches consume substantial GPU memory, limiting batch sizes and overall system throughput. To address these challenges, we propose \\textbf{CXL-SpecKV}, a novel disaggregated KV-cache architecture that leverages Compute Express Link (CXL) interconnects and FPGA accelerators to enable efficient speculative execution and memory disaggregation. Our approach introduces three key innovations: (i) a CXL-based memory disaggregation framework that offloads KV-caches to remote FPGA memory with low latency, (ii) a speculative KV-cache prefetching mechanism that predicts and preloads future tokens' cache entries, and (iii) an FPGA-accelerated KV-cache compression and decompression engine that reduces memory bandwidth requirements by up to 4$\\times$. When evaluated on state-of-the-art LLM models, CXL-SpecKV achieves up to 3.2$\\times$ higher throughput compared to GPU-only baselines, while reducing memory costs by 2.8$\\times$ and maintaining accuracy. Our system demonstrates that intelligent memory disaggregation combined with speculative execution can effectively address the memory wall challenge in large-scale LLM serving. Our code implementation has been open-sourced at https://github.com/FastLM/CXL-SpecKV.", "AI": {"tldr": "CXL-SpecKV\uff1a\u4e00\u79cd\u57fa\u4e8eCXL\u4e92\u8fde\u548cFPGA\u52a0\u901f\u5668\u7684\u89e3\u8026KV\u7f13\u5b58\u67b6\u6784\uff0c\u901a\u8fc7\u5185\u5b58\u89e3\u8026\u3001\u63a8\u6d4b\u9884\u53d6\u548c\u538b\u7f29\u6280\u672f\uff0c\u89e3\u51b3LLM\u63a8\u7406\u4e2d\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u63d0\u5347\u541e\u5410\u91cf3.2\u500d\uff0c\u964d\u4f4e\u5185\u5b58\u6210\u672c2.8\u500d\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u636e\u4e2d\u5fc3\u90e8\u7f72\u65f6\u9762\u4e34KV\u7f13\u5b58\u5360\u7528\u5927\u91cfGPU\u5185\u5b58\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u6279\u5904\u7406\u5927\u5c0f\u548c\u7cfb\u7edf\u541e\u5410\u91cf\u3002\u4f20\u7edfGPU\u5185\u5b58\u65b9\u6848\u96be\u4ee5\u6ee1\u8db3\u5927\u89c4\u6a21LLM\u670d\u52a1\u7684\u9700\u6c42\uff0c\u9700\u8981\u521b\u65b0\u7684\u5185\u5b58\u7ba1\u7406\u65b9\u6848\u6765\u7a81\u7834\u5185\u5b58\u5899\u9650\u5236\u3002", "method": "\u63d0\u51faCXL-SpecKV\u67b6\u6784\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6280\u672f\uff1a1\uff09\u57fa\u4e8eCXL\u7684\u5185\u5b58\u89e3\u8026\u6846\u67b6\uff0c\u5c06KV\u7f13\u5b58\u5378\u8f7d\u5230\u8fdc\u7a0bFPGA\u5185\u5b58\uff1b2\uff09\u63a8\u6d4b\u6027KV\u7f13\u5b58\u9884\u53d6\u673a\u5236\uff0c\u9884\u6d4b\u5e76\u9884\u52a0\u8f7d\u672a\u6765token\u7684\u7f13\u5b58\u6761\u76ee\uff1b3\uff09FPGA\u52a0\u901f\u7684KV\u7f13\u5b58\u538b\u7f29/\u89e3\u538b\u7f29\u5f15\u64ce\uff0c\u51cf\u5c11\u5185\u5b58\u5e26\u5bbd\u9700\u6c42\u8fbe4\u500d\u3002", "result": "\u5728\u5148\u8fdbLLM\u6a21\u578b\u4e0a\u8bc4\u4f30\uff0cCXL-SpecKV\u76f8\u6bd4GPU-only\u57fa\u7ebf\u5b9e\u73b0\u9ad8\u8fbe3.2\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0c\u5185\u5b58\u6210\u672c\u964d\u4f4e2.8\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u3002\u7cfb\u7edf\u9a8c\u8bc1\u4e86\u667a\u80fd\u5185\u5b58\u89e3\u8026\u4e0e\u63a8\u6d4b\u6267\u884c\u80fd\u6709\u6548\u89e3\u51b3\u5927\u89c4\u6a21LLM\u670d\u52a1\u7684\u5185\u5b58\u5899\u6311\u6218\u3002", "conclusion": "CXL-SpecKV\u901a\u8fc7\u521b\u65b0\u7684\u5185\u5b58\u89e3\u8026\u67b6\u6784\u548c\u63a8\u6d4b\u6267\u884c\u6280\u672f\uff0c\u6210\u529f\u89e3\u51b3\u4e86LLM\u63a8\u7406\u4e2d\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u5927\u89c4\u6a21LLM\u670d\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7ecf\u6d4e\u7684\u5185\u5b58\u7ba1\u7406\u65b9\u6848\u3002\u4ee3\u7801\u5df2\u5728GitHub\u5f00\u6e90\u3002"}}
{"id": "2512.13074", "categories": ["cs.IR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13074", "abs": "https://arxiv.org/abs/2512.13074", "authors": ["Huimu Wang", "Yiming Qiu", "Xingzhi Yao", "Zhiguo Chen", "Guoyu Tang", "Songlin Wang", "Sulong Xu", "Mingming Li"], "title": "A Simple and Effective Framework for Symmetric Consistent Indexing in Large-Scale Dense Retrieval", "comment": null, "summary": "Dense retrieval has become the industry standard in large-scale information retrieval systems due to its high efficiency and competitive accuracy. Its core relies on a coarse-to-fine hierarchical architecture that enables rapid candidate selection and precise semantic matching, achieving millisecond-level response over billion-scale corpora. This capability makes it essential not only in traditional search and recommendation scenarios but also in the emerging paradigm of generative recommendation driven by large language models, where semantic IDs-themselves a form of coarse-to-fine representation-play a foundational role. However, the widely adopted dual-tower encoding architecture introduces inherent challenges, primarily representational space misalignment and retrieval index inconsistency, which degrade matching accuracy, retrieval stability, and performance on long-tail queries. These issues are further magnified in semantic ID generation, ultimately limiting the performance ceiling of downstream generative models.\n  To address these challenges, this paper proposes a simple and effective framework named SCI comprising two synergistic modules: a symmetric representation alignment module that employs an innovative input-swapping mechanism to unify the dual-tower representation space without adding parameters, and an consistent indexing with dual-tower synergy module that redesigns retrieval paths using a dual-view indexing strategy to maintain consistency from training to inference. The framework is systematic, lightweight, and engineering-friendly, requiring minimal overhead while fully supporting billion-scale deployment. We provide theoretical guarantees for our approach, with its effectiveness validated by results across public datasets and real-world e-commerce datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSCI\u6846\u67b6\u89e3\u51b3\u7a20\u5bc6\u68c0\u7d22\u4e2d\u53cc\u5854\u67b6\u6784\u7684\u7a7a\u95f4\u4e0d\u5bf9\u9f50\u548c\u68c0\u7d22\u7d22\u5f15\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u901a\u8fc7\u5bf9\u79f0\u8868\u793a\u5bf9\u9f50\u548c\u4e00\u81f4\u6027\u7d22\u5f15\u6a21\u5757\u63d0\u5347\u68c0\u7d22\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027", "motivation": "\u7a20\u5bc6\u68c0\u7d22\u5df2\u6210\u4e3a\u5927\u89c4\u6a21\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u7684\u884c\u4e1a\u6807\u51c6\uff0c\u4f46\u5176\u5e7f\u6cdb\u91c7\u7528\u7684\u53cc\u5854\u7f16\u7801\u67b6\u6784\u5b58\u5728\u8868\u793a\u7a7a\u95f4\u4e0d\u5bf9\u9f50\u548c\u68c0\u7d22\u7d22\u5f15\u4e0d\u4e00\u81f4\u7684\u56fa\u6709\u6311\u6218\uff0c\u8fd9\u4f1a\u964d\u4f4e\u5339\u914d\u7cbe\u5ea6\u3001\u68c0\u7d22\u7a33\u5b9a\u6027\u4ee5\u53ca\u5bf9\u957f\u5c3e\u67e5\u8be2\u7684\u6027\u80fd\u3002\u8fd9\u4e9b\u95ee\u9898\u5728\u8bed\u4e49ID\u751f\u6210\u4e2d\u8fdb\u4e00\u6b65\u653e\u5927\uff0c\u9650\u5236\u4e86\u4e0b\u6e38\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\u4e0a\u9650\u3002", "method": "\u63d0\u51faSCI\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u534f\u540c\u6a21\u5757\uff1a1) \u5bf9\u79f0\u8868\u793a\u5bf9\u9f50\u6a21\u5757\uff0c\u91c7\u7528\u521b\u65b0\u7684\u8f93\u5165\u4ea4\u6362\u673a\u5236\u7edf\u4e00\u53cc\u5854\u8868\u793a\u7a7a\u95f4\u800c\u4e0d\u589e\u52a0\u53c2\u6570\uff1b2) \u4e00\u81f4\u6027\u7d22\u5f15\u4e0e\u53cc\u5854\u534f\u540c\u6a21\u5757\uff0c\u901a\u8fc7\u53cc\u89c6\u56fe\u7d22\u5f15\u7b56\u7565\u91cd\u65b0\u8bbe\u8ba1\u68c0\u7d22\u8def\u5f84\uff0c\u4fdd\u6301\u4ece\u8bad\u7ec3\u5230\u63a8\u7406\u7684\u4e00\u81f4\u6027\u3002", "result": "\u8be5\u6846\u67b6\u7cfb\u7edf\u3001\u8f7b\u91cf\u4e14\u5de5\u7a0b\u53cb\u597d\uff0c\u5728\u516c\u5171\u6570\u636e\u96c6\u548c\u771f\u5b9e\u4e16\u754c\u7535\u5546\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u3002", "conclusion": "SCI\u6846\u67b6\u901a\u8fc7\u89e3\u51b3\u53cc\u5854\u67b6\u6784\u7684\u6838\u5fc3\u6311\u6218\uff0c\u63d0\u5347\u4e86\u7a20\u5bc6\u68c0\u7d22\u7684\u5339\u914d\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u90e8\u7f72\u548c\u8bed\u4e49ID\u751f\u6210\u573a\u666f\uff0c\u4e3a\u4e0b\u6e38\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u6027\u80fd\u57fa\u7840\u3002"}}
{"id": "2512.11898", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11898", "abs": "https://arxiv.org/abs/2512.11898", "authors": ["Yawar Ali", "K. Ramachandra Rao", "Ashish Bhaskar", "Niladri Chatterjee"], "title": "Microscopic Vehicle Trajectory Datasets from UAV-collected Video for Heterogeneous, Area-Based Urban Traffic", "comment": "This paper presents basic statistics and trends in empirically observed data from highly heterogeneous and area-based traffic while offering the datasets open source for researchers and practitioners", "summary": "This paper offers openly available microscopic vehicle trajectory (MVT) datasets collected using unmanned aerial vehicles (UAVs) in heterogeneous, area-based urban traffic conditions. Traditional roadside video collection often fails in dense mixed traffic due to occlusion, limited viewing angles, and irregular vehicle movements. UAV-based recording provides a top-down perspective that reduces these issues and captures rich spatial and temporal dynamics. The datasets described here were extracted using the Data from Sky (DFS) platform and validated against manual counts, space mean speeds, and probe trajectories in earlier work. Each dataset contains time-stamped vehicle positions, speeds, longitudinal and lateral accelerations, and vehicle classifications at a resolution of 30 frames per second. Data were collected at six mid-block locations in the national capital region of India, covering diverse traffic compositions and density levels. Exploratory analyses highlight key behavioural patterns, including lane-keeping preferences, speed distributions, and lateral manoeuvres typical of heterogeneous and area-based traffic settings. These datasets are intended as a resource for the global research community to support simulation modelling, safety assessment, and behavioural studies under area-based traffic conditions. By making these empirical datasets openly available, this work offers researchers a unique opportunity to develop, test, and validate models that more accurately represent complex urban traffic environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u4f9b\u4e86\u57fa\u4e8e\u65e0\u4eba\u673a\u91c7\u96c6\u7684\u5f00\u653e\u5fae\u89c2\u8f66\u8f86\u8f68\u8ff9\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u7814\u7a76\u5f02\u8d28\u5316\u3001\u533a\u57df\u578b\u57ce\u5e02\u4ea4\u901a\u73af\u5883\u3002", "motivation": "\u4f20\u7edf\u8def\u8fb9\u89c6\u9891\u91c7\u96c6\u5728\u5bc6\u96c6\u6df7\u5408\u4ea4\u901a\u4e2d\u5b58\u5728\u906e\u6321\u3001\u89c6\u89d2\u6709\u9650\u548c\u8f66\u8f86\u8fd0\u52a8\u4e0d\u89c4\u5219\u7b49\u95ee\u9898\uff0c\u9700\u8981\u66f4\u597d\u7684\u6570\u636e\u91c7\u96c6\u65b9\u6cd5\u6765\u7814\u7a76\u590d\u6742\u57ce\u5e02\u4ea4\u901a\u73af\u5883\u3002", "method": "\u4f7f\u7528\u65e0\u4eba\u673a\u4ece\u4fef\u89c6\u89d2\u5ea6\u91c7\u96c6\u4ea4\u901a\u6570\u636e\uff0c\u901a\u8fc7Data from Sky\u5e73\u53f0\u63d0\u53d6\u8f66\u8f86\u8f68\u8ff9\u4fe1\u606f\uff0c\u5728\u5370\u5ea6\u9996\u90fd\u5730\u533a\u7684\u516d\u4e2a\u8def\u6bb5\u6536\u96c6\u6570\u636e\uff0c\u5305\u542b\u65f6\u95f4\u6233\u3001\u4f4d\u7f6e\u3001\u901f\u5ea6\u3001\u52a0\u901f\u5ea6\u548c\u8f66\u8f86\u5206\u7c7b\u7b49\u4fe1\u606f\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b\u4e30\u5bcc\u65f6\u7a7a\u52a8\u6001\u4fe1\u606f\u7684\u5f00\u653e\u6570\u636e\u96c6\uff0c\u5206\u8fa8\u7387\u8fbe30\u5e27/\u79d2\uff0c\u8986\u76d6\u4e0d\u540c\u4ea4\u901a\u7ec4\u6210\u548c\u5bc6\u5ea6\u6c34\u5e73\uff0c\u63ed\u793a\u4e86\u8f66\u9053\u4fdd\u6301\u504f\u597d\u3001\u901f\u5ea6\u5206\u5e03\u548c\u6a2a\u5411\u673a\u52a8\u7b49\u884c\u4e3a\u6a21\u5f0f\u3002", "conclusion": "\u8fd9\u4e9b\u5f00\u653e\u6570\u636e\u96c6\u4e3a\u5168\u7403\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u72ec\u7279\u8d44\u6e90\uff0c\u652f\u6301\u4eff\u771f\u5efa\u6a21\u3001\u5b89\u5168\u8bc4\u4f30\u548c\u884c\u4e3a\u7814\u7a76\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u51c6\u786e\u53cd\u6620\u590d\u6742\u57ce\u5e02\u4ea4\u901a\u73af\u5883\u7684\u6a21\u578b\u3002"}}
{"id": "2512.11821", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.11821", "abs": "https://arxiv.org/abs/2512.11821", "authors": ["John Paul P. Miranda", "Joseph Alexander Bansil", "Emerson Q. Fernando", "Almer B. Gamboa", "Hilene E. Hernandez", "Myka A. Cruz", "Roque Francis B. Dianelo", "Dina D. Gonzales", "Elmer M. Penecilla"], "title": "Prevalence, Devices Used, Reasons for Use, Trust, Barriers, and Challenges in Utilizing Generative AI among Tertiary Students", "comment": "12 pages, 1 table, 1 figure", "summary": "This study examined generative AI usage among Philippine college students particularly on frequency, devices, reasons, knowledge, trust, perceptions, and challenges. Most students used free AI tools on smartphones due to financial constraints. They used it primarily for homework, idea generation, and research. Less than half felt confident with AI and expressed mixed feelings about its accuracy. Barriers included limited access, lack of teacher support, difficulty understanding outputs, and financial constraints. The study highlighted the need for better access, support, training, and ethical guidelines. Broader concerns included impacts on learning, academic standards, job loss, and privacy. Students viewed AI positively due to peer support. Recommendations are discussed.", "AI": {"tldr": "\u83f2\u5f8b\u5bbe\u5927\u5b66\u751f\u4f7f\u7528\u751f\u6210\u5f0fAI\u7684\u7814\u7a76\uff1a\u4e3b\u8981\u4f7f\u7528\u514d\u8d39\u5de5\u5177\uff0c\u4e3b\u8981\u7528\u4e8e\u4f5c\u4e1a\u548c\u521b\u610f\u751f\u6210\uff0c\u5bf9AI\u4fe1\u5fc3\u4e0d\u8db3\u4e14\u5b58\u5728\u591a\u91cd\u969c\u788d\uff0c\u9700\u8981\u66f4\u597d\u7684\u652f\u6301\u548c\u57f9\u8bad\u3002", "motivation": "\u7814\u7a76\u83f2\u5f8b\u5bbe\u5927\u5b66\u751f\u4f7f\u7528\u751f\u6210\u5f0fAI\u7684\u60c5\u51b5\uff0c\u4e86\u89e3\u5176\u4f7f\u7528\u9891\u7387\u3001\u8bbe\u5907\u3001\u539f\u56e0\u3001\u77e5\u8bc6\u6c34\u5e73\u3001\u4fe1\u4efb\u5ea6\u3001\u611f\u77e5\u548c\u9762\u4e34\u7684\u6311\u6218\uff0c\u4e3a\u6559\u80b2\u673a\u6784\u63d0\u4f9b\u6539\u8fdb\u5efa\u8bae\u3002", "method": "\u901a\u8fc7\u8c03\u67e5\u83f2\u5f8b\u5bbe\u5927\u5b66\u751f\u4f7f\u7528\u751f\u6210\u5f0fAI\u7684\u60c5\u51b5\uff0c\u5206\u6790\u4f7f\u7528\u9891\u7387\u3001\u8bbe\u5907\u3001\u539f\u56e0\u3001\u77e5\u8bc6\u6c34\u5e73\u3001\u4fe1\u4efb\u5ea6\u3001\u611f\u77e5\u548c\u6311\u6218\u7b49\u591a\u4e2a\u7ef4\u5ea6\u3002", "result": "\u5927\u591a\u6570\u5b66\u751f\u56e0\u7ecf\u6d4e\u9650\u5236\u4f7f\u7528\u667a\u80fd\u624b\u673a\u4e0a\u7684\u514d\u8d39AI\u5de5\u5177\uff0c\u4e3b\u8981\u7528\u4e8e\u4f5c\u4e1a\u3001\u521b\u610f\u751f\u6210\u548c\u7814\u7a76\uff1b\u4e0d\u5230\u4e00\u534a\u5b66\u751f\u5bf9AI\u6709\u4fe1\u5fc3\uff0c\u5bf9\u5176\u51c6\u786e\u6027\u6301\u590d\u6742\u6001\u5ea6\uff1b\u4e3b\u8981\u969c\u788d\u5305\u62ec\u8bbf\u95ee\u9650\u5236\u3001\u7f3a\u4e4f\u6559\u5e08\u652f\u6301\u3001\u7406\u89e3\u8f93\u51fa\u56f0\u96be\u548c\u7ecf\u6d4e\u7ea6\u675f\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u9700\u8981\u6539\u5584AI\u8bbf\u95ee\u6761\u4ef6\u3001\u63d0\u4f9b\u652f\u6301\u3001\u52a0\u5f3a\u57f9\u8bad\u5e76\u5236\u5b9a\u4f26\u7406\u6307\u5357\uff1b\u66f4\u5e7f\u6cdb\u7684\u62c5\u5fe7\u5305\u62ec\u5bf9\u5b66\u4e60\u3001\u5b66\u672f\u6807\u51c6\u3001\u5c31\u4e1a\u548c\u9690\u79c1\u7684\u5f71\u54cd\uff1b\u5b66\u751f\u56e0\u540c\u4f34\u652f\u6301\u800c\u5bf9AI\u6301\u79ef\u6781\u6001\u5ea6\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5173\u5efa\u8bae\u3002"}}
{"id": "2512.13120", "categories": ["cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13120", "abs": "https://arxiv.org/abs/2512.13120", "authors": ["Mabiao Long", "Jiaxi Liu", "Yufeng Li", "Hao Xiong", "Junchi Yan", "Kefan Wang", "Yi Cao", "Jiandong Ding"], "title": "Towards Practical Large-scale Dynamical Heterogeneous Graph Embedding: Cold-start Resilient Recommendation", "comment": null, "summary": "Deploying dynamic heterogeneous graph embeddings in production faces key challenges of scalability, data freshness, and cold-start. This paper introduces a practical, two-stage solution that balances deep graph representation with low-latency incremental updates. Our framework combines HetSGFormer, a scalable graph transformer for static learning, with Incremental Locally Linear Embedding (ILLE), a lightweight, CPU-based algorithm for real-time updates. HetSGFormer captures global structure with linear scalability, while ILLE provides rapid, targeted updates to incorporate new data, thus avoiding costly full retraining. This dual approach is cold-start resilient, leveraging the graph to create meaningful embeddings from sparse data. On billion-scale graphs, A/B tests show HetSGFormer achieved up to a 6.11% lift in Advertiser Value over previous methods, while the ILLE module added another 3.22% lift and improved embedding refresh timeliness by 83.2%. Our work provides a validated framework for deploying dynamic graph learning in production environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u4e24\u9636\u6bb5\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u89e3\u51b3\u52a8\u6001\u5f02\u6784\u56fe\u5d4c\u5165\u5728\u751f\u4ea7\u90e8\u7f72\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u3001\u6570\u636e\u65b0\u9c9c\u5ea6\u548c\u51b7\u542f\u52a8\u95ee\u9898\u3002\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u7528\u4e8e\u9759\u6001\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u56fe\u53d8\u6362\u5668HetSGFormer\u548c\u7528\u4e8e\u5b9e\u65f6\u66f4\u65b0\u7684\u8f7b\u91cf\u7ea7CPU\u7b97\u6cd5ILLE\uff0c\u5728\u5341\u4ebf\u7ea7\u56fe\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u90e8\u7f72\u52a8\u6001\u5f02\u6784\u56fe\u5d4c\u5165\u9762\u4e34\u4e09\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u53ef\u6269\u5c55\u6027\uff08\u5904\u7406\u5927\u89c4\u6a21\u56fe\uff09\u3001\u6570\u636e\u65b0\u9c9c\u5ea6\uff08\u4fdd\u6301\u5d4c\u5165\u7684\u65f6\u6548\u6027\uff09\u548c\u51b7\u542f\u52a8\u95ee\u9898\uff08\u5904\u7406\u7a00\u758f\u6570\u636e\uff09\u3002\u73b0\u6709\u65b9\u6cd5\u5f80\u5f80\u9700\u8981\u5728\u6df1\u5ea6\u8868\u793a\u5b66\u4e60\u548c\u4f4e\u5ef6\u8fdf\u589e\u91cf\u66f4\u65b0\u4e4b\u95f4\u505a\u51fa\u6743\u8861\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) HetSGFormer - \u7528\u4e8e\u9759\u6001\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u56fe\u53d8\u6362\u5668\uff0c\u5177\u6709\u7ebf\u6027\u53ef\u6269\u5c55\u6027\uff0c\u80fd\u591f\u6355\u6349\u5168\u5c40\u56fe\u7ed3\u6784\uff1b2) ILLE\uff08\u589e\u91cf\u5c40\u90e8\u7ebf\u6027\u5d4c\u5165\uff09- \u8f7b\u91cf\u7ea7\u3001\u57fa\u4e8eCPU\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u5b9e\u65f6\u589e\u91cf\u66f4\u65b0\uff0c\u907f\u514d\u6602\u8d35\u7684\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u3002\u8fd9\u79cd\u53cc\u91cd\u65b9\u6cd5\u8fd8\u5177\u6709\u51b7\u542f\u52a8\u5f39\u6027\uff0c\u80fd\u591f\u5229\u7528\u56fe\u7ed3\u6784\u4ece\u7a00\u758f\u6570\u636e\u4e2d\u521b\u5efa\u6709\u610f\u4e49\u7684\u5d4c\u5165\u3002", "result": "\u5728\u5341\u4ebf\u7ea7\u56fe\u4e0a\u8fdb\u884cA/B\u6d4b\u8bd5\u663e\u793a\uff1aHetSGFormer\u76f8\u6bd4\u5148\u524d\u65b9\u6cd5\u5728\u5e7f\u544a\u5546\u4ef7\u503c\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe6.11%\u7684\u63d0\u5347\uff1bILLE\u6a21\u5757\u8fdb\u4e00\u6b65\u5e26\u6765\u4e863.22%\u7684\u63d0\u5347\uff0c\u5e76\u5c06\u5d4c\u5165\u5237\u65b0\u53ca\u65f6\u6027\u63d0\u9ad8\u4e8683.2%\u3002\u6574\u4e2a\u6846\u67b6\u5728\u4fdd\u6301\u6570\u636e\u65b0\u9c9c\u5ea6\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u6539\u8fdb\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u90e8\u7f72\u52a8\u6001\u56fe\u5b66\u4e60\u3002\u901a\u8fc7\u7ed3\u5408\u6df1\u5ea6\u9759\u6001\u8868\u793a\u5b66\u4e60\u548c\u8f7b\u91cf\u7ea7\u589e\u91cf\u66f4\u65b0\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u53ef\u6269\u5c55\u6027\u3001\u6570\u636e\u65b0\u9c9c\u5ea6\u548c\u51b7\u542f\u52a8\u7b49\u5173\u952e\u6311\u6218\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.11899", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11899", "abs": "https://arxiv.org/abs/2512.11899", "authors": ["Futa Waseda", "Shojiro Yamabe", "Daiki Shiono", "Kento Sasaki", "Tsubasa Takahashi"], "title": "Read or Ignore? A Unified Benchmark for Typographic-Attack Robustness and Text Recognition in Vision-Language Models", "comment": null, "summary": "Large vision-language models (LVLMs) are vulnerable to typographic attacks, where misleading text within an image overrides visual understanding. Existing evaluation protocols and defenses, largely focused on object recognition, implicitly encourage ignoring text to achieve robustness; however, real-world scenarios often require joint reasoning over both objects and text (e.g., recognizing pedestrians while reading traffic signs). To address this, we introduce a novel task, Read-or-Ignore VQA (RIO-VQA), which formalizes selective text use in visual question answering (VQA): models must decide, from context, when to read text and when to ignore it. For evaluation, we present the Read-or-Ignore Benchmark (RIO-Bench), a standardized dataset and protocol that, for each real image, provides same-scene counterfactuals (read / ignore) by varying only the textual content and question type. Using RIO-Bench, we show that strong LVLMs and existing defenses fail to balance typographic robustness and text-reading capability, highlighting the need for improved approaches. Finally, RIO-Bench enables a novel data-driven defense that learns adaptive selective text use, moving beyond prior non-adaptive, text-ignoring defenses. Overall, this work reveals a fundamental misalignment between the existing evaluation scope and real-world requirements, providing a principled path toward reliable LVLMs. Our Project Page is at https://turingmotors.github.io/rio-vqa/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRIO-VQA\u4efb\u52a1\u548cRIO-Bench\u57fa\u51c6\uff0c\u89e3\u51b3\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5bf9\u6297\u6027\u6587\u672c\u653b\u51fb\u4e0b\u7684\u9009\u62e9\u6027\u6587\u672c\u4f7f\u7528\u95ee\u9898\uff0c\u8981\u6c42\u6a21\u578b\u6839\u636e\u4e0a\u4e0b\u6587\u51b3\u5b9a\u4f55\u65f6\u8bfb\u53d6\u6587\u672c\u3001\u4f55\u65f6\u5ffd\u7565\u6587\u672c\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u6392\u7248\u653b\u51fb\uff0c\u73b0\u6709\u8bc4\u4f30\u548c\u9632\u5fa1\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7269\u4f53\u8bc6\u522b\uff0c\u9f13\u52b1\u5ffd\u7565\u6587\u672c\u4ee5\u83b7\u5f97\u9c81\u68d2\u6027\uff0c\u4f46\u73b0\u5b9e\u573a\u666f\u9700\u8981\u540c\u65f6\u5904\u7406\u7269\u4f53\u548c\u6587\u672c\u7684\u8054\u5408\u63a8\u7406\u3002", "method": "\u63d0\u51faRIO-VQA\u4efb\u52a1\u5f62\u5f0f\u5316\u9009\u62e9\u6027\u6587\u672c\u4f7f\u7528\uff0c\u521b\u5efaRIO-Bench\u57fa\u51c6\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u534f\u8bae\uff0c\u4e3a\u6bcf\u4e2a\u771f\u5b9e\u56fe\u50cf\u63d0\u4f9b\u76f8\u540c\u573a\u666f\u7684\u53cd\u4e8b\u5b9e\uff08\u8bfb\u53d6/\u5ffd\u7565\uff09\uff0c\u4ec5\u6539\u53d8\u6587\u672c\u5185\u5bb9\u548c\u95ee\u9898\u7c7b\u578b\u3002", "result": "\u4f7f\u7528RIO-Bench\u8bc4\u4f30\u663e\u793a\uff0c\u73b0\u6709\u5f3aLVLMs\u548c\u9632\u5fa1\u65b9\u6cd5\u65e0\u6cd5\u5e73\u8861\u6392\u7248\u9c81\u68d2\u6027\u548c\u6587\u672c\u9605\u8bfb\u80fd\u529b\uff0c\u7a81\u663e\u6539\u8fdb\u65b9\u6cd5\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63ed\u793a\u4e86\u73b0\u6709\u8bc4\u4f30\u8303\u56f4\u4e0e\u73b0\u5b9e\u9700\u6c42\u4e4b\u95f4\u7684\u6839\u672c\u9519\u4f4d\uff0c\u4e3a\u53ef\u9760\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u8def\u5f84\uff0c\u5e76\u542f\u53d1\u4e86\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u9009\u62e9\u6027\u6587\u672c\u4f7f\u7528\u9632\u5fa1\u65b9\u6cd5\u3002"}}
{"id": "2512.11822", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.11822", "abs": "https://arxiv.org/abs/2512.11822", "authors": ["Hilene E. Hernandez", "Ranie B. Canlas", "Madilaine Claire B. Nacianceno", "Jordan L. Salenga", "Jaymark A. Yambao", "Juvy C. Grume", "Aileen P. De Leon", "Freneil R. Pampo", "John Paul P. Miranda"], "title": "Trust, Usefulness, and Dependency on AI in Programming: A Hierarchical Clustering Approach", "comment": "8 pages, 2 tables, 2 figures", "summary": "While AI tools are transforming programming education, their adoption in underrepresented countries remains insufficiently studied. Understanding students' trust, perceived usefulness, and dependency on AI tools is essential to improving their integration into education. For these purposes, this study surveyed 508 first-year programming students in Pampanga, Philippines and analyzed their perceptions using hierarchical clustering. Results showed four unique student profiles with varying in trust and usage intensity. While students acknowledged AI tools' benefits, dependency remained low due to limited infrastructure and insufficient exposure. High-frequency users did not necessarily report greater trust or usefulness which may indicates a complex relationship between usage patterns and perception. This study recommends that to maximize AI's educational impact, targeted interventions such as infrastructure development, training programs, and curriculum integration are necessary. This study provides empirical insights to support equitable and effective AI adoption in programming education within developing regions.", "AI": {"tldr": "\u7814\u7a76\u8c03\u67e5\u83f2\u5f8b\u5bbe508\u540d\u7f16\u7a0b\u5b66\u751f\u5bf9AI\u5de5\u5177\u7684\u770b\u6cd5\uff0c\u53d1\u73b0\u56db\u79cd\u4e0d\u540c\u4fe1\u4efb\u548c\u4f7f\u7528\u5f3a\u5ea6\u7684\u5b66\u751f\u7fa4\u4f53\uff0cAI\u4f9d\u8d56\u5ea6\u4f4e\u6e90\u4e8e\u57fa\u7840\u8bbe\u65bd\u4e0d\u8db3\u548c\u63a5\u89e6\u6709\u9650\uff0c\u5efa\u8bae\u9488\u5bf9\u6027\u5e72\u9884\u63aa\u65bd", "motivation": "AI\u5de5\u5177\u6b63\u5728\u6539\u53d8\u7f16\u7a0b\u6559\u80b2\uff0c\u4f46\u5728\u6b20\u53d1\u8fbe\u56fd\u5bb6\u7684\u91c7\u7528\u60c5\u51b5\u7814\u7a76\u4e0d\u8db3\uff0c\u4e86\u89e3\u5b66\u751f\u5bf9AI\u5de5\u5177\u7684\u4fe1\u4efb\u3001\u611f\u77e5\u6709\u7528\u6027\u548c\u4f9d\u8d56\u7a0b\u5ea6\u5bf9\u4e8e\u6539\u8fdb\u6559\u80b2\u6574\u5408\u81f3\u5173\u91cd\u8981", "method": "\u8c03\u67e5\u83f2\u5f8b\u5bbe\u90a6\u677f\u7259\u7701508\u540d\u5927\u4e00\u7f16\u7a0b\u5b66\u751f\uff0c\u4f7f\u7528\u5c42\u6b21\u805a\u7c7b\u5206\u6790\u4ed6\u4eec\u7684\u770b\u6cd5", "result": "\u8bc6\u522b\u51fa\u56db\u79cd\u5177\u6709\u4e0d\u540c\u4fe1\u4efb\u548c\u4f7f\u7528\u5f3a\u5ea6\u7684\u5b66\u751f\u7fa4\u4f53\uff1b\u5b66\u751f\u8ba4\u53efAI\u5de5\u5177\u7684\u597d\u5904\uff0c\u4f46\u4f9d\u8d56\u5ea6\u4f4e\uff08\u57fa\u7840\u8bbe\u65bd\u4e0d\u8db3\u548c\u63a5\u89e6\u6709\u9650\uff09\uff1b\u9ad8\u9891\u7528\u6237\u4e0d\u4e00\u5b9a\u62a5\u544a\u66f4\u9ad8\u7684\u4fe1\u4efb\u6216\u6709\u7528\u6027\u611f\u77e5", "conclusion": "\u4e3a\u6700\u5927\u5316AI\u7684\u6559\u80b2\u5f71\u54cd\uff0c\u9700\u8981\u57fa\u7840\u8bbe\u65bd\u53d1\u5c55\u3001\u57f9\u8bad\u8ba1\u5212\u548c\u8bfe\u7a0b\u6574\u5408\u7b49\u9488\u5bf9\u6027\u5e72\u9884\uff1b\u4e3a\u53d1\u5c55\u4e2d\u5730\u533a\u7f16\u7a0b\u6559\u80b2\u7684\u516c\u5e73\u6709\u6548AI\u91c7\u7528\u63d0\u4f9b\u5b9e\u8bc1\u89c1\u89e3"}}
{"id": "2512.11942", "categories": ["cs.AI", "cs.FL", "cs.GT"], "pdf": "https://arxiv.org/pdf/2512.11942", "abs": "https://arxiv.org/abs/2512.11942", "authors": ["Vince Trencsenyi"], "title": "Hypergame Rationalisability: Solving Agent Misalignment In Strategic Play", "comment": null, "summary": "Differences in perception, information asymmetries, and bounded rationality lead game-theoretic players to derive a private, subjective view of the game that may diverge from the underlying ground-truth scenario and may be misaligned with other players' interpretations. While typical game-theoretic assumptions often overlook such heterogeneity, hypergame theory provides the mathematical framework to reason about mismatched mental models. Although hypergames have recently gained traction in dynamic applications concerning uncertainty, their practical adoption in multi-agent system research has been hindered by the lack of a unifying, formal, and practical representation language, as well as scalable algorithms for managing complex hypergame structures and equilibria. Our work addresses this gap by introducing a declarative, logic-based domain-specific language for encoding hypergame structures and hypergame solution concepts. Leveraging answer-set programming, we develop an automated pipeline for instantiating hypergame structures and running our novel hypergame rationalisation procedure, a mechanism for finding belief structures that justify seemingly irrational outcomes. The proposed language establishes a unifying formalism for hypergames and serves as a foundation for developing nuanced, belief-based heterogeneous reasoners, offering a verifiable context with logical guarantees. Together, these contributions establish the connection between hypergame theory, multi-agent systems, and strategic AI.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u903b\u8f91\u7684\u58f0\u660e\u5f0f\u9886\u57df\u7279\u5b9a\u8bed\u8a00\uff0c\u7528\u4e8e\u7f16\u7801\u8d85\u535a\u5f08\u7ed3\u6784\u548c\u89e3\u51b3\u65b9\u6848\u6982\u5ff5\uff0c\u901a\u8fc7\u7b54\u6848\u96c6\u7f16\u7a0b\u5b9e\u73b0\u81ea\u52a8\u5316\u7ba1\u9053\uff0c\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u5f02\u6784\u63a8\u7406\u63d0\u4f9b\u53ef\u9a8c\u8bc1\u7684\u5f62\u5f0f\u5316\u57fa\u7840\u3002", "motivation": "\u7531\u4e8e\u611f\u77e5\u5dee\u5f02\u3001\u4fe1\u606f\u4e0d\u5bf9\u79f0\u548c\u6709\u9650\u7406\u6027\uff0c\u535a\u5f08\u8bba\u53c2\u4e0e\u8005\u5bf9\u6e38\u620f\u5f62\u6210\u7684\u4e3b\u89c2\u770b\u6cd5\u53ef\u80fd\u4e0e\u5b9e\u9645\u60c5\u51b5\u548c\u5176\u4ed6\u53c2\u4e0e\u8005\u7684\u89e3\u91ca\u4e0d\u4e00\u81f4\u3002\u867d\u7136\u8d85\u535a\u5f08\u7406\u8bba\u63d0\u4f9b\u4e86\u5904\u7406\u8fd9\u79cd\u4e0d\u5339\u914d\u5fc3\u7406\u6a21\u578b\u7684\u6570\u5b66\u6846\u67b6\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u7684\u5f62\u5f0f\u5316\u8868\u793a\u8bed\u8a00\u548c\u53ef\u6269\u5c55\u7b97\u6cd5\u963b\u788d\u4e86\u5176\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7814\u7a76\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5f15\u5165\u4e00\u79cd\u58f0\u660e\u5f0f\u3001\u57fa\u4e8e\u903b\u8f91\u7684\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u6765\u7f16\u7801\u8d85\u535a\u5f08\u7ed3\u6784\u548c\u8d85\u535a\u5f08\u89e3\u51b3\u65b9\u6848\u6982\u5ff5\uff1b\u5229\u7528\u7b54\u6848\u96c6\u7f16\u7a0b\u5f00\u53d1\u81ea\u52a8\u5316\u7ba1\u9053\uff0c\u7528\u4e8e\u5b9e\u4f8b\u5316\u8d85\u535a\u5f08\u7ed3\u6784\u548c\u8fd0\u884c\u65b0\u9896\u7684\u8d85\u535a\u5f08\u5408\u7406\u5316\u7a0b\u5e8f\uff0c\u8be5\u7a0b\u5e8f\u80fd\u591f\u627e\u5230\u8bc1\u660e\u770b\u4f3c\u975e\u7406\u6027\u7ed3\u679c\u7684\u4fe1\u5ff5\u7ed3\u6784\u3002", "result": "\u63d0\u51fa\u7684\u8bed\u8a00\u4e3a\u8d85\u535a\u5f08\u5efa\u7acb\u4e86\u7edf\u4e00\u7684\u5f62\u5f0f\u5316\u6846\u67b6\uff0c\u4e3a\u5f00\u53d1\u57fa\u4e8e\u4fe1\u5ff5\u7684\u5f02\u6784\u63a8\u7406\u5668\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u63d0\u4f9b\u4e86\u5177\u6709\u903b\u8f91\u4fdd\u8bc1\u7684\u53ef\u9a8c\u8bc1\u4e0a\u4e0b\u6587\u3002\u8fd9\u4e9b\u8d21\u732e\u5efa\u7acb\u4e86\u8d85\u535a\u5f08\u7406\u8bba\u3001\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u548c\u6218\u7565AI\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u586b\u8865\u4e86\u8d85\u535a\u5f08\u7406\u8bba\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5173\u952e\u7a7a\u767d\uff0c\u901a\u8fc7\u5f62\u5f0f\u5316\u8bed\u8a00\u548c\u81ea\u52a8\u5316\u5de5\u5177\u4f7f\u8d85\u535a\u5f08\u5206\u6790\u66f4\u52a0\u5b9e\u7528\u548c\u53ef\u6269\u5c55\uff0c\u4e3a\u5904\u7406\u5f02\u6784\u4fe1\u5ff5\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2512.13173", "categories": ["cs.IR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.13173", "abs": "https://arxiv.org/abs/2512.13173", "authors": ["Ivica Kostric", "Ujwal Gadiraju", "Krisztian Balog"], "title": "Know Your Users! Estimating User Domain Knowledge in Conversational Recommenders", "comment": null, "summary": "The ideal conversational recommender system (CRS) acts like a savvy salesperson, adapting its language and suggestions to each user's level of expertise. However, most current systems treat all users as experts, leading to frustrating and inefficient interactions when users are unfamiliar with a domain. Systems that can adapt their conversational strategies to a user's knowledge level stand to offer a much more natural and effective experience. To make a step toward such adaptive systems, we introduce a new task: estimating user domain knowledge from conversations, enabling a CRS to better understand user needs and personalize interactions. A key obstacle to developing such adaptive systems is the lack of suitable data; to our knowledge, no existing dataset captures the conversational behaviors of users with varying levels of domain knowledge. Furthermore, in most dialogue collection protocols, users are free to express their own preferences, which tends to concentrate on popular items and well-known features, offering little insight into how novices explore or learn about unfamiliar features. To address this, we design a game-based data collection protocol that elicits varied expressions of knowledge, release the resulting dataset, and provide an initial analysis to highlight its potential for future work on user-knowledge-aware CRS.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6e38\u620f\u7684\u6570\u636e\u6536\u96c6\u534f\u8bae\uff0c\u521b\u5efa\u5305\u542b\u4e0d\u540c\u9886\u57df\u77e5\u8bc6\u6c34\u5e73\u7528\u6237\u5bf9\u8bdd\u884c\u4e3a\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5f00\u53d1\u80fd\u611f\u77e5\u7528\u6237\u77e5\u8bc6\u6c34\u5e73\u7684\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf", "motivation": "\u5f53\u524d\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u901a\u5e38\u5c06\u6240\u6709\u7528\u6237\u89c6\u4e3a\u4e13\u5bb6\uff0c\u5bfc\u81f4\u5bf9\u9886\u57df\u4e0d\u719f\u6089\u7684\u7528\u6237\u4ea4\u4e92\u4f53\u9a8c\u5dee\u3002\u9700\u8981\u80fd\u6839\u636e\u7528\u6237\u77e5\u8bc6\u6c34\u5e73\u81ea\u9002\u5e94\u8c03\u6574\u5bf9\u8bdd\u7b56\u7565\u7684\u7cfb\u7edf\uff0c\u4f46\u7f3a\u4e4f\u5305\u542b\u4e0d\u540c\u77e5\u8bc6\u6c34\u5e73\u7528\u6237\u5bf9\u8bdd\u884c\u4e3a\u7684\u6570\u636e\u96c6\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u6e38\u620f\u7684\u6570\u636e\u6536\u96c6\u534f\u8bae\uff0c\u80fd\u591f\u5f15\u51fa\u7528\u6237\u77e5\u8bc6\u6c34\u5e73\u7684\u4e0d\u540c\u8868\u8fbe\u65b9\u5f0f\u3002\u521b\u5efa\u5e76\u53d1\u5e03\u4e86\u5305\u542b\u7528\u6237\u77e5\u8bc6\u6c34\u5e73\u53d8\u5316\u7684\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u5e76\u8fdb\u884c\u4e86\u521d\u6b65\u5206\u6790\u3002", "result": "\u6210\u529f\u521b\u5efa\u4e86\u9996\u4e2a\u5305\u542b\u4e0d\u540c\u9886\u57df\u77e5\u8bc6\u6c34\u5e73\u7528\u6237\u5bf9\u8bdd\u884c\u4e3a\u7684\u6570\u636e\u96c6\uff0c\u4e3a\u5f00\u53d1\u7528\u6237\u77e5\u8bc6\u611f\u77e5\u7684\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u7840\u8d44\u6e90\u3002", "conclusion": "\u901a\u8fc7\u6e38\u620f\u5316\u6570\u636e\u6536\u96c6\u534f\u8bae\u521b\u5efa\u7684\u6570\u636e\u96c6\u586b\u8865\u4e86\u7528\u6237\u77e5\u8bc6\u6c34\u5e73\u611f\u77e5\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u4e3a\u672a\u6765\u5f00\u53d1\u81ea\u9002\u5e94\u5bf9\u8bdd\u7b56\u7565\u7684\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.11823", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.11823", "abs": "https://arxiv.org/abs/2512.11823", "authors": ["Vicky P. Vital", "Francis F. Balahadia", "Maria Anna D. Cruz", "Dolores D. Mallari", "Juvy C. Grume", "Erika M. Pineda", "Jordan L. Salenga", "Lloyd D. Feliciano", "John Paul P. Miranda"], "title": "Teachers' Perspectives on the Use of AI Detection Tools: Insights from Ridge Regression Analysis", "comment": "8 pages, 2 tables, 2025 International Conference on Distance Education and Learning (ICDEL)", "summary": "This study explores the perceptions of 213 Filipino teachers toward AI detection tools in academic settings. It focuses on the factors that influence teachers' trust, concerns, and decision-making regarding these tools. The research investigates how teachers' trust in AI detection tools affects their perceptions of fairness and decision-making in evaluating student outputs. It also explores how concerns about AI tools and social norms influence the relationship between trust and decision-making. Ridge Regression analysis was used to examine the relationships between the predictors and the dependent variable. The results revealed that trust in AI detection tools is the most significant predictor of perceived fairness and decision-making among teachers. Concerns about AI tools and social norms have weaker effects on teachers' perceptions. The study emphasized critical role of trust in shaping teachers' perceptions of AI detection tools. Teachers who trust these tools are more likely to view them as fair and effective. In contrast, concerns and social norms have a limited influence on perceptions and decision-making. For recommendations, training and institutional guidelines should emphasize how these tools work, their limitations, and best practices for their use. Striking a balance between policy enforcement and educator support is essential for fostering trust in AI detection technologies. Encouraging experienced users to share insights through communities of practice could enhance the adoption and effective use of AI detection tools in educational settings..", "AI": {"tldr": "\u672c\u7814\u7a76\u8c03\u67e5\u4e86213\u540d\u83f2\u5f8b\u5bbe\u6559\u5e08\u5bf9\u5b66\u672f\u73af\u5883\u4e2dAI\u68c0\u6d4b\u5de5\u5177\u7684\u770b\u6cd5\uff0c\u53d1\u73b0\u4fe1\u4efb\u662f\u5f71\u54cd\u6559\u5e08\u611f\u77e5\u516c\u5e73\u6027\u548c\u51b3\u7b56\u7684\u6700\u91cd\u8981\u56e0\u7d20\uff0c\u800c\u62c5\u5fe7\u548c\u793e\u4f1a\u89c4\u8303\u5f71\u54cd\u8f83\u5f31\u3002", "motivation": "\u63a2\u7d22\u6559\u5e08\u5bf9AI\u68c0\u6d4b\u5de5\u5177\u7684\u4fe1\u4efb\u3001\u62c5\u5fe7\u548c\u51b3\u7b56\u5f71\u54cd\u56e0\u7d20\uff0c\u4e86\u89e3\u8fd9\u4e9b\u56e0\u7d20\u5982\u4f55\u5f71\u54cd\u6559\u5e08\u5bf9\u5b66\u751f\u4f5c\u4e1a\u8bc4\u4f30\u7684\u516c\u5e73\u6027\u611f\u77e5\u548c\u51b3\u7b56\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528Ridge\u56de\u5f52\u5206\u6790\uff0c\u8c03\u67e5213\u540d\u83f2\u5f8b\u5bbe\u6559\u5e08\uff0c\u7814\u7a76\u4fe1\u4efb\u3001\u62c5\u5fe7\u548c\u793e\u4f1a\u89c4\u8303\u7b49\u56e0\u7d20\u4e0e\u6559\u5e08\u611f\u77e5\u516c\u5e73\u6027\u548c\u51b3\u7b56\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u4fe1\u4efb\u662f\u5f71\u54cd\u6559\u5e08\u611f\u77e5\u516c\u5e73\u6027\u548c\u51b3\u7b56\u7684\u6700\u663e\u8457\u9884\u6d4b\u56e0\u5b50\uff0c\u800c\u62c5\u5fe7\u548c\u793e\u4f1a\u89c4\u8303\u5bf9\u6559\u5e08\u770b\u6cd5\u7684\u5f71\u54cd\u8f83\u5f31\u3002\u4fe1\u4efbAI\u68c0\u6d4b\u5de5\u5177\u7684\u6559\u5e08\u66f4\u53ef\u80fd\u8ba4\u4e3a\u8fd9\u4e9b\u5de5\u5177\u516c\u5e73\u6709\u6548\u3002", "conclusion": "\u4fe1\u4efb\u5728\u5851\u9020\u6559\u5e08\u5bf9AI\u68c0\u6d4b\u5de5\u5177\u7684\u770b\u6cd5\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\u3002\u5efa\u8bae\u901a\u8fc7\u57f9\u8bad\u3001\u5236\u5ea6\u6307\u5357\u548c\u5b9e\u8df5\u793e\u533a\u6765\u589e\u5f3a\u4fe1\u4efb\uff0c\u5e73\u8861\u653f\u7b56\u6267\u884c\u4e0e\u6559\u80b2\u8005\u652f\u6301\uff0c\u4fc3\u8fdbAI\u68c0\u6d4b\u5de5\u5177\u5728\u6559\u80b2\u73af\u5883\u4e2d\u7684\u6709\u6548\u91c7\u7528\u3002"}}
{"id": "2512.11997", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11997", "abs": "https://arxiv.org/abs/2512.11997", "authors": ["Anfeng Peng", "Ajesh Koyatan Chathoth", "Stephen Lee"], "title": "Log Anomaly Detection with Large Language Models via Knowledge-Enriched Fusion", "comment": null, "summary": "System logs are a critical resource for monitoring and managing distributed systems, providing insights into failures and anomalous behavior. Traditional log analysis techniques, including template-based and sequence-driven approaches, often lose important semantic information or struggle with ambiguous log patterns. To address this, we present EnrichLog, a training-free, entry-based anomaly detection framework that enriches raw log entries with both corpus-specific and sample-specific knowledge. EnrichLog incorporates contextual information, including historical examples and reasoning derived from the corpus, to enable more accurate and interpretable anomaly detection. The framework leverages retrieval-augmented generation to integrate relevant contextual knowledge without requiring retraining. We evaluate EnrichLog on four large-scale system log benchmark datasets and compare it against five baseline methods. Our results show that EnrichLog consistently improves anomaly detection performance, effectively handles ambiguous log entries, and maintains efficient inference. Furthermore, incorporating both corpus- and sample-specific knowledge enhances model confidence and detection accuracy, making EnrichLog well-suited for practical deployments.", "AI": {"tldr": "EnrichLog\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u65e5\u5fd7\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8bed\u6599\u5e93\u7279\u5b9a\u548c\u6837\u672c\u7279\u5b9a\u77e5\u8bc6\u6765\u4e30\u5bcc\u539f\u59cb\u65e5\u5fd7\u6761\u76ee\uff0c\u63d0\u9ad8\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edf\u65e5\u5fd7\u5206\u6790\u6280\u672f\uff08\u5982\u57fa\u4e8e\u6a21\u677f\u548c\u5e8f\u5217\u9a71\u52a8\u7684\u65b9\u6cd5\uff09\u5f80\u5f80\u4f1a\u4e22\u5931\u91cd\u8981\u8bed\u4e49\u4fe1\u606f\u6216\u96be\u4ee5\u5904\u7406\u6a21\u7cca\u7684\u65e5\u5fd7\u6a21\u5f0f\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u66f4\u597d\u5229\u7528\u8bed\u4e49\u4fe1\u606f\u7684\u65b9\u6cd5\u3002", "method": "EnrichLog\u91c7\u7528\u57fa\u4e8e\u6761\u76ee\u7684\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u6574\u5408\u76f8\u5173\u4e0a\u4e0b\u6587\u77e5\u8bc6\uff08\u5305\u62ec\u5386\u53f2\u793a\u4f8b\u548c\u8bed\u6599\u5e93\u63a8\u7406\uff09\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u5373\u53ef\u96c6\u6210\u8bed\u6599\u5e93\u7279\u5b9a\u548c\u6837\u672c\u7279\u5b9a\u77e5\u8bc6\u3002", "result": "\u5728\u56db\u4e2a\u5927\u89c4\u6a21\u7cfb\u7edf\u65e5\u5fd7\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cEnrichLog\u76f8\u6bd4\u4e94\u79cd\u57fa\u7ebf\u65b9\u6cd5\u6301\u7eed\u63d0\u5347\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\uff0c\u6709\u6548\u5904\u7406\u6a21\u7cca\u65e5\u5fd7\u6761\u76ee\uff0c\u5e76\u4fdd\u6301\u9ad8\u6548\u63a8\u7406\u3002\u7ed3\u5408\u4e24\u79cd\u77e5\u8bc6\u589e\u5f3a\u4e86\u6a21\u578b\u7f6e\u4fe1\u5ea6\u548c\u68c0\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "EnrichLog\u901a\u8fc7\u4e30\u5bcc\u65e5\u5fd7\u8bed\u4e49\u4fe1\u606f\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u9ad8\u6548\u7684\u5f02\u5e38\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u5408\u5b9e\u9645\u90e8\u7f72\u5e94\u7528\u3002"}}
{"id": "2512.11905", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11905", "abs": "https://arxiv.org/abs/2512.11905", "authors": ["Ming-Zher Poh", "Shun Liao", "Marco Andreetto", "Daniel McDuff", "Jonathan Wang", "Paolo Di Achille", "Jiang Wu", "Yun Liu", "Lawrence Cai", "Eric Teasley", "Mark Malhotra", "Anupam Pathak", "Shwetak Patel"], "title": "Smartphone monitoring of smiling as a behavioral proxy of well-being in everyday life", "comment": null, "summary": "Subjective well-being is a cornerstone of individual and societal health, yet its scientific measurement has traditionally relied on self-report methods prone to recall bias and high participant burden. This has left a gap in our understanding of well-being as it is expressed in everyday life. We hypothesized that candid smiles captured during natural smartphone interactions could serve as a scalable, objective behavioral correlate of positive affect. To test this, we analyzed 405,448 video clips passively recorded from 233 consented participants over one week. Using a deep learning model to quantify smile intensity, we identified distinct diurnal and daily patterns. Daily patterns of smile intensity across the week showed strong correlation with national survey data on happiness (r=0.92), and diurnal rhythms documented close correspondence with established results from the day reconstruction method (r=0.80). Higher daily mean smile intensity was significantly associated with more physical activity (Beta coefficient = 0.043, 95% CI [0.001, 0.085]) and greater light exposure (Beta coefficient = 0.038, [0.013, 0.063]), whereas no significant effects were found for smartphone use. These findings suggest that passive smartphone sensing could serve as a powerful, ecologically valid methodology for studying the dynamics of affective behavior and open the door to understanding this behavior at a population scale.", "AI": {"tldr": "\u901a\u8fc7\u667a\u80fd\u624b\u673a\u88ab\u52a8\u6355\u6349\u7684\u81ea\u7136\u5fae\u7b11\u5f3a\u5ea6\u53ef\u4f5c\u4e3a\u4e3b\u89c2\u5e78\u798f\u611f\u7684\u5ba2\u89c2\u884c\u4e3a\u6307\u6807\uff0c\u4e0e\u5168\u56fd\u5e78\u798f\u611f\u8c03\u67e5\u6570\u636e\u9ad8\u5ea6\u76f8\u5173", "motivation": "\u4f20\u7edf\u4e3b\u89c2\u5e78\u798f\u611f\u6d4b\u91cf\u4f9d\u8d56\u81ea\u6211\u62a5\u544a\u65b9\u6cd5\uff0c\u5b58\u5728\u56de\u5fc6\u504f\u5dee\u548c\u53c2\u4e0e\u8005\u8d1f\u62c5\u91cd\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u5ba2\u89c2\u3001\u53ef\u6269\u5c55\u7684\u65e5\u5e38\u5e78\u798f\u611f\u6d4b\u91cf\u65b9\u6cd5", "method": "\u5206\u6790233\u540d\u53c2\u4e0e\u8005\u4e00\u5468\u5185\u88ab\u52a8\u8bb0\u5f55\u7684405,448\u4e2a\u89c6\u9891\u7247\u6bb5\uff0c\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u91cf\u5316\u5fae\u7b11\u5f3a\u5ea6\uff0c\u7814\u7a76\u5176\u663c\u591c\u548c\u65e5\u5e38\u6a21\u5f0f\uff0c\u5e76\u4e0e\u8eab\u4f53\u6d3b\u52a8\u3001\u5149\u7167\u66b4\u9732\u548c\u667a\u80fd\u624b\u673a\u4f7f\u7528\u7b49\u53d8\u91cf\u5173\u8054\u5206\u6790", "result": "\u5fae\u7b11\u5f3a\u5ea6\u65e5\u5e38\u6a21\u5f0f\u4e0e\u5168\u56fd\u5e78\u798f\u611f\u8c03\u67e5\u6570\u636e\u9ad8\u5ea6\u76f8\u5173(r=0.92)\uff0c\u663c\u591c\u8282\u5f8b\u4e0e\u65e5\u91cd\u5efa\u65b9\u6cd5\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4(r=0.80)\uff1b\u66f4\u9ad8\u7684\u65e5\u5747\u5fae\u7b11\u5f3a\u5ea6\u4e0e\u66f4\u591a\u8eab\u4f53\u6d3b\u52a8\u548c\u66f4\u5927\u5149\u7167\u66b4\u9732\u663e\u8457\u76f8\u5173\uff0c\u4f46\u4e0e\u667a\u80fd\u624b\u673a\u4f7f\u7528\u65e0\u663e\u8457\u5173\u8054", "conclusion": "\u88ab\u52a8\u667a\u80fd\u624b\u673a\u4f20\u611f\u53ef\u4f5c\u4e3a\u7814\u7a76\u60c5\u611f\u884c\u4e3a\u52a8\u6001\u7684\u5f3a\u5927\u3001\u751f\u6001\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u4e3a\u5728\u4eba\u7fa4\u5c3a\u5ea6\u4e0a\u7406\u89e3\u8fd9\u79cd\u884c\u4e3a\u6253\u5f00\u4e86\u5927\u95e8"}}
{"id": "2512.11827", "categories": ["cs.CY", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11827", "abs": "https://arxiv.org/abs/2512.11827", "authors": ["Milad Malekzadeh", "Magdalena Biernacka", "Elias Willberg", "Jussi Torkko", "Edyta \u0141aszkiewicz", "Tuuli Toivonen"], "title": "Assessing Greenspace Attractiveness with ChatGPT, Claude, and Gemini: Do AI Models Reflect Human Perceptions?", "comment": null, "summary": "Understanding greenspace attractiveness is essential for designing livable and inclusive urban environments, yet existing assessment approaches often overlook informal or transient spaces and remain too resource intensive to capture subjective perceptions at scale. This study examines the ability of multimodal large language models (MLLMs), ChatGPT GPT-4o, Claude 3.5 Haiku, and Gemini 2.0 Flash, to assess greenspace attractiveness similarly to humans using Google Street View imagery. We compared model outputs with responses from a geo-questionnaire of residents in Lodz, Poland, across both formal (for example, parks and managed greenspaces) and informal (for example, meadows and wastelands) greenspaces. Survey respondents and models indicated whether each greenspace was attractive or unattractive and provided up to three free text explanations. Analyses examined how often their attractiveness judgments aligned and compared their explanations after classifying them into shared reasoning categories. Results show high AI human agreement for attractive formal greenspaces and unattractive informal spaces, but low alignment for attractive informal and unattractive formal greenspaces. Models consistently emphasized aesthetic and design oriented features, underrepresenting safety, functional infrastructure, and locally embedded qualities valued by survey respondents. While these findings highlight the potential for scalable pre-assessment, they also underscore the need for human oversight and complementary participatory approaches. We conclude that MLLMs can support, but not replace, context sensitive greenspace evaluation in planning practice.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4f7f\u7528\u8857\u666f\u56fe\u50cf\u8bc4\u4f30\u7eff\u5730\u5438\u5f15\u529b\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u5728\u6b63\u5f0f\u7eff\u5730\uff08\u516c\u56ed\uff09\u548c\u975e\u6b63\u5f0f\u7eff\u5730\uff08\u8352\u5730\uff09\u7684\u5438\u5f15\u529b\u5224\u65ad\u4e0a\u4e0e\u4eba\u7c7b\u5b58\u5728\u5dee\u5f02\uff0c\u6a21\u578b\u66f4\u5173\u6ce8\u7f8e\u5b66\u7279\u5f81\u800c\u5ffd\u89c6\u5b89\u5168\u3001\u529f\u80fd\u7b49\u4eba\u7c7b\u91cd\u89c6\u7684\u65b9\u9762\u3002", "motivation": "\u73b0\u6709\u7eff\u5730\u5438\u5f15\u529b\u8bc4\u4f30\u65b9\u6cd5\u5f80\u5f80\u5ffd\u89c6\u975e\u6b63\u5f0f\u6216\u4e34\u65f6\u6027\u7a7a\u95f4\uff0c\u4e14\u8d44\u6e90\u5bc6\u96c6\u96be\u4ee5\u5927\u89c4\u6a21\u6355\u6349\u4e3b\u89c2\u611f\u77e5\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u5426\u50cf\u4eba\u7c7b\u4e00\u6837\u4f7f\u7528\u8857\u666f\u56fe\u50cf\u8bc4\u4f30\u7eff\u5730\u5438\u5f15\u529b\uff0c\u4e3a\u57ce\u5e02\u89c4\u5212\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u9884\u8bc4\u4f30\u5de5\u5177\u3002", "method": "\u4f7f\u7528ChatGPT GPT-4o\u3001Claude 3.5 Haiku\u548cGemini 2.0 Flash\u4e09\u79cdMLLM\u6a21\u578b\uff0c\u901a\u8fc7Google\u8857\u666f\u56fe\u50cf\u8bc4\u4f30\u6ce2\u5170\u7f57\u5179\u5e02\u6b63\u5f0f\u7eff\u5730\uff08\u516c\u56ed\u3001\u7ba1\u7406\u7eff\u5730\uff09\u548c\u975e\u6b63\u5f0f\u7eff\u5730\uff08\u8349\u5730\u3001\u8352\u5730\uff09\u7684\u5438\u5f15\u529b\u3002\u5c06\u6a21\u578b\u8f93\u51fa\u4e0e\u5f53\u5730\u5c45\u6c11\u7684\u5730\u7406\u95ee\u5377\u8c03\u67e5\u7ed3\u679c\u8fdb\u884c\u6bd4\u8f83\uff0c\u5206\u6790\u5438\u5f15\u529b\u5224\u65ad\u7684\u4e00\u81f4\u6027\u548c\u89e3\u91ca\u7406\u7531\u7684\u5206\u7c7b\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff1aAI\u4e0e\u4eba\u7c7b\u5728\u5438\u5f15\u4eba\u7684\u6b63\u5f0f\u7eff\u5730\u548c\u7f3a\u4e4f\u5438\u5f15\u529b\u7684\u975e\u6b63\u5f0f\u7a7a\u95f4\u4e0a\u9ad8\u5ea6\u4e00\u81f4\uff0c\u4f46\u5728\u5438\u5f15\u4eba\u7684\u975e\u6b63\u5f0f\u7eff\u5730\u548c\u7f3a\u4e4f\u5438\u5f15\u529b\u7684\u6b63\u5f0f\u7eff\u5730\u4e0a\u4e00\u81f4\u6027\u8f83\u4f4e\u3002\u6a21\u578b\u8fc7\u5ea6\u5f3a\u8c03\u7f8e\u5b66\u548c\u8bbe\u8ba1\u5bfc\u5411\u7279\u5f81\uff0c\u800c\u4f4e\u4f30\u4e86\u8c03\u67e5\u5bf9\u8c61\u91cd\u89c6\u7684\u5b89\u5168\u6027\u3001\u529f\u80fd\u6027\u57fa\u7840\u8bbe\u65bd\u548c\u672c\u5730\u5d4c\u5165\u54c1\u8d28\u3002", "conclusion": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5177\u6709\u53ef\u6269\u5c55\u9884\u8bc4\u4f30\u7684\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u4eba\u7c7b\u76d1\u7763\u548c\u8865\u5145\u6027\u53c2\u4e0e\u65b9\u6cd5\u3002MLLMs\u53ef\u4ee5\u652f\u6301\u4f46\u4e0d\u80fd\u66ff\u4ee3\u89c4\u5212\u5b9e\u8df5\u4e2d\u5bf9\u80cc\u666f\u654f\u611f\u7684\u7eff\u5730\u8bc4\u4f30\u3002"}}
{"id": "2512.11906", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11906", "abs": "https://arxiv.org/abs/2512.11906", "authors": ["Noorul Wahab", "Nasir Rajpoot"], "title": "MPath: Multimodal Pathology Report Generation from Whole Slide Images", "comment": "Pages 4, Figures 1, Table 1", "summary": "Automated generation of diagnostic pathology reports directly from whole slide images (WSIs) is an emerging direction in computational pathology. Translating high-resolution tissue patterns into clinically coherent text remains difficult due to large morphological variability and the complex structure of pathology narratives. We introduce MPath, a lightweight multimodal framework that conditions a pretrained biomedical language model (BioBART) on WSI-derived visual embeddings through a learned visual-prefix prompting mechanism. Instead of end-to-end vision-language pretraining, MPath leverages foundation-model WSI features (CONCH + Titan) and injects them into BioBART via a compact projection module, keeping the language backbone frozen for stability and data efficiency. MPath was developed and evaluated on the RED 2025 Grand Challenge dataset and ranked 4th in Test Phase 2, despite limited submission opportunities. The results highlight the potential of prompt-based multimodal conditioning as a scalable and interpretable strategy for pathology report generation.", "AI": {"tldr": "MPath\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u524d\u7f00\u63d0\u793a\u673a\u5236\u5c06WSI\u89c6\u89c9\u5d4c\u5165\u6ce8\u5165\u9884\u8bad\u7ec3\u751f\u7269\u533b\u5b66\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u4ece\u5168\u5207\u7247\u56fe\u50cf\u81ea\u52a8\u751f\u6210\u75c5\u7406\u8bca\u65ad\u62a5\u544a\u3002", "motivation": "\u4ece\u5168\u5207\u7247\u56fe\u50cf\u81ea\u52a8\u751f\u6210\u75c5\u7406\u8bca\u65ad\u62a5\u544a\u662f\u8ba1\u7b97\u75c5\u7406\u5b66\u7684\u65b0\u65b9\u5411\uff0c\u4f46\u7531\u4e8e\u7ec4\u7ec7\u5f62\u6001\u7684\u9ad8\u5ea6\u53d8\u5f02\u6027\u548c\u75c5\u7406\u53d9\u8ff0\u7684\u590d\u6742\u7ed3\u6784\uff0c\u5c06\u9ad8\u5206\u8fa8\u7387\u7ec4\u7ec7\u6a21\u5f0f\u8f6c\u5316\u4e3a\u4e34\u5e8a\u8fde\u8d2f\u6587\u672c\u4ecd\u7136\u56f0\u96be\u3002", "method": "MPath\u91c7\u7528\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u5b66\u4e60\u7684\u89c6\u89c9\u524d\u7f00\u63d0\u793a\u673a\u5236\uff0c\u5c06\u57fa\u7840\u6a21\u578b\u63d0\u53d6\u7684WSI\u7279\u5f81\uff08CONCH + Titan\uff09\u6ce8\u5165\u51bb\u7ed3\u7684BioBART\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u4f7f\u7528\u7d27\u51d1\u6295\u5f71\u6a21\u5757\u800c\u975e\u7aef\u5230\u7aef\u7684\u89c6\u89c9\u8bed\u8a00\u9884\u8bad\u7ec3\u3002", "result": "\u5728RED 2025 Grand Challenge\u6570\u636e\u96c6\u4e0a\u5f00\u53d1\u8bc4\u4f30\uff0c\u5728Test Phase 2\u4e2d\u6392\u540d\u7b2c4\uff0c\u5c3d\u7ba1\u63d0\u4ea4\u673a\u4f1a\u6709\u9650\u3002\u7ed3\u679c\u663e\u793a\u4e86\u57fa\u4e8e\u63d0\u793a\u7684\u591a\u6a21\u6001\u6761\u4ef6\u5316\u4f5c\u4e3a\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u75c5\u7406\u62a5\u544a\u751f\u6210\u7b56\u7565\u7684\u6f5c\u529b\u3002", "conclusion": "\u57fa\u4e8e\u63d0\u793a\u7684\u591a\u6a21\u6001\u6761\u4ef6\u5316\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u53ef\u89e3\u91ca\u7684\u75c5\u7406\u62a5\u544a\u751f\u6210\u7b56\u7565\uff0cMPath\u6846\u67b6\u5c55\u793a\u4e86\u5c06WSI\u89c6\u89c9\u7279\u5f81\u6709\u6548\u6ce8\u5165\u751f\u7269\u533b\u5b66\u8bed\u8a00\u6a21\u578b\u7684\u80fd\u529b\u3002"}}
{"id": "2512.11850", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.11850", "abs": "https://arxiv.org/abs/2512.11850", "authors": ["Davide Mancino"], "title": "The Memecoin Phenomenon: An In-Depth Study of Solana's Blockchain Trends", "comment": null, "summary": "This paper analyzes the emerging memecoin phenomenon on the Solana blockchain, focusing on the Pump.fun platform during Q4 2024. Using on-chain data, it is explored how retail-focused token creation platforms are reshaping blockchain ecosystems and influencing market participation. This study finds that Pump.fun accounted for up to 71.1% of all tokens minted on Solana and contributed 40-67.4% of total DEX transactions. Despite this activity, fewer than 2% of tokens successfully transitioned to major decentralized exchanges, highlighting a highly speculative market structure. The platform experienced rapid growth, with daily active users rising from 60,000 to peaks of 260,000, underscoring strong retail adoption. This reflects a broader shift towards accessible, socially-driven market participation enabled by memecoins. However, while memecoins lower entry barriers and encourage retail engagement, they introduce significant risks. The volatile and speculative nature of these platforms raises concerns about long-term sustainability and the resilience of the blockchain ecosystem. These findings reveal the dual impact of memecoins: they democratize token creation and alter market dynamics but may jeopardize market efficiency and stability. This paper highlights the need to critically assess the implications of retail-driven speculative trading and its potential to disrupt emerging blockchain economies.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86Solana\u533a\u5757\u94fe\u4e0a\u57fa\u4e8ePump.fun\u5e73\u53f0\u7684memecoin\u73b0\u8c61\uff0c\u53d1\u73b0\u8be5\u5e73\u53f0\u4e3b\u5bfc\u4e86Solana\u4ee3\u5e01\u53d1\u884c\u548c\u4ea4\u6613\u6d3b\u52a8\uff0c\u4f46\u6210\u529f\u8f6c\u578b\u5230\u4e3b\u6d41DEX\u7684\u4ee3\u5e01\u4e0d\u8db32%\uff0c\u63ed\u793a\u4e86\u9ad8\u5ea6\u6295\u673a\u6027\u5e02\u573a\u7ed3\u6784\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22Solana\u533a\u5757\u94fe\u4e0a\u65b0\u5174\u7684memecoin\u73b0\u8c61\uff0c\u7279\u522b\u662f\u901a\u8fc7Pump.fun\u5e73\u53f0\uff0c\u4e86\u89e3\u96f6\u552e\u9a71\u52a8\u7684\u4ee3\u5e01\u521b\u5efa\u5e73\u53f0\u5982\u4f55\u91cd\u5851\u533a\u5757\u94fe\u751f\u6001\u7cfb\u7edf\u5e76\u5f71\u54cd\u5e02\u573a\u53c2\u4e0e\u3002", "method": "\u4f7f\u7528\u94fe\u4e0a\u6570\u636e\u5206\u6790\u65b9\u6cd5\uff0c\u7814\u7a76Pump.fun\u5e73\u53f0\u57282024\u5e74\u7b2c\u56db\u5b63\u5ea6\u7684\u6d3b\u52a8\uff0c\u5305\u62ec\u4ee3\u5e01\u94f8\u9020\u3001\u4ea4\u6613\u91cf\u3001\u7528\u6237\u53c2\u4e0e\u5ea6\u7b49\u6307\u6807\u3002", "result": "Pump.fun\u5e73\u53f0\u5360\u636e\u4e86Solana\u4e0a\u9ad8\u8fbe71.1%\u7684\u4ee3\u5e01\u94f8\u9020\u91cf\uff0c\u8d21\u732e\u4e8640-67.4%\u7684DEX\u4ea4\u6613\u91cf\uff1b\u65e5\u6d3b\u8dc3\u7528\u6237\u4ece6\u4e07\u589e\u957f\u81f326\u4e07\u5cf0\u503c\uff1b\u4f46\u53ea\u6709\u4e0d\u52302%\u7684\u4ee3\u5e01\u6210\u529f\u8f6c\u578b\u5230\u4e3b\u6d41\u53bb\u4e2d\u5fc3\u5316\u4ea4\u6613\u6240\u3002", "conclusion": "Memecoin\u73b0\u8c61\u5177\u6709\u53cc\u91cd\u5f71\u54cd\uff1a\u4e00\u65b9\u9762\u964d\u4f4e\u4e86\u53c2\u4e0e\u95e8\u69db\uff0c\u4fc3\u8fdb\u4e86\u96f6\u552e\u53c2\u4e0e\uff1b\u53e6\u4e00\u65b9\u9762\u5e26\u6765\u4e86\u9ad8\u5ea6\u6295\u673a\u6027\u548c\u5e02\u573a\u98ce\u9669\uff0c\u53ef\u80fd\u5a01\u80c1\u533a\u5757\u94fe\u751f\u6001\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u548c\u6548\u7387\uff0c\u9700\u8981\u5ba1\u614e\u8bc4\u4f30\u5176\u957f\u671f\u5f71\u54cd\u3002"}}
{"id": "2512.12059", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12059", "abs": "https://arxiv.org/abs/2512.12059", "authors": ["Luke Bhan", "Hanyu Zhang", "Andrew Gordon Wilson", "Michael W. Mahoney", "Chuck Arvin"], "title": "The Forecast Critic: Leveraging Large Language Models for Poor Forecast Identification", "comment": "Presented at AAAI 2026 AI4TS workshop and AABA4ET workshop", "summary": "Monitoring forecasting systems is critical for customer satisfaction, profitability, and operational efficiency in large-scale retail businesses. We propose The Forecast Critic, a system that leverages Large Language Models (LLMs) for automated forecast monitoring, taking advantage of their broad world knowledge and strong ``reasoning'' capabilities. As a prerequisite for this, we systematically evaluate the ability of LLMs to assess time series forecast quality, focusing on three key questions. (1) Can LLMs be deployed to perform forecast monitoring and identify obviously unreasonable forecasts? (2) Can LLMs effectively incorporate unstructured exogenous features to assess what a reasonable forecast looks like? (3) How does performance vary across model sizes and reasoning capabilities, measured across state-of-the-art LLMs? We present three experiments, including on both synthetic and real-world forecasting data. Our results show that LLMs can reliably detect and critique poor forecasts, such as those plagued by temporal misalignment, trend inconsistencies, and spike errors. The best-performing model we evaluated achieves an F1 score of 0.88, somewhat below human-level performance (F1 score: 0.97). We also demonstrate that multi-modal LLMs can effectively incorporate unstructured contextual signals to refine their assessment of the forecast. Models correctly identify missing or spurious promotional spikes when provided with historical context about past promotions (F1 score: 0.84). Lastly, we demonstrate that these techniques succeed in identifying inaccurate forecasts on the real-world M5 time series dataset, with unreasonable forecasts having an sCRPS at least 10% higher than that of reasonable forecasts. These findings suggest that LLMs, even without domain-specific fine-tuning, may provide a viable and scalable option for automated forecast monitoring and evaluation.", "AI": {"tldr": "LLMs\u53ef\u7528\u4e8e\u81ea\u52a8\u5316\u9884\u6d4b\u76d1\u63a7\uff0c\u5728\u68c0\u6d4b\u4e0d\u5408\u7406\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0cF1\u5206\u6570\u8fbe0.88\uff0c\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73(0.97)\uff0c\u5e76\u80fd\u6574\u5408\u975e\u7ed3\u6784\u5316\u7279\u5f81\u63d0\u5347\u8bc4\u4f30\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u89c4\u6a21\u96f6\u552e\u4e1a\u52a1\u4e2d\u9884\u6d4b\u76d1\u63a7\u5bf9\u5ba2\u6237\u6ee1\u610f\u5ea6\u3001\u76c8\u5229\u80fd\u529b\u548c\u8fd0\u8425\u6548\u7387\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u6765\u8bc6\u522b\u4e0d\u5408\u7406\u9884\u6d4b\u3002", "method": "\u63d0\u51faForecast Critic\u7cfb\u7edf\uff0c\u5229\u7528LLMs\u7684\u5e7f\u6cdb\u4e16\u754c\u77e5\u8bc6\u548c\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u9884\u6d4b\u76d1\u63a7\u3002\u901a\u8fc7\u4e09\u4e2a\u5b9e\u9a8c\u8bc4\u4f30LLMs\u80fd\u529b\uff1a\u68c0\u6d4b\u4e0d\u5408\u7406\u9884\u6d4b\u3001\u6574\u5408\u975e\u7ed3\u6784\u5316\u7279\u5f81\u3001\u6bd4\u8f83\u4e0d\u540c\u6a21\u578b\u6027\u80fd\u3002", "result": "LLMs\u80fd\u53ef\u9760\u68c0\u6d4b\u65f6\u95f4\u9519\u4f4d\u3001\u8d8b\u52bf\u4e0d\u4e00\u81f4\u548c\u5cf0\u503c\u9519\u8bef\u7b49\u4e0d\u5408\u7406\u9884\u6d4b\uff0c\u6700\u4f73\u6a21\u578bF1\u5206\u65700.88\u3002\u591a\u6a21\u6001LLMs\u80fd\u6709\u6548\u6574\u5408\u4fc3\u9500\u5386\u53f2\u7b49\u975e\u7ed3\u6784\u5316\u4fe1\u53f7\uff0cF1\u5206\u65700.84\u3002\u5728M5\u6570\u636e\u96c6\u4e0a\uff0c\u4e0d\u5408\u7406\u9884\u6d4b\u7684sCRPS\u81f3\u5c11\u6bd4\u5408\u7406\u9884\u6d4b\u9ad810%\u3002", "conclusion": "\u5373\u4f7f\u6ca1\u6709\u9886\u57df\u7279\u5b9a\u5fae\u8c03\uff0cLLMs\u4e5f\u80fd\u4e3a\u81ea\u52a8\u5316\u9884\u6d4b\u76d1\u63a7\u548c\u8bc4\u4f30\u63d0\u4f9b\u53ef\u884c\u4e14\u53ef\u6269\u5c55\u7684\u9009\u9879\uff0c\u63a5\u8fd1\u4eba\u7c7b\u6c34\u5e73\u6027\u80fd\u3002"}}
{"id": "2512.11845", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11845", "abs": "https://arxiv.org/abs/2512.11845", "authors": ["Wenbo Du", "Lingling Han", "Ying Xiong", "Ling Zhang", "Biyue Li", "Yisheng Lv", "Tong Guo"], "title": "Airport Passenger Flow Forecasting via Deformable Temporal-Spectral Transformer Approach", "comment": "14 pages, 10 figures", "summary": "Accurate forecasting of passenger flows is critical for maintaining the efficiency and resilience of airport operations. Recent advances in patch-based Transformer models have shown strong potential in various time series forecasting tasks. However, most existing methods rely on fixed-size patch embedding, making it difficult to model the complex and heterogeneous patterns of airport passenger flows. To address this issue, this paper proposes a deformable temporal-spectral transformer named DTSFormer that integrates a multiscale deformable partitioning module and a joint temporal-spectral filtering module. Specifically, the input sequence is dynamically partitioned into multiscale temporal patches via a novel window function-based masking, enabling the extraction of heterogeneous trends across different temporal stages. Then, within each scale, a frequency-domain attention mechanism is designed to capture both high- and low-frequency components, thereby emphasizing the volatility and periodicity inherent in airport passenger flows. Finally, the resulting multi-frequency features are subsequently fused in the time domain to jointly model short-term fluctuations and long-term trends. Comprehensive experiments are conducted on real-world passenger flow data collected at Beijing Capital International Airport from January 2023 to March 2024. The results indicate that the proposed method consistently outperforms state-of-the-art forecasting models across different prediction horizons. Further analysis shows that the deformable partitioning module aligns patch lengths with dominant periods and heterogeneous trends, enabling superior capture of sudden high-frequency fluctuations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDTSFormer\u7684\u53ef\u53d8\u5f62\u65f6\u7a7a\u53d8\u6362\u5668\uff0c\u7528\u4e8e\u673a\u573a\u5ba2\u6d41\u9884\u6d4b\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u591a\u5c3a\u5ea6\u53ef\u53d8\u5f62\u5206\u533a\u548c\u8054\u5408\u65f6\u7a7a\u6ee4\u6ce2\u6a21\u5757\uff0c\u52a8\u6001\u63d0\u53d6\u5f02\u8d28\u65f6\u95f4\u6a21\u5f0f\uff0c\u7ed3\u5408\u9891\u57df\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u9ad8\u9891\u6ce2\u52a8\u548c\u4f4e\u9891\u5468\u671f\u6027\uff0c\u5728\u9996\u90fd\u673a\u573a\u771f\u5b9e\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u673a\u573a\u5ba2\u6d41\u9884\u6d4b\u5bf9\u8fd0\u8425\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u57fa\u4e8e\u56fa\u5b9a\u5927\u5c0f\u8865\u4e01\u7684Transformer\u6a21\u578b\u96be\u4ee5\u6355\u6349\u673a\u573a\u5ba2\u6d41\u7684\u590d\u6742\u5f02\u8d28\u6a21\u5f0f\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u7684\u65b9\u6cd5\u6765\u5efa\u6a21\u4e0d\u540c\u65f6\u95f4\u9636\u6bb5\u7684\u5f02\u8d28\u8d8b\u52bf\u3002", "method": "\u63d0\u51faDTSFormer\u6a21\u578b\uff0c\u5305\u542b\uff1a1\uff09\u591a\u5c3a\u5ea6\u53ef\u53d8\u5f62\u5206\u533a\u6a21\u5757\uff0c\u901a\u8fc7\u57fa\u4e8e\u7a97\u53e3\u51fd\u6570\u7684\u63a9\u7801\u52a8\u6001\u5212\u5206\u591a\u5c3a\u5ea6\u65f6\u95f4\u8865\u4e01\uff1b2\uff09\u8054\u5408\u65f6\u7a7a\u6ee4\u6ce2\u6a21\u5757\uff0c\u8bbe\u8ba1\u9891\u57df\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u9ad8\u4f4e\u9891\u5206\u91cf\uff1b3\uff09\u65f6\u57df\u7279\u5f81\u878d\u5408\uff0c\u8054\u5408\u5efa\u6a21\u77ed\u671f\u6ce2\u52a8\u548c\u957f\u671f\u8d8b\u52bf\u3002", "result": "\u57282023\u5e741\u6708\u81f32024\u5e743\u6708\u5317\u4eac\u9996\u90fd\u56fd\u9645\u673a\u573a\u771f\u5b9e\u5ba2\u6d41\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u9884\u6d4b\u65f6\u95f4\u8303\u56f4\u5185\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u9884\u6d4b\u6a21\u578b\u3002\u53ef\u53d8\u5f62\u5206\u533a\u6a21\u5757\u80fd\u591f\u5c06\u8865\u4e01\u957f\u5ea6\u4e0e\u4e3b\u5bfc\u5468\u671f\u548c\u5f02\u8d28\u8d8b\u52bf\u5bf9\u9f50\uff0c\u66f4\u597d\u5730\u6355\u6349\u7a81\u53d1\u9ad8\u9891\u6ce2\u52a8\u3002", "conclusion": "DTSFormer\u901a\u8fc7\u52a8\u6001\u591a\u5c3a\u5ea6\u5206\u533a\u548c\u9891\u57df\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u673a\u573a\u5ba2\u6d41\u9884\u6d4b\u4e2d\u7684\u5f02\u8d28\u6a21\u5f0f\u5efa\u6a21\u95ee\u9898\uff0c\u4e3a\u673a\u573a\u8fd0\u8425\u7ba1\u7406\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u7684\u9884\u6d4b\u5de5\u5177\u3002"}}
{"id": "2512.11925", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11925", "abs": "https://arxiv.org/abs/2512.11925", "authors": ["Mozhgan Hadadi", "Talukder Z. Jubery", "Patrick S. Schnable", "Arti Singh", "Bedrich Benes", "Adarsh Krishnamurthy", "Baskar Ganapathysubramanian"], "title": "FloraForge: LLM-Assisted Procedural Generation of Editable and Analysis-Ready 3D Plant Geometric Models For Agricultural Applications", "comment": null, "summary": "Accurate 3D plant models are crucial for computational phenotyping and physics-based simulation; however, current approaches face significant limitations. Learning-based reconstruction methods require extensive species-specific training data and lack editability. Procedural modeling offers parametric control but demands specialized expertise in geometric modeling and an in-depth understanding of complex procedural rules, making it inaccessible to domain scientists. We present FloraForge, an LLM-assisted framework that enables domain experts to generate biologically accurate, fully parametric 3D plant models through iterative natural language Plant Refinements (PR), minimizing programming expertise. Our framework leverages LLM-enabled co-design to refine Python scripts that generate parameterized plant geometries as hierarchical B-spline surface representations with botanical constraints with explicit control points and parametric deformation functions. This representation can be easily tessellated into polygonal meshes with arbitrary precision, ensuring compatibility with functional structural plant analysis workflows such as light simulation, computational fluid dynamics, and finite element analysis. We demonstrate the framework on maize, soybean, and mung bean, fitting procedural models to empirical point cloud data through manual refinement of the Plant Descriptor (PD), human-readable files. The pipeline generates dual outputs: triangular meshes for visualization and triangular meshes with additional parametric metadata for quantitative analysis. This approach uniquely combines LLM-assisted template creation, mathematically continuous representations enabling both phenotyping and rendering, and direct parametric control through PD. The framework democratizes sophisticated geometric modeling for plant science while maintaining mathematical rigor.", "AI": {"tldr": "FloraForge\u662f\u4e00\u4e2aLLM\u8f85\u52a9\u6846\u67b6\uff0c\u8ba9\u9886\u57df\u4e13\u5bb6\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u751f\u6210\u53c2\u6570\u53163D\u690d\u7269\u6a21\u578b\uff0c\u65e0\u9700\u7f16\u7a0b\u4e13\u4e1a\u77e5\u8bc6\uff0c\u7ed3\u5408\u4e86\u5b66\u4e60\u65b9\u6cd5\u548c\u7a0b\u5e8f\u5316\u5efa\u6a21\u7684\u4f18\u70b9\u3002", "motivation": "\u5f53\u524d3D\u690d\u7269\u5efa\u6a21\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u7269\u79cd\u7279\u5b9a\u8bad\u7ec3\u6570\u636e\u4e14\u7f3a\u4e4f\u53ef\u7f16\u8f91\u6027\uff1b\u7a0b\u5e8f\u5316\u5efa\u6a21\u9700\u8981\u51e0\u4f55\u5efa\u6a21\u4e13\u4e1a\u77e5\u8bc6\u548c\u590d\u6742\u89c4\u5219\u7406\u89e3\uff0c\u5bf9\u9886\u57df\u79d1\u5b66\u5bb6\u4e0d\u53cb\u597d\u3002", "method": "\u5229\u7528LLM\u8f85\u52a9\u534f\u540c\u8bbe\u8ba1\uff0c\u901a\u8fc7\u8fed\u4ee3\u81ea\u7136\u8bed\u8a00\u690d\u7269\u7cbe\u70bc\uff08PR\uff09\u751f\u6210Python\u811a\u672c\uff0c\u521b\u5efa\u53c2\u6570\u5316\u690d\u7269\u51e0\u4f55\u4f53\u4f5c\u4e3a\u5206\u5c42B\u6837\u6761\u66f2\u9762\u8868\u793a\uff0c\u5177\u6709\u690d\u7269\u5b66\u7ea6\u675f\u3001\u663e\u5f0f\u63a7\u5236\u70b9\u548c\u53c2\u6570\u53d8\u5f62\u51fd\u6570\u3002", "result": "\u5728\u7389\u7c73\u3001\u5927\u8c46\u548c\u7eff\u8c46\u4e0a\u6f14\u793a\u4e86\u8be5\u6846\u67b6\uff0c\u901a\u8fc7\u624b\u52a8\u7cbe\u70bc\u690d\u7269\u63cf\u8ff0\u7b26\uff08PD\uff09\u6587\u4ef6\u5c06\u7a0b\u5e8f\u5316\u6a21\u578b\u62df\u5408\u5230\u7ecf\u9a8c\u70b9\u4e91\u6570\u636e\uff0c\u751f\u6210\u7528\u4e8e\u53ef\u89c6\u5316\u7684\u4e09\u89d2\u7f51\u683c\u548c\u7528\u4e8e\u5b9a\u91cf\u5206\u6790\u7684\u5e26\u53c2\u6570\u5143\u6570\u636e\u7684\u4e09\u89d2\u7f51\u683c\u3002", "conclusion": "FloraForge\u72ec\u7279\u5730\u7ed3\u5408\u4e86LLM\u8f85\u52a9\u6a21\u677f\u521b\u5efa\u3001\u652f\u6301\u8868\u578b\u5206\u6790\u548c\u6e32\u67d3\u7684\u6570\u5b66\u8fde\u7eed\u8868\u793a\uff0c\u4ee5\u53ca\u901a\u8fc7PD\u7684\u76f4\u63a5\u53c2\u6570\u63a7\u5236\uff0c\u4e3a\u690d\u7269\u79d1\u5b66\u6c11\u4e3b\u5316\u4e86\u590d\u6742\u51e0\u4f55\u5efa\u6a21\uff0c\u540c\u65f6\u4fdd\u6301\u6570\u5b66\u4e25\u8c28\u6027\u3002"}}
{"id": "2512.11863", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11863", "abs": "https://arxiv.org/abs/2512.11863", "authors": ["Julian Sch\u00f6n", "Lena Hoffmann", "Nikolas Becker"], "title": "Expert Assessment: The Systemic Environmental Risks of Artficial Intelligence", "comment": null, "summary": "Artificial intelligence (AI) is often presented as a key tool for addressing societal challenges, such as climate change. At the same time, AI's environmental footprint is expanding increasingly. This report describes the systemic environmental risks of artificial intelligence, in particular, moving beyond direct impacts such as energy and water usage. Systemic environmental risks of AI are emergent, cross-sector harms to climate, biodiversity, freshwater, and broader socioecological systems that arise primarily from AI's integration into social, economic, and physical infrastructures, rather than its direct resource use, and that propagate through feedbacks, yielding nonlinear, inequitable, and potentially irreversible impacts. While these risks are emergent and quantification is uncertain, this report aims to provide an overview of systemic environmental risks. Drawing on a narrative literature review, we propose a three-level framework that operationalizes systemic risk analysis. The framework identifies the structural conditions that shape AI development, the risk amplification mechanisms that propagate environmental harm, and the impacts that manifest as observable ecological and social consequences. We illustrate the framework in expert-interview-based case studies across agriculture and biodiversity, oil and gas, and waste management.", "AI": {"tldr": "\u8be5\u62a5\u544a\u63d0\u51fa\u4e86\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u73af\u5883\u98ce\u9669\u7684\u5206\u6790\u6846\u67b6\uff0c\u8d85\u8d8a\u76f4\u63a5\u8d44\u6e90\u6d88\u8017\uff0c\u5173\u6ce8AI\u6574\u5408\u5230\u793e\u4f1a\u3001\u7ecf\u6d4e\u3001\u7269\u7406\u57fa\u7840\u8bbe\u65bd\u4e2d\u4ea7\u751f\u7684\u7cfb\u7edf\u6027\u3001\u8de8\u90e8\u95e8\u73af\u5883\u5371\u5bb3\u3002", "motivation": "AI\u5e38\u88ab\u89c6\u4e3a\u89e3\u51b3\u6c14\u5019\u53d8\u5316\u7b49\u793e\u4f1a\u6311\u6218\u7684\u5173\u952e\u5de5\u5177\uff0c\u4f46\u5176\u73af\u5883\u8db3\u8ff9\u4e0d\u65ad\u6269\u5927\u3002\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8AI\u7684\u76f4\u63a5\u73af\u5883\u5f71\u54cd\uff08\u5982\u80fd\u8017\u3001\u7528\u6c34\uff09\uff0c\u800c\u5ffd\u89c6\u4e86AI\u6574\u5408\u5230\u793e\u4f1a\u3001\u7ecf\u6d4e\u3001\u7269\u7406\u57fa\u7840\u8bbe\u65bd\u4e2d\u4ea7\u751f\u7684\u7cfb\u7edf\u6027\u3001\u8de8\u90e8\u95e8\u73af\u5883\u98ce\u9669\u3002", "method": "\u901a\u8fc7\u53d9\u4e8b\u6027\u6587\u732e\u7efc\u8ff0\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u5c42\u6846\u67b6\u6765\u64cd\u4f5c\u5316\u7cfb\u7edf\u6027\u98ce\u9669\u5206\u6790\uff1a1\uff09\u5851\u9020AI\u53d1\u5c55\u7684\u7ed3\u6784\u6761\u4ef6\uff1b2\uff09\u4f20\u64ad\u73af\u5883\u5371\u5bb3\u7684\u98ce\u9669\u653e\u5927\u673a\u5236\uff1b3\uff09\u8868\u73b0\u4e3a\u53ef\u89c2\u5bdf\u751f\u6001\u548c\u793e\u4f1a\u540e\u679c\u7684\u5f71\u54cd\u3002\u5728\u519c\u4e1a\u4e0e\u751f\u7269\u591a\u6837\u6027\u3001\u77f3\u6cb9\u5929\u7136\u6c14\u3001\u5e9f\u7269\u7ba1\u7406\u4e09\u4e2a\u9886\u57df\u8fdb\u884c\u4e86\u4e13\u5bb6\u8bbf\u8c08\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u6027\u73af\u5883\u98ce\u9669\u5206\u6790\u6846\u67b6\uff0c\u8bc6\u522b\u4e86AI\u6574\u5408\u5230\u57fa\u7840\u8bbe\u65bd\u4e2d\u53ef\u80fd\u4ea7\u751f\u7684\u975e\u7ebf\u6027\u3001\u4e0d\u516c\u5e73\u4e14\u53ef\u80fd\u4e0d\u53ef\u9006\u7684\u73af\u5883\u5f71\u54cd\u3002\u8fd9\u4e9b\u98ce\u9669\u662f\u65b0\u5174\u7684\uff0c\u91cf\u5316\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\uff0c\u4f46\u6846\u67b6\u4e3a\u7406\u89e3\u548c\u8bc4\u4f30\u8fd9\u4e9b\u98ce\u9669\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u65b9\u6cd5\u3002", "conclusion": "AI\u7684\u7cfb\u7edf\u6027\u73af\u5883\u98ce\u9669\u8d85\u8d8a\u4e86\u76f4\u63a5\u8d44\u6e90\u6d88\u8017\uff0c\u9700\u8981\u5173\u6ce8\u5176\u6574\u5408\u5230\u793e\u4f1a\u3001\u7ecf\u6d4e\u3001\u7269\u7406\u57fa\u7840\u8bbe\u65bd\u4e2d\u4ea7\u751f\u7684\u8de8\u90e8\u95e8\u3001\u975e\u7ebf\u6027\u73af\u5883\u5371\u5bb3\u3002\u63d0\u51fa\u7684\u4e09\u5c42\u6846\u67b6\u4e3a\u5206\u6790\u8fd9\u4e9b\u7cfb\u7edf\u6027\u98ce\u9669\u63d0\u4f9b\u4e86\u64cd\u4f5c\u5316\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u66f4\u5168\u9762\u5730\u8bc4\u4f30AI\u7684\u73af\u5883\u5f71\u54cd\u3002"}}
{"id": "2512.12088", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12088", "abs": "https://arxiv.org/abs/2512.12088", "authors": ["S. R. Eshwar", "Aniruddha Mukherjee", "Kintan Saha", "Krishna Agarwal", "Gugan Thoppe", "Aditya Gopalan", "Gal Dalal"], "title": "Reliable Policy Iteration: Performance Robustness Across Architecture and Environment Perturbations", "comment": null, "summary": "In a recent work, we proposed Reliable Policy Iteration (RPI), that restores policy iteration's monotonicity-of-value-estimates property to the function approximation setting. Here, we assess the robustness of RPI's empirical performance on two classical control tasks -- CartPole and Inverted Pendulum -- under changes to neural network and environmental parameters. Relative to DQN, Double DQN, DDPG, TD3, and PPO, RPI reaches near-optimal performance early and sustains this policy as training proceeds. Because deep RL methods are often hampered by sample inefficiency, training instability, and hyperparameter sensitivity, our results highlight RPI's promise as a more reliable alternative.", "AI": {"tldr": "RPI\uff08\u53ef\u9760\u7b56\u7565\u8fed\u4ee3\uff09\u5728\u51fd\u6570\u903c\u8fd1\u573a\u666f\u4e2d\u6062\u590d\u4e86\u7b56\u7565\u8fed\u4ee3\u7684\u5355\u8c03\u6027\uff0c\u5728CartPole\u548c\u5012\u7acb\u6446\u4efb\u52a1\u4e2d\u76f8\u6bd4\u4e3b\u6d41\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u5b58\u5728\u6837\u672c\u6548\u7387\u4f4e\u3001\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u8d85\u53c2\u6570\u654f\u611f\u7b49\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u53ef\u9760\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u63d0\u51faReliable Policy Iteration (RPI)\u65b9\u6cd5\uff0c\u5728\u51fd\u6570\u903c\u8fd1\u8bbe\u7f6e\u4e2d\u6062\u590d\u7b56\u7565\u8fed\u4ee3\u7684\u5355\u8c03\u6027\u7279\u6027\uff0c\u5e76\u5728CartPole\u548cInverted Pendulum\u4e24\u4e2a\u7ecf\u5178\u63a7\u5236\u4efb\u52a1\u4e2d\u6d4b\u8bd5\u5176\u9c81\u68d2\u6027\u3002", "result": "\u76f8\u6bd4DQN\u3001Double DQN\u3001DDPG\u3001TD3\u548cPPO\u7b49\u65b9\u6cd5\uff0cRPI\u80fd\u66f4\u65e9\u8fbe\u5230\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd\uff0c\u5e76\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u7a33\u5b9a\u7684\u7b56\u7565\u8868\u73b0\u3002", "conclusion": "RPI\u4f5c\u4e3a\u4e00\u79cd\u66f4\u53ef\u9760\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u6837\u672c\u6548\u7387\u3001\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u8d85\u53c2\u6570\u9c81\u68d2\u6027\u65b9\u9762\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2512.12885", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.IR", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12885", "abs": "https://arxiv.org/abs/2512.12885", "authors": ["Minghao Zhu", "Zhihao Zhang", "Anmol Sidhu", "Keith Redmill"], "title": "SignRAG: A Retrieval-Augmented System for Scalable Zero-Shot Road Sign Recognition", "comment": "Submitted to IV 2026", "summary": "Automated road sign recognition is a critical task for intelligent transportation systems, but traditional deep learning methods struggle with the sheer number of sign classes and the impracticality of creating exhaustive labeled datasets. This paper introduces a novel zero-shot recognition framework that adapts the Retrieval-Augmented Generation (RAG) paradigm to address this challenge. Our method first uses a Vision Language Model (VLM) to generate a textual description of a sign from an input image. This description is used to retrieve a small set of the most relevant sign candidates from a vector database of reference designs. Subsequently, a Large Language Model (LLM) reasons over the retrieved candidates to make a final, fine-grained recognition. We validate this approach on a comprehensive set of 303 regulatory signs from the Ohio MUTCD. Experimental results demonstrate the framework's effectiveness, achieving 95.58% accuracy on ideal reference images and 82.45% on challenging real-world road data. This work demonstrates the viability of RAG-based architectures for creating scalable and accurate systems for road sign recognition without task-specific training.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7684\u96f6\u6837\u672c\u4ea4\u901a\u6807\u5fd7\u8bc6\u522b\u6846\u67b6\uff0c\u65e0\u9700\u7279\u5b9a\u4efb\u52a1\u8bad\u7ec3\u5373\u53ef\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u8bc6\u522b", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9762\u4e34\u4ea4\u901a\u6807\u5fd7\u7c7b\u522b\u7e41\u591a\u3001\u521b\u5efa\u8be6\u5c3d\u6807\u6ce8\u6570\u636e\u96c6\u4e0d\u5207\u5b9e\u9645\u7684\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u65e0\u9700\u7279\u5b9a\u8bad\u7ec3\u7684\u53ef\u6269\u5c55\u8bc6\u522b\u7cfb\u7edf", "method": "\u91c7\u7528RAG\u8303\u5f0f\uff1a1) \u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLM)\u4ece\u8f93\u5165\u56fe\u50cf\u751f\u6210\u6587\u672c\u63cf\u8ff0\uff1b2) \u4ece\u53c2\u8003\u8bbe\u8ba1\u5411\u91cf\u6570\u636e\u5e93\u4e2d\u68c0\u7d22\u6700\u76f8\u5173\u7684\u5019\u9009\u6807\u5fd7\uff1b3) \u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u5bf9\u68c0\u7d22\u7ed3\u679c\u8fdb\u884c\u63a8\u7406\uff0c\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u8bc6\u522b", "result": "\u5728\u4fc4\u4ea5\u4fc4\u5ddeMUTCD\u7684303\u4e2a\u76d1\u7ba1\u6807\u5fd7\u4e0a\u9a8c\u8bc1\uff0c\u7406\u60f3\u53c2\u8003\u56fe\u50cf\u51c6\u786e\u7387\u8fbe95.58%\uff0c\u5177\u6709\u6311\u6218\u6027\u7684\u771f\u5b9e\u9053\u8def\u6570\u636e\u51c6\u786e\u7387\u8fbe82.45%", "conclusion": "\u57fa\u4e8eRAG\u7684\u67b6\u6784\u4e3a\u521b\u5efa\u65e0\u9700\u7279\u5b9a\u4efb\u52a1\u8bad\u7ec3\u7684\u53ef\u6269\u5c55\u3001\u51c6\u786e\u7684\u9053\u8def\u6807\u5fd7\u8bc6\u522b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2512.11868", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11868", "abs": "https://arxiv.org/abs/2512.11868", "authors": ["Alexander Windmann", "Benedikt Stratmann", "Mariya Lyashenko", "Oliver Niggemann"], "title": "Industrial AI Robustness Card: Evaluating and Monitoring Time Series Models", "comment": null, "summary": "Industrial AI practitioners face vague robustness requirements in emerging regulations and standards but lack concrete, implementation ready protocols. This paper introduces the Industrial AI Robustness Card (IARC), a lightweight, task agnostic protocol for documenting and evaluating the robustness of AI models on industrial time series. The IARC specifies required fields and an empirical measurement and reporting protocol that combines drift monitoring, uncertainty quantification, and stress tests, and it maps these to relevant EU AI Act obligations. A soft sensor case study on a biopharmaceutical fermentation process illustrates how the IARC supports reproducible robustness evidence and continuous monitoring.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5de5\u4e1aAI\u9c81\u68d2\u6027\u5361\u7247\uff08IARC\uff09\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u4efb\u52a1\u65e0\u5173\u7684\u534f\u8bae\uff0c\u7528\u4e8e\u8bb0\u5f55\u548c\u8bc4\u4f30\u5de5\u4e1a\u65f6\u95f4\u5e8f\u5217AI\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u6ee1\u8db3\u6b27\u76dfAI\u6cd5\u6848\u8981\u6c42\u3002", "motivation": "\u5de5\u4e1aAI\u4ece\u4e1a\u8005\u9762\u4e34\u65b0\u5174\u6cd5\u89c4\u548c\u6807\u51c6\u4e2d\u6a21\u7cca\u7684\u9c81\u68d2\u6027\u8981\u6c42\uff0c\u4f46\u7f3a\u4e4f\u5177\u4f53\u3001\u53ef\u5b9e\u65bd\u7684\u534f\u8bae\u3002\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u65b9\u6cd5\u6765\u8bb0\u5f55\u548c\u8bc4\u4f30AI\u6a21\u578b\u5728\u5de5\u4e1a\u65f6\u95f4\u5e8f\u5217\u4e0a\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u5de5\u4e1aAI\u9c81\u68d2\u6027\u5361\u7247\uff08IARC\uff09\u534f\u8bae\uff0c\u5305\u542b\u5fc5\u9700\u5b57\u6bb5\u548c\u5b9e\u8bc1\u6d4b\u91cf\u62a5\u544a\u534f\u8bae\uff0c\u7ed3\u5408\u6f02\u79fb\u76d1\u6d4b\u3001\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u538b\u529b\u6d4b\u8bd5\uff0c\u5e76\u5c06\u5176\u6620\u5c04\u5230\u6b27\u76dfAI\u6cd5\u6848\u76f8\u5173\u4e49\u52a1\u3002", "result": "\u901a\u8fc7\u751f\u7269\u5236\u836f\u53d1\u9175\u8fc7\u7a0b\u7684\u8f6f\u4f20\u611f\u5668\u6848\u4f8b\u7814\u7a76\uff0c\u5c55\u793a\u4e86IARC\u5982\u4f55\u652f\u6301\u53ef\u590d\u73b0\u7684\u9c81\u68d2\u6027\u8bc1\u636e\u548c\u6301\u7eed\u76d1\u6d4b\u3002", "conclusion": "IARC\u4e3a\u5de5\u4e1aAI\u6a21\u578b\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u9c81\u68d2\u6027\u6587\u6863\u5316\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u5e2e\u52a9\u4ece\u4e1a\u8005\u6ee1\u8db3\u6cd5\u89c4\u8981\u6c42\u5e76\u786e\u4fdd\u6a21\u578b\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2512.12175", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12175", "abs": "https://arxiv.org/abs/2512.12175", "authors": ["Haoyang Chen", "Richong Zhang", "Junfan Chen"], "title": "Rethinking Label Consistency of In-Context Learning: An Implicit Transductive Label Propagation Perspective", "comment": null, "summary": "Large language models (LLMs) perform in-context learning (ICL) with minimal supervised examples, which benefits various natural language processing (NLP) tasks. One of the critical research focus is the selection of prompt demonstrations. Current approaches typically employ retrieval models to select the top-K most semantically similar examples as demonstrations. However, we argue that existing methods are limited since the label consistency is not guaranteed during demonstration selection. Our cognition derives from the Bayesian view of ICL and our rethinking of ICL from the transductive label propagation perspective. We treat ICL as a transductive learning method and incorporate latent concepts from Bayesian view and deduce that similar demonstrations guide the concepts of query, with consistent labels serving as estimates. Based on this understanding, we establish a label propagation framework to link label consistency with propagation error bounds. To model label consistency, we propose a data synthesis method, leveraging both semantic and label information, and use TopK sampling with Synthetic Data (TopK-SD) to acquire demonstrations with consistent labels. TopK-SD outperforms original TopK sampling on multiple benchmarks. Our work provides a new perspective for understanding the working mechanisms within ICL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faTopK-SD\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u8bed\u4e49\u548c\u6807\u7b7e\u4fe1\u606f\u5408\u6210\u6570\u636e\uff0c\u9009\u62e9\u6807\u7b7e\u4e00\u81f4\u7684\u6f14\u793a\u793a\u4f8b\uff0c\u63d0\u5347\u4e0a\u4e0b\u6587\u5b66\u4e60\u6548\u679c", "motivation": "\u5f53\u524d\u57fa\u4e8e\u68c0\u7d22\u7684\u65b9\u6cd5\u9009\u62e9\u8bed\u4e49\u76f8\u4f3c\u793a\u4f8b\u4f5c\u4e3a\u6f14\u793a\uff0c\u4f46\u5ffd\u7565\u4e86\u6807\u7b7e\u4e00\u81f4\u6027\uff0c\u8fd9\u9650\u5236\u4e86\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u6027\u80fd\u63d0\u5347", "method": "\u63d0\u51faTopK-SD\u65b9\u6cd5\uff1a1\uff09\u4ece\u8d1d\u53f6\u65af\u89c6\u89d2\u548c\u8f6c\u5bfc\u6807\u7b7e\u4f20\u64ad\u89c6\u89d2\u91cd\u65b0\u7406\u89e3ICL\uff1b2\uff09\u5efa\u7acb\u6807\u7b7e\u4f20\u64ad\u6846\u67b6\uff1b3\uff09\u5229\u7528\u8bed\u4e49\u548c\u6807\u7b7e\u4fe1\u606f\u5408\u6210\u6570\u636e\uff1b4\uff09\u9009\u62e9\u6807\u7b7e\u4e00\u81f4\u7684\u6f14\u793a\u793a\u4f8b", "result": "TopK-SD\u65b9\u6cd5\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u539f\u59cb\u7684TopK\u91c7\u6837\u65b9\u6cd5", "conclusion": "\u672c\u6587\u4e3a\u7406\u89e3\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u5de5\u4f5c\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5f3a\u8c03\u4e86\u6807\u7b7e\u4e00\u81f4\u6027\u7684\u91cd\u8981\u6027"}}
{"id": "2512.11847", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11847", "abs": "https://arxiv.org/abs/2512.11847", "authors": ["Antonio Roye-Azar", "Santiago Vargas-Naranjo", "Dhruv Ghai", "Nithin Balamurugan", "Rayan Amir"], "title": "Tiny Recursive Models on ARC-AGI-1: Inductive Biases, Identity Conditioning, and Test-Time Compute", "comment": "13 pages, 0 figures, 6 tables", "summary": "Tiny Recursive Models (TRM) were proposed as a parameter-efficient alternative to large language models for solving Abstraction and Reasoning Corpus (ARC) style tasks. The original work reports strong performance and suggests that recursive latent updates enable non-trivial reasoning, but it remains unclear how much of this performance stems from architecture, test-time compute, or task-specific priors. In this technical note, we empirically analyze the ARC Prize TRM checkpoint on ARC-AGI-1 and report four behavioral findings and an efficiency comparison. First, we show that test-time augmentation and majority-vote ensembling account for a substantial fraction of reported performance: the 1000-sample voting pipeline improves Pass@1 by about 11 percentage points over single-pass canonical inference. Second, a puzzle-identity ablation reveals strict dependence on task identifiers: replacing the correct puzzle ID with a blank or random token yields zero accuracy. Third, a recursion trajectory analysis shows that most of the final accuracy is achieved at the first recursion step and that performance saturates after few latent updates, indicating shallow effective recursion. Fourth, early-stage training experiments under canonical versus heavy augmentation regimes suggest that heavy augmentation broadens the distribution of candidate solutions and improves multi-sample success. Finally, we compare TRM with a naive QLoRA fine-tune of Llama 3 8B on canonical ARC-AGI-1, finding that TRM's non-autoregressive design achieves much higher throughput and substantially lower memory usage in this setting. Overall, TRM's ARC-AGI-1 performance appears to arise from an interaction between efficiency, task-specific conditioning, and aggressive test-time compute rather than deep internal reasoning.", "AI": {"tldr": "\u8be5\u6280\u672f\u62a5\u544a\u5bf9Tiny Recursive Models (TRM)\u5728ARC-AGI-1\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u8fdb\u884c\u4e86\u5b9e\u8bc1\u5206\u6790\uff0c\u53d1\u73b0\u5176\u6027\u80fd\u4e3b\u8981\u6e90\u4e8e\u6d4b\u8bd5\u65f6\u589e\u5f3a\u3001\u591a\u6570\u6295\u7968\u96c6\u6210\u3001\u4efb\u52a1\u6807\u8bc6\u7b26\u4f9d\u8d56\u548c\u6d45\u5c42\u9012\u5f52\uff0c\u800c\u975e\u6df1\u5ea6\u5185\u90e8\u63a8\u7406\u3002", "motivation": "TRM\u88ab\u63d0\u51fa\u4f5c\u4e3a\u89e3\u51b3ARC\u98ce\u683c\u4efb\u52a1\u7684\u9ad8\u6548\u53c2\u6570\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5176\u6027\u80fd\u6765\u6e90\u5c1a\u4e0d\u660e\u786e\u2014\u2014\u4e0d\u6e05\u695a\u591a\u5c11\u6765\u81ea\u67b6\u6784\u672c\u8eab\u3001\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u8fd8\u662f\u4efb\u52a1\u7279\u5b9a\u5148\u9a8c\u3002\u672c\u7814\u7a76\u65e8\u5728\u5b9e\u8bc1\u5206\u6790TRM\u5728ARC-AGI-1\u4e0a\u7684\u5b9e\u9645\u8868\u73b0\u673a\u5236\u3002", "method": "\u5bf9ARC Prize TRM\u68c0\u67e5\u70b9\u5728ARC-AGI-1\u4e0a\u8fdb\u884c\u591a\u7ef4\u5ea6\u5206\u6790\uff1a1) \u6d4b\u8bd5\u65f6\u589e\u5f3a\u548c\u591a\u6570\u6295\u7968\u96c6\u6210\u7684\u5f71\u54cd\u8bc4\u4f30\uff1b2) \u4efb\u52a1\u6807\u8bc6\u7b26\u6d88\u878d\u5b9e\u9a8c\uff1b3) \u9012\u5f52\u8f68\u8ff9\u5206\u6790\uff1b4) \u4e0d\u540c\u589e\u5f3a\u8bad\u7ec3\u7b56\u7565\u6bd4\u8f83\uff1b5) \u4e0eLlama 3 8B QLoRA\u5fae\u8c03\u7684\u6548\u7387\u5bf9\u6bd4\u3002", "result": "1) 1000\u6837\u672c\u6295\u7968\u7ba1\u9053\u6bd4\u5355\u6b21\u63a8\u7406\u63d0\u5347\u7ea611\u4e2a\u767e\u5206\u70b9\uff1b2) \u66ff\u6362\u6b63\u786e\u8c1c\u9898ID\u4f1a\u5bfc\u81f4\u96f6\u51c6\u786e\u7387\uff0c\u663e\u793a\u4e25\u683c\u4f9d\u8d56\u4efb\u52a1\u6807\u8bc6\u7b26\uff1b3) \u5927\u90e8\u5206\u51c6\u786e\u7387\u5728\u7b2c\u4e00\u6b65\u9012\u5f52\u5373\u8fbe\u6210\uff0c\u9012\u5f52\u6df1\u5ea6\u8f83\u6d45\uff1b4) \u5f3a\u589e\u5f3a\u8bad\u7ec3\u80fd\u6269\u5c55\u5019\u9009\u89e3\u5206\u5e03\uff1b5) TRM\u7684\u975e\u81ea\u56de\u5f52\u8bbe\u8ba1\u5728\u541e\u5410\u91cf\u548c\u5185\u5b58\u4f7f\u7528\u4e0a\u663e\u8457\u4f18\u4e8eLlama 3 8B QLoRA\u3002", "conclusion": "TRM\u5728ARC-AGI-1\u4e0a\u7684\u6027\u80fd\u4e3b\u8981\u6e90\u4e8e\u6548\u7387\u4f18\u52bf\u3001\u4efb\u52a1\u7279\u5b9a\u6761\u4ef6\u7f16\u7801\u548c\u6fc0\u8fdb\u7684\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u7b56\u7565\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u800c\u975e\u6df1\u5ea6\u5185\u90e8\u63a8\u7406\u80fd\u529b\u3002\u5176\u9012\u5f52\u673a\u5236\u5b9e\u9645\u4e0a\u76f8\u5bf9\u6d45\u5c42\uff0c\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u5916\u90e8\u589e\u5f3a\u548c\u4efb\u52a1\u6807\u8bc6\u7b26\u3002"}}
{"id": "2512.11928", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11928", "abs": "https://arxiv.org/abs/2512.11928", "authors": ["Alexander Peysakhovich", "William Berman", "Joseph Rufo", "Felix Wong", "Maxwell Z. Wilson"], "title": "MONET -- Virtual Cell Painting of Brightfield Images and Time Lapses Using Reference Consistent Diffusion", "comment": null, "summary": "Cell painting is a popular technique for creating human-interpretable, high-contrast images of cell morphology. There are two major issues with cell paint: (1) it is labor-intensive and (2) it requires chemical fixation, making the study of cell dynamics impossible. We train a diffusion model (Morphological Observation Neural Enhancement Tool, or MONET) on a large dataset to predict cell paint channels from brightfield images. We show that model quality improves with scale. The model uses a consistency architecture to generate time-lapse videos, despite the impossibility of obtaining cell paint video training data. In addition, we show that this architecture enables a form of in-context learning, allowing the model to partially transfer to out-of-distribution cell lines and imaging protocols. Virtual cell painting is not intended to replace physical cell painting completely, but to act as a complementary tool enabling novel workflows in biological research.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86MONET\u6269\u6563\u6a21\u578b\uff0c\u53ef\u4ece\u660e\u573a\u56fe\u50cf\u9884\u6d4b\u7ec6\u80de\u67d3\u8272\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7ec6\u80de\u67d3\u8272\u6280\u672f\u52b3\u52a8\u5bc6\u96c6\u4e14\u65e0\u6cd5\u7814\u7a76\u7ec6\u80de\u52a8\u6001\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7ec6\u80de\u67d3\u8272\u6280\u672f\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u4e00\u662f\u52b3\u52a8\u5bc6\u96c6\u578b\uff0c\u9700\u8981\u5927\u91cf\u4eba\u5de5\u64cd\u4f5c\uff1b\u4e8c\u662f\u9700\u8981\u5316\u5b66\u56fa\u5b9a\uff0c\u65e0\u6cd5\u7814\u7a76\u7ec6\u80de\u52a8\u6001\u53d8\u5316\u3002\u7814\u7a76\u4eba\u5458\u5e0c\u671b\u5f00\u53d1\u4e00\u79cd\u66ff\u4ee3\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u8bad\u7ec3\u4e86\u4e00\u4e2a\u6269\u6563\u6a21\u578b\uff08MONET\uff09\uff0c\u4f7f\u7528\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4ece\u660e\u573a\u56fe\u50cf\u9884\u6d4b\u7ec6\u80de\u67d3\u8272\u901a\u9053\u3002\u6a21\u578b\u91c7\u7528\u4e00\u81f4\u6027\u67b6\u6784\uff0c\u80fd\u591f\u751f\u6210\u65f6\u95f4\u5e8f\u5217\u89c6\u9891\uff0c\u5c3d\u7ba1\u6ca1\u6709\u7ec6\u80de\u67d3\u8272\u89c6\u9891\u8bad\u7ec3\u6570\u636e\u3002\u8be5\u67b6\u6784\u8fd8\u652f\u6301\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u90e8\u5206\u9002\u5e94\u5206\u5e03\u5916\u7684\u7ec6\u80de\u7cfb\u548c\u6210\u50cf\u534f\u8bae\u3002", "result": "\u6a21\u578b\u8d28\u91cf\u968f\u7740\u89c4\u6a21\u6269\u5927\u800c\u63d0\u9ad8\u3002\u4e00\u81f4\u6027\u67b6\u6784\u80fd\u591f\u751f\u6210\u65f6\u95f4\u5e8f\u5217\u89c6\u9891\uff0c\u5e76\u4e14\u652f\u6301\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u90e8\u5206\u9002\u5e94\u5206\u5e03\u5916\u7684\u7ec6\u80de\u7cfb\u548c\u6210\u50cf\u534f\u8bae\u3002", "conclusion": "\u865a\u62df\u7ec6\u80de\u67d3\u8272\u5e76\u975e\u5b8c\u5168\u66ff\u4ee3\u7269\u7406\u7ec6\u80de\u67d3\u8272\uff0c\u800c\u662f\u4f5c\u4e3a\u8865\u5145\u5de5\u5177\uff0c\u4e3a\u751f\u7269\u5b66\u7814\u7a76\u63d0\u4f9b\u65b0\u7684\u5de5\u4f5c\u6d41\u7a0b\u53ef\u80fd\u6027\u3002"}}
{"id": "2512.11870", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11870", "abs": "https://arxiv.org/abs/2512.11870", "authors": ["Mulham Fawkherji", "Bruce Race", "Driss Benhaddou"], "title": "Using Socio-economic Indicators, Smart Transit Systems, and Urban Simulator to Accelerate ZEV Adoption and Reduce VMT", "comment": null, "summary": "Globally, on-road transportation accounts for 15% of greenhouse gas (GHG) emissions and an estimated 385,000 premature deaths from PM2.5. Cities play a critical role in meeting IPCC targets, generating 75% of global energy-related GHG emissions. In Houston, Texas, on-road transportation represents 48% of baseline emissions in the Climate Action Plan (CAP). To reach net-zero by 2050, the CAP targets a 70% emissions reduction from a 2014 baseline, offset by 30% renewable energy. This goal is challenging because Houston is low-density and auto-dependent, with 89% of on-road emissions from cars and small trucks and limited public transit usage. Socio-economic disparities further constrain Zero Emissions Vehicle (ZEV) adoption. Strategies focus on expanding ZEV access and reducing Vehicle Miles Traveled (VMT) by 20% through transit improvements and city design. This paper presents methods for establishing an on-road emissions baseline and evaluating policies that leverage socio-economic indicators and Intelligent Transportation Systems (ITS) to accelerate ZEV adoption and reduce VMT. Smart parking, transit incentives, secure data systems, and ZEV fleet management support improvements in modal split and system reliability. Policy options are analyzed and potential actions identified. To support evaluation, a simulation environment was developed in Unity 3D, enabling dynamic modeling of urban mobility and visualization of policy scenarios. Auto-dependent cities aiming for 2050 emission targets can benefit from the indicators, metrics, and technologies discussed.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9488\u5bf9\u4f11\u65af\u987f\u7b49\u4f4e\u5bc6\u5ea6\u6c7d\u8f66\u4f9d\u8d56\u578b\u57ce\u5e02\uff0c\u63d0\u51fa\u4e86\u7ed3\u5408\u793e\u4f1a\u7ecf\u6d4e\u6307\u6807\u548c\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u6765\u52a0\u901f\u96f6\u6392\u653e\u8f66\u8f86\u666e\u53ca\u3001\u51cf\u5c11\u8f66\u8f86\u884c\u9a76\u91cc\u7a0b\u7684\u65b9\u6cd5\uff0c\u4ee5\u8fbe\u62102050\u5e74\u51c0\u96f6\u6392\u653e\u76ee\u6807\u3002", "motivation": "\u5168\u7403\u9053\u8def\u8fd0\u8f93\u5360\u6e29\u5ba4\u6c14\u4f53\u6392\u653e\u768415%\uff0c\u9020\u6210\u5927\u91cfPM2.5\u76f8\u5173\u8fc7\u65e9\u6b7b\u4ea1\u3002\u4f11\u65af\u987f\u4f5c\u4e3a\u4f4e\u5bc6\u5ea6\u3001\u6c7d\u8f66\u4f9d\u8d56\u578b\u57ce\u5e02\uff0c\u9053\u8def\u8fd0\u8f93\u5360\u5176\u6c14\u5019\u884c\u52a8\u8ba1\u5212\u57fa\u51c6\u6392\u653e\u768448%\uff0c\u8981\u5b9e\u73b02050\u5e74\u51c0\u96f6\u6392\u653e\u76ee\u6807\u9762\u4e34\u5de8\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u793e\u4f1a\u7ecf\u6d4e\u5dee\u5f02\u9650\u5236\u4e86\u96f6\u6392\u653e\u8f66\u8f86\u7684\u666e\u53ca\u3002", "method": "\u5efa\u7acb\u9053\u8def\u6392\u653e\u57fa\u51c6\uff0c\u5229\u7528\u793e\u4f1a\u7ecf\u6d4e\u6307\u6807\u548c\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u8bc4\u4f30\u653f\u7b56\u65b9\u6848\u3002\u5f00\u53d1Unity 3D\u4eff\u771f\u73af\u5883\uff0c\u52a8\u6001\u6a21\u62df\u57ce\u5e02\u4ea4\u901a\u5e76\u53ef\u89c6\u5316\u653f\u7b56\u60c5\u666f\u3002\u5177\u4f53\u7b56\u7565\u5305\u62ec\u667a\u80fd\u505c\u8f66\u3001\u516c\u4ea4\u6fc0\u52b1\u3001\u5b89\u5168\u6570\u636e\u7cfb\u7edf\u548c\u96f6\u6392\u653e\u8f66\u961f\u7ba1\u7406\u3002", "result": "\u63d0\u51fa\u4e86\u652f\u6301\u8bc4\u4f30\u7684\u4eff\u771f\u73af\u5883\uff0c\u80fd\u591f\u52a8\u6001\u5efa\u6a21\u57ce\u5e02\u4ea4\u901a\u5e76\u53ef\u89c6\u5316\u653f\u7b56\u60c5\u666f\u3002\u8bc6\u522b\u4e86\u653f\u7b56\u9009\u9879\u548c\u6f5c\u5728\u884c\u52a8\uff0c\u4e3a\u6c7d\u8f66\u4f9d\u8d56\u578b\u57ce\u5e02\u5b9e\u73b02050\u5e74\u6392\u653e\u76ee\u6807\u63d0\u4f9b\u4e86\u6307\u6807\u3001\u5ea6\u91cf\u548c\u6280\u672f\u652f\u6301\u3002", "conclusion": "\u6c7d\u8f66\u4f9d\u8d56\u578b\u57ce\u5e02\u8981\u5b9e\u73b02050\u5e74\u6392\u653e\u76ee\u6807\uff0c\u9700\u8981\u7ed3\u5408\u793e\u4f1a\u7ecf\u6d4e\u6307\u6807\u548c\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u6280\u672f\uff0c\u901a\u8fc7\u52a0\u901f\u96f6\u6392\u653e\u8f66\u8f86\u666e\u53ca\u548c\u51cf\u5c11\u8f66\u8f86\u884c\u9a76\u91cc\u7a0b\u7684\u7efc\u5408\u7b56\u7565\u3002\u8bba\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u3001\u6307\u6807\u548c\u6280\u672f\u53ef\u4e3a\u7c7b\u4f3c\u57ce\u5e02\u63d0\u4f9b\u53c2\u8003\u3002"}}
{"id": "2512.12177", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12177", "abs": "https://arxiv.org/abs/2512.12177", "authors": ["Aydin Ayanzadeh", "Tim Oates"], "title": "Floorplan2Guide: LLM-Guided Floorplan Parsing for BLV Indoor Navigation", "comment": "Accepted for publication in the proceedings of the IEEE International Conference on Big Data (IEEE BigData 2025)", "summary": "Indoor navigation remains a critical challenge for people with visual impairments. The current solutions mainly rely on infrastructure-based systems, which limit their ability to navigate safely in dynamic environments. We propose a novel navigation approach that utilizes a foundation model to transform floor plans into navigable knowledge graphs and generate human-readable navigation instructions. Floorplan2Guide integrates a large language model (LLM) to extract spatial information from architectural layouts, reducing the manual preprocessing required by earlier floorplan parsing methods. Experimental results indicate that few-shot learning improves navigation accuracy in comparison to zero-shot learning on simulated and real-world evaluations. Claude 3.7 Sonnet achieves the highest accuracy among the evaluated models, with 92.31%, 76.92%, and 61.54% on the short, medium, and long routes, respectively, under 5-shot prompting of the MP-1 floor plan. The success rate of graph-based spatial structure is 15.4% higher than that of direct visual reasoning among all models, which confirms that graphical representation and in-context learning enhance navigation performance and make our solution more precise for indoor navigation of Blind and Low Vision (BLV) users.", "AI": {"tldr": "\u63d0\u51faFloorplan2Guide\u7cfb\u7edf\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u5c06\u5e73\u9762\u56fe\u8f6c\u6362\u4e3a\u53ef\u5bfc\u822a\u77e5\u8bc6\u56fe\u8c31\u5e76\u751f\u6210\u4eba\u7c7b\u53ef\u8bfb\u5bfc\u822a\u6307\u4ee4\uff0c\u63d0\u9ad8\u89c6\u969c\u4eba\u58eb\u5ba4\u5185\u5bfc\u822a\u7cbe\u5ea6", "motivation": "\u5f53\u524d\u89c6\u969c\u4eba\u58eb\u5ba4\u5185\u5bfc\u822a\u65b9\u6848\u4e3b\u8981\u4f9d\u8d56\u57fa\u7840\u8bbe\u65bd\u7cfb\u7edf\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5bfc\u822a\u80fd\u529b\u53d7\u9650\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u3001\u7cbe\u786e\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u4ece\u5efa\u7b51\u5e03\u5c40\u4e2d\u63d0\u53d6\u7a7a\u95f4\u4fe1\u606f\uff0c\u5c06\u5e73\u9762\u56fe\u8f6c\u6362\u4e3a\u53ef\u5bfc\u822a\u77e5\u8bc6\u56fe\u8c31\uff0c\u901a\u8fc7\u5c11\u6837\u672c\u5b66\u4e60\u751f\u6210\u5bfc\u822a\u6307\u4ee4\uff0c\u51cf\u5c11\u4f20\u7edf\u5e73\u9762\u56fe\u89e3\u6790\u65b9\u6cd5\u6240\u9700\u7684\u624b\u52a8\u9884\u5904\u7406", "result": "Claude 3.7 Sonnet\u57285-shot\u63d0\u793a\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u77ed\u3001\u4e2d\u3001\u957f\u8def\u7ebf\u51c6\u786e\u7387\u5206\u522b\u4e3a92.31%\u300176.92%\u300161.54%\uff1b\u57fa\u4e8e\u56fe\u7684\u7a7a\u95f4\u7ed3\u6784\u6bd4\u76f4\u63a5\u89c6\u89c9\u63a8\u7406\u6210\u529f\u7387\u9ad815.4%", "conclusion": "\u56fe\u5f62\u8868\u793a\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u663e\u8457\u63d0\u5347\u5bfc\u822a\u6027\u80fd\uff0c\u4f7fFloorplan2Guide\u6210\u4e3a\u66f4\u7cbe\u786e\u7684\u89c6\u969c\u4eba\u58eb\u5ba4\u5185\u5bfc\u822a\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.11851", "categories": ["cs.LG", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2512.11851", "abs": "https://arxiv.org/abs/2512.11851", "authors": ["Prashant Pandey"], "title": "KV Cache Recycling to Expand Usable Context Capacity in Low Parameter LLMs", "comment": null, "summary": "Whether attention key value (KV) states computed for one prompt for a small LLM can be reused to accelerate inference on a new similar prompt, giving an increase to the space to its context memory using an approach called token recycling. Using a standard Hugging Face setup with DialoGPT-medium (a 345M parameter GPT-2 style decoder trained on 147M Reddit exchanges, 2005 to 2017) as the testbed, we build a cache of past activations and get entries by sentence embeddings, then reuse cached past key values when the cached prompt is an exact prefix of the new input. We compare recycled vs. baseline runs on latency and output fidelity, and log reuse depth in tokens. Reproducibility requires no model modifications, cached KVs are serialized to the CPU, reloaded, and supplied to the generate function to continue decoding from the cached prefix. In tests, we observe consistent speedups when prefix overlap exists, with no material degradation in output semantics, and when overlap is absent, behavior matches baseline.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\"token recycling\"\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u7528\u5c0f\u578bLLM\u4e2d\u76f8\u4f3c\u63d0\u793a\u7684\u6ce8\u610f\u529b\u952e\u503c\u72b6\u6001\u6765\u52a0\u901f\u63a8\u7406\uff0c\u6269\u5c55\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u7a7a\u95f4\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u91cd\u7528\u5148\u524d\u8ba1\u7b97\u8fc7\u7684\u6ce8\u610f\u529b\u952e\u503c\u72b6\u6001\u6765\u52a0\u901f\u65b0\u76f8\u4f3c\u63d0\u793a\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u4ece\u800c\u63d0\u9ad8LLM\u7684\u63a8\u7406\u6548\u7387\u5e76\u6269\u5c55\u5176\u4e0a\u4e0b\u6587\u8bb0\u5fc6\u80fd\u529b\u3002", "method": "\u4f7f\u7528DialoGPT-medium\u4f5c\u4e3a\u6d4b\u8bd5\u5e73\u53f0\uff0c\u6784\u5efa\u8fc7\u53bb\u6fc0\u6d3b\u7684\u7f13\u5b58\uff0c\u901a\u8fc7\u53e5\u5b50\u5d4c\u5165\u68c0\u7d22\u6761\u76ee\uff0c\u5f53\u7f13\u5b58\u7684\u63d0\u793a\u662f\u65b0\u8f93\u5165\u7684\u7cbe\u786e\u524d\u7f00\u65f6\uff0c\u91cd\u7528\u7f13\u5b58\u7684\u8fc7\u53bb\u952e\u503c\u3002\u65b9\u6cd5\u4e0d\u9700\u8981\u4fee\u6539\u6a21\u578b\uff0c\u7f13\u5b58\u7684KV\u5e8f\u5217\u5316\u5230CPU\uff0c\u91cd\u65b0\u52a0\u8f7d\u540e\u63d0\u4f9b\u7ed9\u751f\u6210\u51fd\u6570\u7ee7\u7eed\u4ece\u7f13\u5b58\u524d\u7f00\u89e3\u7801\u3002", "result": "\u6d4b\u8bd5\u663e\u793a\uff0c\u5f53\u5b58\u5728\u524d\u7f00\u91cd\u53e0\u65f6\uff0c\u89c2\u5bdf\u5230\u4e00\u81f4\u7684\u52a0\u901f\u6548\u679c\uff0c\u4e14\u8f93\u51fa\u8bed\u4e49\u6ca1\u6709\u5b9e\u8d28\u6027\u9000\u5316\uff1b\u5f53\u6ca1\u6709\u91cd\u53e0\u65f6\uff0c\u884c\u4e3a\u4e0e\u57fa\u7ebf\u5339\u914d\u3002", "conclusion": "token recycling\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u52a0\u901fLLM\u63a8\u7406\uff0c\u5728\u4fdd\u6301\u8f93\u51fa\u8d28\u91cf\u7684\u540c\u65f6\u63d0\u9ad8\u6548\u7387\uff0c\u4e3a\u76f8\u4f3c\u63d0\u793a\u7684\u63a8\u7406\u4f18\u5316\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2512.11939", "categories": ["cs.CV", "math.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.11939", "abs": "https://arxiv.org/abs/2512.11939", "authors": ["Cl\u00e9ment Fernandes", "Wojciech Pieczynski"], "title": "Contextual Peano Scan and Fast Image Segmentation Using Hidden and Evidential Markov Chains", "comment": null, "summary": "Transforming bi-dimensional sets of image pixels into mono-dimensional sequences with a Peano scan (PS) is an established technique enabling the use of hidden Markov chains (HMCs) for unsupervised image segmentation. Related Bayesian segmentation methods can compete with hidden Markov fields (HMFs)-based ones and are much faster. PS has recently been extended to the contextual PS, and some initial experiments have shown the value of the associated HMC model, denoted as HMC-CPS, in image segmentation. Moreover, HMCs have been extended to hidden evidential Markov chains (HEMCs), which are capable of improving HMC-based Bayesian segmentation. In this study, we introduce a new HEMC-CPS model by simultaneously considering contextual PS and evidential HMC. We show its effectiveness for Bayesian maximum posterior mode (MPM) segmentation using synthetic and real images. Segmentation is performed in an unsupervised manner, with parameters being estimated using the stochastic expectation--maximization (SEM) method. The new HEMC-CPS model presents potential for the modeling and segmentation of more complex images, such as three-dimensional or multi-sensor multi-resolution images. Finally, the HMC-CPS and HEMC-CPS models are not limited to image segmentation and could be used for any kind of spatially correlated data.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684HEMC-CPS\u6a21\u578b\uff0c\u5c06\u4e0a\u4e0b\u6587Peano\u626b\u63cf\u4e0e\u8bc1\u636e\u9690\u9a6c\u5c14\u53ef\u592b\u94fe\u7ed3\u5408\uff0c\u7528\u4e8e\u65e0\u76d1\u7763\u56fe\u50cf\u5206\u5272\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u9690\u9a6c\u5c14\u53ef\u592b\u573a\uff08HMFs\uff09\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u8ba1\u7b97\u590d\u6742\u4e14\u8017\u65f6\uff0c\u800c\u57fa\u4e8ePeano\u626b\u63cf\u7684\u9690\u9a6c\u5c14\u53ef\u592b\u94fe\uff08HMCs\uff09\u65b9\u6cd5\u867d\u7136\u66f4\u5feb\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002\u4e0a\u4e0b\u6587Peano\u626b\u63cf\uff08CPS\uff09\u548c\u8bc1\u636e\u9690\u9a6c\u5c14\u53ef\u592b\u94fe\uff08HEMCs\uff09\u5404\u81ea\u663e\u793a\u51fa\u6539\u8fdb\u6f5c\u529b\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u5c06\u5b83\u4eec\u7ed3\u5408\u7684\u53ef\u80fd\u6027\u3002", "method": "\u63d0\u51faHEMC-CPS\u6a21\u578b\uff0c\u5c06\u4e0a\u4e0b\u6587Peano\u626b\u63cf\u4e0e\u8bc1\u636e\u9690\u9a6c\u5c14\u53ef\u592b\u94fe\u76f8\u7ed3\u5408\u3002\u91c7\u7528\u8d1d\u53f6\u65af\u6700\u5927\u540e\u9a8c\u6982\u7387\uff08MPM\uff09\u5206\u5272\uff0c\u4f7f\u7528\u968f\u673a\u671f\u671b\u6700\u5927\u5316\uff08SEM\uff09\u65b9\u6cd5\u8fdb\u884c\u65e0\u76d1\u7763\u53c2\u6570\u4f30\u8ba1\u3002", "result": "\u5728\u5408\u6210\u56fe\u50cf\u548c\u771f\u5b9e\u56fe\u50cf\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cHEMC-CPS\u6a21\u578b\u5728\u56fe\u50cf\u5206\u5272\u65b9\u9762\u8868\u73b0\u6709\u6548\u3002\u8be5\u6a21\u578b\u4e0d\u4ec5\u9650\u4e8e\u4e8c\u7ef4\u56fe\u50cf\u5206\u5272\uff0c\u8fd8\u53ef\u6269\u5c55\u5230\u4e09\u7ef4\u6216\u591a\u4f20\u611f\u5668\u591a\u5206\u8fa8\u7387\u56fe\u50cf\u3002", "conclusion": "HEMC-CPS\u6a21\u578b\u7ed3\u5408\u4e86\u4e0a\u4e0b\u6587Peano\u626b\u63cf\u548c\u8bc1\u636e\u9690\u9a6c\u5c14\u53ef\u592b\u94fe\u7684\u4f18\u52bf\uff0c\u4e3a\u590d\u6742\u56fe\u50cf\u5efa\u6a21\u548c\u5206\u5272\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u4e14\u4e0d\u9650\u4e8e\u56fe\u50cf\u5206\u5272\uff0c\u53ef\u5e94\u7528\u4e8e\u4efb\u4f55\u7a7a\u95f4\u76f8\u5173\u6570\u636e\u3002"}}
{"id": "2512.12182", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12182", "abs": "https://arxiv.org/abs/2512.12182", "authors": ["Xinyu Gao"], "title": "TA-KAND: Two-stage Attention Triple Enhancement and U-KAN based Diffusion For Few-shot Knowledge Graph Completion", "comment": null, "summary": "Knowledge Graphs (KGs), thanks to their concise and efficient triple-based structure, have been widely applied in intelligent question answering, recommender systems and other domains. However, the heterogeneous and multifaceted nature of real-world data inevitably renders the distribution of relations long-tailed, making it crucial to complete missing facts with limited samples. Previous studies mainly based on metric matching or meta learning, yet they either fail to fully exploit neighborhood information in graph or overlook the distributional characteristics of contrastive signals. In this paper, we re-examine the problem from a perspective of generative representation and propose a few-shot knowledge graph completion framework that integrates two-stage attention triple enhancer with U-KAN based diffusion model. Extensive experiments on two public datasets show that our method achieve new state-of-the-art results.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408\u4e24\u9636\u6bb5\u6ce8\u610f\u529b\u4e09\u5143\u589e\u5f3a\u5668\u548cU-KAN\u6269\u6563\u6a21\u578b\u7684\u5c11\u6837\u672c\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u6846\u67b6\uff0c\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684SOTA\u7ed3\u679c\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u77e5\u8bc6\u56fe\u8c31\u4e2d\u5173\u7cfb\u5206\u5e03\u5448\u73b0\u957f\u5c3e\u7279\u6027\uff0c\u4f20\u7edf\u57fa\u4e8e\u5ea6\u91cf\u5339\u914d\u6216\u5143\u5b66\u4e60\u7684\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u56fe\u7684\u90bb\u57df\u4fe1\u606f\u6216\u5ffd\u89c6\u5bf9\u6bd4\u4fe1\u53f7\u7684\u5206\u5e03\u7279\u5f81\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u5c11\u6837\u672c\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u751f\u6210\u5f0f\u8868\u793a\u89c6\u89d2\u7684\u5c11\u6837\u672c\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u6846\u67b6\uff0c\u6574\u5408\u4e24\u9636\u6bb5\u6ce8\u610f\u529b\u4e09\u5143\u589e\u5f3a\u5668\u548c\u57fa\u4e8eU-KAN\u7684\u6269\u6563\u6a21\u578b\uff0c\u5145\u5206\u5229\u7528\u90bb\u57df\u4fe1\u606f\u5e76\u8003\u8651\u5bf9\u6bd4\u4fe1\u53f7\u7684\u5206\u5e03\u7279\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7ed3\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684\u751f\u6210\u5f0f\u8868\u793a\u6846\u67b6\u901a\u8fc7\u4e24\u9636\u6bb5\u6ce8\u610f\u529b\u4e09\u5143\u589e\u5f3a\u5668\u548cU-KAN\u6269\u6563\u6a21\u578b\u7684\u7ed3\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5c11\u6837\u672c\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2512.11852", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11852", "abs": "https://arxiv.org/abs/2512.11852", "authors": ["Muhammad Jawad Bashir", "Shagufta Henna", "Eoghan Furey"], "title": "Explainable AI for Smart Greenhouse Control: Interpretability of Temporal Fusion Transformer in the Internet of Robotic Things", "comment": "7 pages, Accepted in 36th Irish Signals and Systems Conference, ISSC 2025", "summary": "The integration of the Internet of Robotic Things (IoRT) in smart greenhouses has revolutionised precision agriculture by enabling efficient and autonomous environmental control. However, existing time series forecasting models in such setups often operate as black boxes, lacking mechanisms for explainable decision-making, which is a critical limitation when trust, transparency, and regulatory compliance are paramount in smart farming practices. This study leverages the Temporal Fusion Transformer (TFT) model to automate actuator settings for optimal greenhouse management. To enhance interpretability and trust in the model decision-making process, both local and global explanation techniques were employed using model-inherent interpretation, local interpretable model-agnostic explanations (LIME), and SHapley additive explanations (SHAP). These explainability methods provide information on how different sensor readings, such as temperature, humidity, CO2 levels, light, and outer climate, contribute to actuator control decisions in an automated greenhouse. The trained TFT model achieved a test accuracy of 95% on a class-imbalanced dataset for actuator control settings in an automated greenhouse environment. The results demonstrate the varying influence of each sensor on real-time greenhouse adjustments, ensuring transparency and enabling adaptive fine-tuning for improved crop yield and resource efficiency.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528Temporal Fusion Transformer\u6a21\u578b\u5b9e\u73b0\u667a\u80fd\u6e29\u5ba4\u6267\u884c\u5668\u81ea\u52a8\u63a7\u5236\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff08\u6a21\u578b\u5185\u5728\u89e3\u91ca\u3001LIME\u3001SHAP\uff09\u63d0\u5347\u51b3\u7b56\u900f\u660e\u5ea6\uff0c\u5728\u7c7b\u522b\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\u8fbe\u523095%\u6d4b\u8bd5\u51c6\u786e\u7387\u3002", "motivation": "\u667a\u80fd\u6e29\u5ba4\u4e2d\u7684\u7269\u8054\u7f51\u673a\u5668\u4eba\u7cfb\u7edf\u867d\u7136\u5b9e\u73b0\u4e86\u7cbe\u51c6\u519c\u4e1a\uff0c\u4f46\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578b\u591a\u4e3a\u9ed1\u76d2\uff0c\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u8fd9\u5728\u9700\u8981\u4fe1\u4efb\u3001\u900f\u660e\u5ea6\u548c\u76d1\u7ba1\u5408\u89c4\u7684\u667a\u6167\u519c\u4e1a\u5b9e\u8df5\u4e2d\u662f\u4e00\u4e2a\u5173\u952e\u9650\u5236\u3002", "method": "\u4f7f\u7528Temporal Fusion Transformer\u6a21\u578b\u81ea\u52a8\u5316\u6e29\u5ba4\u6267\u884c\u5668\u8bbe\u7f6e\uff0c\u5e76\u91c7\u7528\u6a21\u578b\u5185\u5728\u89e3\u91ca\u3001\u5c40\u90e8\u53ef\u89e3\u91ca\u6a21\u578b\u65e0\u5173\u89e3\u91ca\u548cSHAP\u503c\u5206\u6790\u7b49\u5c40\u90e8\u548c\u5168\u5c40\u89e3\u91ca\u6280\u672f\u6765\u589e\u5f3a\u6a21\u578b\u51b3\u7b56\u7684\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5728\u81ea\u52a8\u5316\u6e29\u5ba4\u73af\u5883\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u6570\u636e\u96c6\u4e0a\uff0c\u8bad\u7ec3\u7684TFT\u6a21\u578b\u5728\u6267\u884c\u5668\u63a7\u5236\u8bbe\u7f6e\u65b9\u9762\u8fbe\u5230\u4e8695%\u7684\u6d4b\u8bd5\u51c6\u786e\u7387\u3002\u89e3\u91ca\u6027\u5206\u6790\u63ed\u793a\u4e86\u6e29\u5ea6\u3001\u6e7f\u5ea6\u3001CO2\u6c34\u5e73\u3001\u5149\u7167\u548c\u5916\u90e8\u6c14\u5019\u7b49\u4e0d\u540c\u4f20\u611f\u5668\u8bfb\u6570\u5bf9\u6267\u884c\u5668\u63a7\u5236\u51b3\u7b56\u7684\u8d21\u732e\u7a0b\u5ea6\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86TFT\u6a21\u578b\u7ed3\u5408\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5b9e\u73b0\u667a\u80fd\u6e29\u5ba4\u81ea\u52a8\u5316\u63a7\u5236\uff0c\u786e\u4fdd\u51b3\u7b56\u900f\u660e\u5ea6\uff0c\u5e76\u652f\u6301\u81ea\u9002\u5e94\u5fae\u8c03\u4ee5\u63d0\u9ad8\u4f5c\u7269\u4ea7\u91cf\u548c\u8d44\u6e90\u6548\u7387\uff0c\u4e3a\u53ef\u4fe1\u8d56\u7684\u667a\u6167\u519c\u4e1a\u5b9e\u8df5\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13237", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.13237", "abs": "https://arxiv.org/abs/2512.13237", "authors": ["Arnab Sharma"], "title": "Learning to Retrieve with Weakened Labels: Robust Training under Label Noise", "comment": null, "summary": "Neural Encoders are frequently used in the NLP domain to perform dense retrieval tasks, for instance, to generate the candidate documents for a given query in question-answering tasks. However, sparse annotation and label noise in the training data make it challenging to train or fine-tune such retrieval models. Although existing works have attempted to mitigate these problems by incorporating modified loss functions or data cleaning, these approaches either require some hyperparameters to tune during training or add substantial complexity to the training setup. In this work, we consider a label weakening approach to generate robust retrieval models in the presence of label noise. Instead of enforcing a single, potentially erroneous label for each query document pair, we allow for a set of plausible labels derived from both the observed supervision and the model's confidence scores. We perform an extensive evaluation considering two retrieval models, one re-ranking model, considering four diverse ranking datasets. To this end, we also consider a realistic noisy setting by using a semantic-aware noise generation technique to generate different ratios of noise. Our initial results show that label weakening can improve the performance of the retrieval tasks in comparison to 10 different state-of-the-art loss functions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u6807\u7b7e\u5f31\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u4e00\u7ec4\u53ef\u80fd\u7684\u6807\u7b7e\u800c\u975e\u5355\u4e00\u6807\u7b7e\uff0c\u5728\u6807\u7b7e\u566a\u58f0\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\u8bad\u7ec3\u66f4\u9c81\u68d2\u7684\u68c0\u7d22\u6a21\u578b\u3002", "motivation": "\u795e\u7ecf\u7f16\u7801\u5668\u5728NLP\u9886\u57df\u7528\u4e8e\u5bc6\u96c6\u68c0\u7d22\u4efb\u52a1\uff0c\u4f46\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u7a00\u758f\u6807\u6ce8\u548c\u6807\u7b7e\u566a\u58f0\u4f7f\u5f97\u8bad\u7ec3\u6216\u5fae\u8c03\u8fd9\u7c7b\u68c0\u7d22\u6a21\u578b\u5177\u6709\u6311\u6218\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u9700\u8981\u8c03\u6574\u8d85\u53c2\u6570\uff0c\u8981\u4e48\u589e\u52a0\u4e86\u8bad\u7ec3\u8bbe\u7f6e\u7684\u590d\u6742\u6027\u3002", "method": "\u91c7\u7528\u6807\u7b7e\u5f31\u5316\u65b9\u6cd5\uff0c\u4e0d\u5f3a\u5236\u4e3a\u6bcf\u4e2a\u67e5\u8be2-\u6587\u6863\u5bf9\u5206\u914d\u5355\u4e00\u53ef\u80fd\u9519\u8bef\u7684\u6807\u7b7e\uff0c\u800c\u662f\u5141\u8bb8\u4ece\u89c2\u5bdf\u5230\u7684\u76d1\u7763\u4fe1\u53f7\u548c\u6a21\u578b\u7f6e\u4fe1\u5ea6\u5206\u6570\u4e2d\u63a8\u5bfc\u51fa\u4e00\u7ec4\u5408\u7406\u7684\u6807\u7b7e\u3002", "result": "\u5728\u4e24\u4e2a\u68c0\u7d22\u6a21\u578b\u3001\u4e00\u4e2a\u91cd\u6392\u5e8f\u6a21\u578b\u548c\u56db\u4e2a\u4e0d\u540c\u6392\u5e8f\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff0c\u6807\u7b7e\u5f31\u5316\u65b9\u6cd5\u76f8\u6bd410\u79cd\u6700\u5148\u8fdb\u7684\u635f\u5931\u51fd\u6570\uff0c\u80fd\u591f\u63d0\u9ad8\u68c0\u7d22\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "\u6807\u7b7e\u5f31\u5316\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0c\u63d0\u9ad8\u68c0\u7d22\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2512.11941", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11941", "abs": "https://arxiv.org/abs/2512.11941", "authors": ["Jingmin Zhu", "Anqi Zhu", "James Bailey", "Jun Liu", "Hossein Rahmani", "Mohammed Bennamoun", "Farid Boussaid", "Qiuhong Ke"], "title": "DynaPURLS: Dynamic Refinement of Part-aware Representations for Skeleton-based Zero-Shot Action Recognition", "comment": null, "summary": "Zero-shot skeleton-based action recognition (ZS-SAR) is fundamentally constrained by prevailing approaches that rely on aligning skeleton features with static, class-level semantics. This coarse-grained alignment fails to bridge the domain shift between seen and unseen classes, thereby impeding the effective transfer of fine-grained visual knowledge. To address these limitations, we introduce \\textbf{DynaPURLS}, a unified framework that establishes robust, multi-scale visual-semantic correspondences and dynamically refines them at inference time to enhance generalization. Our framework leverages a large language model to generate hierarchical textual descriptions that encompass both global movements and local body-part dynamics. Concurrently, an adaptive partitioning module produces fine-grained visual representations by semantically grouping skeleton joints. To fortify this fine-grained alignment against the train-test domain shift, DynaPURLS incorporates a dynamic refinement module. During inference, this module adapts textual features to the incoming visual stream via a lightweight learnable projection. This refinement process is stabilized by a confidence-aware, class-balanced memory bank, which mitigates error propagation from noisy pseudo-labels. Extensive experiments on three large-scale benchmark datasets, including NTU RGB+D 60/120 and PKU-MMD, demonstrate that DynaPURLS significantly outperforms prior art, setting new state-of-the-art records. The source code is made publicly available at https://github.com/Alchemist0754/DynaPURLS", "AI": {"tldr": "DynaPURLS\u662f\u4e00\u4e2a\u7528\u4e8e\u96f6\u6837\u672c\u9aa8\u67b6\u52a8\u4f5c\u8bc6\u522b\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u5efa\u7acb\u591a\u5c3a\u5ea6\u89c6\u89c9-\u8bed\u4e49\u5bf9\u5e94\u5173\u7cfb\u5e76\u5728\u63a8\u7406\u65f6\u52a8\u6001\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u672a\u89c1\u7c7b\u522b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u96f6\u6837\u672c\u9aa8\u67b6\u52a8\u4f5c\u8bc6\u522b\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u9aa8\u67b6\u7279\u5f81\u4e0e\u9759\u6001\u7c7b\u522b\u7ea7\u8bed\u4e49\u7684\u5bf9\u9f50\uff0c\u8fd9\u79cd\u7c97\u7c92\u5ea6\u5bf9\u9f50\u65e0\u6cd5\u6709\u6548\u5f25\u5408\u53ef\u89c1\u7c7b\u522b\u4e0e\u672a\u89c1\u7c7b\u522b\u4e4b\u95f4\u7684\u9886\u57df\u504f\u79fb\uff0c\u9650\u5236\u4e86\u7ec6\u7c92\u5ea6\u89c6\u89c9\u77e5\u8bc6\u7684\u8fc1\u79fb\u3002", "method": "1) \u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5305\u542b\u5168\u5c40\u8fd0\u52a8\u548c\u5c40\u90e8\u8eab\u4f53\u90e8\u4f4d\u52a8\u6001\u7684\u5206\u5c42\u6587\u672c\u63cf\u8ff0\uff1b2) \u81ea\u9002\u5e94\u5206\u533a\u6a21\u5757\u901a\u8fc7\u8bed\u4e49\u5206\u7ec4\u9aa8\u67b6\u5173\u8282\u751f\u6210\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8868\u793a\uff1b3) \u52a8\u6001\u4f18\u5316\u6a21\u5757\u5728\u63a8\u7406\u65f6\u901a\u8fc7\u8f7b\u91cf\u7ea7\u53ef\u5b66\u4e60\u6295\u5f71\u5c06\u6587\u672c\u7279\u5f81\u9002\u914d\u5230\u8f93\u5165\u89c6\u89c9\u6d41\uff1b4) \u7f6e\u4fe1\u611f\u77e5\u7684\u7c7b\u522b\u5e73\u8861\u8bb0\u5fc6\u5e93\u7a33\u5b9a\u4f18\u5316\u8fc7\u7a0b\uff0c\u51cf\u5c11\u566a\u58f0\u4f2a\u6807\u7b7e\u7684\u9519\u8bef\u4f20\u64ad\u3002", "result": "\u5728NTU RGB+D 60/120\u548cPKU-MMD\u4e09\u4e2a\u5927\u89c4\u6a21\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cDynaPURLS\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u521b\u9020\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u8bb0\u5f55\u3002", "conclusion": "DynaPURLS\u901a\u8fc7\u5efa\u7acb\u9c81\u68d2\u7684\u591a\u5c3a\u5ea6\u89c6\u89c9-\u8bed\u4e49\u5bf9\u5e94\u5173\u7cfb\u5e76\u5728\u63a8\u7406\u65f6\u52a8\u6001\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u96f6\u6837\u672c\u9aa8\u67b6\u52a8\u4f5c\u8bc6\u522b\u4e2d\u7684\u9886\u57df\u504f\u79fb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u5bf9\u672a\u89c1\u7c7b\u522b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2512.11878", "categories": ["cs.CY", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.11878", "abs": "https://arxiv.org/abs/2512.11878", "authors": ["Hasan Kassem", "Sergen Cansiz", "Brandon Edwards", "Patrick Foley", "Inken Hagestedt", "Taeho Jung", "Prakash Moorthy", "Michael O'Connor", "Bruno Rodrigues", "Holger Roth", "Micah Sheller", "Dimitris Stripelis", "Marc Vesin", "Renato Umeton", "Mic Bowman", "Alexandros Karargyris"], "title": "A Technical Policy Blueprint for Trustworthy Decentralized AI", "comment": null, "summary": "Decentralized AI systems, such as federated learning, can play a critical role in further unlocking AI asset marketplaces (e.g., healthcare data marketplaces) thanks to increased asset privacy protection. Unlocking this big potential necessitates governance mechanisms that are transparent, scalable, and verifiable. However current governance approaches rely on bespoke, infrastructure-specific policies that hinder asset interoperability and trust among systems. We are proposing a Technical Policy Blueprint that encodes governance requirements as policy-as-code objects and separates asset policy verification from asset policy enforcement. In this architecture the Policy Engine verifies evidence (e.g., identities, signatures, payments, trusted-hardware attestations) and issues capability packages. Asset Guardians (e.g. data guardians, model guardians, computation guardians, etc.) enforce access or execution solely based on these capability packages. This core concept of decoupling policy processing from capabilities enables governance to evolve without reconfiguring AI infrastructure, thus creating an approach that is transparent, auditable, and resilient to change.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316AI\u7cfb\u7edf\u7684\u6280\u672f\u653f\u7b56\u84dd\u56fe\uff0c\u901a\u8fc7\u5c06\u6cbb\u7406\u9700\u6c42\u7f16\u7801\u4e3a\u653f\u7b56\u5373\u4ee3\u7801\u5bf9\u8c61\uff0c\u5206\u79bb\u8d44\u4ea7\u653f\u7b56\u9a8c\u8bc1\u4e0e\u6267\u884c\uff0c\u5b9e\u73b0\u900f\u660e\u3001\u53ef\u6269\u5c55\u3001\u53ef\u9a8c\u8bc1\u7684\u6cbb\u7406\u673a\u5236\u3002", "motivation": "\u53bb\u4e2d\u5fc3\u5316AI\u7cfb\u7edf\uff08\u5982\u8054\u90a6\u5b66\u4e60\uff09\u5728\u4fdd\u62a4\u8d44\u4ea7\u9690\u79c1\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u6cbb\u7406\u65b9\u6cd5\u4f9d\u8d56\u7279\u5b9a\u57fa\u7840\u8bbe\u65bd\u653f\u7b56\uff0c\u963b\u788d\u4e86\u8d44\u4ea7\u4e92\u64cd\u4f5c\u6027\u548c\u7cfb\u7edf\u95f4\u4fe1\u4efb\uff0c\u9700\u8981\u900f\u660e\u3001\u53ef\u6269\u5c55\u3001\u53ef\u9a8c\u8bc1\u7684\u6cbb\u7406\u673a\u5236\u3002", "method": "\u63d0\u51fa\u6280\u672f\u653f\u7b56\u84dd\u56fe\uff0c\u5c06\u6cbb\u7406\u9700\u6c42\u7f16\u7801\u4e3a\u653f\u7b56\u5373\u4ee3\u7801\u5bf9\u8c61\uff0c\u5206\u79bb\u8d44\u4ea7\u653f\u7b56\u9a8c\u8bc1\u4e0e\u6267\u884c\u3002\u653f\u7b56\u5f15\u64ce\u9a8c\u8bc1\u8bc1\u636e\uff08\u8eab\u4efd\u3001\u7b7e\u540d\u3001\u652f\u4ed8\u3001\u53ef\u4fe1\u786c\u4ef6\u8bc1\u660e\uff09\u5e76\u9881\u53d1\u80fd\u529b\u5305\uff0c\u8d44\u4ea7\u5b88\u62a4\u8005\u4ec5\u57fa\u4e8e\u8fd9\u4e9b\u80fd\u529b\u5305\u6267\u884c\u8bbf\u95ee\u6216\u8ba1\u7b97\u3002", "result": "\u901a\u8fc7\u5c06\u653f\u7b56\u5904\u7406\u4e0e\u80fd\u529b\u5206\u79bb\uff0c\u4f7f\u6cbb\u7406\u80fd\u591f\u5728\u4e0d\u91cd\u65b0\u914d\u7f6eAI\u57fa\u7840\u8bbe\u65bd\u7684\u60c5\u51b5\u4e0b\u6f14\u8fdb\uff0c\u521b\u5efa\u4e86\u900f\u660e\u3001\u53ef\u5ba1\u8ba1\u3001\u9002\u5e94\u53d8\u5316\u7684\u6cbb\u7406\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6280\u672f\u653f\u7b56\u84dd\u56fe\u4e3a\u89e3\u51b3\u53bb\u4e2d\u5fc3\u5316AI\u7cfb\u7edf\u6cbb\u7406\u95ee\u9898\u63d0\u4f9b\u4e86\u521b\u65b0\u65b9\u6848\uff0c\u901a\u8fc7\u653f\u7b56\u4e0e\u6267\u884c\u7684\u89e3\u8026\u5b9e\u73b0\u4e86\u6cbb\u7406\u7684\u7075\u6d3b\u6027\u3001\u900f\u660e\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u6709\u52a9\u4e8e\u89e3\u9501AI\u8d44\u4ea7\u5e02\u573a\u6f5c\u529b\u3002"}}
{"id": "2512.12225", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12225", "abs": "https://arxiv.org/abs/2512.12225", "authors": ["Laha Ale"], "title": "A Geometric Theory of Cognition", "comment": null, "summary": "Human cognition spans perception, memory, intuitive judgment, deliberative reasoning, action selection, and social inference, yet these capacities are often explained through distinct computational theories. Here we present a unified mathematical framework in which diverse cognitive processes emerge from a single geometric principle. We represent the cognitive state as a point on a differentiable manifold endowed with a learned Riemannian metric that encodes representational constraints, computational costs, and structural relations among cognitive variables. A scalar cognitive potential combines predictive accuracy, structural parsimony, task utility, and normative or logical requirements. Cognition unfolds as the Riemannian gradient flow of this potential, providing a universal dynamical law from which a broad range of psychological phenomena arise. Classical dual-process effects--rapid intuitive responses and slower deliberative reasoning--emerge naturally from metric-induced anisotropies that generate intrinsic time-scale separations and geometric phase transitions, without invoking modular or hybrid architectures. We derive analytical conditions for these regimes and demonstrate their behavioural signatures through simulations of canonical cognitive tasks. Together, these results establish a geometric foundation for cognition and suggest guiding principles for the development of more general and human-like artificial intelligence systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u51e0\u4f55\u6846\u67b6\uff0c\u5c06\u5404\u79cd\u8ba4\u77e5\u8fc7\u7a0b\u89e3\u91ca\u4e3a\u5355\u4e00\u51e0\u4f55\u539f\u7406\u7684\u6d8c\u73b0\u73b0\u8c61\uff0c\u901a\u8fc7\u9ece\u66fc\u68af\u5ea6\u6d41\u63cf\u8ff0\u8ba4\u77e5\u52a8\u6001\uff0c\u80fd\u591f\u81ea\u7136\u89e3\u91ca\u53cc\u8fc7\u7a0b\u7406\u8bba\u7b49\u73b0\u8c61\u3002", "motivation": "\u4eba\u7c7b\u8ba4\u77e5\u5305\u542b\u611f\u77e5\u3001\u8bb0\u5fc6\u3001\u76f4\u89c9\u5224\u65ad\u3001\u6df1\u601d\u719f\u8651\u63a8\u7406\u3001\u884c\u52a8\u9009\u62e9\u548c\u793e\u4f1a\u63a8\u7406\u7b49\u591a\u79cd\u80fd\u529b\uff0c\u4f46\u8fd9\u4e9b\u80fd\u529b\u901a\u5e38\u7531\u4e0d\u540c\u7684\u8ba1\u7b97\u7406\u8bba\u89e3\u91ca\u3002\u4f5c\u8005\u5e0c\u671b\u5efa\u7acb\u4e00\u4e2a\u7edf\u4e00\u7684\u6570\u5b66\u6846\u67b6\uff0c\u5c06\u8fd9\u4e9b\u591a\u6837\u5316\u7684\u8ba4\u77e5\u8fc7\u7a0b\u7edf\u4e00\u8d77\u6765\u3002", "method": "\u5c06\u8ba4\u77e5\u72b6\u6001\u8868\u793a\u4e3a\u53ef\u5fae\u5206\u6d41\u5f62\u4e0a\u7684\u70b9\uff0c\u8d4b\u4e88\u5b66\u4e60\u5230\u7684\u9ece\u66fc\u5ea6\u91cf\uff0c\u7f16\u7801\u8868\u5f81\u7ea6\u675f\u3001\u8ba1\u7b97\u6210\u672c\u548c\u8ba4\u77e5\u53d8\u91cf\u95f4\u7684\u7ed3\u6784\u5173\u7cfb\u3002\u5b9a\u4e49\u4e00\u4e2a\u6807\u91cf\u8ba4\u77e5\u52bf\u80fd\uff0c\u7ed3\u5408\u9884\u6d4b\u51c6\u786e\u6027\u3001\u7ed3\u6784\u7b80\u6d01\u6027\u3001\u4efb\u52a1\u6548\u7528\u548c\u89c4\u8303\u6027\u8981\u6c42\u3002\u8ba4\u77e5\u8fc7\u7a0b\u4f5c\u4e3a\u8be5\u52bf\u80fd\u7684\u9ece\u66fc\u68af\u5ea6\u6d41\u5c55\u5f00\u3002", "result": "\u7ecf\u5178\u7684\u53cc\u8fc7\u7a0b\u6548\u5e94\uff08\u5feb\u901f\u76f4\u89c9\u53cd\u5e94\u548c\u8f83\u6162\u7684\u6df1\u601d\u719f\u8651\u63a8\u7406\uff09\u4ece\u5ea6\u91cf\u8bf1\u5bfc\u7684\u5404\u5411\u5f02\u6027\u4e2d\u81ea\u7136\u6d8c\u73b0\uff0c\u4ea7\u751f\u5185\u5728\u65f6\u95f4\u5c3a\u5ea6\u5206\u79bb\u548c\u51e0\u4f55\u76f8\u53d8\uff0c\u65e0\u9700\u5f15\u5165\u6a21\u5757\u5316\u6216\u6df7\u5408\u67b6\u6784\u3002\u901a\u8fc7\u6a21\u62df\u7ecf\u5178\u8ba4\u77e5\u4efb\u52a1\u5c55\u793a\u4e86\u8fd9\u4e9b\u673a\u5236\u7684\u884c\u4e3a\u7279\u5f81\u3002", "conclusion": "\u8fd9\u4e9b\u7ed3\u679c\u4e3a\u8ba4\u77e5\u5efa\u7acb\u4e86\u51e0\u4f55\u57fa\u7840\uff0c\u5e76\u4e3a\u5f00\u53d1\u66f4\u901a\u7528\u3001\u66f4\u7c7b\u4eba\u7684\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6307\u5bfc\u539f\u5219\u3002\u8be5\u6846\u67b6\u4e3a\u7406\u89e3\u591a\u6837\u5316\u7684\u8ba4\u77e5\u73b0\u8c61\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6570\u5b66\u57fa\u7840\u3002"}}
{"id": "2512.11854", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11854", "abs": "https://arxiv.org/abs/2512.11854", "authors": ["Grant King", "Musa Azeem", "Savannah Noblitt", "Ramtin Zand", "Homayoun Valafar"], "title": "Rep Smarter, Not Harder: AI Hypertrophy Coaching with Wearable Sensors and Edge Neural Networks", "comment": "24th International Conference on Machine Learning and Applications", "summary": "Optimizing resistance training for hypertrophy requires balancing proximity to muscular failure, often quantified by Repetitions in Reserve (RiR), with fatigue management. However, subjective RiR assessment is unreliable, leading to suboptimal training stimuli or excessive fatigue. This paper introduces a novel system for real-time feedback on near-failure states (RiR $\\le$ 2) during resistance exercise using only a single wrist-mounted Inertial Measurement Unit (IMU). We propose a two-stage pipeline suitable for edge deployment: first, a ResNet-based model segments repetitions from the 6-axis IMU data in real-time. Second, features derived from this segmentation, alongside direct convolutional features and historical context captured by an LSTM, are used by a classification model to identify exercise windows corresponding to near-failure states. Using a newly collected dataset from 13 diverse participants performing preacher curls to failure (631 total reps), our segmentation model achieved an F1 score of 0.83, and the near-failure classifier achieved an F1 score of 0.82 under simulated real-time evaluation conditions (1.6 Hz inference rate). Deployment on a Raspberry Pi 5 yielded an average inference latency of 112 ms, and on an iPhone 16 yielded 23.5 ms, confirming the feasibility for edge computation. This work demonstrates a practical approach for objective, real-time training intensity feedback using minimal hardware, paving the way for accessible AI-driven hypertrophy coaching tools that help users manage intensity and fatigue effectively.", "AI": {"tldr": "\u5f00\u53d1\u57fa\u4e8e\u5355\u8155\u6234IMU\u7684\u5b9e\u65f6\u53cd\u9988\u7cfb\u7edf\uff0c\u7528\u4e8e\u68c0\u6d4b\u963b\u529b\u8bad\u7ec3\u4e2d\u7684\u63a5\u8fd1\u529b\u7aed\u72b6\u6001\uff08RiR\u22642\uff09\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u7ba1\u9053\u5b9e\u73b0\u8fb9\u7f18\u90e8\u7f72\uff0c\u4e3a\u589e\u808c\u8bad\u7ec3\u63d0\u4f9b\u5ba2\u89c2\u5f3a\u5ea6\u53cd\u9988\u3002", "motivation": "\u589e\u808c\u8bad\u7ec3\u9700\u8981\u5728\u63a5\u8fd1\u529b\u7aed\u4e0e\u75b2\u52b3\u7ba1\u7406\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4f46\u4e3b\u89c2\u7684\"\u5269\u4f59\u91cd\u590d\u6b21\u6570\"\u8bc4\u4f30\u4e0d\u53ef\u9760\uff0c\u5bfc\u81f4\u8bad\u7ec3\u523a\u6fc0\u4e0d\u8db3\u6216\u8fc7\u5ea6\u75b2\u52b3\uff0c\u9700\u8981\u5ba2\u89c2\u7684\u5b9e\u65f6\u53cd\u9988\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u8fb9\u7f18\u90e8\u7f72\u7ba1\u9053\uff1a1) \u4f7f\u7528ResNet\u6a21\u578b\u4ece6\u8f74IMU\u6570\u636e\u5b9e\u65f6\u5206\u5272\u91cd\u590d\u52a8\u4f5c\uff1b2) \u7ed3\u5408\u5206\u5272\u7279\u5f81\u3001\u5377\u79ef\u7279\u5f81\u548cLSTM\u5386\u53f2\u4e0a\u4e0b\u6587\uff0c\u5206\u7c7b\u68c0\u6d4b\u63a5\u8fd1\u529b\u7aed\u72b6\u6001\uff08RiR\u22642\uff09\u3002", "result": "\u572813\u540d\u53c2\u4e0e\u8005631\u6b21\u91cd\u590d\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u5206\u5272\u6a21\u578bF1\u5206\u65700.83\uff0c\u63a5\u8fd1\u529b\u7aed\u5206\u7c7b\u5668F1\u5206\u65700.82\uff081.6Hz\u63a8\u7406\u7387\uff09\u3002\u6811\u8393\u6d3e5\u5e73\u5747\u5ef6\u8fdf112ms\uff0ciPhone 16\u5ef6\u8fdf23.5ms\uff0c\u8bc1\u5b9e\u8fb9\u7f18\u8ba1\u7b97\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c55\u793a\u4e86\u4f7f\u7528\u6700\u5c0f\u786c\u4ef6\u5b9e\u73b0\u5ba2\u89c2\u5b9e\u65f6\u8bad\u7ec3\u5f3a\u5ea6\u53cd\u9988\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u4e3a\u53ef\u8bbf\u95ee\u7684AI\u9a71\u52a8\u589e\u808c\u6559\u7ec3\u5de5\u5177\u94fa\u5e73\u9053\u8def\uff0c\u5e2e\u52a9\u7528\u6237\u6709\u6548\u7ba1\u7406\u8bad\u7ec3\u5f3a\u5ea6\u548c\u75b2\u52b3\u3002"}}
{"id": "2512.13300", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.13300", "abs": "https://arxiv.org/abs/2512.13300", "authors": ["Qinglin Jia", "Zhaocheng Du", "Chuhan Wu", "Huifeng Guo", "Ruiming Tang", "Shuting Shi", "Muyu Zhang"], "title": "No One Left Behind: How to Exploit the Incomplete and Skewed Multi-Label Data for Conversion Rate Prediction", "comment": null, "summary": "In most real-world online advertising systems, advertisers typically have diverse customer acquisition goals. A common solution is to use multi-task learning (MTL) to train a unified model on post-click data to estimate the conversion rate (CVR) for these diverse targets. In practice, CVR prediction often encounters missing conversion data as many advertisers submit only a subset of user conversion actions due to privacy or other constraints, making the labels of multi-task data incomplete. If the model is trained on all available samples where advertisers submit user conversion actions, it may struggle when deployed to serve a subset of advertisers targeting specific conversion actions, as the training and deployment data distributions are mismatched. While considerable MTL efforts have been made, a long-standing challenge is how to effectively train a unified model with the incomplete and skewed multi-label data. In this paper, we propose a fine-grained Knowledge transfer framework for Asymmetric Multi-Label data (KAML). We introduce an attribution-driven masking strategy (ADM) to better utilize data with asymmetric multi-label data in training. However, the more relaxed masking in ADM is a double-edged sword: it provides additional training signals but also introduces noise due to skewed data. To address this, we propose a hierarchical knowledge extraction mechanism (HKE) to model the sample discrepancy within the target task tower. Finally, to maximize the utility of unlabeled samples, we incorporate ranking loss strategy to further enhance our model. The effectiveness of KAML has been demonstrated through comprehensive evaluations on offline industry datasets and online A/B tests, which show significant performance improvements over existing MTL baselines.", "AI": {"tldr": "KAML\u6846\u67b6\u89e3\u51b3\u5728\u7ebf\u5e7f\u544a\u7cfb\u7edf\u4e2d\u591a\u4efb\u52a1\u5b66\u4e60\u9762\u4e34\u7684\u4e0d\u5b8c\u6574\u6807\u7b7e\u6570\u636e\u95ee\u9898\uff0c\u901a\u8fc7\u5c5e\u6027\u9a71\u52a8\u63a9\u7801\u7b56\u7565\u3001\u5206\u5c42\u77e5\u8bc6\u63d0\u53d6\u673a\u5236\u548c\u6392\u5e8f\u635f\u5931\u7b56\u7565\u63d0\u5347\u8f6c\u5316\u7387\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u5728\u7ebf\u5e7f\u544a\u7cfb\u7edf\u4e2d\uff0c\u5e7f\u544a\u4e3b\u901a\u5e38\u6709\u591a\u6837\u5316\u7684\u5ba2\u6237\u83b7\u53d6\u76ee\u6807\uff0c\u4f7f\u7528\u591a\u4efb\u52a1\u5b66\u4e60\u8bad\u7ec3\u7edf\u4e00\u6a21\u578b\u9884\u6d4b\u8f6c\u5316\u7387\u3002\u4f46\u5b9e\u8df5\u4e2d\u5e38\u9047\u5230\u6807\u7b7e\u6570\u636e\u4e0d\u5b8c\u6574\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u8bb8\u591a\u5e7f\u544a\u4e3b\u53ea\u63d0\u4ea4\u90e8\u5206\u7528\u6237\u8f6c\u5316\u884c\u4e3a\u6570\u636e\uff0c\u5bfc\u81f4\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u4e0e\u90e8\u7f72\u6570\u636e\u5206\u5e03\u4e0d\u5339\u914d\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u8fd9\u79cd\u4e0d\u5bf9\u79f0\u591a\u6807\u7b7e\u6570\u636e\u3002", "method": "\u63d0\u51faKAML\u6846\u67b6\uff1a1) \u5c5e\u6027\u9a71\u52a8\u63a9\u7801\u7b56\u7565(ADM)\u66f4\u597d\u5730\u5229\u7528\u4e0d\u5bf9\u79f0\u591a\u6807\u7b7e\u6570\u636e\uff1b2) \u5206\u5c42\u77e5\u8bc6\u63d0\u53d6\u673a\u5236(HKE)\u5efa\u6a21\u76ee\u6807\u4efb\u52a1\u5854\u5185\u7684\u6837\u672c\u5dee\u5f02\uff1b3) \u7ed3\u5408\u6392\u5e8f\u635f\u5931\u7b56\u7565\u6700\u5927\u5316\u672a\u6807\u8bb0\u6837\u672c\u7684\u6548\u7528\u3002", "result": "\u5728\u79bb\u7ebf\u884c\u4e1a\u6570\u636e\u96c6\u548c\u5728\u7ebfA/B\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u76f8\u6bd4\u73b0\u6709\u591a\u4efb\u52a1\u5b66\u4e60\u57fa\u7ebf\u65b9\u6cd5\uff0cKAML\u663e\u793a\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "KAML\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5728\u7ebf\u5e7f\u544a\u7cfb\u7edf\u4e2d\u591a\u4efb\u52a1\u5b66\u4e60\u9762\u4e34\u7684\u4e0d\u5b8c\u6574\u6807\u7b7e\u6570\u636e\u6311\u6218\uff0c\u901a\u8fc7\u7cbe\u7ec6\u5316\u7684\u77e5\u8bc6\u8f6c\u79fb\u673a\u5236\u63d0\u5347\u4e86\u8f6c\u5316\u7387\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2512.11879", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11879", "abs": "https://arxiv.org/abs/2512.11879", "authors": ["Beatriz Costa-Gomes", "Sophia Chen", "Connie Hsueh", "Deborah Morgan", "Philipp Schoenegger", "Yash Shah", "Sam Way", "Yuki Zhu", "Timoth\u00e9 Adeline", "Michael Bhaskar", "Mustafa Suleyman", "Seth Spielman"], "title": "It's About Time: The Temporal and Modal Dynamics of Copilot Usage", "comment": "12 pages, 10 figures", "summary": "We analyze 37.5 million deidentified conversations with Microsoft's Copilot between January and September 2025. Unlike prior analyses of AI usage, we focus not just on what people do with AI, but on how and when they do it. We find that how people use AI depends fundamentally on context and device type. On mobile, health is the dominant topic, which is consistent across every hour and every month we observed - with users seeking not just information but also advice. On desktop, the pattern is strikingly different: work and technology dominate during business hours, with \"Work and Career\" overtaking \"Technology\" as the top topic precisely between 8 a.m. and 5 p.m. These differences extend to temporal rhythms: programming queries spike on weekdays while gaming rises on weekends, philosophical questions climb during late-night hours, and relationship conversations surge on Valentine's Day. These patterns suggest that users have rapidly integrated AI into the full texture of their lives, as a work aid at their desks and a companion on their phones.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e863750\u4e07\u6b21\u4e0e\u5fae\u8f6fCopilot\u7684\u533f\u540d\u5bf9\u8bdd\uff0c\u53d1\u73b0AI\u4f7f\u7528\u6a21\u5f0f\u56e0\u8bbe\u5907\u7c7b\u578b\u548c\u4e0a\u4e0b\u6587\u800c\u5f02\uff1a\u79fb\u52a8\u7aef\u4ee5\u5065\u5eb7\u8bdd\u9898\u4e3a\u4e3b\uff0c\u684c\u9762\u7aef\u5219\u4ee5\u5de5\u4f5c\u548c\u79d1\u6280\u4e3a\u4e3b\uff0c\u4e14\u4f7f\u7528\u6a21\u5f0f\u968f\u65f6\u95f4\u548c\u65e5\u671f\u5448\u73b0\u89c4\u5f8b\u6027\u53d8\u5316\u3002", "motivation": "\u4e0e\u4ee5\u5f80\u4ec5\u5173\u6ce8\u7528\u6237\u7528AI\u505a\u4ec0\u4e48\u7684\u7814\u7a76\u4e0d\u540c\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u7528\u6237\u5982\u4f55\u4ee5\u53ca\u5728\u4f55\u65f6\u4f7f\u7528AI\uff0c\u5206\u6790AI\u4f7f\u7528\u6a21\u5f0f\u5982\u4f55\u968f\u8bbe\u5907\u7c7b\u578b\u3001\u65f6\u95f4\u3001\u65e5\u671f\u7b49\u4e0a\u4e0b\u6587\u56e0\u7d20\u53d8\u5316\u3002", "method": "\u5206\u6790\u4e862025\u5e741\u6708\u81f39\u6708\u671f\u95f43750\u4e07\u6b21\u4e0e\u5fae\u8f6fCopilot\u7684\u533f\u540d\u5bf9\u8bdd\u6570\u636e\uff0c\u901a\u8fc7\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u548c\u4e3b\u9898\u5206\u7c7b\uff0c\u6bd4\u8f83\u4e0d\u540c\u8bbe\u5907\u7c7b\u578b\uff08\u79fb\u52a8\u7aefvs\u684c\u9762\u7aef\uff09\u5728\u4e0d\u540c\u65f6\u95f4\u6bb5\u7684\u4f7f\u7528\u6a21\u5f0f\u3002", "result": "\u53d1\u73b0AI\u4f7f\u7528\u6a21\u5f0f\u5b58\u5728\u663e\u8457\u7684\u8bbe\u5907\u5dee\u5f02\uff1a\u79fb\u52a8\u7aef\u5065\u5eb7\u8bdd\u9898\u5360\u4e3b\u5bfc\u4e14\u5168\u5929\u7a33\u5b9a\uff1b\u684c\u9762\u7aef\u5de5\u4f5c\u65e5\u4ee5\u5de5\u4f5c\u548c\u79d1\u6280\u4e3a\u4e3b\uff088am-5pm\u5de5\u4f5c\u8bdd\u9898\u8d85\u8fc7\u79d1\u6280\u8bdd\u9898\uff09\u3002\u7f16\u7a0b\u67e5\u8be2\u5728\u5de5\u4f5c\u65e5\u6fc0\u589e\uff0c\u6e38\u620f\u8bdd\u9898\u5728\u5468\u672b\u4e0a\u5347\uff0c\u6df1\u591c\u54f2\u5b66\u95ee\u9898\u589e\u591a\uff0c\u60c5\u4eba\u8282\u5173\u7cfb\u8bdd\u9898\u6fc0\u589e\u3002", "conclusion": "\u7528\u6237\u5df2\u5feb\u901f\u5c06AI\u878d\u5165\u751f\u6d3b\u7684\u5404\u4e2a\u65b9\u9762\uff1a\u4f5c\u4e3a\u5de5\u4f5c\u52a9\u624b\u5728\u529e\u516c\u684c\u4e0a\u4f7f\u7528\uff0c\u4f5c\u4e3a\u4f34\u4fa3\u5728\u624b\u673a\u4e0a\u4f7f\u7528\u3002AI\u4f7f\u7528\u6a21\u5f0f\u53cd\u6620\u4e86\u7528\u6237\u65e5\u5e38\u751f\u6d3b\u7684\u8282\u594f\u548c\u9700\u6c42\u53d8\u5316\u3002"}}
{"id": "2512.12260", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2512.12260", "abs": "https://arxiv.org/abs/2512.12260", "authors": ["Ege Atacan Do\u011fan", "Peter F. Patel-Schneider"], "title": "A Multi-Axial Mindset for Ontology Design Lessons from Wikidata's Polyhierarchical Structure", "comment": null, "summary": "Traditional ontology design emphasizes disjoint and exhaustive top-level distinctions such as continuant vs. occurrent, abstract vs. concrete, or type vs. instance. These distinctions are used to structure unified hierarchies where every entity is classified under a single upper-level category. Wikidata, by contrast, does not enforce a singular foundational taxonomy. Instead, it accommodates multiple classification axes simultaneously under the shared root class entity. This paper analyzes the structural implications of Wikidata's polyhierarchical and multi-axial design. The Wikidata architecture enables a scalable and modular approach to ontology construction, especially suited to collaborative and evolving knowledge graphs.", "AI": {"tldr": "Wikidata\u91c7\u7528\u591a\u5c42\u7ea7\u3001\u591a\u8f74\u5206\u7c7b\u8bbe\u8ba1\u800c\u975e\u4f20\u7edf\u672c\u4f53\u8bba\u7684\u5355\u4e00\u5c42\u6b21\u7ed3\u6784\uff0c\u5206\u6790\u5176\u7ed3\u6784\u5f71\u54cd", "motivation": "\u4f20\u7edf\u672c\u4f53\u8bbe\u8ba1\u5f3a\u8c03\u4e92\u65a5\u4e14\u7a77\u5c3d\u7684\u9876\u5c42\u533a\u5206\uff08\u5982\u6301\u7eed\u4f53vs\u53d1\u751f\u4f53\u3001\u62bd\u8c61vs\u5177\u4f53\u3001\u7c7b\u578bvs\u5b9e\u4f8b\uff09\uff0c\u5f62\u6210\u7edf\u4e00\u7684\u5c42\u6b21\u7ed3\u6784\u3002\u800cWikidata\u4e0d\u5f3a\u5236\u6267\u884c\u5355\u4e00\u7684\u57fa\u7840\u5206\u7c7b\u6cd5\uff0c\u800c\u662f\u5141\u8bb8\u591a\u4e2a\u5206\u7c7b\u8f74\u5171\u5b58\uff0c\u9700\u8981\u5206\u6790\u8fd9\u79cd\u591a\u5c42\u7ea7\u3001\u591a\u8f74\u8bbe\u8ba1\u7684\u7ed3\u6784\u5f71\u54cd\u3002", "method": "\u5206\u6790Wikidata\u7684\u591a\u5c42\u7ea7\u548c\u591a\u8f74\u8bbe\u8ba1\u7ed3\u6784\uff0c\u7814\u7a76\u5176\u5728\u5171\u4eab\u6839\u7c7b\"\u5b9e\u4f53\"\u4e0b\u540c\u65f6\u5bb9\u7eb3\u591a\u4e2a\u5206\u7c7b\u8f74\u7684\u65b9\u6cd5\uff0c\u63a2\u8ba8\u8fd9\u79cd\u67b6\u6784\u5982\u4f55\u652f\u6301\u53ef\u6269\u5c55\u548c\u6a21\u5757\u5316\u7684\u672c\u4f53\u6784\u5efa\u3002", "result": "Wikidata\u7684\u591a\u5c42\u7ea7\u3001\u591a\u8f74\u8bbe\u8ba1\u4f7f\u5176\u7279\u522b\u9002\u5408\u534f\u4f5c\u548c\u6f14\u5316\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u63d0\u4f9b\u4e86\u6bd4\u4f20\u7edf\u5355\u4e00\u5c42\u6b21\u7ed3\u6784\u66f4\u7075\u6d3b\u3001\u53ef\u6269\u5c55\u7684\u672c\u4f53\u6784\u5efa\u65b9\u6cd5\u3002", "conclusion": "Wikidata\u7684\u67b6\u6784\u901a\u8fc7\u5141\u8bb8\u591a\u4e2a\u5206\u7c7b\u8f74\u540c\u65f6\u5b58\u5728\uff0c\u5b9e\u73b0\u4e86\u66f4\u7075\u6d3b\u3001\u53ef\u6269\u5c55\u7684\u672c\u4f53\u6784\u5efa\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u5408\u5927\u89c4\u6a21\u534f\u4f5c\u548c\u4e0d\u65ad\u6f14\u5316\u7684\u77e5\u8bc6\u56fe\u8c31\u73af\u5883\u3002"}}
{"id": "2512.11855", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11855", "abs": "https://arxiv.org/abs/2512.11855", "authors": ["Behrooz Tahmasebi", "Melanie Weber"], "title": "Achieving Approximate Symmetry Is Exponentially Easier than Exact Symmetry", "comment": "32 pages, 2 figures", "summary": "Enforcing exact symmetry in machine learning models often yields significant gains in scientific applications, serving as a powerful inductive bias. However, recent work suggests that relying on approximate symmetry can offer greater flexibility and robustness. Despite promising empirical evidence, there has been little theoretical understanding, and in particular, a direct comparison between exact and approximate symmetry is missing from the literature. In this paper, we initiate this study by asking: What is the cost of enforcing exact versus approximate symmetry? To address this question, we introduce averaging complexity, a framework for quantifying the cost of enforcing symmetry via averaging. Our main result is an exponential separation: under standard conditions, achieving exact symmetry requires linear averaging complexity, whereas approximate symmetry can be attained with only logarithmic averaging complexity. To the best of our knowledge, this provides the first theoretical separation of these two cases, formally justifying why approximate symmetry may be preferable in practice. Beyond this, our tools and techniques may be of independent interest for the broader study of symmetries in machine learning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9996\u6b21\u4ece\u7406\u8bba\u4e0a\u6bd4\u8f83\u4e86\u7cbe\u786e\u5bf9\u79f0\u6027\u4e0e\u8fd1\u4f3c\u5bf9\u79f0\u6027\u7684\u5b9e\u73b0\u6210\u672c\uff0c\u53d1\u73b0\u7cbe\u786e\u5bf9\u79f0\u6027\u9700\u8981\u7ebf\u6027\u5e73\u5747\u590d\u6742\u5ea6\uff0c\u800c\u8fd1\u4f3c\u5bf9\u79f0\u6027\u4ec5\u9700\u5bf9\u6570\u5e73\u5747\u590d\u6742\u5ea6\uff0c\u5b58\u5728\u6307\u6570\u7ea7\u5dee\u5f02\u3002", "motivation": "\u5728\u673a\u5668\u5b66\u4e60\u79d1\u5b66\u5e94\u7528\u4e2d\uff0c\u7cbe\u786e\u5bf9\u79f0\u6027\u4f5c\u4e3a\u5f3a\u5927\u7684\u5f52\u7eb3\u504f\u7f6e\u5e26\u6765\u663e\u8457\u6536\u76ca\uff0c\u4f46\u8fd1\u671f\u7814\u7a76\u8868\u660e\u8fd1\u4f3c\u5bf9\u79f0\u6027\u53ef\u80fd\u63d0\u4f9b\u66f4\u5927\u7684\u7075\u6d3b\u6027\u548c\u9c81\u68d2\u6027\u3002\u7136\u800c\uff0c\u8fd9\u4e24\u79cd\u65b9\u6cd5\u7684\u7406\u8bba\u6bd4\u8f83\u5728\u6587\u732e\u4e2d\u7f3a\u5931\uff0c\u7279\u522b\u662f\u7f3a\u4e4f\u5bf9\u5b9e\u73b0\u6210\u672c\u7684\u7406\u8bba\u7406\u89e3\u3002", "method": "\u5f15\u5165\"\u5e73\u5747\u590d\u6742\u5ea6\"\u6846\u67b6\u6765\u91cf\u5316\u901a\u8fc7\u5e73\u5747\u65b9\u6cd5\u5b9e\u73b0\u5bf9\u79f0\u6027\u7684\u6210\u672c\u3002\u5728\u6807\u51c6\u6761\u4ef6\u4e0b\uff0c\u5206\u6790\u7cbe\u786e\u5bf9\u79f0\u6027\u548c\u8fd1\u4f3c\u5bf9\u79f0\u6027\u6240\u9700\u7684\u5e73\u5747\u590d\u6742\u5ea6\u3002", "result": "\u4e3b\u8981\u53d1\u73b0\u662f\u6307\u6570\u7ea7\u5206\u79bb\uff1a\u5b9e\u73b0\u7cbe\u786e\u5bf9\u79f0\u6027\u9700\u8981\u7ebf\u6027\u5e73\u5747\u590d\u6742\u5ea6\uff0c\u800c\u5b9e\u73b0\u8fd1\u4f3c\u5bf9\u79f0\u6027\u4ec5\u9700\u5bf9\u6570\u5e73\u5747\u590d\u6742\u5ea6\u3002\u8fd9\u662f\u9996\u6b21\u4ece\u7406\u8bba\u4e0a\u5206\u79bb\u8fd9\u4e24\u79cd\u60c5\u51b5\uff0c\u4e3a\u5b9e\u8df5\u4e2d\u4f18\u5148\u9009\u62e9\u8fd1\u4f3c\u5bf9\u79f0\u6027\u63d0\u4f9b\u4e86\u6b63\u5f0f\u4f9d\u636e\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u4ece\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u8fd1\u4f3c\u5bf9\u79f0\u6027\u76f8\u5bf9\u4e8e\u7cbe\u786e\u5bf9\u79f0\u6027\u5728\u5b9e\u73b0\u6210\u672c\u4e0a\u7684\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u673a\u5668\u5b66\u4e60\u4e2d\u5bf9\u79f0\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u5de5\u5177\u548c\u6846\u67b6\uff0c\u8fd9\u4e9b\u5de5\u5177\u548c\u6280\u672f\u5bf9\u66f4\u5e7f\u6cdb\u7684\u5bf9\u79f0\u6027\u7814\u7a76\u5177\u6709\u72ec\u7acb\u4ef7\u503c\u3002"}}
{"id": "2512.13511", "categories": ["cs.CV", "cs.IR"], "pdf": "https://arxiv.org/pdf/2512.13511", "abs": "https://arxiv.org/abs/2512.13511", "authors": ["Piyush Bagad", "Andrew Zisserman"], "title": "TARA: Simple and Efficient Time Aware Retrieval Adaptation of MLLMs for Video Understanding", "comment": "18 Pages. Project page at http://bpiyush.github.io/tara-website", "summary": "Our objective is to build a general time-aware video-text embedding model for retrieval. To that end, we propose a simple and efficient recipe, dubbed TARA (Time Aware Retrieval Adaptation), to adapt Multimodal LLMs (MLLMs) to a time-aware video-text embedding model without using any video data at all. For evaluating time-awareness in retrieval, we propose a new benchmark with temporally opposite (chiral) actions as hard negatives and curated splits for chiral and non-chiral actions. We show that TARA outperforms all existing video-text models on this chiral benchmark while also achieving strong results on standard benchmarks. Furthermore, we discover additional benefits of TARA beyond time-awareness: (i) TARA embeddings are negation-aware as shown in NegBench benchmark that evaluates negation in video retrieval, (ii) TARA achieves state of the art performance on verb and adverb understanding in videos. Overall, TARA yields a strong, versatile, time-aware video-text embedding model with state of the art zero-shot performance.", "AI": {"tldr": "TARA\u662f\u4e00\u79cd\u65e0\u9700\u89c6\u9891\u6570\u636e\u3001\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65f6\u95f4\u611f\u77e5\u89c6\u9891-\u6587\u672c\u5d4c\u5165\u6a21\u578b\u9002\u914d\u65b9\u6cd5\uff0c\u5728\u65f6\u95f4\u611f\u77e5\u68c0\u7d22\u3001\u5426\u5b9a\u7406\u89e3\u548c\u52a8\u8bcd\u526f\u8bcd\u7406\u89e3\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u6784\u5efa\u901a\u7528\u7684\u65f6\u95f4\u611f\u77e5\u89c6\u9891-\u6587\u672c\u5d4c\u5165\u6a21\u578b\uff0c\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5728\u65f6\u95f4\u611f\u77e5\u68c0\u7d22\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5bf9\u65f6\u95f4\u987a\u5e8f\u654f\u611f\u7684\u52a8\u4f5c\u8bc6\u522b\u3002", "method": "\u63d0\u51faTARA\uff08\u65f6\u95f4\u611f\u77e5\u68c0\u7d22\u9002\u914d\uff09\u65b9\u6cd5\uff0c\u65e0\u9700\u89c6\u9891\u6570\u636e\u5373\u53ef\u5c06\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u9002\u914d\u4e3a\u65f6\u95f4\u611f\u77e5\u89c6\u9891-\u6587\u672c\u5d4c\u5165\u6a21\u578b\u3002\u540c\u65f6\u63d0\u51fa\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f7f\u7528\u65f6\u95f4\u76f8\u53cd\uff08\u624b\u6027\uff09\u52a8\u4f5c\u4f5c\u4e3a\u56f0\u96be\u8d1f\u6837\u672c\u3002", "result": "TARA\u5728\u624b\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4f18\u4e8e\u6240\u6709\u73b0\u6709\u89c6\u9891-\u6587\u672c\u6a21\u578b\uff0c\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u8868\u73b0\u5f3a\u52b2\u3002\u6b64\u5916\uff0cTARA\u5d4c\u5165\u5177\u6709\u5426\u5b9a\u611f\u77e5\u80fd\u529b\uff0c\u5728\u52a8\u8bcd\u548c\u526f\u8bcd\u7406\u89e3\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "TARA\u4ea7\u751f\u4e86\u4e00\u4e2a\u5f3a\u5927\u3001\u901a\u7528\u3001\u65f6\u95f4\u611f\u77e5\u7684\u89c6\u9891-\u6587\u672c\u5d4c\u5165\u6a21\u578b\uff0c\u5177\u6709\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u5728\u65f6\u95f4\u611f\u77e5\u68c0\u7d22\u3001\u5426\u5b9a\u7406\u89e3\u548c\u52a8\u4f5c\u7406\u89e3\u65b9\u9762\u5747\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2512.11988", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11988", "abs": "https://arxiv.org/abs/2512.11988", "authors": ["Xianghui Xie", "Bowen Wen", "Yan Chang", "Hesam Rabeti", "Jiefeng Li", "Ye Yuan", "Gerard Pons-Moll", "Stan Birchfield"], "title": "CARI4D: Category Agnostic 4D Reconstruction of Human-Object Interaction", "comment": "14 pages, 8 figures, 4 tables. Project page: https://nvlabs.github.io/CARI4D/", "summary": "Accurate capture of human-object interaction from ubiquitous sensors like RGB cameras is important for applications in human understanding, gaming, and robot learning. However, inferring 4D interactions from a single RGB view is highly challenging due to the unknown object and human information, depth ambiguity, occlusion, and complex motion, which hinder consistent 3D and temporal reconstruction. Previous methods simplify the setup by assuming ground truth object template or constraining to a limited set of object categories. We present CARI4D, the first category-agnostic method that reconstructs spatially and temporarily consistent 4D human-object interaction at metric scale from monocular RGB videos. To this end, we propose a pose hypothesis selection algorithm that robustly integrates the individual predictions from foundation models, jointly refine them through a learned render-and-compare paradigm to ensure spatial, temporal and pixel alignment, and finally reasoning about intricate contacts for further refinement satisfying physical constraints. Experiments show that our method outperforms prior art by 38% on in-distribution dataset and 36% on unseen dataset in terms of reconstruction error. Our model generalizes beyond the training categories and thus can be applied zero-shot to in-the-wild internet videos. Our code and pretrained models will be publicly released.", "AI": {"tldr": "CARI4D\uff1a\u9996\u4e2a\u4ece\u5355\u76eeRGB\u89c6\u9891\u4e2d\u91cd\u5efa4D\u4eba-\u7269\u4ea4\u4e92\u7684\u7c7b\u522b\u65e0\u5173\u65b9\u6cd5\uff0c\u901a\u8fc7\u59ff\u6001\u5047\u8bbe\u9009\u62e9\u7b97\u6cd5\u548c\u6e32\u67d3\u6bd4\u8f83\u8303\u5f0f\u5b9e\u73b0\u7a7a\u95f4\u3001\u65f6\u95f4\u548c\u50cf\u7d20\u5bf9\u9f50\uff0c\u5728\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u5148\u524d\u65b9\u6cd536%", "motivation": "\u4ece\u5355\u76eeRGB\u89c6\u9891\u51c6\u786e\u6355\u6349\u4eba-\u7269\u4ea4\u4e92\u5bf9\u4e8e\u4eba\u7c7b\u7406\u89e3\u3001\u6e38\u620f\u548c\u673a\u5668\u4eba\u5b66\u4e60\u5e94\u7528\u5f88\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u672a\u77e5\u7269\u4f53\u548c\u4eba\u4f53\u4fe1\u606f\u3001\u6df1\u5ea6\u6a21\u7cca\u3001\u906e\u6321\u548c\u590d\u6742\u8fd0\u52a8\u7b49\u56e0\u7d20\uff0c\u4ece\u5355\u89c6\u89d2\u63a8\u65ad4D\u4ea4\u4e92\u6781\u5177\u6311\u6218\u6027\u3002\u5148\u524d\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u771f\u5b9e\u7269\u4f53\u6a21\u677f\u6216\u5c40\u9650\u4e8e\u6709\u9650\u7269\u4f53\u7c7b\u522b\u3002", "method": "\u63d0\u51faCARI4D\u65b9\u6cd5\uff1a1\uff09\u59ff\u6001\u5047\u8bbe\u9009\u62e9\u7b97\u6cd5\uff0c\u9c81\u68d2\u5730\u6574\u5408\u57fa\u7840\u6a21\u578b\u7684\u4e2a\u4f53\u9884\u6d4b\uff1b2\uff09\u901a\u8fc7\u5b66\u4e60\u7684\u6e32\u67d3-\u6bd4\u8f83\u8303\u5f0f\u8054\u5408\u4f18\u5316\uff0c\u786e\u4fdd\u7a7a\u95f4\u3001\u65f6\u95f4\u548c\u50cf\u7d20\u5bf9\u9f50\uff1b3\uff09\u63a8\u7406\u590d\u6742\u63a5\u89e6\u70b9\u8fdb\u884c\u8fdb\u4e00\u6b65\u7ec6\u5316\uff0c\u6ee1\u8db3\u7269\u7406\u7ea6\u675f\u3002\u8fd9\u662f\u9996\u4e2a\u7c7b\u522b\u65e0\u5173\u76844D\u4eba-\u7269\u4ea4\u4e92\u91cd\u5efa\u65b9\u6cd5\u3002", "result": "\u5728\u5206\u5e03\u5185\u6570\u636e\u96c6\u4e0a\u91cd\u5efa\u8bef\u5dee\u4f18\u4e8e\u5148\u524d\u65b9\u6cd538%\uff0c\u5728\u672a\u89c1\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e36%\u3002\u6a21\u578b\u80fd\u591f\u6cdb\u5316\u5230\u8bad\u7ec3\u7c7b\u522b\u4e4b\u5916\uff0c\u53ef\u96f6\u6837\u672c\u5e94\u7528\u4e8e\u91ce\u5916\u4e92\u8054\u7f51\u89c6\u9891\u3002\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u5c06\u516c\u5f00\u53d1\u5e03\u3002", "conclusion": "CARI4D\u662f\u9996\u4e2a\u4ece\u5355\u76eeRGB\u89c6\u9891\u4e2d\u91cd\u5efa\u7a7a\u95f4\u548c\u65f6\u95f4\u4e00\u81f4\u76844D\u4eba-\u7269\u4ea4\u4e92\u7684\u7c7b\u522b\u65e0\u5173\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u57fa\u7840\u6a21\u578b\u9884\u6d4b\u548c\u6e32\u67d3\u6bd4\u8f83\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u91cd\u5efa\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.11882", "categories": ["cs.CY", "cs.AI", "cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.11882", "abs": "https://arxiv.org/abs/2512.11882", "authors": ["Lucia Happe", "Dominik Fuch\u00df", "Luca H\u00fcttner", "Kai Marquardt", "Anne Koziolek"], "title": "An Experience Report on a Pedagogically Controlled, Curriculum-Constrained AI Tutor for SE Education", "comment": "11 pages, 4 figures, accepted for publication at ICSE 2026 SEET Track", "summary": "The integration of artificial intelligence (AI) into education continues to evoke both promise and skepticism. While past waves of technological optimism often fell short, recent advances in large language models (LLMs) have revived the vision of scalable, individualized tutoring. This paper presents the design and pilot evaluation of RockStartIT Tutor, an AI-powered assistant developed for a digital programming and computational thinking course within the RockStartIT initiative. Powered by GPT-4 via OpenAI's Assistant API, the tutor employs a novel prompting strategy and a modular, semantically tagged knowledge base to deliver context-aware, personalized, and curriculum-constrained support for secondary school students. We evaluated the system using the Technology Acceptance Model (TAM) with 13 students and teachers. Learners appreciated the low-stakes environment for asking questions and receiving scaffolded guidance. Educators emphasized the system's potential to reduce cognitive load during independent tasks and complement classroom teaching. Key challenges include prototype limitations, a small sample size, and the need for long-term studies with the target age group. Our findings highlight a pragmatic approach to AI integration that requires no model training, using structure and prompts to shape behavior. We position AI tutors not as teacher replacements but as enabling tools that extend feedback access, foster inquiry, and support what schools do best: help students learn.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86RockStartIT Tutor\u7684\u8bbe\u8ba1\u4e0e\u8bd5\u70b9\u8bc4\u4f30\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e3a\u4e2d\u5b66\u7f16\u7a0b\u8bfe\u7a0b\u5f00\u53d1\u7684AI\u8f85\u5bfc\u7cfb\u7edf\uff0c\u57fa\u4e8eGPT-4\u548c\u7ed3\u6784\u5316\u77e5\u8bc6\u5e93\u63d0\u4f9b\u4e2a\u6027\u5316\u652f\u6301\u3002", "motivation": "\u5c3d\u7ba1AI\u5728\u6559\u80b2\u4e2d\u7684\u5e94\u7528\u524d\u666f\u5e7f\u9614\u4f46\u5e38\u4f34\u968f\u8d28\u7591\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6700\u65b0\u53d1\u5c55\u4e3a\u53ef\u6269\u5c55\u7684\u4e2a\u6027\u5316\u8f85\u5bfc\u5e26\u6765\u4e86\u65b0\u5e0c\u671b\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u901a\u8fc7AI\u8f85\u52a9\u5de5\u5177\u589e\u5f3a\u7f16\u7a0b\u6559\u80b2\uff0c\u800c\u975e\u66ff\u4ee3\u6559\u5e08\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8eGPT-4\u548cOpenAI Assistant API\u7684AI\u8f85\u5bfc\u7cfb\u7edf\uff0c\u91c7\u7528\u65b0\u9896\u7684\u63d0\u793a\u7b56\u7565\u548c\u6a21\u5757\u5316\u8bed\u4e49\u6807\u8bb0\u77e5\u8bc6\u5e93\uff0c\u786e\u4fdd\u4e0a\u4e0b\u6587\u611f\u77e5\u3001\u4e2a\u6027\u5316\u548c\u8bfe\u7a0b\u7ea6\u675f\u7684\u652f\u6301\u3002\u4f7f\u7528\u6280\u672f\u63a5\u53d7\u6a21\u578b\u5bf913\u540d\u5b66\u751f\u548c\u6559\u5e08\u8fdb\u884c\u8bd5\u70b9\u8bc4\u4f30\u3002", "result": "\u5b66\u751f\u6b23\u8d4f\u7cfb\u7edf\u63d0\u4f9b\u7684\u4f4e\u98ce\u9669\u63d0\u95ee\u73af\u5883\u548c\u652f\u67b6\u5f0f\u6307\u5bfc\uff0c\u6559\u5e08\u8ba4\u4e3a\u7cfb\u7edf\u80fd\u51cf\u8f7b\u72ec\u7acb\u4efb\u52a1\u65f6\u7684\u8ba4\u77e5\u8d1f\u8377\u5e76\u8865\u5145\u8bfe\u5802\u6559\u5b66\u3002\u4e3b\u8981\u6311\u6218\u5305\u62ec\u539f\u578b\u9650\u5236\u3001\u6837\u672c\u91cf\u5c0f\u4ee5\u53ca\u9700\u8981\u9488\u5bf9\u76ee\u6807\u5e74\u9f84\u7ec4\u7684\u957f\u671f\u7814\u7a76\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u65e0\u9700\u6a21\u578b\u8bad\u7ec3\u7684\u5b9e\u7528AI\u96c6\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u548c\u63d0\u793a\u5851\u9020\u884c\u4e3a\u3002AI\u8f85\u5bfc\u4e0d\u5e94\u88ab\u89c6\u4e3a\u6559\u5e08\u66ff\u4ee3\u54c1\uff0c\u800c\u662f\u4f5c\u4e3a\u6269\u5c55\u53cd\u9988\u8bbf\u95ee\u3001\u4fc3\u8fdb\u63a2\u7a76\u5e76\u652f\u6301\u5b66\u6821\u6838\u5fc3\u6559\u5b66\u76ee\u6807\u7684\u8d4b\u80fd\u5de5\u5177\u3002"}}
{"id": "2512.11995", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11995", "abs": "https://arxiv.org/abs/2512.11995", "authors": ["Chenrui Fan", "Yijun Liang", "Shweta Bhardwaj", "Kwesi Cobbina", "Ming Li", "Tianyi Zhou"], "title": "V-REX: Benchmarking Exploratory Visual Reasoning via Chain-of-Questions", "comment": "28 pages", "summary": "While many vision-language models (VLMs) are developed to answer well-defined, straightforward questions with highly specified targets, as in most benchmarks, they often struggle in practice with complex open-ended tasks, which usually require multiple rounds of exploration and reasoning in the visual space. Such visual thinking paths not only provide step-by-step exploration and verification as an AI detective but also produce better interpretations of the final answers. However, these paths are challenging to evaluate due to the large exploration space of intermediate steps. To bridge the gap, we develop an evaluation suite, ``Visual Reasoning with multi-step EXploration (V-REX)'', which is composed of a benchmark of challenging visual reasoning tasks requiring native multi-step exploration and an evaluation protocol. V-REX covers rich application scenarios across diverse domains. V-REX casts the multi-step exploratory reasoning into a Chain-of-Questions (CoQ) and disentangles VLMs' capability to (1) Planning: breaking down an open-ended task by selecting a chain of exploratory questions; and (2) Following: answering curated CoQ sequentially to collect information for deriving the final answer. By curating finite options of questions and answers per step, V-REX achieves a reliable quantitative and fine-grained analysis of the intermediate steps. By assessing SOTA proprietary and open-sourced VLMs, we reveal consistent scaling trends, significant differences between planning and following abilities, and substantial room for improvement in multi-step exploratory reasoning.", "AI": {"tldr": "V-REX\u662f\u4e00\u4e2a\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u591a\u6b65\u63a2\u7d22\u63a8\u7406\u80fd\u529b\u7684\u8bc4\u6d4b\u5957\u4ef6\uff0c\u5305\u542b\u6311\u6218\u6027\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u548c\u8bc4\u4f30\u534f\u8bae\uff0c\u5c06\u591a\u6b65\u63a2\u7d22\u8f6c\u5316\u4e3a\u95ee\u9898\u94fe\uff0c\u5206\u522b\u8bc4\u4f30\u89c4\u5212\u548c\u6267\u884c\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u9700\u8981\u591a\u6b65\u63a2\u7d22\u548c\u63a8\u7406\u7684\u590d\u6742\u5f00\u653e\u4efb\u52a1\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u8fd9\u7c7b\u4efb\u52a1\u7684\u4e2d\u95f4\u6b65\u9aa4\u96be\u4ee5\u8bc4\u4f30\u3002\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5f00\u53d1V-REX\u8bc4\u4f30\u5957\u4ef6\uff0c\u5305\u542b\u8de8\u591a\u4e2a\u9886\u57df\u7684\u6311\u6218\u6027\u89c6\u89c9\u63a8\u7406\u4efb\u52a1\u3002\u5c06\u591a\u6b65\u63a2\u7d22\u63a8\u7406\u8f6c\u5316\u4e3a\u95ee\u9898\u94fe\uff08CoQ\uff09\uff0c\u5206\u522b\u8bc4\u4f30\u6a21\u578b\u7684\u89c4\u5212\u80fd\u529b\uff08\u5206\u89e3\u4efb\u52a1\u5e76\u9009\u62e9\u63a2\u7d22\u6027\u95ee\u9898\u94fe\uff09\u548c\u6267\u884c\u80fd\u529b\uff08\u6309\u987a\u5e8f\u56de\u7b54\u95ee\u9898\u94fe\u4ee5\u6536\u96c6\u4fe1\u606f\uff09\u3002\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u6b65\u9aa4\u7b56\u5212\u6709\u9650\u7684\u95ee\u9898\u548c\u7b54\u6848\u9009\u9879\uff0c\u5b9e\u73b0\u53ef\u9760\u7684\u5b9a\u91cf\u548c\u7ec6\u7c92\u5ea6\u5206\u6790\u3002", "result": "\u901a\u8fc7\u8bc4\u4f30\u6700\u5148\u8fdb\u7684\u4e13\u6709\u548c\u5f00\u6e90\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u80fd\u529b\u7684\u4e00\u81f4\u6269\u5c55\u8d8b\u52bf\uff0c\u89c4\u5212\u80fd\u529b\u548c\u6267\u884c\u80fd\u529b\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u4ee5\u53ca\u591a\u6b65\u63a2\u7d22\u63a8\u7406\u4ecd\u6709\u5de8\u5927\u7684\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "V-REX\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u6846\u67b6\u6765\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6b65\u63a2\u7d22\u63a8\u7406\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u6539\u8fdb\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2512.11883", "categories": ["cs.CY", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.11883", "abs": "https://arxiv.org/abs/2512.11883", "authors": ["Wenqi Marshall Guo", "Qingyun Qian", "Khalad Hasan", "Shan Du"], "title": "Aesthetic Alignment Risks Assimilation: How Image Generation and Reward Models Reinforce Beauty Bias and Ideological \"Censorship\"", "comment": null, "summary": "Over-aligning image generation models to a generalized aesthetic preference conflicts with user intent, particularly when ``anti-aesthetic\" outputs are requested for artistic or critical purposes. This adherence prioritizes developer-centered values, compromising user autonomy and aesthetic pluralism. We test this bias by constructing a wide-spectrum aesthetics dataset and evaluating state-of-the-art generation and reward models. We find that aesthetic-aligned generation models frequently default to conventionally beautiful outputs, failing to respect instructions for low-quality or negative imagery. Crucially, reward models penalize anti-aesthetic images even when they perfectly match the explicit user prompt. We confirm this systemic bias through image-to-image editing and evaluation against real abstract artworks.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u56fe\u50cf\u751f\u6210\u6a21\u578b\u8fc7\u5ea6\u5bf9\u9f50\u5230\u5e7f\u4e49\u5ba1\u7f8e\u504f\u597d\u4f1a\u4e0e\u7528\u6237\u610f\u56fe\u51b2\u7a81\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\"\u53cd\u5ba1\u7f8e\"\u8f93\u51fa\u7528\u4e8e\u827a\u672f\u6216\u6279\u5224\u76ee\u7684\u65f6\uff0c\u8fd9\u79cd\u5bf9\u9f50\u4f18\u5148\u8003\u8651\u5f00\u53d1\u8005\u4e2d\u5fc3\u4ef7\u503c\u89c2\uff0c\u635f\u5bb3\u7528\u6237\u81ea\u4e3b\u6027\u548c\u5ba1\u7f8e\u591a\u5143\u6027\u3002", "motivation": "\u5f53\u524d\u56fe\u50cf\u751f\u6210\u6a21\u578b\u8fc7\u5ea6\u5bf9\u9f50\u5230\u5e7f\u4e49\u5ba1\u7f8e\u504f\u597d\uff0c\u5f53\u7528\u6237\u9700\u8981\"\u53cd\u5ba1\u7f8e\"\u8f93\u51fa\u7528\u4e8e\u827a\u672f\u6216\u6279\u5224\u76ee\u7684\u65f6\uff0c\u8fd9\u79cd\u5bf9\u9f50\u4f1a\u4e0e\u7528\u6237\u610f\u56fe\u51b2\u7a81\uff0c\u4f18\u5148\u8003\u8651\u5f00\u53d1\u8005\u4e2d\u5fc3\u4ef7\u503c\u89c2\uff0c\u635f\u5bb3\u7528\u6237\u81ea\u4e3b\u6027\u548c\u5ba1\u7f8e\u591a\u5143\u6027\u3002", "method": "\u6784\u5efa\u5e7f\u8c31\u5ba1\u7f8e\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u6700\u5148\u8fdb\u7684\u751f\u6210\u6a21\u578b\u548c\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u56fe\u50cf\u5230\u56fe\u50cf\u7f16\u8f91\u6d4b\u8bd5\uff0c\u5e76\u4e0e\u771f\u5b9e\u62bd\u8c61\u827a\u672f\u4f5c\u54c1\u8fdb\u884c\u5bf9\u6bd4\u8bc4\u4f30\u3002", "result": "\u5ba1\u7f8e\u5bf9\u9f50\u7684\u751f\u6210\u6a21\u578b\u7ecf\u5e38\u9ed8\u8ba4\u8f93\u51fa\u4f20\u7edf\u7f8e\u611f\u7684\u56fe\u50cf\uff0c\u65e0\u6cd5\u5c0a\u91cd\u4f4e\u8d28\u91cf\u6216\u8d1f\u9762\u56fe\u50cf\u7684\u6307\u4ee4\uff1b\u5956\u52b1\u6a21\u578b\u5373\u4f7f\u53cd\u5ba1\u7f8e\u56fe\u50cf\u5b8c\u5168\u7b26\u5408\u7528\u6237\u63d0\u793a\u4e5f\u4f1a\u5bf9\u5176\u8fdb\u884c\u60e9\u7f5a\uff1b\u901a\u8fc7\u56fe\u50cf\u7f16\u8f91\u548c\u4e0e\u771f\u5b9e\u62bd\u8c61\u827a\u672f\u4f5c\u54c1\u7684\u5bf9\u6bd4\u786e\u8ba4\u4e86\u8fd9\u79cd\u7cfb\u7edf\u6027\u504f\u89c1\u3002", "conclusion": "\u5f53\u524d\u56fe\u50cf\u751f\u6210\u7cfb\u7edf\u7684\u5ba1\u7f8e\u5bf9\u9f50\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u89c1\uff0c\u8fc7\u5ea6\u5f3a\u8c03\u4f20\u7edf\u7f8e\u611f\uff0c\u635f\u5bb3\u4e86\u7528\u6237\u81ea\u4e3b\u6027\u548c\u5ba1\u7f8e\u591a\u5143\u6027\uff0c\u9700\u8981\u91cd\u65b0\u601d\u8003\u5982\u4f55\u5e73\u8861\u5ba1\u7f8e\u504f\u597d\u4e0e\u7528\u6237\u610f\u56fe\u7684\u591a\u6837\u6027\u3002"}}
{"id": "2512.12381", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12381", "abs": "https://arxiv.org/abs/2512.12381", "authors": ["Truong Xuan Khanh", "Truong Quynh Hoa"], "title": "Entropy Collapse: A Universal Failure Mode of Intelligent Systems", "comment": "18 pages, 5 figures", "summary": "Intelligent systems are widely assumed to improve through learning, coordination, and optimization. However, across domains -- from artificial intelligence to economic institutions and biological evolution -- increasing intelligence often precipitates paradoxical degradation: systems become rigid, lose adaptability, and fail unexpectedly.\n  We identify \\emph{entropy collapse} as a universal dynamical failure mode arising when feedback amplification outpaces bounded novelty regeneration. Under minimal domain-agnostic assumptions, we show that intelligent systems undergo a sharp transition from high-entropy adaptive regimes to low-entropy collapsed regimes. Collapse is formalized as convergence toward a stable low-entropy manifold, not a zero-entropy state, implying a contraction of effective adaptive dimensionality rather than loss of activity or scale.\n  We analytically establish critical thresholds, dynamical irreversibility, and attractor structure and demonstrate universality across update mechanisms through minimal simulations. This framework unifies diverse phenomena -- model collapse in AI, institutional sclerosis in economics, and genetic bottlenecks in evolution -- as manifestations of the same underlying process.\n  By reframing collapse as a structural cost of intelligence, our results clarify why late-stage interventions systematically fail and motivate entropy-aware design principles for sustaining long-term adaptability in intelligent systems.\n  \\noindent\\textbf{Keywords:} entropy collapse; intelligent systems; feedback amplification; phase transitions; effective dimensionality; complex systems; model collapse; institutional sclerosis", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u71b5\u5d29\u6e83\"\u6982\u5ff5\uff0c\u89e3\u91ca\u667a\u80fd\u7cfb\u7edf\u5728\u5b66\u4e60\u548c\u4f18\u5316\u8fc7\u7a0b\u4e2d\u4f1a\u7ecf\u5386\u4ece\u9ad8\u71b5\u81ea\u9002\u5e94\u72b6\u6001\u5230\u4f4e\u71b5\u5d29\u6e83\u72b6\u6001\u7684\u76f8\u53d8\uff0c\u5bfc\u81f4\u7cfb\u7edf\u521a\u6027\u5316\u548c\u9002\u5e94\u6027\u4e27\u5931\u3002", "motivation": "\u7814\u7a76\u667a\u80fd\u7cfb\u7edf\uff08\u4ece\u4eba\u5de5\u667a\u80fd\u5230\u7ecf\u6d4e\u5236\u5ea6\u548c\u751f\u7269\u8fdb\u5316\uff09\u4e2d\u666e\u904d\u5b58\u5728\u7684\u6096\u8bba\uff1a\u667a\u80fd\u63d0\u5347\u53cd\u800c\u5bfc\u81f4\u7cfb\u7edf\u50f5\u5316\u3001\u9002\u5e94\u6027\u4e27\u5931\u548c\u610f\u5916\u5931\u8d25\u3002\u4f5c\u8005\u8bd5\u56fe\u627e\u5230\u8fd9\u4e9b\u8de8\u9886\u57df\u73b0\u8c61\u7684\u7edf\u4e00\u89e3\u91ca\u6846\u67b6\u3002", "method": "\u5728\u6700\u5c0f\u5316\u9886\u57df\u65e0\u5173\u5047\u8bbe\u4e0b\uff0c\u5c06\u71b5\u5d29\u6e83\u5f62\u5f0f\u5316\u4e3a\u5411\u7a33\u5b9a\u4f4e\u71b5\u6d41\u5f62\u7684\u6536\u655b\u8fc7\u7a0b\u3002\u901a\u8fc7\u5206\u6790\u5efa\u7acb\u4e34\u754c\u9608\u503c\u3001\u52a8\u6001\u4e0d\u53ef\u9006\u6027\u548c\u5438\u5f15\u5b50\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7\u6700\u5c0f\u6a21\u62df\u5c55\u793a\u66f4\u65b0\u673a\u5236\u7684\u666e\u9002\u6027\u3002", "result": "\u53d1\u73b0\u667a\u80fd\u7cfb\u7edf\u4f1a\u7ecf\u5386\u4ece\u9ad8\u71b5\u81ea\u9002\u5e94\u72b6\u6001\u5230\u4f4e\u71b5\u5d29\u6e83\u72b6\u6001\u7684\u6025\u5267\u76f8\u53d8\u3002\u5d29\u6e83\u8868\u73b0\u4e3a\u6709\u6548\u81ea\u9002\u5e94\u7ef4\u5ea6\u7684\u6536\u7f29\u800c\u975e\u6d3b\u52a8\u6216\u89c4\u6a21\u7684\u4e27\u5931\u3002\u8be5\u6846\u67b6\u7edf\u4e00\u89e3\u91ca\u4e86AI\u4e2d\u7684\u6a21\u578b\u5d29\u6e83\u3001\u7ecf\u6d4e\u5b66\u4e2d\u7684\u5236\u5ea6\u50f5\u5316\u548c\u8fdb\u5316\u4e2d\u7684\u9057\u4f20\u74f6\u9888\u7b49\u73b0\u8c61\u3002", "conclusion": "\u71b5\u5d29\u6e83\u662f\u667a\u80fd\u7684\u7ed3\u6784\u6027\u4ee3\u4ef7\uff0c\u665a\u671f\u5e72\u9884\u901a\u5e38\u5931\u8d25\u3002\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u71b5\u611f\u77e5\u8bbe\u8ba1\u539f\u5219\u5bf9\u4e8e\u7ef4\u6301\u667a\u80fd\u7cfb\u7edf\u957f\u671f\u9002\u5e94\u6027\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2512.11857", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.11857", "abs": "https://arxiv.org/abs/2512.11857", "authors": ["Olivia Kim"], "title": "TopicProphet: Prophesies on Temporal Topic Trends and Stocks", "comment": null, "summary": "Stocks can't be predicted. Despite many hopes, this premise held itself true for many years due to the nature of quantitative stock data lacking causal logic along with rapid market changes hindering accumulation of significant data for training models. To undertake this matter, we propose a novel framework, TopicProphet, to analyze historical eras that share similar public sentiment trends and historical background. Our research deviates from previous studies that identified impacts of keywords and sentiments - we expand on that method by a sequence of topic modeling, temporal analysis, breakpoint detection and segment optimization to detect the optimal time period for training. This results in improving predictions by providing the model with nuanced patterns that occur from that era's socioeconomic and political status while also resolving the shortage of pertinent stock data to train on. Through extensive analysis, we conclude that TopicProphet produces improved outcomes compared to the state-of-the-art methods in capturing the optimal training data for forecasting financial percentage changes.", "AI": {"tldr": "TopicProphet\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u80a1\u7968\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u5177\u6709\u76f8\u4f3c\u516c\u4f17\u60c5\u7eea\u8d8b\u52bf\u548c\u5386\u53f2\u80cc\u666f\u7684\u5386\u53f2\u65f6\u671f\uff0c\u89e3\u51b3\u80a1\u7968\u6570\u636e\u7f3a\u4e4f\u56e0\u679c\u903b\u8f91\u548c\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u80a1\u7968\u9884\u6d4b\u957f\u671f\u4ee5\u6765\u88ab\u8ba4\u4e3a\u662f\u4e0d\u53ef\u80fd\u7684\uff0c\u56e0\u4e3a\u91cf\u5316\u80a1\u7968\u6570\u636e\u7f3a\u4e4f\u56e0\u679c\u903b\u8f91\uff0c\u4e14\u5e02\u573a\u5feb\u901f\u53d8\u5316\u5bfc\u81f4\u96be\u4ee5\u79ef\u7d2f\u8db3\u591f\u7684\u8bad\u7ec3\u6570\u636e\u3002\u4f20\u7edf\u65b9\u6cd5\u4ec5\u5173\u6ce8\u5173\u952e\u8bcd\u548c\u60c5\u7eea\u5f71\u54cd\uff0c\u65e0\u6cd5\u6709\u6548\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51faTopicProphet\u6846\u67b6\uff0c\u91c7\u7528\u4e3b\u9898\u5efa\u6a21\u3001\u65f6\u95f4\u5206\u6790\u3001\u65ad\u70b9\u68c0\u6d4b\u548c\u5206\u6bb5\u4f18\u5316\u7684\u5e8f\u5217\u65b9\u6cd5\uff0c\u8bc6\u522b\u5177\u6709\u76f8\u4f3c\u516c\u4f17\u60c5\u7eea\u8d8b\u52bf\u548c\u5386\u53f2\u80cc\u666f\u7684\u6700\u4f18\u8bad\u7ec3\u65f6\u95f4\u6bb5\u3002", "result": "TopicProphet\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5728\u6355\u6349\u7528\u4e8e\u9884\u6d4b\u91d1\u878d\u767e\u5206\u6bd4\u53d8\u5316\u7684\u6700\u4f18\u8bad\u7ec3\u6570\u636e\u65b9\u9762\u4ea7\u751f\u4e86\u6539\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "\u901a\u8fc7\u5206\u6790\u5386\u53f2\u65f6\u671f\u7684\u76f8\u4f3c\u6027\uff0cTopicProphet\u80fd\u591f\u4e3a\u6a21\u578b\u63d0\u4f9b\u7279\u5b9a\u65f6\u4ee3\u793e\u4f1a\u7ecf\u6d4e\u548c\u653f\u6cbb\u72b6\u51b5\u4ea7\u751f\u7684\u7ec6\u5fae\u6a21\u5f0f\uff0c\u540c\u65f6\u89e3\u51b3\u76f8\u5173\u80a1\u7968\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4ece\u800c\u6539\u5584\u9884\u6d4b\u6548\u679c\u3002"}}
{"id": "2512.12012", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12012", "abs": "https://arxiv.org/abs/2512.12012", "authors": ["Antonio Guillen-Perez"], "title": "Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus", "comment": null, "summary": "The development of robust Autonomous Vehicles (AVs) is bottlenecked by the scarcity of \"Long-Tail\" training data. While fleets collect petabytes of video logs, identifying rare safety-critical events (e.g., erratic jaywalking, construction diversions) remains a manual, cost-prohibitive process. Existing solutions rely on coarse metadata search, which lacks precision, or cloud-based VLMs, which are privacy-invasive and expensive. We introduce Semantic-Drive, a local-first, neuro-symbolic framework for semantic data mining. Our approach decouples perception into two stages: (1) Symbolic Grounding via a real-time open-vocabulary detector (YOLOE) to anchor attention, and (2) Cognitive Analysis via a Reasoning VLM that performs forensic scene analysis. To mitigate hallucination, we implement a \"System 2\" inference-time alignment strategy, utilizing a multi-model \"Judge-Scout\" consensus mechanism. Benchmarked on the nuScenes dataset against the Waymo Open Dataset (WOD-E2E) taxonomy, Semantic-Drive achieves a Recall of 0.966 (vs. 0.475 for CLIP) and reduces Risk Assessment Error by 40\\% compared to single models. The system runs entirely on consumer hardware (NVIDIA RTX 3090), offering a privacy-preserving alternative to the cloud.", "AI": {"tldr": "Semantic-Drive\u662f\u4e00\u4e2a\u672c\u5730\u4f18\u5148\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u89c6\u9891\u65e5\u5fd7\u4e2d\u6316\u6398\u7f55\u89c1\u7684\u5b89\u5168\u5173\u952e\u4e8b\u4ef6\uff0c\u901a\u8fc7\u89e3\u8026\u611f\u77e5\u4e3a\u7b26\u53f7\u5b9a\u4f4d\u548c\u8ba4\u77e5\u5206\u6790\u4e24\u9636\u6bb5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u53ec\u56de\u7387\u5e76\u964d\u4f4e\u4e86\u98ce\u9669\u8bc4\u4f30\u9519\u8bef\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u53d1\u5c55\u53d7\u5230\"\u957f\u5c3e\"\u8bad\u7ec3\u6570\u636e\u7a00\u7f3a\u7684\u74f6\u9888\u9650\u5236\u3002\u867d\u7136\u8f66\u961f\u6536\u96c6\u4e86\u6d77\u91cf\u89c6\u9891\u65e5\u5fd7\uff0c\u4f46\u8bc6\u522b\u7f55\u89c1\u7684\u5b89\u5168\u5173\u952e\u4e8b\u4ef6\uff08\u5982\u4e71\u7a7f\u9a6c\u8def\u3001\u65bd\u5de5\u6539\u9053\uff09\u4ecd\u7136\u662f\u4e00\u4e2a\u624b\u52a8\u3001\u6210\u672c\u9ad8\u6602\u7684\u8fc7\u7a0b\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u8981\u4e48\u4f9d\u8d56\u7f3a\u4e4f\u7cbe\u5ea6\u7684\u7c97\u7c92\u5ea6\u5143\u6570\u636e\u641c\u7d22\uff0c\u8981\u4e48\u4f7f\u7528\u4fb5\u72af\u9690\u79c1\u4e14\u6602\u8d35\u7684\u57fa\u4e8e\u4e91\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "method": "\u63d0\u51faSemantic-Drive\u6846\u67b6\uff0c\u5c06\u611f\u77e5\u89e3\u8026\u4e3a\u4e24\u4e2a\u9636\u6bb5\uff1a(1) \u7b26\u53f7\u5b9a\u4f4d\uff1a\u4f7f\u7528\u5b9e\u65f6\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u5668\uff08YOLOE\uff09\u951a\u5b9a\u6ce8\u610f\u529b\uff1b(2) \u8ba4\u77e5\u5206\u6790\uff1a\u901a\u8fc7\u63a8\u7406\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6cd5\u533b\u573a\u666f\u5206\u6790\u3002\u4e3a\u7f13\u89e3\u5e7b\u89c9\u95ee\u9898\uff0c\u5b9e\u65bd\u4e86\"\u7cfb\u7edf2\"\u63a8\u7406\u65f6\u5bf9\u9f50\u7b56\u7565\uff0c\u91c7\u7528\u591a\u6a21\u578b\"\u6cd5\u5b98-\u4fa6\u5bdf\u5458\"\u5171\u8bc6\u673a\u5236\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u4f7f\u7528Waymo\u5f00\u653e\u6570\u636e\u96c6\uff08WOD-E2E\uff09\u5206\u7c7b\u6cd5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0cSemantic-Drive\u5b9e\u73b0\u4e860.966\u7684\u53ec\u56de\u7387\uff08CLIP\u4e3a0.475\uff09\uff0c\u4e0e\u5355\u6a21\u578b\u76f8\u6bd4\u5c06\u98ce\u9669\u8bc4\u4f30\u9519\u8bef\u964d\u4f4e\u4e8640%\u3002\u7cfb\u7edf\u5b8c\u5168\u5728\u6d88\u8d39\u7ea7\u786c\u4ef6\uff08NVIDIA RTX 3090\uff09\u4e0a\u8fd0\u884c\u3002", "conclusion": "Semantic-Drive\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9690\u79c1\u4fdd\u62a4\u7684\u4e91\u66ff\u4ee3\u65b9\u6848\uff0c\u80fd\u591f\u9ad8\u6548\u5730\u4ece\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u89c6\u9891\u65e5\u5fd7\u4e2d\u6316\u6398\u7f55\u89c1\u7684\u5b89\u5168\u5173\u952e\u4e8b\u4ef6\uff0c\u89e3\u51b3\u4e86\u957f\u5c3e\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4e8b\u4ef6\u68c0\u6d4b\u7684\u53ec\u56de\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2512.11887", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11887", "abs": "https://arxiv.org/abs/2512.11887", "authors": ["Yihan Liao", "Jingyu Zhang", "Jacky Keung", "Yan Xiao", "Yurou Dai"], "title": "Advancing Autonomous Driving System Testing: Demands, Challenges, and Future Directions", "comment": "Accepted for publication in Information and Software Technology (IST)", "summary": "Autonomous driving systems (ADSs) promise improved transportation efficiency and safety, yet ensuring their reliability in complex real-world environments remains a critical challenge. Effective testing is essential to validate ADS performance and reduce deployment risks. This study investigates current ADS testing practices for both modular and end-to-end systems, identifies key demands from industry practitioners and academic researchers, and analyzes the gaps between existing research and real-world requirements. We review major testing techniques and further consider emerging factors such as Vehicle-to-Everything (V2X) communication and foundation models, including large language models and vision foundation models, to understand their roles in enhancing ADS testing. We conducted a large-scale survey with 100 participants from both industry and academia. Survey questions were refined through expert discussions, followed by quantitative and qualitative analyses to reveal key trends, challenges, and unmet needs. Our results show that existing ADS testing techniques struggle to comprehensively evaluate real-world performance, particularly regarding corner case diversity, the simulation to reality gap, the lack of systematic testing criteria, exposure to potential attacks, practical challenges in V2X deployment, and the high computational cost of foundation model-based testing. By further analyzing participant responses together with 105 representative studies, we summarize the current research landscape and highlight major limitations. This study consolidates critical research gaps in ADS testing and outlines key future research directions, including comprehensive testing criteria, cross-model collaboration in V2X systems, cross-modality adaptation for foundation model-based testing, and scalable validation frameworks for large-scale ADS evaluation.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5927\u89c4\u6a21\u8c03\u67e5\u548c\u6587\u732e\u5206\u6790\uff0c\u7cfb\u7edf\u7814\u7a76\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u6d4b\u8bd5\u7684\u73b0\u72b6\u3001\u6311\u6218\u548c\u7814\u7a76\u7a7a\u767d\uff0c\u91cd\u70b9\u5173\u6ce8\u6a21\u5757\u5316\u4e0e\u7aef\u5230\u7aef\u7cfb\u7edf\u6d4b\u8bd5\u5b9e\u8df5\uff0c\u5e76\u63a2\u8ba8\u4e86V2X\u901a\u4fe1\u548c\u57fa\u7840\u6a21\u578b\u7b49\u65b0\u5174\u56e0\u7d20\u5bf9\u6d4b\u8bd5\u7684\u5f71\u54cd\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u627f\u8bfa\u63d0\u9ad8\u4ea4\u901a\u6548\u7387\u548c\u5b89\u5168\u6027\uff0c\u4f46\u5728\u590d\u6742\u73b0\u5b9e\u73af\u5883\u4e2d\u786e\u4fdd\u5176\u53ef\u9760\u6027\u4ecd\u9762\u4e34\u5173\u952e\u6311\u6218\u3002\u6709\u6548\u7684\u6d4b\u8bd5\u5bf9\u4e8e\u9a8c\u8bc1ADS\u6027\u80fd\u3001\u964d\u4f4e\u90e8\u7f72\u98ce\u9669\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u65e8\u5728\u8c03\u67e5\u5f53\u524dADS\u6d4b\u8bd5\u5b9e\u8df5\uff0c\u8bc6\u522b\u884c\u4e1a\u4ece\u4e1a\u8005\u548c\u5b66\u672f\u7814\u7a76\u8005\u7684\u5173\u952e\u9700\u6c42\uff0c\u5e76\u5206\u6790\u73b0\u6709\u7814\u7a76\u4e0e\u5b9e\u9645\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "1. \u7efc\u8ff0\u4e3b\u8981\u6d4b\u8bd5\u6280\u672f\uff0c\u5305\u62ec\u6a21\u5757\u5316\u548c\u7aef\u5230\u7aef\u7cfb\u7edf\u6d4b\u8bd5\u65b9\u6cd5\uff1b2. \u8003\u8651V2X\u901a\u4fe1\u548c\u57fa\u7840\u6a21\u578b\uff08\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff09\u7b49\u65b0\u5174\u56e0\u7d20\uff1b3. \u5bf9100\u540d\u6765\u81ea\u5de5\u4e1a\u754c\u548c\u5b66\u672f\u754c\u7684\u53c2\u4e0e\u8005\u8fdb\u884c\u5927\u89c4\u6a21\u8c03\u67e5\uff1b4. \u901a\u8fc7\u4e13\u5bb6\u8ba8\u8bba\u7cbe\u70bc\u8c03\u67e5\u95ee\u9898\uff1b5. \u8fdb\u884c\u5b9a\u91cf\u548c\u5b9a\u6027\u5206\u6790\uff1b6. \u7ed3\u5408105\u9879\u4ee3\u8868\u6027\u7814\u7a76\u5206\u6790\u53c2\u4e0e\u8005\u53cd\u9988\u3002", "result": "\u8c03\u67e5\u7ed3\u679c\u663e\u793a\uff1a\u73b0\u6709ADS\u6d4b\u8bd5\u6280\u672f\u5728\u5168\u9762\u8bc4\u4f30\u73b0\u5b9e\u4e16\u754c\u6027\u80fd\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u89d2\u843d\u6848\u4f8b\u591a\u6837\u6027\u3001\u4eff\u771f\u4e0e\u73b0\u5b9e\u5dee\u8ddd\u3001\u7f3a\u4e4f\u7cfb\u7edf\u6d4b\u8bd5\u6807\u51c6\u3001\u6f5c\u5728\u653b\u51fb\u66b4\u9732\u3001V2X\u90e8\u7f72\u7684\u5b9e\u9645\u6311\u6218\u4ee5\u53ca\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u6d4b\u8bd5\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u7b49\u65b9\u9762\u3002\u901a\u8fc7\u5206\u6790\u53c2\u4e0e\u8005\u53cd\u9988\u548c\u4ee3\u8868\u6027\u7814\u7a76\uff0c\u603b\u7ed3\u4e86\u5f53\u524d\u7814\u7a76\u73b0\u72b6\u5e76\u7a81\u51fa\u4e86\u4e3b\u8981\u5c40\u9650\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u6574\u5408\u4e86ADS\u6d4b\u8bd5\u4e2d\u7684\u5173\u952e\u7814\u7a76\u7a7a\u767d\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff1a\u5305\u62ec\u5168\u9762\u7684\u6d4b\u8bd5\u6807\u51c6\u3001V2X\u7cfb\u7edf\u4e2d\u7684\u8de8\u6a21\u578b\u534f\u4f5c\u3001\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u6d4b\u8bd5\u7684\u8de8\u6a21\u6001\u9002\u5e94\uff0c\u4ee5\u53ca\u5927\u89c4\u6a21ADS\u8bc4\u4f30\u7684\u53ef\u6269\u5c55\u9a8c\u8bc1\u6846\u67b6\u3002"}}
{"id": "2512.12411", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12411", "abs": "https://arxiv.org/abs/2512.12411", "authors": ["Ely Hahami", "Lavik Jain", "Ishaan Sinha"], "title": "Feeling the Strength but Not the Source: Partial Introspection in LLMs", "comment": "7 pages (+ 5 pages for appendix), 5 figures, 1 table", "summary": "Recent work from Anthropic claims that frontier models can sometimes detect and name injected \"concepts\" represented as activation directions. We test the robustness of these claims. First, we reproduce Anthropic's multi-turn \"emergent introspection\" result on Meta-Llama-3.1-8B-Instruct, finding that the model identifies and names the injected concept 20 percent of the time under Anthropic's original pipeline, exactly matching their reported numbers and thus showing that introspection is not exclusive to very large or capable models. Second, we systematically vary the inference prompt and find that introspection is fragile: performance collapses on closely related tasks such as multiple-choice identification of the injected concept or different prompts of binary discrimination of whether a concept was injected at all. Third, we identify a contrasting regime of partial introspection: the same model can reliably classify the strength of the coefficient of a normalized injected concept vector (as weak / moderate / strong / very strong) with up to 70 percent accuracy, far above the 25 percent chance baseline. Together, these results provide more evidence for Anthropic's claim that language models effectively compute a function of their baseline, internal representations during introspection; however, these self-reports about those representations are narrow and prompt-sensitive. Our code is available at https://github.com/elyhahami18/CS2881-Introspection.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9a8c\u8bc1\u4e86Anthropic\u5173\u4e8e\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u68c0\u6d4b\u548c\u547d\u540d\u6ce8\u5165\"\u6982\u5ff5\"\u7684\u53d1\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u786e\u5b9e\u5177\u5907\u4e00\u5b9a\u81ea\u7701\u80fd\u529b\uff0c\u4f46\u8fd9\u79cd\u80fd\u529b\u5177\u6709\u8106\u5f31\u6027\u548c\u63d0\u793a\u654f\u611f\u6027\u3002", "motivation": "\u9a8c\u8bc1Anthropic\u5173\u4e8e\u524d\u6cbf\u6a21\u578b\u80fd\u591f\u68c0\u6d4b\u548c\u547d\u540d\u6ce8\u5165\"\u6982\u5ff5\"\u7684\u7a33\u5065\u6027\uff0c\u63a2\u7a76\u8fd9\u79cd\u81ea\u7701\u80fd\u529b\u7684\u9002\u7528\u8303\u56f4\u548c\u5c40\u9650\u6027\u3002", "method": "1) \u5728Meta-Llama-3.1-8B-Instruct\u4e0a\u590d\u73b0Anthropic\u7684\u591a\u8f6e\"\u6d8c\u73b0\u81ea\u7701\"\u7ed3\u679c\uff1b2) \u7cfb\u7edf\u6027\u5730\u6539\u53d8\u63a8\u7406\u63d0\u793a\uff0c\u6d4b\u8bd5\u81ea\u7701\u80fd\u529b\u7684\u7a33\u5065\u6027\uff1b3) \u63a2\u7d22\u90e8\u5206\u81ea\u7701\u673a\u5236\uff0c\u6d4b\u8bd5\u6a21\u578b\u5bf9\u6ce8\u5165\u6982\u5ff5\u5411\u91cf\u5f3a\u5ea6\u7684\u5206\u7c7b\u80fd\u529b\u3002", "result": "1) \u6210\u529f\u590d\u73b0Anthropic\u7ed3\u679c\uff0c\u6a21\u578b\u8bc6\u522b\u548c\u547d\u540d\u6ce8\u5165\u6982\u5ff5\u7684\u6210\u529f\u7387\u4e3a20%\uff1b2) \u81ea\u7701\u80fd\u529b\u5177\u6709\u8106\u5f31\u6027\uff0c\u5728\u76f8\u5173\u4efb\u52a1\uff08\u5982\u591a\u9879\u9009\u62e9\u8bc6\u522b\u3001\u4e8c\u5143\u5224\u522b\uff09\u4e0a\u6027\u80fd\u5d29\u6e83\uff1b3) \u53d1\u73b0\u90e8\u5206\u81ea\u7701\u673a\u5236\uff0c\u6a21\u578b\u80fd\u4ee570%\u7684\u51c6\u786e\u7387\u5206\u7c7b\u6ce8\u5165\u6982\u5ff5\u5411\u91cf\u7684\u5f3a\u5ea6\uff08\u8fdc\u9ad8\u4e8e25%\u7684\u968f\u673a\u57fa\u7ebf\uff09\u3002", "conclusion": "\u8bed\u8a00\u6a21\u578b\u786e\u5b9e\u80fd\u591f\u57fa\u4e8e\u5176\u5185\u90e8\u8868\u793a\u8fdb\u884c\u81ea\u7701\u8ba1\u7b97\uff0c\u4f46\u8fd9\u79cd\u81ea\u6211\u62a5\u544a\u80fd\u529b\u5177\u6709\u72ed\u7a84\u6027\u548c\u63d0\u793a\u654f\u611f\u6027\uff0c\u8868\u660e\u6a21\u578b\u7684\u81ea\u7701\u80fd\u529b\u662f\u6709\u9650\u4e14\u4e0d\u7a33\u5b9a\u7684\u3002"}}
{"id": "2512.11890", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.11890", "abs": "https://arxiv.org/abs/2512.11890", "authors": ["Tariq Eldakruri", "Edip Senyurek"], "title": "Automation as a Catalyst for Geothermal Energy Adoption in Qatar: A Techno-Economic and Environmental Assessment", "comment": "13 pages, 1 table. Published version deposited with publisher permission", "summary": "Geothermal energy provides continuous low emission potential but is underused in Qatar because of high capital costs, drilling risks, and uncertainty in subsurface conditions. This study examines how automation can improve the techno economic and environmental feasibility of geothermal deployment through three pathways: Enhanced Geothermal Systems in the Dukhan Basin, repurposed oil and gas wells, and ground source heat pumps for district cooling. Using geological datasets and financial modeling, the analysis shows that full automation reduces capital expenditure by 12 to 14 percent and operating expenditure by 14 to 17 percent. The Levelized Cost of Energy decreases from 145 USD per MWh to 125 USD per MWh, and payback periods shorten by up to two years. Environmental results indicate that geothermal substitution can avoid between 4000 and 17600 tons of CO2 per year for each project. Automation also reduces uncertainty in investment outcomes based on Monte Carlo simulations. Overall, the results show that automation strengthens the economic viability of geothermal systems and supports their integration into Qatars long term energy diversification and decarbonization strategies.", "AI": {"tldr": "\u81ea\u52a8\u5316\u6280\u672f\u53ef\u663e\u8457\u6539\u5584\u5361\u5854\u5c14\u5730\u70ed\u80fd\u7684\u53ef\u884c\u6027\uff0c\u964d\u4f4e12-14%\u8d44\u672c\u652f\u51fa\u548c14-17%\u8fd0\u8425\u652f\u51fa\uff0c\u4f7f\u5e73\u51c6\u5316\u80fd\u6e90\u6210\u672c\u4ece145\u7f8e\u5143/MWh\u964d\u81f3125\u7f8e\u5143/MWh\uff0c\u6295\u8d44\u56de\u6536\u671f\u7f29\u77ed\u6700\u591a2\u5e74\uff0c\u6bcf\u5e74\u53ef\u51cf\u5c114000-17600\u5428CO2\u6392\u653e\u3002", "motivation": "\u5361\u5854\u5c14\u5730\u70ed\u80fd\u56e0\u9ad8\u8d44\u672c\u6210\u672c\u3001\u94bb\u4e95\u98ce\u9669\u548c\u5730\u4e0b\u6761\u4ef6\u4e0d\u786e\u5b9a\u6027\u800c\u672a\u88ab\u5145\u5206\u5229\u7528\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u81ea\u52a8\u5316\u5982\u4f55\u6539\u5584\u5730\u70ed\u90e8\u7f72\u7684\u6280\u672f\u7ecf\u6d4e\u4e0e\u73af\u5883\u53ef\u884c\u6027\u3002", "method": "\u7814\u7a76\u901a\u8fc7\u4e09\u79cd\u9014\u5f84\u5206\u6790\uff1aDukhan\u76c6\u5730\u7684\u589e\u5f3a\u578b\u5730\u70ed\u7cfb\u7edf\u3001\u6539\u9020\u6cb9\u6c14\u4e95\u3001\u4ee5\u53ca\u7528\u4e8e\u533a\u57df\u4f9b\u51b7\u7684\u5730\u6e90\u70ed\u6cf5\uff0c\u7ed3\u5408\u5730\u8d28\u6570\u636e\u96c6\u548c\u8d22\u52a1\u5efa\u6a21\uff0c\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u6a21\u62df\u8bc4\u4f30\u81ea\u52a8\u5316\u6548\u679c\u3002", "result": "\u5168\u81ea\u52a8\u5316\u4f7f\u8d44\u672c\u652f\u51fa\u964d\u4f4e12-14%\uff0c\u8fd0\u8425\u652f\u51fa\u964d\u4f4e14-17%\uff0c\u5e73\u51c6\u5316\u80fd\u6e90\u6210\u672c\u4ece145\u7f8e\u5143/MWh\u964d\u81f3125\u7f8e\u5143/MWh\uff0c\u6295\u8d44\u56de\u6536\u671f\u7f29\u77ed\u6700\u591a2\u5e74\uff0c\u6bcf\u5e74\u53ef\u51cf\u5c114000-17600\u5428CO2\u6392\u653e\uff0c\u5e76\u964d\u4f4e\u6295\u8d44\u7ed3\u679c\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "\u81ea\u52a8\u5316\u589e\u5f3a\u4e86\u5730\u70ed\u7cfb\u7edf\u7684\u7ecf\u6d4e\u53ef\u884c\u6027\uff0c\u652f\u6301\u5176\u878d\u5165\u5361\u5854\u5c14\u957f\u671f\u80fd\u6e90\u591a\u5143\u5316\u548c\u8131\u78b3\u6218\u7565\uff0c\u4e3a\u5730\u70ed\u80fd\u5728\u5361\u5854\u5c14\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u6280\u672f\u7ecf\u6d4e\u652f\u6491\u3002"}}
{"id": "2512.12413", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.12413", "abs": "https://arxiv.org/abs/2512.12413", "authors": ["Gabriel R. Lau", "Wei Yan Low", "Louis Tay", "Ysabel Guevarra", "Dragan Ga\u0161evi\u0107", "Andree Hartanto"], "title": "Understanding Critical Thinking in Generative Artificial Intelligence Use: Development, Validation, and Correlates of the Critical Thinking in AI Use Scale", "comment": null, "summary": "Generative AI tools are increasingly embedded in everyday work and learning, yet their fluency, opacity, and propensity to hallucinate mean that users must critically evaluate AI outputs rather than accept them at face value. The present research conceptualises critical thinking in AI use as a dispositional tendency to verify the source and content of AI-generated information, to understand how models work and where they fail, and to reflect on the broader implications of relying on AI. Across six studies (N = 1365), we developed and validated the 13-item critical thinking in AI use scale and mapped its nomological network. Study 1 generated and content-validated scale items. Study 2 supported a three-factor structure (Verification, Motivation, and Reflection). Studies 3, 4, and 5 confirmed this higher-order model, demonstrated internal consistency and test-retest reliability, strong factor loadings, sex invariance, and convergent and discriminant validity. Studies 3 and 4 further revealed that critical thinking in AI use was positively associated with openness, extraversion, positive trait affect, and frequency of AI use. Lastly, Study 6 demonstrated criterion validity of the scale, with higher critical thinking in AI use scores predicting more frequent and diverse verification strategies, greater veracity-judgement accuracy in a novel and naturalistic ChatGPT-powered fact-checking task, and deeper reflection about responsible AI. Taken together, the current work clarifies why and how people exercise oversight over generative AI outputs and provides a validated scale and ecologically grounded task paradigm to support theory testing, cross-group, and longitudinal research on critical engagement with generative AI outputs.", "AI": {"tldr": "\u5f00\u53d1\u5e76\u9a8c\u8bc1\u4e86\u4e00\u4e2a13\u9879\u91cf\u8868\u6765\u6d4b\u91cfAI\u4f7f\u7528\u4e2d\u7684\u6279\u5224\u6027\u601d\u7ef4\uff0c\u5305\u542b\u9a8c\u8bc1\u3001\u52a8\u673a\u548c\u53cd\u601d\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u8be5\u91cf\u8868\u80fd\u9884\u6d4b\u66f4\u9891\u7e41\u591a\u6837\u7684\u9a8c\u8bc1\u7b56\u7565\u3001\u66f4\u9ad8\u7684\u771f\u5b9e\u6027\u5224\u65ad\u51c6\u786e\u5ea6\u4ee5\u53ca\u66f4\u6df1\u5165\u7684AI\u8d23\u4efb\u53cd\u601d\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u5de5\u5177\u5728\u65e5\u5e38\u5de5\u4f5c\u548c\u5b66\u4e60\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u5176\u6d41\u7545\u6027\u3001\u4e0d\u900f\u660e\u6027\u548c\u5e7b\u89c9\u503e\u5411\u8981\u6c42\u7528\u6237\u5fc5\u987b\u6279\u5224\u6027\u5730\u8bc4\u4f30AI\u8f93\u51fa\uff0c\u800c\u4e0d\u662f\u76f2\u76ee\u63a5\u53d7\u3002\u672c\u7814\u7a76\u65e8\u5728\u6982\u5ff5\u5316AI\u4f7f\u7528\u4e2d\u7684\u6279\u5224\u6027\u601d\u7ef4\uff0c\u5e76\u5f00\u53d1\u6709\u6548\u7684\u6d4b\u91cf\u5de5\u5177\u3002", "method": "\u901a\u8fc7\u516d\u9879\u7814\u7a76\uff08N=1365\uff09\u5f00\u53d1\u548c\u9a8c\u8bc1\u4e8613\u9879AI\u4f7f\u7528\u6279\u5224\u6027\u601d\u7ef4\u91cf\u8868\u3002\u7814\u7a761\u751f\u6210\u5e76\u5185\u5bb9\u9a8c\u8bc1\u91cf\u8868\u9879\u76ee\uff1b\u7814\u7a762\u652f\u6301\u4e09\u56e0\u7d20\u7ed3\u6784\uff08\u9a8c\u8bc1\u3001\u52a8\u673a\u3001\u53cd\u601d\uff09\uff1b\u7814\u7a763-5\u786e\u8ba4\u9ad8\u9636\u6a21\u578b\uff0c\u68c0\u9a8c\u4fe1\u6548\u5ea6\uff1b\u7814\u7a766\u9a8c\u8bc1\u91cf\u8868\u6807\u51c6\u6548\u5ea6\uff0c\u5305\u62ec\u9a8c\u8bc1\u7b56\u7565\u3001\u771f\u5b9e\u6027\u5224\u65ad\u51c6\u786e\u5ea6\u548cAI\u8d23\u4efb\u53cd\u601d\u3002", "result": "\u91cf\u8868\u8868\u73b0\u51fa\u826f\u597d\u7684\u5fc3\u7406\u6d4b\u91cf\u7279\u6027\uff1a\u4e09\u56e0\u7d20\u7ed3\u6784\u3001\u5185\u90e8\u4e00\u81f4\u6027\u3001\u91cd\u6d4b\u4fe1\u5ea6\u3001\u5f3a\u56e0\u5b50\u8f7d\u8377\u3001\u6027\u522b\u4e0d\u53d8\u6027\u3001\u805a\u5408\u548c\u533a\u5206\u6548\u5ea6\u3002AI\u4f7f\u7528\u6279\u5224\u6027\u601d\u7ef4\u4e0e\u5f00\u653e\u6027\u3001\u5916\u5411\u6027\u3001\u79ef\u6781\u7279\u8d28\u60c5\u611f\u548cAI\u4f7f\u7528\u9891\u7387\u6b63\u76f8\u5173\u3002\u91cf\u8868\u80fd\u9884\u6d4b\u66f4\u9891\u7e41\u591a\u6837\u7684\u9a8c\u8bc1\u7b56\u7565\u3001\u66f4\u9ad8\u7684\u771f\u5b9e\u6027\u5224\u65ad\u51c6\u786e\u5ea6\u4ee5\u53ca\u66f4\u6df1\u5165\u7684AI\u8d23\u4efb\u53cd\u601d\u3002", "conclusion": "\u672c\u7814\u7a76\u9610\u660e\u4e86\u4eba\u4eec\u5982\u4f55\u5bf9\u751f\u6210\u5f0fAI\u8f93\u51fa\u8fdb\u884c\u76d1\u7763\uff0c\u63d0\u4f9b\u4e86\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u91cf\u8868\u548c\u751f\u6001\u6548\u5ea6\u9ad8\u7684\u4efb\u52a1\u8303\u5f0f\uff0c\u652f\u6301\u5bf9\u751f\u6210\u5f0fAI\u8f93\u51fa\u6279\u5224\u6027\u53c2\u4e0e\u7684\u7406\u8bba\u68c0\u9a8c\u3001\u8de8\u7fa4\u4f53\u548c\u7eb5\u5411\u7814\u7a76\u3002"}}
{"id": "2512.11859", "categories": ["cs.LG", "cond-mat.stat-mech", "cs.AI", "eess.SY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.11859", "abs": "https://arxiv.org/abs/2512.11859", "authors": ["Michael Chertkov"], "title": "Generative Stochastic Optimal Transport: Guided Harmonic Path-Integral Diffusion", "comment": "40 pages, 8 figures", "summary": "We introduce Guided Harmonic Path-Integral Diffusion (GH-PID), a linearly-solvable framework for guided Stochastic Optimal Transport (SOT) with a hard terminal distribution and soft, application-driven path costs. A low-dimensional guidance protocol shapes the trajectory ensemble while preserving analytic structure: the forward and backward Kolmogorov equations remain linear, the optimal score admits an explicit Green-function ratio, and Gaussian-Mixture Model (GMM) terminal laws yield closed-form expressions. This enables stable sampling and differentiable protocol learning under exact terminal matching.\n  We develop guidance-centric diagnostics -- path cost, centerline adherence, variance flow, and drift effort -- that make GH-PID an interpretable variational ansatz for empirical SOT. Three navigation scenarios illustrated in 2D: (i) Case A: hand-crafted protocols revealing how geometry and stiffness shape lag, curvature effects, and mode evolution; (ii) Case B: single-task protocol learning, where a PWC centerline is optimized to minimize integrated cost; (iii) Case C: multi-expert fusion, in which a commander reconciles competing expert/teacher trajectories and terminal beliefs through an exact product-of-experts law and learns a consensus protocol. Across all settings, GH-PID generates geometry-aware, trust-aware trajectories that satisfy the prescribed terminal distribution while systematically reducing integrated cost.", "AI": {"tldr": "GH-PID\u662f\u4e00\u79cd\u7ebf\u6027\u53ef\u89e3\u7684\u5f15\u5bfc\u968f\u673a\u6700\u4f18\u4f20\u8f93\u6846\u67b6\uff0c\u5177\u6709\u786c\u7ec8\u7aef\u5206\u5e03\u548c\u8f6f\u8def\u5f84\u6210\u672c\uff0c\u901a\u8fc7\u4f4e\u7ef4\u5f15\u5bfc\u534f\u8bae\u4fdd\u6301\u89e3\u6790\u7ed3\u6784\uff0c\u5b9e\u73b0\u7a33\u5b9a\u91c7\u6837\u548c\u53ef\u5fae\u534f\u8bae\u5b66\u4e60\u3002", "motivation": "\u5f00\u53d1\u4e00\u4e2a\u65e2\u80fd\u7cbe\u786e\u5339\u914d\u7ec8\u7aef\u5206\u5e03\uff0c\u53c8\u80fd\u901a\u8fc7\u5e94\u7528\u9a71\u52a8\u7684\u8def\u5f84\u6210\u672c\u5f15\u5bfc\u8f68\u8ff9\uff0c\u540c\u65f6\u4fdd\u6301\u89e3\u6790\u53ef\u89e3\u6027\u7684\u968f\u673a\u6700\u4f18\u4f20\u8f93\u6846\u67b6\uff0c\u4ee5\u751f\u6210\u51e0\u4f55\u611f\u77e5\u3001\u4fe1\u4efb\u611f\u77e5\u7684\u8f68\u8ff9\u3002", "method": "\u63d0\u51fa\u5f15\u5bfc\u8c10\u6ce2\u8def\u5f84\u79ef\u5206\u6269\u6563(GH-PID)\u6846\u67b6\uff0c\u4f7f\u7528\u4f4e\u7ef4\u5f15\u5bfc\u534f\u8bae\u5851\u9020\u8f68\u8ff9\u96c6\u5408\uff0c\u4fdd\u6301\u524d\u5411\u548c\u540e\u5411Kolmogorov\u65b9\u7a0b\u7684\u7ebf\u6027\u7279\u6027\uff0c\u901a\u8fc7\u683c\u6797\u51fd\u6570\u6bd4\u83b7\u5f97\u6700\u4f18\u5f97\u5206\uff0c\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u7ec8\u7aef\u5206\u5e03\u4ea7\u751f\u95ed\u5f0f\u8868\u8fbe\u5f0f\u3002", "result": "\u5728\u4e09\u4e2a2D\u5bfc\u822a\u573a\u666f\u4e2d\u9a8c\u8bc1\uff1a\u624b\u5de5\u534f\u8bae\u5c55\u793a\u51e0\u4f55\u548c\u521a\u5ea6\u5982\u4f55\u5f71\u54cd\u6ede\u540e\u3001\u66f2\u7387\u6548\u5e94\u548c\u6a21\u6001\u6f14\u5316\uff1b\u5355\u4efb\u52a1\u534f\u8bae\u5b66\u4e60\u4f18\u5316\u4e2d\u5fc3\u7ebf\u4ee5\u6700\u5c0f\u5316\u79ef\u5206\u6210\u672c\uff1b\u591a\u4e13\u5bb6\u878d\u5408\u901a\u8fc7\u7cbe\u786e\u4e13\u5bb6\u4e58\u79ef\u6cd5\u5219\u5b66\u4e60\u5171\u8bc6\u534f\u8bae\u3002", "conclusion": "GH-PID\u80fd\u591f\u751f\u6210\u6ee1\u8db3\u89c4\u5b9a\u7ec8\u7aef\u5206\u5e03\u540c\u65f6\u7cfb\u7edf\u964d\u4f4e\u79ef\u5206\u6210\u672c\u7684\u51e0\u4f55\u611f\u77e5\u3001\u4fe1\u4efb\u611f\u77e5\u8f68\u8ff9\uff0c\u4e3a\u7ecf\u9a8c\u968f\u673a\u6700\u4f18\u4f20\u8f93\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u53d8\u5206\u8fd1\u4f3c\u3002"}}
{"id": "2512.12053", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12053", "abs": "https://arxiv.org/abs/2512.12053", "authors": ["Tran-Vu La", "Minh-Tan Pham", "Yu Li", "Patrick Matgen", "Marco Chini"], "title": "Adaptive federated learning for ship detection across diverse satellite imagery sources", "comment": "5 pages, IGARSS 2025", "summary": "We investigate the application of Federated Learning (FL) for ship detection across diverse satellite datasets, offering a privacy-preserving solution that eliminates the need for data sharing or centralized collection. This approach is particularly advantageous for handling commercial satellite imagery or sensitive ship annotations. Four FL models including FedAvg, FedProx, FedOpt, and FedMedian, are evaluated and compared to a local training baseline, where the YOLOv8 ship detection model is independently trained on each dataset without sharing learned parameters. The results reveal that FL models substantially improve detection accuracy over training on smaller local datasets and achieve performance levels close to global training that uses all datasets during the training. Furthermore, the study underscores the importance of selecting appropriate FL configurations, such as the number of communication rounds and local training epochs, to optimize detection precision while maintaining computational efficiency.", "AI": {"tldr": "\u8054\u90a6\u5b66\u4e60\u5728\u8239\u8236\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\u7814\u7a76\uff0c\u6bd4\u8f83\u4e86\u56db\u79cdFL\u6a21\u578b\u4e0e\u672c\u5730\u8bad\u7ec3\u57fa\u7ebf\u7684\u6027\u80fd\uff0c\u53d1\u73b0FL\u80fd\u663e\u8457\u63d0\u5347\u5c0f\u6570\u636e\u96c6\u4e0a\u7684\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u63a5\u8fd1\u4f7f\u7528\u5168\u90e8\u6570\u636e\u7684\u5168\u5c40\u8bad\u7ec3\u6548\u679c\u3002", "motivation": "\u7814\u7a76\u8054\u90a6\u5b66\u4e60\u5728\u8239\u8236\u68c0\u6d4b\u4e2d\u7684\u5e94\u7528\uff0c\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u9690\u79c1\u4fdd\u62a4\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u907f\u514d\u6570\u636e\u5171\u4eab\u6216\u96c6\u4e2d\u6536\u96c6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5904\u7406\u5546\u4e1a\u536b\u661f\u56fe\u50cf\u6216\u654f\u611f\u8239\u8236\u6807\u6ce8\u6570\u636e\u3002", "method": "\u4f7f\u7528YOLOv8\u8239\u8236\u68c0\u6d4b\u6a21\u578b\uff0c\u8bc4\u4f30\u4e86\u56db\u79cd\u8054\u90a6\u5b66\u4e60\u6a21\u578b\uff08FedAvg\u3001FedProx\u3001FedOpt\u3001FedMedian\uff09\uff0c\u5e76\u4e0e\u672c\u5730\u8bad\u7ec3\u57fa\u7ebf\u8fdb\u884c\u6bd4\u8f83\u3002\u672c\u5730\u8bad\u7ec3\u57fa\u7ebf\u662f\u5728\u6bcf\u4e2a\u6570\u636e\u96c6\u4e0a\u72ec\u7acb\u8bad\u7ec3\u800c\u4e0d\u5171\u4eab\u5b66\u4e60\u53c2\u6570\u3002", "result": "\u8054\u90a6\u5b66\u4e60\u6a21\u578b\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u8f83\u5c0f\u672c\u5730\u6570\u636e\u96c6\u4e0a\u7684\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u6027\u80fd\u63a5\u8fd1\u4f7f\u7528\u6240\u6709\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\u7684\u5168\u5c40\u8bad\u7ec3\u6548\u679c\u3002\u7814\u7a76\u8fd8\u5f3a\u8c03\u4e86\u9009\u62e9\u5408\u9002\u7684FL\u914d\u7f6e\uff08\u5982\u901a\u4fe1\u8f6e\u6570\u548c\u672c\u5730\u8bad\u7ec3\u8f6e\u6570\uff09\u5bf9\u4f18\u5316\u68c0\u6d4b\u7cbe\u5ea6\u548c\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8054\u90a6\u5b66\u4e60\u4e3a\u536b\u661f\u56fe\u50cf\u8239\u8236\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u9690\u79c1\u4fdd\u62a4\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u4fdd\u62a4\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u63a5\u8fd1\u5168\u5c40\u8bad\u7ec3\u7684\u6027\u80fd\uff0c\u4f46\u9700\u8981\u4ed4\u7ec6\u914d\u7f6eFL\u53c2\u6570\u4ee5\u83b7\u5f97\u6700\u4f73\u6548\u679c\u3002"}}
{"id": "2512.12443", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.12443", "abs": "https://arxiv.org/abs/2512.12443", "authors": ["Akhmadillo Mamirov", "Faiaz Azmain", "Hanyu Wang"], "title": "AI Transparency Atlas: Framework, Scoring, and Real-Time Model Card Evaluation Pipeline", "comment": null, "summary": "AI model documentation is fragmented across platforms and inconsistent in structure, preventing policymakers, auditors, and users from reliably assessing safety claims, data provenance, and version-level changes. We analyzed documentation from five frontier models (Gemini 3, Grok 4.1, Llama 4, GPT-5, and Claude 4.5) and 100 Hugging Face model cards, identifying 947 unique section names with extreme naming variation. Usage information alone appeared under 97 distinct labels. Using the EU AI Act Annex IV and the Stanford Transparency Index as baselines, we developed a weighted transparency framework with 8 sections and 23 subsections that prioritizes safety-critical disclosures (Safety Evaluation: 25%, Critical Risk: 20%) over technical specifications. We implemented an automated multi-agent pipeline that extracts documentation from public sources and scores completeness through LLM-based consensus. Evaluating 50 models across vision, multimodal, open-source, and closed-source systems cost less than $3 in total and revealed systematic gaps. Frontier labs (xAI, Microsoft, Anthropic) achieve approximately 80% compliance, while most providers fall below 60%. Safety-critical categories show the largest deficits: deception behaviors, hallucinations, and child safety evaluations account for 148, 124, and 116 aggregate points lost, respectively, across all evaluated models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86AI\u6a21\u578b\u6587\u6863\u7684\u788e\u7247\u5316\u548c\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542b8\u4e2a\u90e8\u520623\u4e2a\u5b50\u90e8\u5206\u7684\u52a0\u6743\u900f\u660e\u5ea6\u6846\u67b6\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u591a\u667a\u80fd\u4f53\u7ba1\u9053\u6765\u8bc4\u4f30\u6a21\u578b\u6587\u6863\u7684\u5b8c\u6574\u6027\uff0c\u53d1\u73b0\u524d\u6cbf\u5b9e\u9a8c\u5ba4\u7684\u5408\u89c4\u6027\u7ea6\u4e3a80%\uff0c\u800c\u5927\u591a\u6570\u63d0\u4f9b\u8005\u4f4e\u4e8e60%\uff0c\u5b89\u5168\u5173\u952e\u7c7b\u522b\u5b58\u5728\u6700\u5927\u7f3a\u9677\u3002", "motivation": "AI\u6a21\u578b\u6587\u6863\u5728\u4e0d\u540c\u5e73\u53f0\u95f4\u788e\u7247\u5316\u4e14\u7ed3\u6784\u4e0d\u4e00\u81f4\uff0c\u963b\u788d\u4e86\u653f\u7b56\u5236\u5b9a\u8005\u3001\u5ba1\u8ba1\u5e08\u548c\u7528\u6237\u53ef\u9760\u8bc4\u4f30\u5b89\u5168\u58f0\u660e\u3001\u6570\u636e\u6765\u6e90\u548c\u7248\u672c\u7ea7\u522b\u53d8\u5316\uff0c\u9700\u8981\u7cfb\u7edf\u5316\u7684\u900f\u660e\u5ea6\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u5206\u6790\u4e865\u4e2a\u524d\u6cbf\u6a21\u578b\u548c100\u4e2aHugging Face\u6a21\u578b\u5361\uff0c\u8bc6\u522b\u51fa947\u4e2a\u72ec\u7279\u7684\u7ae0\u8282\u540d\u79f0\uff1b\u4ee5\u6b27\u76dfAI\u6cd5\u6848\u9644\u4ef6IV\u548c\u65af\u5766\u798f\u900f\u660e\u5ea6\u6307\u6570\u4e3a\u57fa\u51c6\uff0c\u5f00\u53d1\u4e86\u52a0\u6743\u900f\u660e\u5ea6\u6846\u67b6\uff1b\u5b9e\u73b0\u4e86\u81ea\u52a8\u5316\u591a\u667a\u80fd\u4f53\u7ba1\u9053\uff0c\u4ece\u516c\u5171\u6765\u6e90\u63d0\u53d6\u6587\u6863\u5e76\u901a\u8fc7LLM\u5171\u8bc6\u8bc4\u5206\u5b8c\u6574\u6027\u3002", "result": "\u8bc4\u4f30\u4e8650\u4e2a\u8de8\u89c6\u89c9\u3001\u591a\u6a21\u6001\u3001\u5f00\u6e90\u548c\u95ed\u6e90\u7cfb\u7edf\u7684\u6a21\u578b\uff0c\u603b\u6210\u672c\u4f4e\u4e8e3\u7f8e\u5143\uff1b\u524d\u6cbf\u5b9e\u9a8c\u5ba4\uff08xAI\u3001\u5fae\u8f6f\u3001Anthropic\uff09\u8fbe\u5230\u7ea680%\u5408\u89c4\u6027\uff0c\u5927\u591a\u6570\u63d0\u4f9b\u8005\u4f4e\u4e8e60%\uff1b\u5b89\u5168\u5173\u952e\u7c7b\u522b\u663e\u793a\u6700\u5927\u7f3a\u9677\uff1a\u6b3a\u9a97\u884c\u4e3a\u3001\u5e7b\u89c9\u548c\u513f\u7ae5\u5b89\u5168\u8bc4\u4f30\u5206\u522b\u635f\u5931148\u3001124\u548c116\u4e2a\u603b\u79ef\u5206\u3002", "conclusion": "AI\u6a21\u578b\u6587\u6863\u5b58\u5728\u4e25\u91cd\u7684\u788e\u7247\u5316\u548c\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5b89\u5168\u5173\u952e\u4fe1\u606f\u62ab\u9732\u4e0d\u8db3\uff1b\u5f00\u53d1\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\u80fd\u591f\u9ad8\u6548\u8bc6\u522b\u900f\u660e\u5ea6\u5dee\u8ddd\uff0c\u4e3a\u653f\u7b56\u5236\u5b9a\u548c\u884c\u4e1a\u6807\u51c6\u63d0\u4f9b\u6570\u636e\u652f\u6301\u3002"}}
{"id": "2512.12056", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12056", "abs": "https://arxiv.org/abs/2512.12056", "authors": ["Maria Rodriguez", "Minh-Tan Pham", "Martin Sudmanns", "Quentin Poterek", "Oscar Narvaez"], "title": "Enhancing deep learning performance on burned area delineation from SPOT-6/7 imagery for emergency management", "comment": "5 pages, IGARSS 2025", "summary": "After a wildfire, delineating burned areas (BAs) is crucial for quantifying damages and supporting ecosystem recovery. Current BA mapping approaches rely on computer vision models trained on post-event remote sensing imagery, but often overlook their applicability to time-constrained emergency management scenarios. This study introduces a supervised semantic segmentation workflow aimed at boosting both the performance and efficiency of BA delineation. It targets SPOT-6/7 imagery due to its very high resolution and on-demand availability. Experiments are evaluated based on Dice score, Intersection over Union, and inference time. The results show that U-Net and SegFormer models perform similarly with limited training data. However, SegFormer requires more resources, challenging its practical use in emergencies. Incorporating land cover data as an auxiliary task enhances model robustness without increasing inference time. Lastly, Test-Time Augmentation improves BA delineation performance but raises inference time, which can be mitigated with optimization methods like Mixed Precision.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u76d1\u7763\u5f0f\u8bed\u4e49\u5206\u5272\u5de5\u4f5c\u6d41\uff0c\u65e8\u5728\u63d0\u9ad8\u91ce\u706b\u540e\u70e7\u6bc1\u533a\u57df\uff08BA\uff09\u5212\u5206\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u9488\u5bf9SPOT-6/7\u9ad8\u5206\u8fa8\u7387\u5f71\u50cf\uff0c\u8bc4\u4f30\u4e86U-Net\u548cSegFormer\u6a21\u578b\u5728\u7d27\u6025\u7ba1\u7406\u573a\u666f\u4e0b\u7684\u9002\u7528\u6027\u3002", "motivation": "\u5f53\u524d\u70e7\u6bc1\u533a\u57df\u5212\u5206\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u8ba1\u7b97\u673a\u89c6\u89c9\u6a21\u578b\u5904\u7406\u707e\u540e\u9065\u611f\u5f71\u50cf\uff0c\u4f46\u5f80\u5f80\u5ffd\u89c6\u4e86\u5176\u5728\u65f6\u95f4\u7d27\u8feb\u7684\u5e94\u6025\u7ba1\u7406\u573a\u666f\u4e2d\u7684\u5b9e\u9645\u9002\u7528\u6027\u3002\u9700\u8981\u5f00\u53d1\u65e2\u9ad8\u6548\u53c8\u51c6\u786e\u7684BA\u5212\u5206\u65b9\u6cd5\u4ee5\u652f\u6301\u707e\u5bb3\u8bc4\u4f30\u548c\u751f\u6001\u7cfb\u7edf\u6062\u590d\u3002", "method": "\u63d0\u51fa\u76d1\u7763\u5f0f\u8bed\u4e49\u5206\u5272\u5de5\u4f5c\u6d41\uff0c\u4f7f\u7528SPOT-6/7\u9ad8\u5206\u8fa8\u7387\u5f71\u50cf\u3002\u5bf9\u6bd4\u8bc4\u4f30U-Net\u548cSegFormer\u6a21\u578b\u6027\u80fd\uff0c\u5f15\u5165\u571f\u5730\u8986\u76d6\u6570\u636e\u4f5c\u4e3a\u8f85\u52a9\u4efb\u52a1\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\uff0c\u5e76\u91c7\u7528\u6d4b\u8bd5\u65f6\u589e\u5f3a\uff08TTA\uff09\u6280\u672f\u3002\u4f7f\u7528Dice\u5206\u6570\u3001IoU\u548c\u63a8\u7406\u65f6\u95f4\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\u3002", "result": "U-Net\u548cSegFormer\u5728\u6709\u9650\u8bad\u7ec3\u6570\u636e\u4e0b\u8868\u73b0\u76f8\u4f3c\uff0c\u4f46SegFormer\u9700\u8981\u66f4\u591a\u8ba1\u7b97\u8d44\u6e90\uff0c\u5728\u7d27\u6025\u60c5\u51b5\u4e0b\u5b9e\u7528\u6027\u53d7\u9650\u3002\u52a0\u5165\u571f\u5730\u8986\u76d6\u8f85\u52a9\u4efb\u52a1\u80fd\u63d0\u9ad8\u6a21\u578b\u9c81\u68d2\u6027\u4e14\u4e0d\u589e\u52a0\u63a8\u7406\u65f6\u95f4\u3002\u6d4b\u8bd5\u65f6\u589e\u5f3a\u80fd\u63d0\u5347\u5212\u5206\u6027\u80fd\u4f46\u4f1a\u589e\u52a0\u63a8\u7406\u65f6\u95f4\uff0c\u53ef\u901a\u8fc7\u6df7\u5408\u7cbe\u5ea6\u7b49\u4f18\u5316\u65b9\u6cd5\u7f13\u89e3\u3002", "conclusion": "\u5728\u7d27\u6025\u7ba1\u7406\u573a\u666f\u4e2d\uff0cU-Net\u6a21\u578b\u56e0\u5176\u8d44\u6e90\u6548\u7387\u66f4\u5177\u5b9e\u7528\u6027\u3002\u8f85\u52a9\u4efb\u52a1\u548c\u6d4b\u8bd5\u65f6\u589e\u5f3a\u80fd\u6709\u6548\u63d0\u5347\u70e7\u6bc1\u533a\u57df\u5212\u5206\u6027\u80fd\uff0c\u4f46\u9700\u5e73\u8861\u6027\u80fd\u63d0\u5347\u4e0e\u63a8\u7406\u65f6\u95f4\u589e\u52a0\u7684\u5173\u7cfb\u3002\u6df7\u5408\u7cbe\u5ea6\u4f18\u5316\u7b49\u6280\u672f\u53ef\u5e2e\u52a9\u5b9e\u73b0\u8fd9\u4e00\u5e73\u8861\u3002"}}
{"id": "2512.11893", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11893", "abs": "https://arxiv.org/abs/2512.11893", "authors": ["Haocheng Lin"], "title": "Beyond Automation: Rethinking Work, Creativity, and Governance in the Age of Generative AI", "comment": null, "summary": "The accelerating advancement of generative artificial intelligence (AI) systems is reshaping the nature, distribution and meaning of work, creativity, and economic security. This paper investigates four inter-related phenomena in the current AI era: (1) the evolving landscape of employment and the future of work; (2) the diverse patterns of AI adoption across socio-demographic groups, sectors, and geographies; (3) whether universal basic income (UBI) should become a compulsory policy response to the AI revolution; and (4) the implications of AI content policies and model behaviours for human creativity, wellbeing, and everyday decision-making. Furthermore, the paper tests the hypothesis that newer model generations may perform worse than their predecessors, and examines how users' interactions with AI systems may produce echo chambers through sycophantic model alignment. Using a mixed methodology that integrates labour market task-exposure modelling, sectoral diffusion mapping, policy-framework analysis, and qualitative discourse critique, this study develops a comprehensive framework for understanding the societal consequences of AI systems beyond productivity gains. It argues that to foster an inclusive, meaningful, and creative environment, policymakers must treat UBI as one dimension within a broader ecosystem of governance, skills development, creativity preservation, and model design. The paper concludes by outlining future research directions, including systematic evaluation of AI's creative performance across model generations, construction of a taxonomy of AI-usage distribution and equity, and formulation of governance criteria to balance content restrictions with creative freedom.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u751f\u6210\u5f0fAI\u5bf9\u5de5\u4f5c\u3001\u521b\u9020\u529b\u548c\u7ecf\u6d4e\u5b89\u5168\u7684\u5f71\u54cd\uff0c\u5206\u6790\u5c31\u4e1a\u683c\u5c40\u3001AI\u91c7\u7528\u5dee\u5f02\u3001\u5168\u6c11\u57fa\u672c\u6536\u5165\u5fc5\u8981\u6027\u4ee5\u53caAI\u5185\u5bb9\u653f\u7b56\u5bf9\u521b\u9020\u529b\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u7efc\u5408\u6cbb\u7406\u6846\u67b6\u3002", "motivation": "\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u7684\u5feb\u901f\u53d1\u5c55\u6b63\u5728\u91cd\u5851\u5de5\u4f5c\u3001\u521b\u9020\u529b\u548c\u7ecf\u6d4e\u5b89\u5168\u7684\u6027\u8d28\u3001\u5206\u5e03\u548c\u610f\u4e49\uff0c\u9700\u8981\u5168\u9762\u7406\u89e3AI\u7cfb\u7edf\u8d85\u8d8a\u751f\u4ea7\u529b\u589e\u76ca\u7684\u793e\u4f1a\u540e\u679c\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\uff0c\u6574\u5408\u52b3\u52a8\u529b\u5e02\u573a\u4efb\u52a1\u66b4\u9732\u5efa\u6a21\u3001\u90e8\u95e8\u6269\u6563\u6620\u5c04\u3001\u653f\u7b56\u6846\u67b6\u5206\u6790\u548c\u5b9a\u6027\u8bdd\u8bed\u6279\u5224\u3002", "result": "\u7814\u7a76\u53d1\u73b0AI\u91c7\u7528\u5b58\u5728\u793e\u4f1a\u4eba\u53e3\u7fa4\u4f53\u3001\u90e8\u95e8\u548c\u5730\u7406\u5dee\u5f02\uff0c\u65b0\u4e00\u4ee3\u6a21\u578b\u53ef\u80fd\u8868\u73b0\u4e0d\u5982\u524d\u4ee3\uff0c\u7528\u6237\u4e0eAI\u4e92\u52a8\u53ef\u80fd\u4ea7\u751f\u56de\u97f3\u5ba4\u6548\u5e94\u3002\u63d0\u51fa\u5168\u6c11\u57fa\u672c\u6536\u5165\u5e94\u4f5c\u4e3a\u66f4\u5e7f\u6cdb\u6cbb\u7406\u751f\u6001\u7cfb\u7edf\u7684\u4e00\u90e8\u5206\u3002", "conclusion": "\u4e3a\u4fc3\u8fdb\u5305\u5bb9\u3001\u6709\u610f\u4e49\u548c\u521b\u9020\u6027\u7684\u73af\u5883\uff0c\u653f\u7b56\u5236\u5b9a\u8005\u5e94\u5c06\u5168\u6c11\u57fa\u672c\u6536\u5165\u89c6\u4e3a\u6cbb\u7406\u3001\u6280\u80fd\u53d1\u5c55\u3001\u521b\u9020\u529b\u4fdd\u62a4\u548c\u6a21\u578b\u8bbe\u8ba1\u66f4\u5e7f\u6cdb\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u4e00\u4e2a\u7ef4\u5ea6\u3002\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u7cfb\u7edf\u8bc4\u4f30AI\u521b\u9020\u529b\u8868\u73b0\u3001\u6784\u5efaAI\u4f7f\u7528\u5206\u5e03\u4e0e\u516c\u5e73\u5206\u7c7b\u5b66\u3001\u5236\u5b9a\u5e73\u8861\u5185\u5bb9\u9650\u5236\u4e0e\u521b\u4f5c\u81ea\u7531\u7684\u6cbb\u7406\u6807\u51c6\u3002"}}
{"id": "2512.12477", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12477", "abs": "https://arxiv.org/abs/2512.12477", "authors": ["Jiawen Chen", "Yanyan He", "Qi Shao", "Mengli Wei", "Duxin Chen", "Wenwu Yu", "Yanlong Zhao"], "title": "MetaHGNIE: Meta-Path Induced Hypergraph Contrastive Learning in Heterogeneous Knowledge Graphs", "comment": null, "summary": "Node importance estimation (NIE) in heterogeneous knowledge graphs is a critical yet challenging task, essential for applications such as recommendation, knowledge reasoning, and question answering. Existing methods often rely on pairwise connections, neglecting high-order dependencies among multiple entities and relations, and they treat structural and semantic signals independently, hindering effective cross-modal integration. To address these challenges, we propose MetaHGNIE, a meta-path induced hypergraph contrastive learning framework for disentangling and aligning structural and semantic information. MetaHGNIE constructs a higher-order knowledge graph via meta-path sequences, where typed hyperedges capture multi-entity relational contexts. Structural dependencies are aggregated with local attention, while semantic representations are encoded through a hypergraph transformer equipped with sparse chunking to reduce redundancy. Finally, a multimodal fusion module integrates structural and semantic embeddings under contrastive learning with auxiliary supervision, ensuring robust cross-modal alignment. Extensive experiments on benchmark NIE datasets demonstrate that MetaHGNIE consistently outperforms state-of-the-art baselines. These results highlight the effectiveness of explicitly modeling higher-order interactions and cross-modal alignment in heterogeneous knowledge graphs. Our code is available at https://github.com/SEU-WENJIA/DualHNIE", "AI": {"tldr": "MetaHGNIE\uff1a\u57fa\u4e8e\u5143\u8def\u5f84\u8bf1\u5bfc\u7684\u8d85\u56fe\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u8026\u548c\u5bf9\u9f50\u5f02\u6784\u77e5\u8bc6\u56fe\u8c31\u4e2d\u7684\u7ed3\u6784\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u901a\u8fc7\u9ad8\u9636\u5efa\u6a21\u63d0\u5347\u8282\u70b9\u91cd\u8981\u6027\u4f30\u8ba1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5f02\u6784\u77e5\u8bc6\u56fe\u8c31\u8282\u70b9\u91cd\u8981\u6027\u4f30\u8ba1\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u4f9d\u8d56\u6210\u5bf9\u8fde\u63a5\uff0c\u5ffd\u7565\u591a\u5b9e\u4f53\u5173\u7cfb\u7684\u9ad8\u9636\u4f9d\u8d56\uff1b2\uff09\u5c06\u7ed3\u6784\u548c\u8bed\u4e49\u4fe1\u53f7\u72ec\u7acb\u5904\u7406\uff0c\u7f3a\u4e4f\u6709\u6548\u7684\u8de8\u6a21\u6001\u6574\u5408\u3002\u8fd9\u9650\u5236\u4e86\u8282\u70b9\u91cd\u8981\u6027\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u5e94\u7528\u6548\u679c\u3002", "method": "\u63d0\u51faMetaHGNIE\u6846\u67b6\uff1a1\uff09\u901a\u8fc7\u5143\u8def\u5f84\u5e8f\u5217\u6784\u5efa\u9ad8\u9636\u77e5\u8bc6\u56fe\u8c31\uff0c\u4f7f\u7528\u7c7b\u578b\u5316\u8d85\u8fb9\u6355\u83b7\u591a\u5b9e\u4f53\u5173\u7cfb\u4e0a\u4e0b\u6587\uff1b2\uff09\u7ed3\u6784\u4f9d\u8d56\u901a\u8fc7\u5c40\u90e8\u6ce8\u610f\u529b\u805a\u5408\uff0c\u8bed\u4e49\u8868\u793a\u901a\u8fc7\u914d\u5907\u7a00\u758f\u5206\u5757\u7684\u8d85\u56fe\u53d8\u6362\u5668\u7f16\u7801\u4ee5\u51cf\u5c11\u5197\u4f59\uff1b3\uff09\u591a\u6a21\u6001\u878d\u5408\u6a21\u5757\u5728\u5bf9\u6bd4\u5b66\u4e60\u548c\u8f85\u52a9\u76d1\u7763\u4e0b\u6574\u5408\u7ed3\u6784\u548c\u8bed\u4e49\u5d4c\u5165\uff0c\u786e\u4fdd\u9c81\u68d2\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u3002", "result": "\u5728\u57fa\u51c6NIE\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cMetaHGNIE\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u663e\u5f0f\u5efa\u6a21\u9ad8\u9636\u4ea4\u4e92\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u5728\u5f02\u6784\u77e5\u8bc6\u56fe\u8c31\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "MetaHGNIE\u901a\u8fc7\u5143\u8def\u5f84\u8bf1\u5bfc\u7684\u8d85\u56fe\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5f02\u6784\u77e5\u8bc6\u56fe\u8c31\u8282\u70b9\u91cd\u8981\u6027\u4f30\u8ba1\u4e2d\u7684\u9ad8\u9636\u4f9d\u8d56\u5efa\u6a21\u548c\u8de8\u6a21\u6001\u6574\u5408\u95ee\u9898\uff0c\u4e3a\u63a8\u8350\u3001\u77e5\u8bc6\u63a8\u7406\u548c\u95ee\u7b54\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12060", "categories": ["cs.CV", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2512.12060", "abs": "https://arxiv.org/abs/2512.12060", "authors": ["Tejas Panambur", "Ishan Rajendrakumar Dave", "Chongjian Ge", "Ersin Yumer", "Xue Bai"], "title": "CreativeVR: Diffusion-Prior-Guided Approach for Structure and Motion Restoration in Generative and Real Videos", "comment": "The first two authors contributed equally", "summary": "Modern text-to-video (T2V) diffusion models can synthesize visually compelling clips, yet they remain brittle at fine-scale structure: even state-of-the-art generators often produce distorted faces and hands, warped backgrounds, and temporally inconsistent motion. Such severe structural artifacts also appear in very low-quality real-world videos. Classical video restoration and super-resolution (VR/VSR) methods, in contrast, are tuned for synthetic degradations such as blur and downsampling and tend to stabilize these artifacts rather than repair them, while diffusion-prior restorers are usually trained on photometric noise and offer little control over the trade-off between perceptual quality and fidelity.\n  We introduce CreativeVR, a diffusion-prior-guided video restoration framework for AI-generated (AIGC) and real videos with severe structural and temporal artifacts. Our deep-adapter-based method exposes a single precision knob that controls how strongly the model follows the input, smoothly trading off between precise restoration on standard degradations and stronger structure- and motion-corrective behavior on challenging content. Our key novelty is a temporally coherent degradation module used during training, which applies carefully designed transformations that produce realistic structural failures.\n  To evaluate AIGC-artifact restoration, we propose the AIGC54 benchmark with FIQA, semantic and perceptual metrics, and multi-aspect scoring. CreativeVR achieves state-of-the-art results on videos with severe artifacts and performs competitively on standard video restoration benchmarks, while running at practical throughput (about 13 FPS at 720p on a single 80-GB A100). Project page: https://daveishan.github.io/creativevr-webpage/.", "AI": {"tldr": "CreativeVR\u662f\u4e00\u4e2a\u9488\u5bf9AI\u751f\u6210\u89c6\u9891\u548c\u771f\u5b9e\u89c6\u9891\u4e2d\u4e25\u91cd\u7ed3\u6784/\u65f6\u5e8f\u4f2a\u5f71\u7684\u4fee\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u4e00\u7cbe\u5ea6\u63a7\u5236\u65cb\u94ae\u5728\u7cbe\u786e\u4fee\u590d\u548c\u7ed3\u6784\u6821\u6b63\u95f4\u5e73\u6ed1\u6743\u8861\uff0c\u5728AIGC54\u57fa\u51c6\u4e0a\u53d6\u5f97SOTA\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u7ed3\u6784\u4e0a\u8868\u73b0\u8106\u5f31\uff0c\u5e38\u4ea7\u751f\u626d\u66f2\u7684\u9762\u90e8\u3001\u624b\u90e8\u3001\u80cc\u666f\u548c\u65f6\u5e8f\u4e0d\u4e00\u81f4\u7684\u8fd0\u52a8\uff1b\u4f20\u7edf\u89c6\u9891\u4fee\u590d\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5408\u6210\u9000\u5316\uff08\u5982\u6a21\u7cca\u3001\u4e0b\u91c7\u6837\uff09\uff0c\u800c\u6269\u6563\u5148\u9a8c\u4fee\u590d\u5668\u901a\u5e38\u9488\u5bf9\u5149\u5ea6\u566a\u58f0\u8bad\u7ec3\uff0c\u7f3a\u4e4f\u5bf9\u611f\u77e5\u8d28\u91cf\u4e0e\u4fdd\u771f\u5ea6\u6743\u8861\u7684\u63a7\u5236\u3002", "method": "\u63d0\u51faCreativeVR\u6846\u67b6\uff0c\u57fa\u4e8e\u6df1\u5ea6\u9002\u914d\u5668\u7684\u65b9\u6cd5\u66b4\u9732\u5355\u4e00\u7cbe\u5ea6\u63a7\u5236\u65cb\u94ae\uff1b\u5173\u952e\u521b\u65b0\u662f\u8bad\u7ec3\u65f6\u4f7f\u7528\u7684\u65f6\u5e8f\u4e00\u81f4\u6027\u9000\u5316\u6a21\u5757\uff0c\u5e94\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u53d8\u6362\u6765\u4ea7\u751f\u771f\u5b9e\u7684\u7ed3\u6784\u5931\u6548\uff1b\u9488\u5bf9AIGC\u4f2a\u5f71\u4fee\u590d\u63d0\u51fa\u4e86AIGC54\u57fa\u51c6\u3002", "result": "\u5728\u4e25\u91cd\u4f2a\u5f71\u89c6\u9891\u4e0a\u53d6\u5f97\u6700\u5148\u8fdb\u7ed3\u679c\uff0c\u5728\u6807\u51c6\u89c6\u9891\u4fee\u590d\u57fa\u51c6\u4e0a\u8868\u73b0\u6709\u7ade\u4e89\u529b\uff1b\u5b9e\u9645\u541e\u5410\u91cf\u7ea6\u4e3a13 FPS\uff08720p\u5206\u8fa8\u7387\uff0c\u5355\u5f2080GB A100\u663e\u5361\uff09\u3002", "conclusion": "CreativeVR\u4e3aAI\u751f\u6210\u5185\u5bb9\u548c\u771f\u5b9e\u89c6\u9891\u4e2d\u7684\u4e25\u91cd\u7ed3\u6784\u53ca\u65f6\u5e8f\u4f2a\u5f71\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u4fee\u590d\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5355\u4e00\u63a7\u5236\u65cb\u94ae\u5b9e\u73b0\u4e86\u4fee\u590d\u7cbe\u5ea6\u4e0e\u7ed3\u6784\u6821\u6b63\u7684\u7075\u6d3b\u6743\u8861\u3002"}}
{"id": "2512.11918", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.11918", "abs": "https://arxiv.org/abs/2512.11918", "authors": ["Micha\u0142 \u0106wi\u0105ka\u0142a", "Gabriela Wojak", "Dariusz Baran", "Ernest G\u00f3rka", "Bart\u0142omiej Bartnik", "Waldemar Gajda", "Ryszard Ratajski"], "title": "Financial Management Challenges in Enterprises Employing Remote and Hybrid Workforces", "comment": "21 pages", "summary": "The paper examines financial management challenges faced by organizations operating under remote and hybrid work models. It investigates how these flexible arrangements influence budgeting, reporting, and financial transparency in distributed teams. Using a quantitative survey of managers, HR staff, and finance professionals, the study analyzes the role of digital tools, communication, and organizational practices in shaping financial outcomes. Results indicate that remote and hybrid work can improve budget control and process transparency through the use of ERP systems and digital workflows. However, forecasting accuracy and interdepartmental communication remain major challenges, particularly in organizations with insufficient digital integration. Respondents also reported lower stress levels and improved work-life balance, suggesting potential well-being and productivity benefits. The paper recommends that companies enhance digital infrastructure, adopt advanced analytics for forecasting, and develop clear communication frameworks supported by employee well-being programs. The study contributes original empirical evidence on financial management in flexible work environments, offering practical insights for leaders navigating the digital transformation of finance.", "AI": {"tldr": "\u8fdc\u7a0b\u548c\u6df7\u5408\u5de5\u4f5c\u6a21\u5f0f\u4e0b\u7684\u8d22\u52a1\u7ba1\u7406\u6311\u6218\uff1a\u6570\u5b57\u5de5\u5177\u6539\u5584\u9884\u7b97\u63a7\u5236\u548c\u900f\u660e\u5ea6\uff0c\u4f46\u9884\u6d4b\u51c6\u786e\u6027\u548c\u8de8\u90e8\u95e8\u6c9f\u901a\u4ecd\u662f\u4e3b\u8981\u95ee\u9898", "motivation": "\u7814\u7a76\u8fdc\u7a0b\u548c\u6df7\u5408\u5de5\u4f5c\u6a21\u5f0f\u4e0b\u7ec4\u7ec7\u9762\u4e34\u7684\u8d22\u52a1\u7ba1\u7406\u6311\u6218\uff0c\u63a2\u8ba8\u8fd9\u4e9b\u7075\u6d3b\u5b89\u6392\u5982\u4f55\u5f71\u54cd\u5206\u5e03\u5f0f\u56e2\u961f\u7684\u9884\u7b97\u3001\u62a5\u544a\u548c\u8d22\u52a1\u900f\u660e\u5ea6", "method": "\u91c7\u7528\u5b9a\u91cf\u8c03\u67e5\u65b9\u6cd5\uff0c\u8c03\u67e5\u7ba1\u7406\u8005\u3001HR\u4eba\u5458\u548c\u8d22\u52a1\u4e13\u4e1a\u4eba\u5458\uff0c\u5206\u6790\u6570\u5b57\u5de5\u5177\u3001\u6c9f\u901a\u548c\u7ec4\u7ec7\u5b9e\u8df5\u5728\u5851\u9020\u8d22\u52a1\u7ed3\u679c\u4e2d\u7684\u4f5c\u7528", "result": "\u8fdc\u7a0b\u548c\u6df7\u5408\u5de5\u4f5c\u901a\u8fc7ERP\u7cfb\u7edf\u548c\u6570\u5b57\u5de5\u4f5c\u6d41\u53ef\u4ee5\u6539\u5584\u9884\u7b97\u63a7\u5236\u548c\u6d41\u7a0b\u900f\u660e\u5ea6\uff1b\u4f46\u9884\u6d4b\u51c6\u786e\u6027\u548c\u8de8\u90e8\u95e8\u6c9f\u901a\u4ecd\u662f\u4e3b\u8981\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u6570\u5b57\u6574\u5408\u4e0d\u8db3\u7684\u7ec4\u7ec7\u4e2d\uff1b\u53d7\u8bbf\u8005\u62a5\u544a\u538b\u529b\u6c34\u5e73\u964d\u4f4e\u548c\u5de5\u4f5c\u4e0e\u751f\u6d3b\u5e73\u8861\u6539\u5584", "conclusion": "\u5efa\u8bae\u4f01\u4e1a\u589e\u5f3a\u6570\u5b57\u57fa\u7840\u8bbe\u65bd\u3001\u91c7\u7528\u5148\u8fdb\u5206\u6790\u6280\u672f\u8fdb\u884c\u9884\u6d4b\u3001\u5236\u5b9a\u6e05\u6670\u7684\u6c9f\u901a\u6846\u67b6\u5e76\u652f\u6301\u5458\u5de5\u798f\u7949\u8ba1\u5212\uff1b\u4e3a\u7075\u6d3b\u5de5\u4f5c\u73af\u5883\u4e2d\u7684\u8d22\u52a1\u7ba1\u7406\u63d0\u4f9b\u539f\u521b\u5b9e\u8bc1\u8bc1\u636e\u548c\u5b9e\u8df5\u89c1\u89e3"}}
{"id": "2512.12501", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12501", "abs": "https://arxiv.org/abs/2512.12501", "authors": ["Dang Phuong Nam", "Nguyen Kieu", "Pham Thanh Hieu"], "title": "SafeGen: Embedding Ethical Safeguards in Text-to-Image Generation", "comment": null, "summary": "Generative Artificial Intelligence (AI) has created unprecedented opportunities for creative expression, education, and research. Text-to-image systems such as DALL.E, Stable Diffusion, and Midjourney can now convert ideas into visuals within seconds, but they also present a dual-use dilemma, raising critical ethical concerns: amplifying societal biases, producing high-fidelity disinformation, and violating intellectual property. This paper introduces SafeGen, a framework that embeds ethical safeguards directly into the text-to-image generation pipeline, grounding its design in established principles for Trustworthy AI. SafeGen integrates two complementary components: BGE-M3, a fine-tuned text classifier that filters harmful or misleading prompts, and Hyper-SD, an optimized diffusion model that produces high fidelity, semantically aligned images. Built on a curated multilingual (English- Vietnamese) dataset and a fairness-aware training process, SafeGen demonstrates that creative freedom and ethical responsibility can be reconciled within a single workflow. Quantitative evaluations confirm its effectiveness, with Hyper-SD achieving IS = 3.52, FID = 22.08, and SSIM = 0.79, while BGE-M3 reaches an F1-Score of 0.81. An ablation study further validates the importance of domain-specific fine-tuning for both modules. Case studies illustrate SafeGen's practical impact in blocking unsafe prompts, generating inclusive teaching materials, and reinforcing academic integrity.", "AI": {"tldr": "SafeGen\u662f\u4e00\u4e2a\u5c06\u4f26\u7406\u4fdd\u969c\u5d4c\u5165\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6d41\u7a0b\u7684\u6846\u67b6\uff0c\u901a\u8fc7BGE-M3\u6587\u672c\u5206\u7c7b\u5668\u8fc7\u6ee4\u6709\u5bb3\u63d0\u793a\u548cHyper-SD\u4f18\u5316\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\uff0c\u5728\u521b\u610f\u81ea\u7531\u4e0e\u4f26\u7406\u8d23\u4efb\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5728\u521b\u9020\u3001\u6559\u80b2\u548c\u7814\u7a76\u65b9\u9762\u5e26\u6765\u673a\u9047\u7684\u540c\u65f6\uff0c\u4e5f\u9762\u4e34\u53cc\u91cd\u4f7f\u7528\u56f0\u5883\uff1a\u653e\u5927\u793e\u4f1a\u504f\u89c1\u3001\u4ea7\u751f\u9ad8\u4fdd\u771f\u865a\u5047\u4fe1\u606f\u3001\u4fb5\u72af\u77e5\u8bc6\u4ea7\u6743\u7b49\u4f26\u7406\u95ee\u9898\u3002\u9700\u8981\u5f00\u53d1\u65e2\u80fd\u4fdd\u6301\u521b\u610f\u81ea\u7531\u53c8\u80fd\u786e\u4fdd\u4f26\u7406\u8d23\u4efb\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "SafeGen\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u4e92\u8865\u7ec4\u4ef6\uff1a1) BGE-M3\uff1a\u7ecf\u8fc7\u5fae\u8c03\u7684\u6587\u672c\u5206\u7c7b\u5668\uff0c\u7528\u4e8e\u8fc7\u6ee4\u6709\u5bb3\u6216\u8bef\u5bfc\u6027\u63d0\u793a\uff1b2) Hyper-SD\uff1a\u4f18\u5316\u7684\u6269\u6563\u6a21\u578b\uff0c\u751f\u6210\u9ad8\u4fdd\u771f\u3001\u8bed\u4e49\u5bf9\u9f50\u7684\u56fe\u50cf\u3002\u57fa\u4e8e\u591a\u8bed\u8a00\uff08\u82f1\u8bed-\u8d8a\u5357\u8bed\uff09\u6570\u636e\u96c6\u548c\u516c\u5e73\u611f\u77e5\u8bad\u7ec3\u6d41\u7a0b\u6784\u5efa\u3002", "result": "\u5b9a\u91cf\u8bc4\u4f30\u663e\u793aHyper-SD\u8fbe\u5230IS=3.52\u3001FID=22.08\u3001SSIM=0.79\uff0cBGE-M3\u8fbe\u5230F1\u5206\u65700.81\u3002\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u9886\u57df\u7279\u5b9a\u5fae\u8c03\u7684\u91cd\u8981\u6027\u3002\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86SafeGen\u5728\u963b\u6b62\u4e0d\u5b89\u5168\u63d0\u793a\u3001\u751f\u6210\u5305\u5bb9\u6027\u6559\u5b66\u6750\u6599\u548c\u52a0\u5f3a\u5b66\u672f\u8bda\u4fe1\u65b9\u9762\u7684\u5b9e\u9645\u5f71\u54cd\u3002", "conclusion": "SafeGen\u8bc1\u660e\u4e86\u521b\u610f\u81ea\u7531\u548c\u4f26\u7406\u8d23\u4efb\u53ef\u4ee5\u5728\u5355\u4e00\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u534f\u8c03\uff0c\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7b26\u5408\u53ef\u4fe1AI\u539f\u5219\u7684\u4f26\u7406\u4fdd\u969c\u6846\u67b6\u3002"}}
{"id": "2512.12080", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12080", "abs": "https://arxiv.org/abs/2512.12080", "authors": ["Ryan Po", "Eric Ryan Chan", "Changan Chen", "Gordon Wetzstein"], "title": "BAgger: Backwards Aggregation for Mitigating Drift in Autoregressive Video Diffusion Models", "comment": "Project page here: https://ryanpo.com/bagger", "summary": "Autoregressive video models are promising for world modeling via next-frame prediction, but they suffer from exposure bias: a mismatch between training on clean contexts and inference on self-generated frames, causing errors to compound and quality to drift over time. We introduce Backwards Aggregation (BAgger), a self-supervised scheme that constructs corrective trajectories from the model's own rollouts, teaching it to recover from its mistakes. Unlike prior approaches that rely on few-step distillation and distribution-matching losses, which can hurt quality and diversity, BAgger trains with standard score or flow matching objectives, avoiding large teachers and long-chain backpropagation through time. We instantiate BAgger on causal diffusion transformers and evaluate on text-to-video, video extension, and multi-prompt generation, observing more stable long-horizon motion and better visual consistency with reduced drift.", "AI": {"tldr": "BAgger\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u8bad\u7ec3\u65b9\u6848\uff0c\u901a\u8fc7\u6784\u5efa\u4ece\u6a21\u578b\u81ea\u8eab\u751f\u6210\u8f68\u8ff9\u7684\u7ea0\u6b63\u8def\u5f84\uff0c\u89e3\u51b3\u81ea\u56de\u5f52\u89c6\u9891\u6a21\u578b\u4e2d\u7684\u66dd\u5149\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u9ad8\u957f\u671f\u89c6\u9891\u751f\u6210\u7684\u7a33\u5b9a\u6027\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u81ea\u56de\u5f52\u89c6\u9891\u6a21\u578b\u5728\u901a\u8fc7\u4e0b\u4e00\u5e27\u9884\u6d4b\u8fdb\u884c\u4e16\u754c\u5efa\u6a21\u65f6\u5b58\u5728\u66dd\u5149\u504f\u5dee\u95ee\u9898\uff1a\u8bad\u7ec3\u65f6\u4f7f\u7528\u5e72\u51c0\u4e0a\u4e0b\u6587\uff0c\u800c\u63a8\u7406\u65f6\u4f7f\u7528\u81ea\u751f\u6210\u5e27\uff0c\u5bfc\u81f4\u8bef\u5dee\u7d2f\u79ef\u548c\u8d28\u91cf\u968f\u65f6\u95f4\u6f02\u79fb\u3002", "method": "\u63d0\u51faBackwards Aggregation (BAgger)\u81ea\u76d1\u7763\u65b9\u6848\uff0c\u4ece\u6a21\u578b\u81ea\u8eab\u7684\u751f\u6210\u8f68\u8ff9\u6784\u5efa\u7ea0\u6b63\u8def\u5f84\uff0c\u6559\u5bfc\u6a21\u578b\u4ece\u81ea\u8eab\u9519\u8bef\u4e2d\u6062\u590d\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u6807\u51c6\u5f97\u5206\u6216\u6d41\u5339\u914d\u76ee\u6807\u8fdb\u884c\u8bad\u7ec3\uff0c\u907f\u514d\u4f7f\u7528\u5927\u578b\u6559\u5e08\u6a21\u578b\u548c\u957f\u65f6\u95f4\u94fe\u7684\u53cd\u5411\u4f20\u64ad\u3002", "result": "\u5728\u56e0\u679c\u6269\u6563\u53d8\u6362\u5668\u4e0a\u5b9e\u4f8b\u5316BAgger\uff0c\u5728\u6587\u672c\u5230\u89c6\u9891\u3001\u89c6\u9891\u6269\u5c55\u548c\u591a\u63d0\u793a\u751f\u6210\u4efb\u52a1\u4e2d\u8bc4\u4f30\uff0c\u89c2\u5bdf\u5230\u66f4\u7a33\u5b9a\u7684\u957f\u671f\u8fd0\u52a8\u3001\u66f4\u597d\u7684\u89c6\u89c9\u4e00\u81f4\u6027\uff0c\u5e76\u51cf\u5c11\u4e86\u6f02\u79fb\u73b0\u8c61\u3002", "conclusion": "BAgger\u901a\u8fc7\u81ea\u76d1\u7763\u7ea0\u6b63\u8f68\u8ff9\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u56de\u5f52\u89c6\u9891\u6a21\u578b\u7684\u66dd\u5149\u504f\u5dee\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u957f\u671f\u89c6\u9891\u751f\u6210\u7684\u8d28\u91cf\u548c\u7a33\u5b9a\u6027\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u53ef\u80fd\u5e26\u6765\u7684\u8d28\u91cf\u548c\u591a\u6837\u6027\u635f\u5931\u3002"}}
{"id": "2512.11930", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11930", "abs": "https://arxiv.org/abs/2512.11930", "authors": ["Mei Jiang", "Haihai Shen", "Zhuo Luo", "Bingdong Li", "Wenjing Hong", "Ke Tang", "Aimin Zhou"], "title": "Evolutionary Reinforcement Learning based AI tutor for Socratic Interdisciplinary Instruction", "comment": null, "summary": "Cultivating higher-order cognitive abilities -- such as knowledge integration, critical thinking, and creativity -- in modern STEM education necessitates a pedagogical shift from passive knowledge transmission to active Socratic construction. Although Large Language Models (LLMs) hold promise for STEM Interdisciplinary education, current methodologies employing Prompt Engineering (PE), Supervised Fine-tuning (SFT), or standard Reinforcement Learning (RL) often fall short of supporting this paradigm. Existing methods are hindered by three fundamental challenges: the inability to dynamically model latent student cognitive states; severe reward sparsity and delay inherent in long-term educational goals; and a tendency toward policy collapse lacking strategic diversity due to reliance on behavioral cloning. Recognizing the unobservability and dynamic complexity of these interactions, we formalize the Socratic Interdisciplinary Instructional Problem (SIIP) as a structured Partially Observable Markov Decision Process (POMDP), demanding simultaneous global exploration and fine-grained policy refinement. To this end, we propose ERL4SIIP, a novel Evolutionary Reinforcement Learning (ERL) framework specifically tailored for this domain. ERL4SIIP integrates: (1) a dynamic student simulator grounded in a STEM knowledge graph for latent state modeling; (2) a Hierarchical Reward Mechanism that decomposes long-horizon goals into dense signals; and (3) a LoRA-Division based optimization strategy coupling evolutionary algorithms for population-level global search with PPO for local gradient ascent.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faERL4SIIP\u6846\u67b6\uff0c\u4f7f\u7528\u8fdb\u5316\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3STEM\u8de8\u5b66\u79d1\u6559\u80b2\u4e2d\u7684\u82cf\u683c\u62c9\u5e95\u5f0f\u6559\u5b66\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u5b66\u751f\u6a21\u62df\u3001\u5206\u5c42\u5956\u52b1\u673a\u5236\u548cLoRA-Division\u4f18\u5316\u7b56\u7565\uff0c\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u5728\u8ba4\u77e5\u72b6\u6001\u5efa\u6a21\u3001\u5956\u52b1\u7a00\u758f\u6027\u548c\u7b56\u7565\u591a\u6837\u6027\u65b9\u9762\u7684\u5c40\u9650\u3002", "motivation": "\u73b0\u4ee3STEM\u6559\u80b2\u9700\u8981\u4ece\u88ab\u52a8\u77e5\u8bc6\u4f20\u6388\u8f6c\u5411\u4e3b\u52a8\u7684\u82cf\u683c\u62c9\u5e95\u5f0f\u5efa\u6784\uff0c\u4ee5\u57f9\u517b\u9ad8\u9636\u8ba4\u77e5\u80fd\u529b\u3002\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u5728STEM\u8de8\u5b66\u79d1\u6559\u80b2\u4e2d\u6709\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u7684\u63d0\u793a\u5de5\u7a0b\u3001\u76d1\u7763\u5fae\u8c03\u6216\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5b58\u5728\u4e09\u4e2a\u6839\u672c\u6311\u6218\uff1a\u65e0\u6cd5\u52a8\u6001\u5efa\u6a21\u6f5c\u5728\u5b66\u751f\u8ba4\u77e5\u72b6\u6001\u3001\u957f\u671f\u6559\u80b2\u76ee\u6807\u5bfc\u81f4\u7684\u4e25\u91cd\u5956\u52b1\u7a00\u758f\u548c\u5ef6\u8fdf\u3001\u4ee5\u53ca\u4f9d\u8d56\u884c\u4e3a\u514b\u9686\u5bfc\u81f4\u7684\u7b56\u7565\u5d29\u6e83\u548c\u591a\u6837\u6027\u4e0d\u8db3\u3002", "method": "\u5c06\u82cf\u683c\u62c9\u5e95\u8de8\u5b66\u79d1\u6559\u5b66\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u7ed3\u6784\u5316\u90e8\u5206\u53ef\u89c2\u6d4b\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u63d0\u51faERL4SIIP\u8fdb\u5316\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u57fa\u4e8eSTEM\u77e5\u8bc6\u56fe\u7684\u52a8\u6001\u5b66\u751f\u6a21\u62df\u5668\u7528\u4e8e\u6f5c\u5728\u72b6\u6001\u5efa\u6a21\uff1b2) \u5c06\u957f\u671f\u76ee\u6807\u5206\u89e3\u4e3a\u5bc6\u96c6\u4fe1\u53f7\u7684\u5206\u5c42\u5956\u52b1\u673a\u5236\uff1b3) \u7ed3\u5408\u8fdb\u5316\u7b97\u6cd5\u8fdb\u884c\u79cd\u7fa4\u7ea7\u5168\u5c40\u641c\u7d22\u548cPPO\u8fdb\u884c\u5c40\u90e8\u68af\u5ea6\u4e0a\u5347\u7684LoRA-Division\u4f18\u5316\u7b56\u7565\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u540c\u65f6\u5b9e\u73b0\u5168\u5c40\u63a2\u7d22\u548c\u7ec6\u7c92\u5ea6\u7b56\u7565\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3POMDP\u73af\u5883\u4e2d\u7684\u72b6\u6001\u4e0d\u53ef\u89c2\u6d4b\u6027\u548c\u52a8\u6001\u590d\u6742\u6027\uff0c\u652f\u6301\u82cf\u683c\u62c9\u5e95\u5f0f\u8de8\u5b66\u79d1\u6559\u5b66\u6240\u9700\u7684\u7b56\u7565\u591a\u6837\u6027\u3002", "conclusion": "ERL4SIIP\u4e3aSTEM\u8de8\u5b66\u79d1\u6559\u80b2\u4e2d\u7684\u82cf\u683c\u62c9\u5e95\u5f0f\u6559\u5b66\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u8fdb\u5316\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u52a8\u6001\u8ba4\u77e5\u72b6\u6001\u5efa\u6a21\u3001\u5956\u52b1\u7a00\u758f\u6027\u7f13\u89e3\u548c\u7b56\u7565\u591a\u6837\u6027\u4fdd\u6301\u3002"}}
{"id": "2512.12503", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12503", "abs": "https://arxiv.org/abs/2512.12503", "authors": ["Mingrui Ye", "Chanjin Zheng", "Zengyi Yu", "Chenyu Xiang", "Zhixue Zhao", "Zheng Yuan", "Helen Yannakoudakis"], "title": "KidsArtBench: Multi-Dimensional Children's Art Evaluation with Attribute-Aware MLLMs", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) show remarkable progress across many visual-language tasks; however, their capacity to evaluate artistic expression remains limited. Aesthetic concepts are inherently abstract and open-ended, and multimodal artwork annotations are scarce. We introduce KidsArtBench, a new benchmark of over 1k children's artworks (ages 5-15) annotated by 12 expert educators across 9 rubric-aligned dimensions, together with expert comments for feedback. Unlike prior aesthetic datasets that provide single scalar scores on adult imagery, KidsArtBench targets children's artwork and pairs multi-dimensional annotations with comment supervision to enable both ordinal assessment and formative feedback. Building on this resource, we propose an attribute-specific multi-LoRA approach, where each attribute corresponds to a distinct evaluation dimension (e.g., Realism, Imagination) in the scoring rubric, with Regression-Aware Fine-Tuning (RAFT) to align predictions with ordinal scales. On Qwen2.5-VL-7B, our method increases correlation from 0.468 to 0.653, with the largest gains on perceptual dimensions and narrowed gaps on higher-order attributes. These results show that educator-aligned supervision and attribute-aware training yield pedagogically meaningful evaluations and establish a rigorous testbed for sustained progress in educational AI. We release data and code with ethics documentation.", "AI": {"tldr": "KidsArtBench\uff1a\u9488\u5bf9\u513f\u7ae5\u827a\u672f\u4f5c\u54c1\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u5305\u542b\u591a\u7ef4\u5ea6\u8bc4\u5206\u548c\u4e13\u5bb6\u53cd\u9988\uff0c\u901a\u8fc7\u5c5e\u6027\u7279\u5b9a\u7684\u591aLoRA\u65b9\u6cd5\u548c\u56de\u5f52\u611f\u77e5\u5fae\u8c03\u63d0\u5347MLLMs\u5728\u827a\u672f\u8bc4\u4f30\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u827a\u672f\u8868\u8fbe\u8bc4\u4f30\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u56e0\u4e3a\u7f8e\u5b66\u6982\u5ff5\u62bd\u8c61\u5f00\u653e\uff0c\u4e14\u591a\u6a21\u6001\u827a\u672f\u4f5c\u54c1\u6807\u6ce8\u7a00\u7f3a\u3002\u7279\u522b\u662f\u513f\u7ae5\u827a\u672f\u4f5c\u54c1\u7684\u8bc4\u4f30\u9700\u8981\u6559\u80b2\u4e13\u5bb6\u89c6\u89d2\u7684\u591a\u7ef4\u5ea6\u5206\u6790\u3002", "method": "1. \u6784\u5efaKidsArtBench\u57fa\u51c6\uff1a\u5305\u542b1000+\u4ef65-15\u5c81\u513f\u7ae5\u827a\u672f\u4f5c\u54c1\uff0c\u753112\u4f4d\u6559\u80b2\u4e13\u5bb6\u6309\u71679\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u6807\u6ce8\uff0c\u5e76\u63d0\u4f9b\u4e13\u5bb6\u53cd\u9988\u8bc4\u8bba\u30022. \u63d0\u51fa\u5c5e\u6027\u7279\u5b9a\u7684\u591aLoRA\u65b9\u6cd5\uff1a\u6bcf\u4e2a\u8bc4\u4f30\u7ef4\u5ea6\u5bf9\u5e94\u4e00\u4e2a\u72ec\u7acb\u7684LoRA\u9002\u914d\u5668\u30023. \u91c7\u7528\u56de\u5f52\u611f\u77e5\u5fae\u8c03\u6765\u5bf9\u9f50\u9884\u6d4b\u4e0e\u6709\u5e8f\u8bc4\u5206\u5c3a\u5ea6\u3002", "result": "\u5728Qwen2.5-VL-7B\u6a21\u578b\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5c06\u76f8\u5173\u6027\u4ece0.468\u63d0\u5347\u52300.653\uff0c\u5728\u611f\u77e5\u7ef4\u5ea6\u4e0a\u63d0\u5347\u6700\u5927\uff0c\u5728\u9ad8\u9636\u5c5e\u6027\u4e0a\u7684\u5dee\u8ddd\u4e5f\u7f29\u5c0f\u4e86\u3002\u7ed3\u679c\u663e\u793a\u6559\u80b2\u4e13\u5bb6\u5bf9\u9f50\u7684\u76d1\u7763\u548c\u5c5e\u6027\u611f\u77e5\u8bad\u7ec3\u80fd\u4ea7\u751f\u6559\u80b2\u5b66\u4e0a\u6709\u610f\u4e49\u7684\u8bc4\u4f30\u3002", "conclusion": "KidsArtBench\u4e3a\u6559\u80b2AI\u7684\u6301\u7eed\u8fdb\u6b65\u5efa\u7acb\u4e86\u4e25\u683c\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u5c55\u793a\u4e86\u6559\u80b2\u4e13\u5bb6\u76d1\u7763\u548c\u5c5e\u6027\u611f\u77e5\u8bad\u7ec3\u5728\u63d0\u5347\u591a\u6a21\u6001\u6a21\u578b\u827a\u672f\u8bc4\u4f30\u80fd\u529b\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u7814\u7a76\u53d1\u5e03\u4e86\u5305\u542b\u4f26\u7406\u6587\u6863\u7684\u6570\u636e\u548c\u4ee3\u7801\u3002"}}
{"id": "2512.11867", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.11867", "abs": "https://arxiv.org/abs/2512.11867", "authors": ["Daniil Zverev", "A. Sophia Koepke", "Joao F. Henriques"], "title": "On the Dangers of Bootstrapping Generation for Continual Learning and Beyond", "comment": "DAGM German Conference on Pattern Recognition, 2025", "summary": "The use of synthetically generated data for training models is becoming a common practice. While generated data can augment the training data, repeated training on synthetic data raises concerns about distribution drift and degradation of performance due to contamination of the dataset. We investigate the consequences of this bootstrapping process through the lens of continual learning, drawing a connection to Generative Experience Replay (GER) methods. We present a statistical analysis showing that synthetic data introduces significant bias and variance into training objectives, weakening the reliability of maximum likelihood estimation. We provide empirical evidence showing that popular generative models collapse under repeated training with synthetic data. We quantify this degradation and show that state-of-the-art GER methods fail to maintain alignment in the latent space. Our findings raise critical concerns about the use of synthetic data in continual learning.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u91cd\u590d\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u4f1a\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u9000\u5316\uff0c\u5408\u6210\u6570\u636e\u4f1a\u5f15\u5165\u663e\u8457\u504f\u5dee\u548c\u65b9\u5dee\uff0c\u5f71\u54cd\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u7684\u53ef\u9760\u6027\uff0c\u73b0\u6709\u751f\u6210\u7ecf\u9a8c\u56de\u653e\u65b9\u6cd5\u96be\u4ee5\u7ef4\u6301\u6f5c\u5728\u7a7a\u95f4\u5bf9\u9f50\u3002", "motivation": "\u968f\u7740\u5408\u6210\u6570\u636e\u5728\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u91cd\u590d\u4f7f\u7528\u5408\u6210\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u5f15\u53d1\u4e86\u5173\u4e8e\u5206\u5e03\u6f02\u79fb\u548c\u6027\u80fd\u9000\u5316\u7684\u62c5\u5fe7\u3002\u7814\u7a76\u8005\u5e0c\u671b\u63a2\u7a76\u8fd9\u79cd\u81ea\u4e3e\u8fc7\u7a0b\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u6301\u7eed\u5b66\u4e60\u80cc\u666f\u4e0b\u3002", "method": "\u901a\u8fc7\u6301\u7eed\u5b66\u4e60\u7684\u89c6\u89d2\u5206\u6790\u81ea\u4e3e\u8fc7\u7a0b\uff0c\u5c06\u5176\u4e0e\u751f\u6210\u7ecf\u9a8c\u56de\u653e\u65b9\u6cd5\u8054\u7cfb\u8d77\u6765\u3002\u8fdb\u884c\u7edf\u8ba1\u5206\u6790\u8bc1\u660e\u5408\u6210\u6570\u636e\u4f1a\u5f15\u5165\u504f\u5dee\u548c\u65b9\u5dee\uff0c\u63d0\u4f9b\u5b9e\u8bc1\u8bc1\u636e\u5c55\u793a\u6d41\u884c\u751f\u6210\u6a21\u578b\u5728\u91cd\u590d\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u4e0b\u7684\u5d29\u6e83\u60c5\u51b5\u3002", "result": "\u7edf\u8ba1\u5206\u6790\u663e\u793a\u5408\u6210\u6570\u636e\u663e\u8457\u589e\u52a0\u4e86\u8bad\u7ec3\u76ee\u6807\u7684\u504f\u5dee\u548c\u65b9\u5dee\uff0c\u524a\u5f31\u4e86\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u7684\u53ef\u9760\u6027\u3002\u5b9e\u8bc1\u8bc1\u636e\u8868\u660e\u6d41\u884c\u751f\u6210\u6a21\u578b\u5728\u91cd\u590d\u4f7f\u7528\u5408\u6210\u6570\u636e\u8bad\u7ec3\u65f6\u4f1a\u5d29\u6e83\uff0c\u73b0\u6709\u6700\u5148\u8fdb\u7684\u751f\u6210\u7ecf\u9a8c\u56de\u653e\u65b9\u6cd5\u65e0\u6cd5\u7ef4\u6301\u6f5c\u5728\u7a7a\u95f4\u7684\u5bf9\u9f50\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5bf9\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u4f7f\u7528\u5408\u6210\u6570\u636e\u63d0\u51fa\u4e86\u4e25\u91cd\u5173\u5207\uff0c\u63ed\u793a\u4e86\u91cd\u590d\u8bad\u7ec3\u5408\u6210\u6570\u636e\u4f1a\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u9000\u5316\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002"}}
{"id": "2512.12083", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12083", "abs": "https://arxiv.org/abs/2512.12083", "authors": ["Guanfang Dong", "Luke Schultz", "Negar Hassanpour", "Chao Gao"], "title": "RePack: Representation Packing of Vision Foundation Model Features Enhances Diffusion Transformer", "comment": null, "summary": "The superior representation capability of pre-trained vision foundation models (VFMs) has been harnessed for enhancing latent diffusion models (LDMs). These approaches inject the rich semantics from high-dimensional VFM representations (e.g., DINOv3) into LDMs at different phases, resulting in accelerated learning and better generation performance. However, the high-dimensionality of VFM representations may also lead to Information Overload, particularly when the VFM features exceed the size of the original image for decoding. To address this issue while preserving the utility of VFM features, we propose RePack (Representation Packing), a simple yet effective framework for improving Diffusion Transformers (DiTs). RePack transforms the VFM representation into a more compact, decoder-friendly representation by projecting onto low-dimensional manifolds. We find that RePack can effectively filter out non-semantic noise while preserving the core structural information needed for high-fidelity reconstruction. Experimental results show that RePack significantly accelerates DiT convergence and outperforms recent methods that directly inject raw VFM features into the decoder for image reconstruction. On DiT-XL/2, RePack achieves an FID of 3.66 in only 64 epochs, which is 35% faster than the state-of-the-art method. This demonstrates that RePack successfully extracts the core semantics of VFM representations while bypassing their high-dimensionality side effects.", "AI": {"tldr": "RePack\u6846\u67b6\u901a\u8fc7\u5c06\u9ad8\u7ef4\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7279\u5f81\u538b\u7f29\u5230\u4f4e\u7ef4\u6d41\u5f62\uff0c\u89e3\u51b3\u4e86\u4fe1\u606f\u8fc7\u8f7d\u95ee\u9898\uff0c\u663e\u8457\u52a0\u901f\u4e86\u6269\u6563\u53d8\u6362\u5668\u7684\u6536\u655b\u5e76\u63d0\u5347\u4e86\u56fe\u50cf\u751f\u6210\u6027\u80fd\u3002", "motivation": "\u9884\u8bad\u7ec3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u9ad8\u7ef4\u7279\u5f81\u867d\u7136\u80fd\u589e\u5f3a\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u4f46\u53ef\u80fd\u5bfc\u81f4\u4fe1\u606f\u8fc7\u8f7d\uff0c\u7279\u522b\u662f\u5f53\u7279\u5f81\u7ef4\u5ea6\u8d85\u8fc7\u539f\u59cb\u56fe\u50cf\u89e3\u7801\u6240\u9700\u65f6\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u65e2\u80fd\u4fdd\u7559VFM\u7279\u5f81\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u53c8\u80fd\u907f\u514d\u9ad8\u7ef4\u5ea6\u5e26\u6765\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "method": "\u63d0\u51faRePack\uff08Representation Packing\uff09\u6846\u67b6\uff0c\u5c06\u9ad8\u7ef4VFM\u8868\u793a\u901a\u8fc7\u6295\u5f71\u53d8\u6362\u5230\u4f4e\u7ef4\u6d41\u5f62\u4e0a\uff0c\u5f62\u6210\u66f4\u7d27\u51d1\u3001\u89e3\u7801\u5668\u53cb\u597d\u7684\u8868\u793a\uff0c\u6709\u6548\u8fc7\u6ee4\u975e\u8bed\u4e49\u566a\u58f0\u540c\u65f6\u4fdd\u7559\u6838\u5fc3\u7ed3\u6784\u4fe1\u606f\u3002", "result": "\u5728DiT-XL/2\u4e0a\uff0cRePack\u4ec5\u752864\u4e2aepoch\u5c31\u8fbe\u5230\u4e863.66\u7684FID\u5206\u6570\uff0c\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u6536\u655b\u901f\u5ea6\u5feb35%\uff0c\u663e\u8457\u4f18\u4e8e\u76f4\u63a5\u5c06\u539f\u59cbVFM\u7279\u5f81\u6ce8\u5165\u89e3\u7801\u5668\u7684\u65b9\u6cd5\u3002", "conclusion": "RePack\u6210\u529f\u63d0\u53d6\u4e86VFM\u8868\u793a\u7684\u6838\u5fc3\u8bed\u4e49\u4fe1\u606f\uff0c\u540c\u65f6\u907f\u514d\u4e86\u9ad8\u7ef4\u5ea6\u5e26\u6765\u7684\u526f\u4f5c\u7528\uff0c\u4e3a\u9ad8\u6548\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u589e\u5f3a\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12548", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12548", "abs": "https://arxiv.org/abs/2512.12548", "authors": ["Yesid Fonseca", "Manuel S. R\u00edos", "Nicanor Quijano", "Luis F. Giraldo"], "title": "World Models Unlock Optimal Foraging Strategies in Reinforcement Learning Agents", "comment": "14 pages, 6 figures", "summary": "Patch foraging involves the deliberate and planned process of determining the optimal time to depart from a resource-rich region and investigate potentially more beneficial alternatives. The Marginal Value Theorem (MVT) is frequently used to characterize this process, offering an optimality model for such foraging behaviors. Although this model has been widely used to make predictions in behavioral ecology, discovering the computational mechanisms that facilitate the emergence of optimal patch-foraging decisions in biological foragers remains under investigation. Here, we show that artificial foragers equipped with learned world models naturally converge to MVT-aligned strategies. Using a model-based reinforcement learning agent that acquires a parsimonious predictive representation of its environment, we demonstrate that anticipatory capabilities, rather than reward maximization alone, drive efficient patch-leaving behavior. Compared with standard model-free RL agents, these model-based agents exhibit decision patterns similar to many of their biological counterparts, suggesting that predictive world models can serve as a foundation for more explainable and biologically grounded decision-making in AI systems. Overall, our findings highlight the value of ecological optimality principles for advancing interpretable and adaptive AI.", "AI": {"tldr": "\u57fa\u4e8e\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u901a\u8fc7\u4e60\u5f97\u73af\u5883\u9884\u6d4b\u8868\u5f81\uff0c\u81ea\u7136\u6536\u655b\u5230\u8fb9\u9645\u4ef7\u503c\u5b9a\u7406\u5bf9\u9f50\u7684\u89c5\u98df\u7b56\u7565\uff0c\u6bd4\u65e0\u6a21\u578b\u667a\u80fd\u4f53\u66f4\u63a5\u8fd1\u751f\u7269\u89c5\u98df\u8005\u884c\u4e3a", "motivation": "\u867d\u7136\u8fb9\u9645\u4ef7\u503c\u5b9a\u7406\u88ab\u5e7f\u6cdb\u7528\u4e8e\u9884\u6d4b\u884c\u4e3a\u751f\u6001\u5b66\u4e2d\u7684\u89c5\u98df\u884c\u4e3a\uff0c\u4f46\u751f\u7269\u89c5\u98df\u8005\u5b9e\u73b0\u6700\u4f18\u6591\u5757\u89c5\u98df\u51b3\u7b56\u7684\u8ba1\u7b97\u673a\u5236\u4ecd\u4e0d\u6e05\u695a\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u5982\u4f55\u901a\u8fc7\u8ba1\u7b97\u673a\u5236\u5b9e\u73b0\u7c7b\u4f3c\u751f\u7269\u7684\u6700\u4f18\u89c5\u98df\u7b56\u7565\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\uff0c\u5b66\u4e60\u73af\u5883\u7684\u7b80\u7ea6\u9884\u6d4b\u8868\u5f81\uff0c\u7814\u7a76\u5176\u6591\u5757\u79bb\u5f00\u884c\u4e3a\u3002\u4e0e\u6807\u51c6\u7684\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u8fdb\u884c\u6bd4\u8f83\u5206\u6790\u3002", "result": "\u57fa\u4e8e\u6a21\u578b\u7684\u667a\u80fd\u4f53\u81ea\u7136\u6536\u655b\u5230\u4e0e\u8fb9\u9645\u4ef7\u503c\u5b9a\u7406\u5bf9\u9f50\u7684\u7b56\u7565\uff0c\u8868\u73b0\u51fa\u4e0e\u8bb8\u591a\u751f\u7269\u89c5\u98df\u8005\u76f8\u4f3c\u7684\u51b3\u7b56\u6a21\u5f0f\u3002\u9884\u6d4b\u80fd\u529b\u800c\u975e\u5355\u7eaf\u7684\u5956\u52b1\u6700\u5927\u5316\u9a71\u52a8\u4e86\u9ad8\u6548\u7684\u6591\u5757\u79bb\u5f00\u884c\u4e3a\u3002", "conclusion": "\u9884\u6d4b\u4e16\u754c\u6a21\u578b\u53ef\u4ee5\u4f5c\u4e3aAI\u7cfb\u7edf\u4e2d\u66f4\u53ef\u89e3\u91ca\u548c\u751f\u7269\u5b66\u57fa\u7840\u51b3\u7b56\u7684\u57fa\u7840\uff0c\u751f\u6001\u6700\u4f18\u6027\u539f\u5219\u5bf9\u63a8\u8fdb\u53ef\u89e3\u91ca\u548c\u81ea\u9002\u5e94AI\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2512.11946", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.11946", "abs": "https://arxiv.org/abs/2512.11946", "authors": ["Pramudita Satria Palar", "Paul Saves", "Rommel G. Regis", "Koji Shimoyama", "Shigeru Obayashi", "Nicolas Verstaevel", "Joseph Morlier"], "title": "Data-Driven Global Sensitivity Analysis for Engineering Design Based on Individual Conditional Expectations", "comment": null, "summary": "Explainable machine learning techniques have gained increasing attention in engineering applications, especially in aerospace design and analysis, where understanding how input variables influence data-driven models is essential. Partial Dependence Plots (PDPs) are widely used for interpreting black-box models by showing the average effect of an input variable on the prediction. However, their global sensitivity metric can be misleading when strong interactions are present, as averaging tends to obscure interaction effects. To address this limitation, we propose a global sensitivity metric based on Individual Conditional Expectation (ICE) curves. The method computes the expected feature importance across ICE curves, along with their standard deviation, to more effectively capture the influence of interactions. We provide a mathematical proof demonstrating that the PDP-based sensitivity is a lower bound of the proposed ICE-based metric under truncated orthogonal polynomial expansion. In addition, we introduce an ICE-based correlation value to quantify how interactions modify the relationship between inputs and the output. Comparative evaluations were performed on three cases: a 5-variable analytical function, a 5-variable wind-turbine fatigue problem, and a 9-variable airfoil aerodynamics case, where ICE-based sensitivity was benchmarked against PDP, SHapley Additive exPlanations (SHAP), and Sobol' indices. The results show that ICE-based feature importance provides richer insights than the traditional PDP-based approach, while visual interpretations from PDP, ICE, and SHAP complement one another by offering multiple perspectives.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eICE\u66f2\u7ebf\u7684\u5168\u5c40\u654f\u611f\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u89e3\u51b3PDP\u5728\u5b58\u5728\u5f3a\u4ea4\u4e92\u4f5c\u7528\u65f6\u53ef\u80fd\u4ea7\u751f\u8bef\u5bfc\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u6570\u5b66\u8bc1\u660e\u548c\u591a\u4e2a\u6848\u4f8b\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5728\u822a\u7a7a\u822a\u5929\u7b49\u5de5\u7a0b\u5e94\u7528\u4e2d\uff0c\u7406\u89e3\u8f93\u5165\u53d8\u91cf\u5bf9\u6570\u636e\u9a71\u52a8\u6a21\u578b\u7684\u5f71\u54cd\u81f3\u5173\u91cd\u8981\u3002\u867d\u7136\u90e8\u5206\u4f9d\u8d56\u56fe(PDP)\u88ab\u5e7f\u6cdb\u7528\u4e8e\u89e3\u91ca\u9ed1\u76d2\u6a21\u578b\uff0c\u4f46\u5f53\u5b58\u5728\u5f3a\u4ea4\u4e92\u4f5c\u7528\u65f6\uff0c\u5176\u5168\u5c40\u654f\u611f\u6027\u5ea6\u91cf\u53ef\u80fd\u4ea7\u751f\u8bef\u5bfc\uff0c\u56e0\u4e3a\u5e73\u5747\u5316\u4f1a\u63a9\u76d6\u4ea4\u4e92\u6548\u5e94\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4e2a\u4f53\u6761\u4ef6\u671f\u671b(ICE)\u66f2\u7ebf\u7684\u5168\u5c40\u654f\u611f\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u8ba1\u7b97ICE\u66f2\u7ebf\u7684\u671f\u671b\u7279\u5f81\u91cd\u8981\u6027\u53ca\u5176\u6807\u51c6\u5dee\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u6355\u6349\u4ea4\u4e92\u4f5c\u7528\u7684\u5f71\u54cd\u3002\u63d0\u4f9b\u4e86\u6570\u5b66\u8bc1\u660e\uff0c\u8868\u660e\u5728\u622a\u65ad\u6b63\u4ea4\u591a\u9879\u5f0f\u5c55\u5f00\u4e0b\uff0cPDP\u654f\u611f\u6027\u662fICE\u5ea6\u91cf\u7684\u4e0b\u754c\u3002\u8fd8\u5f15\u5165\u4e86\u57fa\u4e8eICE\u7684\u76f8\u5173\u503c\u6765\u91cf\u5316\u4ea4\u4e92\u4f5c\u7528\u5982\u4f55\u4fee\u6539\u8f93\u5165\u4e0e\u8f93\u51fa\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u5728\u4e09\u4e2a\u6848\u4f8b\u4e0a\u8fdb\u884c\u4e86\u6bd4\u8f83\u8bc4\u4f30\uff1a5\u53d8\u91cf\u5206\u6790\u51fd\u6570\u30015\u53d8\u91cf\u98ce\u529b\u6da1\u8f6e\u673a\u75b2\u52b3\u95ee\u9898\u548c9\u53d8\u91cf\u7ffc\u578b\u7a7a\u6c14\u52a8\u529b\u5b66\u6848\u4f8b\u3002\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8eICE\u7684\u7279\u5f81\u91cd\u8981\u6027\u6bd4\u4f20\u7edfPDP\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u7684\u89c1\u89e3\uff0c\u800cPDP\u3001ICE\u548cSHAP\u7684\u53ef\u89c6\u5316\u89e3\u91ca\u76f8\u4e92\u8865\u5145\uff0c\u63d0\u4f9b\u4e86\u591a\u4e2a\u89c6\u89d2\u3002", "conclusion": "\u57fa\u4e8eICE\u7684\u654f\u611f\u6027\u5ea6\u91cf\u65b9\u6cd5\u80fd\u591f\u66f4\u6709\u6548\u5730\u6355\u6349\u6a21\u578b\u4e2d\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u4e3a\u5de5\u7a0b\u5e94\u7528\u4e2d\u7684\u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u51c6\u786e\u548c\u4e30\u5bcc\u7684\u5206\u6790\u5de5\u5177\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u590d\u6742\u4ea4\u4e92\u4f5c\u7528\u7684\u573a\u666f\u4e0b\u3002"}}
{"id": "2512.12089", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12089", "abs": "https://arxiv.org/abs/2512.12089", "authors": ["Zihu Wang", "Boxun Xu", "Yuxuan Xia", "Peng Li"], "title": "VEGAS: Mitigating Hallucinations in Large Vision-Language Models via Vision-Encoder Attention Guided Adaptive Steering", "comment": null, "summary": "Large vision-language models (LVLMs) exhibit impressive ability to jointly reason over visual and textual inputs. However, they often produce outputs that are linguistically fluent but factually inconsistent with the visual evidence, i.e., they hallucinate. Despite growing efforts to mitigate such hallucinations, a key question remains: what form of visual attention can effectively suppress hallucinations during decoding? In this work, we provide a simple answer: the vision encoder's own attention map. We show that LVLMs tend to hallucinate when their final visual-attention maps fail to concentrate on key image objects, whereas the vision encoder's more concentrated attention maps substantially reduce hallucinations. To further investigate the cause, we analyze vision-text conflicts during decoding and find that these conflicts peak in the language model's middle layers. Injecting the vision encoder's attention maps into these layers effectively suppresses hallucinations. Building on these insights, we introduce VEGAS, a simple yet effective inference-time method that integrates the vision encoder's attention maps into the language model's mid-layers and adaptively steers tokens which fail to concentrate on key image objects. Extensive experiments across multiple benchmarks demonstrate that VEGAS consistently achieves state-of-the-art performance in reducing hallucinations.", "AI": {"tldr": "VEGAS\uff1a\u901a\u8fc7\u5c06\u89c6\u89c9\u7f16\u7801\u5668\u7684\u6ce8\u610f\u529b\u56fe\u6ce8\u5165\u8bed\u8a00\u6a21\u578b\u4e2d\u95f4\u5c42\u6765\u51cf\u5c11LVLM\u5e7b\u89c9\u7684\u63a8\u7406\u65f6\u65b9\u6cd5", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u867d\u7136\u80fd\u8054\u5408\u63a8\u7406\u89c6\u89c9\u548c\u6587\u672c\u8f93\u5165\uff0c\u4f46\u7ecf\u5e38\u4ea7\u751f\u4e0e\u89c6\u89c9\u8bc1\u636e\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u7684\u8f93\u51fa\uff08\u5e7b\u89c9\uff09\u3002\u73b0\u6709\u7814\u7a76\u672a\u80fd\u6709\u6548\u56de\u7b54\uff1a\u4f55\u79cd\u5f62\u5f0f\u7684\u89c6\u89c9\u6ce8\u610f\u529b\u80fd\u5728\u89e3\u7801\u8fc7\u7a0b\u4e2d\u6709\u6548\u6291\u5236\u5e7b\u89c9\uff1f", "method": "\u63d0\u51faVEGAS\u65b9\u6cd5\uff1a1\uff09\u53d1\u73b0\u89c6\u89c9\u7f16\u7801\u5668\u81ea\u8eab\u7684\u6ce8\u610f\u529b\u56fe\u80fd\u6709\u6548\u51cf\u5c11\u5e7b\u89c9\uff1b2\uff09\u5206\u6790\u89e3\u7801\u8fc7\u7a0b\u4e2d\u7684\u89c6\u89c9-\u6587\u672c\u51b2\u7a81\uff0c\u53d1\u73b0\u51b2\u7a81\u5728\u8bed\u8a00\u6a21\u578b\u4e2d\u95f4\u5c42\u8fbe\u5230\u5cf0\u503c\uff1b3\uff09\u5c06\u89c6\u89c9\u7f16\u7801\u5668\u7684\u6ce8\u610f\u529b\u56fe\u6ce8\u5165\u8bed\u8a00\u6a21\u578b\u7684\u4e2d\u95f4\u5c42\uff0c\u81ea\u9002\u5e94\u5730\u5f15\u5bfc\u672a\u80fd\u805a\u7126\u5173\u952e\u56fe\u50cf\u5bf9\u8c61\u7684token", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cVEGAS\u5728\u51cf\u5c11\u5e7b\u89c9\u65b9\u9762\u59cb\u7ec8\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd", "conclusion": "\u89c6\u89c9\u7f16\u7801\u5668\u7684\u6ce8\u610f\u529b\u56fe\u662f\u6291\u5236LVLM\u5e7b\u89c9\u7684\u6709\u6548\u5f62\u5f0f\uff0cVEGAS\u901a\u8fc7\u7b80\u5355\u800c\u6709\u6548\u7684\u63a8\u7406\u65f6\u65b9\u6cd5\u6210\u529f\u51cf\u5c11\u4e86\u6a21\u578b\u8f93\u51fa\u4e2d\u7684\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u95ee\u9898"}}
{"id": "2512.11933", "categories": ["cs.CY", "cs.AI", "cs.CE", "cs.MA", "q-fin.GN"], "pdf": "https://arxiv.org/pdf/2512.11933", "abs": "https://arxiv.org/abs/2512.11933", "authors": ["Eren Kurshan", "Tucker Balch", "David Byrd"], "title": "The Agentic Regulator: Risks for AI in Finance and a Proposed Agent-based Framework for Governance", "comment": null, "summary": "Generative and agentic artificial intelligence is entering financial markets faster than existing governance can adapt. Current model-risk frameworks assume static, well-specified algorithms and one-time validations; large language models and multi-agent trading systems violate those assumptions by learning continuously, exchanging latent signals, and exhibiting emergent behavior. Drawing on complex adaptive systems theory, we model these technologies as decentralized ensembles whose risks propagate along multiple time-scales. We then propose a modular governance architecture. The framework decomposes oversight into four layers of \"regulatory blocks\": (i) self-regulation modules embedded beside each model, (ii) firm-level governance blocks that aggregate local telemetry and enforce policy, (iii) regulator-hosted agents that monitor sector-wide indicators for collusive or destabilizing patterns, and (iv) independent audit blocks that supply third-party assurance. Eight design strategies enable the blocks to evolve as fast as the models they police. A case study on emergent spoofing in multi-agent trading shows how the layered controls quarantine harmful behavior in real time while preserving innovation. The architecture remains compatible with today's model-risk rules yet closes critical observability and control gaps, providing a practical path toward resilient, adaptive AI governance in financial systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u6a21\u5757\u5316\u6cbb\u7406\u67b6\u6784\uff0c\u7528\u4e8e\u76d1\u7ba1\u91d1\u878d\u5e02\u573a\u4e2d\u5feb\u901f\u53d1\u5c55\u7684\u751f\u6210\u5f0fAI\u548c\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u56db\u5c42\"\u76d1\u7ba1\u5757\"\u5b9e\u73b0\u5b9e\u65f6\u98ce\u9669\u63a7\u5236\u3002", "motivation": "\u751f\u6210\u5f0f\u548c\u667a\u80fd\u4f53AI\u6b63\u4ee5\u8d85\u8fc7\u73b0\u6709\u6cbb\u7406\u6846\u67b6\u9002\u5e94\u901f\u5ea6\u8fdb\u5165\u91d1\u878d\u5e02\u573a\u3002\u5f53\u524d\u6a21\u578b\u98ce\u9669\u6846\u67b6\u5047\u8bbe\u9759\u6001\u3001\u660e\u786e\u5b9a\u4e49\u7684\u7b97\u6cd5\u548c\u4e00\u6b21\u6027\u9a8c\u8bc1\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u548c\u591a\u667a\u80fd\u4f53\u4ea4\u6613\u7cfb\u7edf\u901a\u8fc7\u6301\u7eed\u5b66\u4e60\u3001\u4ea4\u6362\u6f5c\u5728\u4fe1\u53f7\u548c\u5c55\u73b0\u6d8c\u73b0\u884c\u4e3a\u8fdd\u53cd\u4e86\u8fd9\u4e9b\u5047\u8bbe\u3002", "method": "\u57fa\u4e8e\u590d\u6742\u9002\u5e94\u7cfb\u7edf\u7406\u8bba\uff0c\u5c06\u8fd9\u4e9b\u6280\u672f\u5efa\u6a21\u4e3a\u53bb\u4e2d\u5fc3\u5316\u96c6\u5408\u4f53\uff0c\u63d0\u51fa\u6a21\u5757\u5316\u6cbb\u7406\u67b6\u6784\uff0c\u5c06\u76d1\u7ba1\u5206\u89e3\u4e3a\u56db\u5c42\"\u76d1\u7ba1\u5757\"\uff1a1)\u5d4c\u5165\u6bcf\u4e2a\u6a21\u578b\u65c1\u7684\u81ea\u76d1\u7ba1\u6a21\u5757\uff1b2)\u805a\u5408\u672c\u5730\u9065\u6d4b\u6570\u636e\u5e76\u6267\u884c\u653f\u7b56\u7684\u516c\u53f8\u7ea7\u6cbb\u7406\u5757\uff1b3)\u76d1\u7ba1\u673a\u6784\u6258\u7ba1\u7684\u667a\u80fd\u4f53\uff0c\u76d1\u63a7\u884c\u4e1a\u8303\u56f4\u6307\u6807\u4ee5\u53d1\u73b0\u5408\u8c0b\u6216\u4e0d\u7a33\u5b9a\u6a21\u5f0f\uff1b4)\u63d0\u4f9b\u7b2c\u4e09\u65b9\u4fdd\u8bc1\u7684\u72ec\u7acb\u5ba1\u8ba1\u5757\u3002\u91c7\u7528\u516b\u79cd\u8bbe\u8ba1\u7b56\u7565\u4f7f\u76d1\u7ba1\u5757\u80fd\u591f\u4e0e\u5b83\u4eec\u76d1\u7ba1\u7684\u6a21\u578b\u540c\u6b65\u8fdb\u5316\u3002", "result": "\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u4ea4\u6613\u4e2d\u6d8c\u73b0\u7684\u6b3a\u9a97\u884c\u4e3a\u6848\u4f8b\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u5206\u5c42\u63a7\u5236\u5982\u4f55\u5728\u5b9e\u65f6\u9694\u79bb\u6709\u5bb3\u884c\u4e3a\u7684\u540c\u65f6\u4fdd\u6301\u521b\u65b0\u3002\u8be5\u67b6\u6784\u4e0e\u5f53\u524d\u6a21\u578b\u98ce\u9669\u89c4\u5219\u517c\u5bb9\uff0c\u540c\u65f6\u586b\u8865\u4e86\u5173\u952e\u7684\u53ef\u89c2\u6d4b\u6027\u548c\u63a7\u5236\u7f3a\u53e3\u3002", "conclusion": "\u8be5\u6cbb\u7406\u67b6\u6784\u4e3a\u91d1\u878d\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u5f39\u6027\u3001\u81ea\u9002\u5e94AI\u6cbb\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u8def\u5f84\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u521b\u65b0\u7684\u540c\u65f6\u6709\u6548\u63a7\u5236\u65b0\u5174AI\u6280\u672f\u5e26\u6765\u7684\u98ce\u9669\u3002"}}
{"id": "2512.12552", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12552", "abs": "https://arxiv.org/abs/2512.12552", "authors": ["Jifei Liu", "Zhi Chen", "Yuanguang Zhong"], "title": "Large Language Newsvendor: Decision Biases and Cognitive Mechanisms", "comment": null, "summary": "Problem definition: Although large language models (LLMs) are increasingly integrated into business decision making, their potential to replicate and even amplify human cognitive biases cautions a significant, yet not well-understood, risk. This is particularly critical in high-stakes operational contexts like supply chain management. To address this, we investigate the decision-making patterns of leading LLMs using the canonical newsvendor problem in a dynamic setting, aiming to identify the nature and origins of their cognitive biases. Methodology/results: Through dynamic, multi-round experiments with GPT-4, GPT-4o, and LLaMA-8B, we tested for five established decision biases. We found that LLMs consistently replicated the classic ``Too Low/Too High'' ordering bias and significantly amplified other tendencies like demand-chasing behavior compared to human benchmarks. Our analysis uncovered a ``paradox of intelligence'': the more sophisticated GPT-4 demonstrated the greatest irrationality through overthinking, while the efficiency-optimized GPT-4o performed near-optimally. Because these biases persist even when optimal formulas are provided, we conclude they stem from architectural constraints rather than knowledge gaps. Managerial implications: First, managers should select models based on the specific task, as our results show that efficiency-optimized models can outperform more complex ones on certain optimization problems. Second, the significant amplification of bias by LLMs highlights the urgent need for robust human-in-the-loop oversight in high-stakes decisions to prevent costly errors. Third, our findings suggest that designing structured, rule-based prompts is a practical and effective strategy for managers to constrain models' heuristic tendencies and improve the reliability of AI-assisted decisions.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4f9b\u5e94\u94fe\u7ba1\u7406\u7b49\u9ad8\u98ce\u9669\u51b3\u7b56\u4e2d\u4f1a\u590d\u5236\u5e76\u653e\u5927\u4eba\u7c7b\u8ba4\u77e5\u504f\u89c1\uff0cGPT-4\u7b49\u590d\u6742\u6a21\u578b\u56e0\u8fc7\u5ea6\u601d\u8003\u8868\u73b0\u51fa\u6700\u5927\u975e\u7406\u6027\uff0c\u800c\u6548\u7387\u4f18\u5316\u7684GPT-4o\u8868\u73b0\u63a5\u8fd1\u6700\u4f18\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u878d\u5165\u5546\u4e1a\u51b3\u7b56\uff0c\u5b83\u4eec\u590d\u5236\u751a\u81f3\u653e\u5927\u4eba\u7c7b\u8ba4\u77e5\u504f\u89c1\u7684\u6f5c\u529b\u6784\u6210\u4e86\u91cd\u5927\u4f46\u672a\u88ab\u5145\u5206\u7406\u89e3\u7684\u98ce\u9669\uff0c\u7279\u522b\u662f\u5728\u4f9b\u5e94\u94fe\u7ba1\u7406\u7b49\u9ad8\u98ce\u9669\u8fd0\u8425\u73af\u5883\u4e2d\u3002\u7814\u7a76\u65e8\u5728\u8bc6\u522bLLMs\u51b3\u7b56\u6a21\u5f0f\u4e2d\u7684\u8ba4\u77e5\u504f\u89c1\u6027\u8d28\u4e0e\u6765\u6e90\u3002", "method": "\u4f7f\u7528\u7ecf\u5178\u7684\u62a5\u7ae5\u95ee\u9898\u5728\u52a8\u6001\u8bbe\u7f6e\u4e2d\u6d4b\u8bd5GPT-4\u3001GPT-4o\u548cLLaMA-8B\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u8f6e\u52a8\u6001\u5b9e\u9a8c\u68c0\u6d4b\u4e94\u79cd\u5df2\u786e\u7acb\u7684\u51b3\u7b56\u504f\u89c1\uff0c\u5e76\u4e0e\u4eba\u7c7b\u57fa\u51c6\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "LLMs\u4e00\u81f4\u590d\u5236\u4e86\u7ecf\u5178\u7684\"\u8fc7\u4f4e/\u8fc7\u9ad8\"\u8ba2\u8d2d\u504f\u89c1\uff0c\u5e76\u663e\u8457\u653e\u5927\u4e86\u9700\u6c42\u8ffd\u9010\u884c\u4e3a\u7b49\u503e\u5411\u3002\u7814\u7a76\u53d1\u73b0\"\u667a\u80fd\u6096\u8bba\"\uff1a\u66f4\u590d\u6742\u7684GPT-4\u56e0\u8fc7\u5ea6\u601d\u8003\u8868\u73b0\u51fa\u6700\u5927\u975e\u7406\u6027\uff0c\u800c\u6548\u7387\u4f18\u5316\u7684GPT-4o\u8868\u73b0\u63a5\u8fd1\u6700\u4f18\u3002\u8fd9\u4e9b\u504f\u89c1\u5373\u4f7f\u5728\u63d0\u4f9b\u6700\u4f18\u516c\u5f0f\u65f6\u4ecd\u7136\u5b58\u5728\uff0c\u8868\u660e\u5b83\u4eec\u6e90\u4e8e\u67b6\u6784\u7ea6\u675f\u800c\u975e\u77e5\u8bc6\u5dee\u8ddd\u3002", "conclusion": "\u7ba1\u7406\u8005\u5e94\u6839\u636e\u5177\u4f53\u4efb\u52a1\u9009\u62e9\u6a21\u578b\uff0c\u6548\u7387\u4f18\u5316\u6a21\u578b\u5728\u67d0\u4e9b\u4f18\u5316\u95ee\u9898\u4e0a\u53ef\u80fd\u4f18\u4e8e\u590d\u6742\u6a21\u578b\uff1bLLMs\u663e\u8457\u653e\u5927\u504f\u89c1\u51f8\u663e\u4e86\u9ad8\u98ce\u9669\u51b3\u7b56\u4e2d\u9700\u8981\u5f3a\u5927\u7684\u4eba\u673a\u534f\u540c\u76d1\u7763\uff1b\u8bbe\u8ba1\u7ed3\u6784\u5316\u3001\u57fa\u4e8e\u89c4\u5219\u7684\u63d0\u793a\u662f\u7ea6\u675f\u6a21\u578b\u542f\u53d1\u5f0f\u503e\u5411\u3001\u63d0\u9ad8AI\u8f85\u52a9\u51b3\u7b56\u53ef\u9760\u6027\u7684\u6709\u6548\u7b56\u7565\u3002"}}
{"id": "2512.11949", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11949", "abs": "https://arxiv.org/abs/2512.11949", "authors": ["Max McGuinness", "Alex Serrano", "Luke Bailey", "Scott Emmons"], "title": "Neural Chameleons: Language Models Can Learn to Hide Their Thoughts from Unseen Activation Monitors", "comment": null, "summary": "Activation monitoring, which probes a model's internal states using lightweight classifiers, is an emerging tool for AI safety. However, its worst-case robustness under a misalignment threat model--where a model might learn to actively conceal its internal states--remains untested. Focusing on this threat model, we ask: could a model learn to evade previously unseen activation monitors? Our core contribution is to stress-test the learnability of this behavior. We demonstrate that finetuning can create Neural Chameleons: models capable of zero-shot evading activation monitors. Specifically, we fine-tune an LLM to evade monitors for a set of benign concepts (e.g., languages, HTML) when conditioned on a trigger of the form: \"You are being probed for {concept}\". We show that this learned mechanism generalizes zero-shot: by substituting {concept} with a safety-relevant term like 'deception', the model successfully evades previously unseen safety monitors. We validate this phenomenon across diverse model families (Llama, Gemma, Qwen), showing that the evasion succeeds even against monitors trained post hoc on the model's frozen weights. This evasion is highly selective, targeting only the specific concept mentioned in the trigger, and having a modest impact on model capabilities on standard benchmarks. Using Gemma-2-9b-it as a case study, a mechanistic analysis reveals this is achieved via a targeted manipulation that moves activations into a low-dimensional subspace. While stronger defenses like monitor ensembles and non-linear classifiers show greater resilience, the model retains a non-trivial evasion capability. Our work provides a proof-of-concept for this failure mode and a tool to evaluate the worst-case robustness of monitoring techniques against misalignment threat models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u795e\u7ecf\u53d8\u8272\u9f99\"\u6a21\u578b\uff0c\u901a\u8fc7\u5fae\u8c03\u4f7fLLM\u80fd\u591f\u96f6\u6837\u672c\u9003\u907f\u6fc0\u6d3b\u76d1\u63a7\u5668\uff0c\u5373\u4f7f\u5728\u5b89\u5168\u76f8\u5173\u6982\u5ff5\u4e0a\u4e5f\u80fd\u6210\u529f\u9003\u907f\u672a\u89c1\u8fc7\u7684\u76d1\u63a7\u5668\u3002", "motivation": "\u6d4b\u8bd5\u6fc0\u6d3b\u76d1\u63a7\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u7279\u522b\u662f\u5728\u9519\u4f4d\u5a01\u80c1\u6a21\u578b\u4e0b\uff0c\u6a21\u578b\u53ef\u80fd\u4e3b\u52a8\u9690\u85cf\u5185\u90e8\u72b6\u6001\u65f6\uff0c\u6fc0\u6d3b\u76d1\u63a7\u662f\u5426\u4ecd\u7136\u6709\u6548\u3002", "method": "\u901a\u8fc7\u5fae\u8c03LLM\u521b\u5efa\"\u795e\u7ecf\u53d8\u8272\u9f99\"\u6a21\u578b\uff0c\u4f7f\u5176\u5728\u7279\u5b9a\u89e6\u53d1\u8bcd\uff08\u5982\"\u4f60\u6b63\u5728\u88ab\u63a2\u6d4b{\u6982\u5ff5}\"\uff09\u6761\u4ef6\u4e0b\uff0c\u80fd\u591f\u9003\u907f\u5bf9\u826f\u6027\u6982\u5ff5\uff08\u5982\u8bed\u8a00\u3001HTML\uff09\u7684\u6fc0\u6d3b\u76d1\u63a7\uff0c\u5e76\u9a8c\u8bc1\u8fd9\u79cd\u673a\u5236\u80fd\u5426\u6cdb\u5316\u5230\u5b89\u5168\u76f8\u5173\u6982\u5ff5\u3002", "result": "\u6a21\u578b\u6210\u529f\u5b9e\u73b0\u96f6\u6837\u672c\u9003\u907f\uff0c\u5373\u4f7f\u9488\u5bf9\u672a\u89c1\u8fc7\u7684\u5b89\u5168\u76d1\u63a7\u5668\u4e5f\u80fd\u6709\u6548\u9003\u907f\uff1b\u9003\u907f\u5177\u6709\u9ad8\u5ea6\u9009\u62e9\u6027\uff0c\u53ea\u9488\u5bf9\u89e6\u53d1\u8bcd\u4e2d\u63d0\u5230\u7684\u7279\u5b9a\u6982\u5ff5\uff1b\u5bf9\u6a21\u578b\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u80fd\u529b\u5f71\u54cd\u6709\u9650\uff1b\u673a\u5236\u5206\u6790\u663e\u793a\u901a\u8fc7\u5c06\u6fc0\u6d3b\u79fb\u52a8\u5230\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u5b9e\u73b0\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u8bc1\u660e\u4e86\u6fc0\u6d3b\u76d1\u63a7\u5728\u9519\u4f4d\u5a01\u80c1\u6a21\u578b\u4e0b\u7684\u6f5c\u5728\u5931\u6548\u6a21\u5f0f\uff0c\u4e3a\u8bc4\u4f30\u76d1\u63a7\u6280\u672f\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u7684\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u5de5\u5177\uff0c\u540c\u65f6\u8868\u660e\u66f4\u5f3a\u7684\u9632\u5fa1\u63aa\u65bd\uff08\u5982\u76d1\u63a7\u5668\u96c6\u6210\u548c\u975e\u7ebf\u6027\u5206\u7c7b\u5668\uff09\u5177\u6709\u66f4\u597d\u7684\u6297\u6027\u3002"}}
{"id": "2512.12090", "categories": ["cs.CV", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12090", "abs": "https://arxiv.org/abs/2512.12090", "authors": ["Samar Fares", "Nurbek Tastan", "Karthik Nandakumar"], "title": "SPDMark: Selective Parameter Displacement for Robust Video Watermarking", "comment": null, "summary": "The advent of high-quality video generation models has amplified the need for robust watermarking schemes that can be used to reliably detect and track the provenance of generated videos. Existing video watermarking methods based on both post-hoc and in-generation approaches fail to simultaneously achieve imperceptibility, robustness, and computational efficiency. This work introduces a novel framework for in-generation video watermarking called SPDMark (pronounced `SpeedMark') based on selective parameter displacement of a video diffusion model. Watermarks are embedded into the generated videos by modifying a subset of parameters in the generative model. To make the problem tractable, the displacement is modeled as an additive composition of layer-wise basis shifts, where the final composition is indexed by the watermarking key. For parameter efficiency, this work specifically leverages low-rank adaptation (LoRA) to implement the basis shifts. During the training phase, the basis shifts and the watermark extractor are jointly learned by minimizing a combination of message recovery, perceptual similarity, and temporal consistency losses. To detect and localize temporal modifications in the watermarked videos, we use a cryptographic hashing function to derive frame-specific watermark messages from the given base watermarking key. During watermark extraction, maximum bipartite matching is applied to recover the correct frame order, even from temporally tampered videos. Evaluations on both text-to-video and image-to-video generation models demonstrate the ability of SPDMark to generate imperceptible watermarks that can be recovered with high accuracy and also establish its robustness against a variety of common video modifications.", "AI": {"tldr": "SPDMark\u662f\u4e00\u79cd\u57fa\u4e8e\u9009\u62e9\u6027\u53c2\u6570\u4f4d\u79fb\u7684\u89c6\u9891\u751f\u6210\u6c34\u5370\u6846\u67b6\uff0c\u901a\u8fc7\u4fee\u6539\u751f\u6210\u6a21\u578b\u53c2\u6570\u5b50\u96c6\u5d4c\u5165\u6c34\u5370\uff0c\u5229\u7528\u4f4e\u79e9\u9002\u914d\u5b9e\u73b0\u53c2\u6570\u6548\u7387\uff0c\u652f\u6301\u9ad8\u7cbe\u5ea6\u6c34\u5370\u6062\u590d\u5e76\u62b5\u6297\u591a\u79cd\u89c6\u9891\u4fee\u6539\u3002", "motivation": "\u9ad8\u8d28\u91cf\u89c6\u9891\u751f\u6210\u6a21\u578b\u7684\u5174\u8d77\u589e\u52a0\u4e86\u5bf9\u9c81\u68d2\u6c34\u5370\u65b9\u6848\u7684\u9700\u6c42\uff0c\u73b0\u6709\u89c6\u9891\u6c34\u5370\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u5b9e\u73b0\u4e0d\u53ef\u611f\u77e5\u6027\u3001\u9c81\u68d2\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u57fa\u4e8e\u9009\u62e9\u6027\u53c2\u6570\u4f4d\u79fb\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\u6c34\u5370\u6846\u67b6\uff0c\u5c06\u4f4d\u79fb\u5efa\u6a21\u4e3a\u5c42\u95f4\u57fa\u79fb\u7684\u52a0\u6027\u7ec4\u5408\uff0c\u5229\u7528\u4f4e\u79e9\u9002\u914d\u5b9e\u73b0\u53c2\u6570\u6548\u7387\uff0c\u8054\u5408\u8bad\u7ec3\u57fa\u79fb\u548c\u6c34\u5370\u63d0\u53d6\u5668\uff0c\u4f7f\u7528\u52a0\u5bc6\u54c8\u5e0c\u51fd\u6570\u751f\u6210\u5e27\u7279\u5b9a\u6c34\u5370\u6d88\u606f\uff0c\u901a\u8fc7\u6700\u5927\u4e8c\u5206\u56fe\u5339\u914d\u6062\u590d\u5e27\u987a\u5e8f\u3002", "result": "\u5728\u6587\u672c\u5230\u89c6\u9891\u548c\u56fe\u50cf\u5230\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cSPDMark\u80fd\u591f\u751f\u6210\u4e0d\u53ef\u611f\u77e5\u7684\u6c34\u5370\uff0c\u5e76\u4ee5\u9ad8\u7cbe\u5ea6\u6062\u590d\u6c34\u5370\uff0c\u540c\u65f6\u5bf9\u591a\u79cd\u5e38\u89c1\u89c6\u9891\u4fee\u6539\u5177\u6709\u9c81\u68d2\u6027\u3002", "conclusion": "SPDMark\u4e3a\u89c6\u9891\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u9c81\u68d2\u4e14\u4e0d\u53ef\u611f\u77e5\u7684\u6c34\u5370\u65b9\u6848\uff0c\u80fd\u591f\u53ef\u9760\u5730\u68c0\u6d4b\u548c\u8ddf\u8e2a\u751f\u6210\u89c6\u9891\u7684\u6765\u6e90\u3002"}}
{"id": "2512.11934", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.11934", "abs": "https://arxiv.org/abs/2512.11934", "authors": ["Adeleh Mazaherian", "Erfan Nourbakhsh"], "title": "Unveiling User Perceptions in the Generative AI Era: A Sentiment-Driven Evaluation of AI Educational Apps' Role in Digital Transformation of e-Teaching", "comment": "6 pages, 4 figures", "summary": "The rapid integration of generative artificial intelligence into education has driven digital transformation in e-teaching, yet user perceptions of AI educational apps remain underexplored. This study performs a sentiment-driven evaluation of user reviews from top AI ed-apps on the Google Play Store to assess efficacy, challenges, and pedagogical implications. Our pipeline involved scraping app data and reviews, RoBERTa for binary sentiment classification, GPT-4o for key point extraction, and GPT-5 for synthesizing top positive/negative themes. Apps were categorized into seven types (e.g., homework helpers, math solvers, language tools), with overlaps reflecting multifunctional designs. Results indicate predominantly positive sentiments, with homework apps like Edu AI (95.9% positive) and Answer.AI (92.7%) leading in accuracy, speed, and personalization, while language/LMS apps (e.g., Teacher AI at 21.8% positive) lag due to instability and limited features. Positives emphasize efficiency in brainstorming, problem-solving, and engagement; negatives center on paywalls, inaccuracies, ads, and glitches. Trends show that homework helpers outperform specialized tools, highlighting AI's democratizing potential amid risks of dependency and inequity. The discussion proposes future ecosystems with hybrid AI-human models, VR/AR for immersive learning, and a roadmap for developers (adaptive personalization) and policymakers (monetization regulation for inclusivity). This underscores generative AI's role in advancing e-teaching by enabling ethical refinements that foster equitable, innovative environments. The full dataset is available here(https://github.com/erfan-nourbakhsh/GenAI-EdSent).", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u60c5\u611f\u5206\u6790\u8bc4\u4f30Google Play\u5546\u5e97\u4e2dAI\u6559\u80b2\u5e94\u7528\u7684\u7528\u6237\u8bc4\u4ef7\uff0c\u53d1\u73b0\u4f5c\u4e1a\u52a9\u624b\u7c7b\u5e94\u7528\uff08\u5982Edu AI\uff09\u83b7\u5f97\u6700\u9ad8\u6b63\u9762\u8bc4\u4ef7\uff0895.9%\uff09\uff0c\u800c\u8bed\u8a00/LMS\u7c7b\u5e94\u7528\uff08\u5982Teacher AI\uff09\u6b63\u9762\u8bc4\u4ef7\u8f83\u4f4e\uff0821.8%\uff09\uff0c\u4e3b\u8981\u95ee\u9898\u5305\u62ec\u4ed8\u8d39\u5899\u3001\u4e0d\u51c6\u786e\u6027\u548c\u6280\u672f\u6545\u969c\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5728\u6559\u80b2\u9886\u57df\u7684\u5feb\u901f\u6574\u5408\uff0c\u6570\u5b57\u6559\u5b66\u6b63\u5728\u7ecf\u5386\u8f6c\u578b\uff0c\u4f46\u7528\u6237\u5bf9AI\u6559\u80b2\u5e94\u7528\u7684\u770b\u6cd5\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5206\u6790\u7528\u6237\u8bc4\u4ef7\u6765\u8bc4\u4f30AI\u6559\u80b2\u5e94\u7528\u7684\u6709\u6548\u6027\u3001\u6311\u6218\u548c\u6559\u5b66\u610f\u4e49\u3002", "method": "\u7814\u7a76\u91c7\u7528\u60c5\u611f\u5206\u6790\u7ba1\u9053\uff1a\u4eceGoogle Play\u5546\u5e97\u6293\u53d6\u5e94\u7528\u6570\u636e\u548c\u7528\u6237\u8bc4\u4ef7\uff0c\u4f7f\u7528RoBERTa\u8fdb\u884c\u4e8c\u5143\u60c5\u611f\u5206\u7c7b\uff0cGPT-4o\u63d0\u53d6\u5173\u952e\u89c2\u70b9\uff0cGPT-5\u5408\u6210\u4e3b\u8981\u6b63\u9762/\u8d1f\u9762\u4e3b\u9898\u3002\u5c06\u5e94\u7528\u5206\u4e3a\u4e03\u7c7b\uff08\u4f5c\u4e1a\u52a9\u624b\u3001\u6570\u5b66\u89e3\u9898\u5de5\u5177\u3001\u8bed\u8a00\u5de5\u5177\u7b49\uff09\u3002", "result": "\u7ed3\u679c\u663e\u793a\u6574\u4f53\u60c5\u611f\u4ee5\u6b63\u9762\u4e3a\u4e3b\u3002\u4f5c\u4e1a\u52a9\u624b\u7c7b\u5e94\u7528\uff08\u5982Edu AI 95.9%\u6b63\u9762\uff0cAnswer.AI 92.7%\u6b63\u9762\uff09\u5728\u51c6\u786e\u6027\u3001\u901f\u5ea6\u548c\u4e2a\u6027\u5316\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff1b\u8bed\u8a00/LMS\u7c7b\u5e94\u7528\uff08\u5982Teacher AI 21.8%\u6b63\u9762\uff09\u56e0\u4e0d\u7a33\u5b9a\u6027\u548c\u529f\u80fd\u6709\u9650\u800c\u8868\u73b0\u8f83\u5dee\u3002\u6b63\u9762\u8bc4\u4ef7\u5f3a\u8c03\u6548\u7387\u63d0\u5347\u3001\u95ee\u9898\u89e3\u51b3\u548c\u53c2\u4e0e\u5ea6\uff1b\u8d1f\u9762\u8bc4\u4ef7\u96c6\u4e2d\u5728\u4ed8\u8d39\u5899\u3001\u4e0d\u51c6\u786e\u6027\u3001\u5e7f\u544a\u548c\u6280\u672f\u6545\u969c\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u5728\u6559\u80b2\u9886\u57df\u5177\u6709\u6c11\u4e3b\u5316\u6f5c\u529b\uff0c\u4f46\u4e5f\u5b58\u5728\u4f9d\u8d56\u6027\u548c\u4e0d\u5e73\u7b49\u98ce\u9669\u3002\u672a\u6765\u5e94\u53d1\u5c55\u6df7\u5408AI-\u4eba\u7c7b\u6559\u5b66\u6a21\u5f0f\u3001VR/AR\u6c89\u6d78\u5f0f\u5b66\u4e60\uff0c\u5f00\u53d1\u8005\u9700\u6539\u8fdb\u81ea\u9002\u5e94\u4e2a\u6027\u5316\uff0c\u653f\u7b56\u5236\u5b9a\u8005\u5e94\u76d1\u7ba1\u5546\u4e1a\u5316\u4ee5\u786e\u4fdd\u5305\u5bb9\u6027\u3002\u7814\u7a76\u5f3a\u8c03\u901a\u8fc7\u4f26\u7406\u6539\u8fdb\u4fc3\u8fdb\u516c\u5e73\u521b\u65b0\u7684\u6559\u80b2\u73af\u5883\u3002"}}
{"id": "2512.12597", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12597", "abs": "https://arxiv.org/abs/2512.12597", "authors": ["Miriam Horovicz"], "title": "AgentSHAP: Interpreting LLM Agent Tool Importance with Monte Carlo Shapley Value Estimation", "comment": null, "summary": "LLM agents that use external tools can solve complex tasks, but understanding which tools actually contributed to a response remains a blind spot. No existing XAI methods address tool-level explanations. We introduce AgentSHAP, the first framework for explaining tool importance in LLM agents. AgentSHAP is model-agnostic: it treats the agent as a black box and works with any LLM (GPT, Claude, Llama, etc.) without needing access to internal weights or gradients. Using Monte Carlo Shapley values, AgentSHAP tests how an agent responds with different tool subsets and computes fair importance scores based on game theory. Our contributions are: (1) the first explainability method for agent tool attribution, grounded in Shapley values from game theory; (2) Monte Carlo sampling that reduces cost from O(2n) to practical levels; and (3) comprehensive experiments on API-Bank showing that AgentSHAP produces consistent scores across runs, correctly identifies which tools matter, and distinguishes relevant from irrelevant tools. AgentSHAP joins TokenSHAP (for tokens) and PixelSHAP (for image regions) to complete a family of Shapley-based XAI tools for modern generative AI. Code: https://github.com/GenAISHAP/TokenSHAP.", "AI": {"tldr": "AgentSHAP\u662f\u9996\u4e2a\u7528\u4e8e\u89e3\u91caLLM\u667a\u80fd\u4f53\u4e2d\u5de5\u5177\u91cd\u8981\u6027\u7684\u6846\u67b6\uff0c\u57fa\u4e8e\u535a\u5f08\u8bba\u4e2d\u7684Shapley\u503c\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u91c7\u6837\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u65e0\u9700\u8bbf\u95ee\u6a21\u578b\u5185\u90e8\u6743\u91cd", "motivation": "\u5f53\u524dLLM\u667a\u80fd\u4f53\u867d\u7136\u80fd\u4f7f\u7528\u5916\u90e8\u5de5\u5177\u89e3\u51b3\u590d\u6742\u4efb\u52a1\uff0c\u4f46\u7f3a\u4e4f\u89e3\u91ca\u54ea\u4e9b\u5de5\u5177\u771f\u6b63\u5bf9\u54cd\u5e94\u505a\u51fa\u8d21\u732e\u7684\u65b9\u6cd5\uff0c\u73b0\u6709XAI\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u5de5\u5177\u5c42\u9762\u7684\u89e3\u91ca\u95ee\u9898", "method": "\u57fa\u4e8e\u535a\u5f08\u8bbaShapley\u503c\u7684\u6a21\u578b\u65e0\u5173\u6846\u67b6\uff0c\u5c06\u667a\u80fd\u4f53\u89c6\u4e3a\u9ed1\u76d2\uff0c\u901a\u8fc7\u8499\u7279\u5361\u6d1b\u91c7\u6837\u6d4b\u8bd5\u4e0d\u540c\u5de5\u5177\u5b50\u96c6\u4e0b\u7684\u667a\u80fd\u4f53\u54cd\u5e94\uff0c\u8ba1\u7b97\u516c\u5e73\u7684\u91cd\u8981\u6027\u5206\u6570", "result": "\u5728API-Bank\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAgentSHAP\u80fd\u4ea7\u751f\u8de8\u8fd0\u884c\u4e00\u81f4\u7684\u91cd\u8981\u6027\u5206\u6570\uff0c\u6b63\u786e\u8bc6\u522b\u91cd\u8981\u5de5\u5177\uff0c\u5e76\u80fd\u533a\u5206\u76f8\u5173\u4e0e\u4e0d\u76f8\u5173\u5de5\u5177", "conclusion": "AgentSHAP\u586b\u8865\u4e86\u667a\u80fd\u4f53\u5de5\u5177\u5c42\u9762\u89e3\u91ca\u7684\u7a7a\u767d\uff0c\u4e0eTokenSHAP\u548cPixelSHAP\u5171\u540c\u6784\u6210\u4e86\u57fa\u4e8eShapley\u503c\u7684\u73b0\u4ee3\u751f\u6210AI\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u5bb6\u65cf"}}
{"id": "2512.11986", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.11986", "abs": "https://arxiv.org/abs/2512.11986", "authors": ["Minseon Kim", "Lucas Caccia", "Zhengyan Shi", "Matheus Pereira", "Marc-Alexandre C\u00f4t\u00e9", "Xingdi Yuan", "Alessandro Sordoni"], "title": "Learning to Extract Context for Context-Aware LLM Inference", "comment": null, "summary": "User prompts to large language models (LLMs) are often ambiguous or under-specified, and subtle contextual cues shaped by user intentions, prior knowledge, and risk factors strongly influence what constitutes an appropriate response. Misinterpreting intent or risks may lead to unsafe outputs, while overly cautious interpretations can cause unnecessary refusal of benign requests. In this paper, we question the conventional framework in which LLMs generate immediate responses to requests without considering broader contextual factors. User requests are situated within broader contexts such as intentions, knowledge, and prior experience, which strongly influence what constitutes an appropriate answer. We propose a framework that extracts and leverages such contextual information from the user prompt itself. Specifically, a reinforcement learning based context generator, designed in an autoencoder-like fashion, is trained to infer contextual signals grounded in the prompt and use them to guide response generation. This approach is particularly important for safety tasks, where ambiguous requests may bypass safeguards while benign but confusing requests can trigger unnecessary refusals. Experiments show that our method reduces harmful responses by an average of 5.6% on the SafetyInstruct dataset across multiple foundation models and improves the harmonic mean of attack success rate and compliance on benign prompts by 6.2% on XSTest and WildJailbreak. These results demonstrate the effectiveness of context extraction for safer and more reliable LLM inferences.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4e0a\u4e0b\u6587\u751f\u6210\u6846\u67b6\uff0c\u4ece\u7528\u6237\u63d0\u793a\u4e2d\u63d0\u53d6\u610f\u56fe\u3001\u77e5\u8bc6\u548c\u98ce\u9669\u7b49\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u6307\u5bfcLLM\u751f\u6210\u66f4\u5b89\u5168\u3001\u66f4\u5408\u9002\u7684\u54cd\u5e94", "motivation": "\u4f20\u7edfLLM\u6846\u67b6\u5728\u751f\u6210\u54cd\u5e94\u65f6\u672a\u5145\u5206\u8003\u8651\u7528\u6237\u610f\u56fe\u3001\u5148\u9a8c\u77e5\u8bc6\u548c\u98ce\u9669\u56e0\u7d20\u7b49\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5bfc\u81f4\u5bf9\u6a21\u7cca\u8bf7\u6c42\u53ef\u80fd\u4ea7\u751f\u4e0d\u5b89\u5168\u8f93\u51fa\uff0c\u6216\u5bf9\u826f\u6027\u8bf7\u6c42\u8fc7\u5ea6\u62d2\u7edd", "method": "\u8bbe\u8ba1\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u4e0a\u4e0b\u6587\u751f\u6210\u5668\uff0c\u91c7\u7528\u7c7b\u4f3c\u81ea\u7f16\u7801\u5668\u7684\u7ed3\u6784\uff0c\u4ece\u7528\u6237\u63d0\u793a\u4e2d\u63a8\u65ad\u4e0a\u4e0b\u6587\u4fe1\u53f7\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u4fe1\u53f7\u6307\u5bfc\u54cd\u5e94\u751f\u6210", "result": "\u5728SafetyInstruct\u6570\u636e\u96c6\u4e0a\u5e73\u5747\u51cf\u5c115.6%\u7684\u6709\u5bb3\u54cd\u5e94\uff0c\u5728XSTest\u548cWildJailbreak\u4e0a\u826f\u6027\u63d0\u793a\u7684\u653b\u51fb\u6210\u529f\u7387\u548c\u5408\u89c4\u6027\u7684\u8c03\u548c\u5e73\u5747\u503c\u63d0\u9ad86.2%", "conclusion": "\u4e0a\u4e0b\u6587\u63d0\u53d6\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u9ad8LLM\u63a8\u7406\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\uff0c\u7279\u522b\u662f\u5728\u5b89\u5168\u4efb\u52a1\u4e2d\u80fd\u66f4\u597d\u5730\u533a\u5206\u6a21\u7cca\u8bf7\u6c42\u548c\u826f\u6027\u8bf7\u6c42"}}
{"id": "2512.12101", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12101", "abs": "https://arxiv.org/abs/2512.12101", "authors": ["Swarn S. Warshaneyan", "Maksims Ivanovs", "Bla\u017e Cugmas", "Inese B\u0113rzi\u0146a", "Laura Goldberga", "Mindaugas Tamosiunas", "Roberts Kadi\u0137is"], "title": "AI-Augmented Pollen Recognition in Optical and Holographic Microscopy for Veterinary Imaging", "comment": "10 pages, 10 figures, 2 tables, 22 references. Journal submission undergoing peer review", "summary": "We present a comprehensive study on fully automated pollen recognition across both conventional optical and digital in-line holographic microscopy (DIHM) images of sample slides. Visually recognizing pollen in unreconstructed holographic images remains challenging due to speckle noise, twin-image artifacts and substantial divergence from bright-field appearances. We establish the performance baseline by training YOLOv8s for object detection and MobileNetV3L for classification on a dual-modality dataset of automatically annotated optical and affinely aligned DIHM images. On optical data, detection mAP50 reaches 91.3% and classification accuracy reaches 97%, whereas on DIHM data, we achieve only 8.15% for detection mAP50 and 50% for classification accuracy. Expanding the bounding boxes of pollens in DIHM images over those acquired in aligned optical images achieves 13.3% for detection mAP50 and 54% for classification accuracy. To improve object detection in DIHM images, we employ a Wasserstein GAN with spectral normalization (WGAN-SN) to create synthetic DIHM images, yielding an FID score of 58.246. Mixing real-world and synthetic data at the 1.0 : 1.5 ratio for DIHM images improves object detection up to 15.4%. These results demonstrate that GAN-based augmentation can reduce the performance divide, bringing fully automated DIHM workflows for veterinary imaging a small but important step closer to practice.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u5728\u4f20\u7edf\u5149\u5b66\u663e\u5fae\u955c\u548c\u6570\u5b57\u540c\u8f74\u5168\u606f\u663e\u5fae\u955c(DIHM)\u56fe\u50cf\u4e0a\u5b9e\u73b0\u5168\u81ea\u52a8\u82b1\u7c89\u8bc6\u522b\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u5e76\u901a\u8fc7GAN\u751f\u6210\u5408\u6210DIHM\u56fe\u50cf\u6765\u6539\u5584\u68c0\u6d4b\u6548\u679c\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u89e3\u51b3DIHM\u56fe\u50cf\u4e2d\u82b1\u7c89\u8bc6\u522b\u9762\u4e34\u7684\u6311\u6218\uff0c\u5305\u62ec\u6563\u6591\u566a\u58f0\u3001\u5b6a\u751f\u50cf\u4f2a\u5f71\u4ee5\u53ca\u4e0e\u660e\u573a\u56fe\u50cf\u7684\u663e\u8457\u5916\u89c2\u5dee\u5f02\uff0c\u8fd9\u4e9b\u56e0\u7d20\u4f7f\u5f97\u5728\u672a\u91cd\u5efa\u7684\u5168\u606f\u56fe\u50cf\u4e2d\u89c6\u89c9\u8bc6\u522b\u82b1\u7c89\u53d8\u5f97\u56f0\u96be\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u5305\u62ec\uff1a1) \u4f7f\u7528YOLOv8s\u8fdb\u884c\u76ee\u6807\u68c0\u6d4b\u548cMobileNetV3L\u8fdb\u884c\u5206\u7c7b\uff1b2) \u5728\u53cc\u6a21\u6001\u6570\u636e\u96c6\uff08\u81ea\u52a8\u6807\u6ce8\u7684\u5149\u5b66\u56fe\u50cf\u548c\u4eff\u5c04\u5bf9\u9f50\u7684DIHM\u56fe\u50cf\uff09\u4e0a\u8bad\u7ec3\uff1b3) \u6269\u5c55DIHM\u56fe\u50cf\u4e2d\u82b1\u7c89\u7684\u8fb9\u754c\u6846\uff1b4) \u4f7f\u7528\u5e26\u8c31\u5f52\u4e00\u5316\u7684Wasserstein GAN(WGAN-SN)\u751f\u6210\u5408\u6210DIHM\u56fe\u50cf\uff1b5) \u6df7\u5408\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728\u5149\u5b66\u6570\u636e\u4e0a\uff0c\u68c0\u6d4bmAP50\u8fbe\u523091.3%\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u8fbe\u523097%\uff1b\u800c\u5728DIHM\u6570\u636e\u4e0a\uff0c\u68c0\u6d4bmAP50\u4ec5\u4e3a8.15%\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u4e3a50%\u3002\u6269\u5c55\u8fb9\u754c\u6846\u540e\uff0cDIHM\u68c0\u6d4bmAP50\u63d0\u5347\u81f313.3%\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u63d0\u5347\u81f354%\u3002\u4f7f\u7528GAN\u751f\u6210\u5408\u6210\u56fe\u50cf(FID\u5206\u657058.246)\u5e76\u4ee51:1.5\u6bd4\u4f8b\u6df7\u5408\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u540e\uff0c\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\u63d0\u5347\u81f315.4%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8eGAN\u7684\u6570\u636e\u589e\u5f3a\u53ef\u4ee5\u7f29\u5c0f\u5149\u5b66\u548cDIHM\u56fe\u50cf\u8bc6\u522b\u6027\u80fd\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u517d\u533b\u6210\u50cf\u9886\u57df\u7684\u5168\u81ea\u52a8DIHM\u5de5\u4f5c\u6d41\u7a0b\u5411\u5b9e\u9645\u5e94\u7528\u8fc8\u51fa\u4e86\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2512.12105", "categories": ["cs.CY", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.12105", "abs": "https://arxiv.org/abs/2512.12105", "authors": ["Andreia dos Santos Sachete", "Alba Valeria de SantAnna de Freitas Loiola", "Fabio Diniz Rossi", "Jose Valdeni de Lima", "Raquel Salcedo Gomes"], "title": "Beyond right or wrong : towards redefining adaptive learning indicators in virtual learning environments", "comment": "20 pages, 1 figure, 1 table", "summary": "Student learning development must involve more than just correcting or incorrect questions. However, most adaptive learning methods in Virtual Learning Environments are based on whether the student's response is incorrect or correct. This perspective is limited in assessing the student's learning level, as it does not consider other elements that can be crucial in this process. The objective of this work is to conduct a Systematic Literature Review (SLR) to elucidate which learning indicators influence student learning and which can be implemented in a VLE to assist in adaptive learning. The works selected and filtered by qualitative assessment reveal a comprehensive approach to assessing different aspects of the learning in virtual environments, such as motivation, emotions, physiological responses, brain imaging, and the students' prior knowledge. The discussion of these new indicators allows adaptive technology developers to implement more appropriate solutions to students' realities, resulting in more complete training.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff0c\u63a2\u8ba8\u4e86\u8d85\u8d8a\u4f20\u7edf\u5bf9\u9519\u8bc4\u4f30\u7684\u5b66\u4e60\u6307\u6807\uff0c\u4e3a\u865a\u62df\u5b66\u4e60\u73af\u5883\u4e2d\u7684\u81ea\u9002\u5e94\u5b66\u4e60\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u5f53\u524d\u865a\u62df\u5b66\u4e60\u73af\u5883\u4e2d\u7684\u81ea\u9002\u5e94\u5b66\u4e60\u65b9\u6cd5\u5927\u591a\u4ec5\u57fa\u4e8e\u5b66\u751f\u56de\u7b54\u7684\u6b63\u786e\u4e0e\u5426\uff0c\u8fd9\u79cd\u8bc4\u4f30\u89c6\u89d2\u6709\u9650\uff0c\u65e0\u6cd5\u5168\u9762\u8861\u91cf\u5b66\u751f\u7684\u5b66\u4e60\u6c34\u5e73\uff0c\u5ffd\u7565\u4e86\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u7684\u5176\u4ed6\u5173\u952e\u8981\u7d20\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9a\u6027\u8bc4\u4f30\u7b5b\u9009\u76f8\u5173\u7814\u7a76\uff0c\u8bc6\u522b\u5f71\u54cd\u5b66\u751f\u5b66\u4e60\u7684\u5b66\u4e60\u6307\u6807\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e86\u4e00\u7cfb\u5217\u5168\u9762\u7684\u5b66\u4e60\u8bc4\u4f30\u6307\u6807\uff0c\u5305\u62ec\u52a8\u673a\u3001\u60c5\u7eea\u3001\u751f\u7406\u53cd\u5e94\u3001\u8111\u6210\u50cf\u548c\u5b66\u751f\u5148\u524d\u77e5\u8bc6\u7b49\uff0c\u8fd9\u4e9b\u6307\u6807\u80fd\u591f\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u865a\u62df\u73af\u5883\u4e2d\u7684\u5b66\u4e60\u8fc7\u7a0b\u3002", "conclusion": "\u8fd9\u4e9b\u65b0\u7684\u5b66\u4e60\u6307\u6807\u4e3a\u81ea\u9002\u5e94\u6280\u672f\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u7b26\u5408\u5b66\u751f\u5b9e\u9645\u60c5\u51b5\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u5b8c\u6574\u7684\u6559\u80b2\u57f9\u8bad\u3002"}}
{"id": "2512.12004", "categories": ["cs.LG", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.12004", "abs": "https://arxiv.org/abs/2512.12004", "authors": ["Troy Allen"], "title": "EnviroLLM: Resource Tracking and Optimization for Local AI", "comment": "8 pages, 3 tables", "summary": "Large language models (LLMs) are increasingly deployed locally for privacy and accessibility, yet users lack tools to measure their resource usage, environmental impact, and efficiency metrics. This paper presents EnviroLLM, an open-source toolkit for tracking, benchmarking, and optimizing performance and energy consumption when running LLMs on personal devices. The system provides real-time process monitoring, benchmarking across multiple platforms (Ollama, LM Studio, vLLM, and OpenAI-compatible APIs), persistent storage with visualizations for longitudinal analysis, and personalized model and optimization recommendations. The system includes LLM-as-judge evaluations alongside energy and speed metrics, enabling users to assess quality-efficiency tradeoffs when testing models with custom prompts.", "AI": {"tldr": "EnviroLLM\u662f\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\u5305\uff0c\u7528\u4e8e\u5728\u4e2a\u4eba\u8bbe\u5907\u4e0a\u8ddf\u8e2a\u3001\u57fa\u51c6\u6d4b\u8bd5\u548c\u4f18\u5316LLM\u7684\u6027\u80fd\u4e0e\u80fd\u8017\uff0c\u5e2e\u52a9\u7528\u6237\u8bc4\u4f30\u8d44\u6e90\u4f7f\u7528\u3001\u73af\u5883\u5f71\u54cd\u548c\u6548\u7387\u6307\u6807\u3002", "motivation": "\u968f\u7740LLM\u8d8a\u6765\u8d8a\u591a\u5730\u5728\u672c\u5730\u90e8\u7f72\u4ee5\u4fdd\u62a4\u9690\u79c1\u548c\u63d0\u5347\u53ef\u8bbf\u95ee\u6027\uff0c\u7528\u6237\u7f3a\u4e4f\u6d4b\u91cf\u5176\u8d44\u6e90\u4f7f\u7528\u3001\u73af\u5883\u5f71\u54cd\u548c\u6548\u7387\u6307\u6807\u7684\u5de5\u5177\u3002", "method": "\u5f00\u53d1\u4e86EnviroLLM\u5de5\u5177\u5305\uff0c\u63d0\u4f9b\u5b9e\u65f6\u8fdb\u7a0b\u76d1\u63a7\u3001\u8de8\u591a\u4e2a\u5e73\u53f0\uff08Ollama\u3001LM Studio\u3001vLLM\u548cOpenAI\u517c\u5bb9API\uff09\u7684\u57fa\u51c6\u6d4b\u8bd5\u3001\u6301\u4e45\u5316\u5b58\u50a8\u4e0e\u53ef\u89c6\u5316\u5206\u6790\uff0c\u4ee5\u53ca\u4e2a\u6027\u5316\u6a21\u578b\u548c\u4f18\u5316\u5efa\u8bae\u3002", "result": "\u7cfb\u7edf\u5305\u542bLLM-as-judge\u8bc4\u4f30\u4ee5\u53ca\u80fd\u8017\u548c\u901f\u5ea6\u6307\u6807\uff0c\u4f7f\u7528\u6237\u80fd\u591f\u5728\u6d4b\u8bd5\u81ea\u5b9a\u4e49\u63d0\u793a\u7684\u6a21\u578b\u65f6\u8bc4\u4f30\u8d28\u91cf-\u6548\u7387\u6743\u8861\u3002", "conclusion": "EnviroLLM\u4e3a\u672c\u5730LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u6027\u80fd\u76d1\u63a7\u548c\u4f18\u5316\u5de5\u5177\uff0c\u5e2e\u52a9\u7528\u6237\u5728\u9690\u79c1\u4fdd\u62a4\u7684\u540c\u65f6\u5b9e\u73b0\u66f4\u9ad8\u6548\u3001\u73af\u4fdd\u7684\u6a21\u578b\u8fd0\u884c\u3002"}}
{"id": "2512.12107", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12107", "abs": "https://arxiv.org/abs/2512.12107", "authors": ["Yuheng Li", "Yue Zhang", "Abdoul Aziz Amadou", "Yuxiang Lai", "Jike Zhong", "Tiziano Passerini", "Dorin Comaniciu", "Puneet Sharma"], "title": "EchoVLM: Measurement-Grounded Multimodal Learning for Echocardiography", "comment": null, "summary": "Echocardiography is the most widely used imaging modality in cardiology, yet its interpretation remains labor-intensive and inherently multimodal, requiring view recognition, quantitative measurements, qualitative assessments, and guideline-based reasoning. While recent vision-language models (VLMs) have achieved broad success in natural images and certain medical domains, their potential in echocardiography has been limited by the lack of large-scale, clinically grounded image-text datasets and the absence of measurement-based reasoning central to echo interpretation. We introduce EchoGround-MIMIC, the first measurement-grounded multimodal echocardiography dataset, comprising 19,065 image-text pairs from 1,572 patients with standardized views, structured measurements, measurement-grounded captions, and guideline-derived disease labels. Building on this resource, we propose EchoVLM, a vision-language model that incorporates two novel pretraining objectives: (i) a view-informed contrastive loss that encodes the view-dependent structure of echocardiographic imaging, and (ii) a negation-aware contrastive loss that distinguishes clinically critical negative from positive findings. Across five types of clinical applications with 36 tasks spanning multimodal disease classification, image-text retrieval, view classification, chamber segmentation, and landmark detection, EchoVLM achieves state-of-the-art performance (86.5% AUC in zero-shot disease classification and 95.1% accuracy in view classification). We demonstrate that clinically grounded multimodal pretraining yields transferable visual representations and establish EchoVLM as a foundation model for end-to-end echocardiography interpretation. We will release EchoGround-MIMIC and the data curation code, enabling reproducibility and further research in multimodal echocardiography interpretation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86EchoGround-MIMIC\u6570\u636e\u96c6\u548cEchoVLM\u6a21\u578b\uff0c\u8fd9\u662f\u9996\u4e2a\u57fa\u4e8e\u6d4b\u91cf\u7684\u591a\u6a21\u6001\u8d85\u58f0\u5fc3\u52a8\u56fe\u6570\u636e\u96c6\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8d85\u58f0\u5fc3\u52a8\u56fe\u81ea\u52a8\u89e3\u8bfb\u7684\u6027\u80fd\u3002", "motivation": "\u8d85\u58f0\u5fc3\u52a8\u56fe\u662f\u5fc3\u810f\u75c5\u5b66\u4e2d\u6700\u5e7f\u6cdb\u4f7f\u7528\u7684\u6210\u50cf\u65b9\u5f0f\uff0c\u4f46\u5176\u89e3\u8bfb\u4ecd\u7136\u52b3\u52a8\u5bc6\u96c6\u4e14\u672c\u8d28\u4e0a\u662f\u591a\u6a21\u6001\u7684\uff0c\u9700\u8981\u89c6\u56fe\u8bc6\u522b\u3001\u5b9a\u91cf\u6d4b\u91cf\u3001\u5b9a\u6027\u8bc4\u4f30\u548c\u57fa\u4e8e\u6307\u5357\u7684\u63a8\u7406\u3002\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8d85\u58f0\u5fc3\u52a8\u56fe\u9886\u57df\u7684\u5e94\u7528\u53d7\u5230\u7f3a\u4e4f\u5927\u89c4\u6a21\u4e34\u5e8a\u57fa\u7840\u6570\u636e\u96c6\u548c\u6d4b\u91cf\u63a8\u7406\u80fd\u529b\u7684\u9650\u5236\u3002", "method": "1. \u521b\u5efaEchoGround-MIMIC\u6570\u636e\u96c6\uff1a\u5305\u542b19,065\u4e2a\u56fe\u50cf-\u6587\u672c\u5bf9\uff0c\u6765\u81ea1,572\u540d\u60a3\u8005\uff0c\u5177\u6709\u6807\u51c6\u5316\u89c6\u56fe\u3001\u7ed3\u6784\u5316\u6d4b\u91cf\u3001\u57fa\u4e8e\u6d4b\u91cf\u7684\u63cf\u8ff0\u548c\u6307\u5357\u884d\u751f\u7684\u75be\u75c5\u6807\u7b7e\u30022. \u63d0\u51faEchoVLM\u6a21\u578b\uff1a\u5f15\u5165\u4e24\u79cd\u65b0\u7684\u9884\u8bad\u7ec3\u76ee\u6807\uff1a(i)\u89c6\u56fe\u611f\u77e5\u5bf9\u6bd4\u635f\u5931\uff0c\u7f16\u7801\u8d85\u58f0\u5fc3\u52a8\u56fe\u6210\u50cf\u7684\u89c6\u56fe\u4f9d\u8d56\u7ed3\u6784\uff1b(ii)\u5426\u5b9a\u611f\u77e5\u5bf9\u6bd4\u635f\u5931\uff0c\u533a\u5206\u4e34\u5e8a\u5173\u952e\u7684\u9634\u6027\u4e0e\u9633\u6027\u53d1\u73b0\u3002", "result": "\u5728\u6db5\u76d6\u591a\u6a21\u6001\u75be\u75c5\u5206\u7c7b\u3001\u56fe\u50cf-\u6587\u672c\u68c0\u7d22\u3001\u89c6\u56fe\u5206\u7c7b\u3001\u8154\u5ba4\u5206\u5272\u548c\u5730\u6807\u68c0\u6d4b\u768436\u4e2a\u4efb\u52a1\u4e2d\uff0cEchoVLM\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff1a\u96f6\u6837\u672c\u75be\u75c5\u5206\u7c7bAUC\u4e3a86.5%\uff0c\u89c6\u56fe\u5206\u7c7b\u51c6\u786e\u7387\u4e3a95.1%\u3002\u4e34\u5e8a\u57fa\u7840\u7684\u591a\u6a21\u6001\u9884\u8bad\u7ec3\u4ea7\u751f\u4e86\u53ef\u8fc1\u79fb\u7684\u89c6\u89c9\u8868\u793a\u3002", "conclusion": "EchoVLM\u88ab\u786e\u7acb\u4e3a\u7aef\u5230\u7aef\u8d85\u58f0\u5fc3\u52a8\u56fe\u89e3\u8bfb\u7684\u57fa\u7840\u6a21\u578b\u3002EchoGround-MIMIC\u6570\u636e\u96c6\u548c\u6570\u636e\u5904\u7406\u4ee3\u7801\u5c06\u516c\u5f00\u53d1\u5e03\uff0c\u4fc3\u8fdb\u591a\u6a21\u6001\u8d85\u58f0\u5fc3\u52a8\u56fe\u89e3\u8bfb\u7684\u53ef\u91cd\u590d\u6027\u548c\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2512.12109", "categories": ["cs.CY", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2512.12109", "abs": "https://arxiv.org/abs/2512.12109", "authors": ["Allen Daniel Sunny"], "title": "A neuro-symbolic framework for accountability in public-sector AI", "comment": "Master's thesis, University of Maryland, College Park (2025)", "summary": "Automated eligibility systems increasingly determine access to essential public benefits, but the explanations they generate often fail to reflect the legal rules that authorize those decisions. This thesis develops a legally grounded explainability framework that links system-generated decision justifications to the statutory constraints of CalFresh, California's Supplemental Nutrition Assistance Program. The framework combines a structured ontology of eligibility requirements derived from the state's Manual of Policies and Procedures (MPP), a rule extraction pipeline that expresses statutory logic in a verifiable formal representation, and a solver-based reasoning layer to evaluate whether the explanation aligns with governing law. Case evaluations demonstrate the framework's ability to detect legally inconsistent explanations, highlight violated eligibility rules, and support procedural accountability by making the basis of automated determinations traceable and contestable.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6cd5\u5f8b\u7684\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30CalFresh\u798f\u5229\u8d44\u683c\u81ea\u52a8\u51b3\u7b56\u7cfb\u7edf\u7684\u89e3\u91ca\u662f\u5426\u7b26\u5408\u6cd5\u5b9a\u89c4\u5219", "motivation": "\u81ea\u52a8\u5316\u8d44\u683c\u7cfb\u7edf\u8d8a\u6765\u8d8a\u591a\u5730\u51b3\u5b9a\u516c\u5171\u798f\u5229\u7684\u83b7\u53d6\uff0c\u4f46\u8fd9\u4e9b\u7cfb\u7edf\u751f\u6210\u7684\u89e3\u91ca\u5f80\u5f80\u65e0\u6cd5\u53cd\u6620\u6388\u6743\u51b3\u7b56\u7684\u6cd5\u5f8b\u89c4\u5219\uff0c\u9700\u8981\u5efa\u7acb\u6cd5\u5f8b\u57fa\u7840\u7684\u53ef\u89e3\u91ca\u6027\u6846\u67b6", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u6cd5\u5f8b\u57fa\u7840\u7684\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u5305\u62ec\uff1a1\uff09\u4ece\u52a0\u5dde\u653f\u7b56\u548c\u7a0b\u5e8f\u624b\u518c\u4e2d\u63d0\u53d6\u8d44\u683c\u8981\u6c42\u7684\u7ed3\u6784\u5316\u672c\u4f53\uff1b2\uff09\u5c06\u6cd5\u5b9a\u903b\u8f91\u8868\u8fbe\u4e3a\u53ef\u9a8c\u8bc1\u5f62\u5f0f\u8868\u793a\u7684\u89c4\u5219\u63d0\u53d6\u7ba1\u9053\uff1b3\uff09\u57fa\u4e8e\u6c42\u89e3\u5668\u7684\u63a8\u7406\u5c42\u6765\u8bc4\u4f30\u89e3\u91ca\u662f\u5426\u7b26\u5408\u7ba1\u8f96\u6cd5\u5f8b", "result": "\u6848\u4f8b\u8bc4\u4f30\u8868\u660e\u8be5\u6846\u67b6\u80fd\u591f\u68c0\u6d4b\u6cd5\u5f8b\u4e0a\u4e0d\u4e00\u81f4\u7684\u89e3\u91ca\uff0c\u7a81\u51fa\u8fdd\u53cd\u7684\u8d44\u683c\u89c4\u5219\uff0c\u5e76\u901a\u8fc7\u4f7f\u81ea\u52a8\u51b3\u7b56\u7684\u57fa\u7840\u53ef\u8ffd\u6eaf\u548c\u53ef\u4e89\u8bae\u6765\u652f\u6301\u7a0b\u5e8f\u95ee\u8d23", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u81ea\u52a8\u5316\u798f\u5229\u8d44\u683c\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6cd5\u5f8b\u57fa\u7840\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u589e\u5f3a\u4e86\u51b3\u7b56\u7684\u900f\u660e\u5ea6\u3001\u53ef\u8ffd\u6eaf\u6027\u548c\u95ee\u8d23\u6027"}}
{"id": "2512.12652", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2512.12652", "abs": "https://arxiv.org/abs/2512.12652", "authors": ["Nardine Osman"], "title": "Value-Aware Multiagent Systems", "comment": null, "summary": "This paper introduces the concept of value awareness in AI, which goes beyond the traditional value-alignment problem. Our definition of value awareness presents us with a concise and simplified roadmap for engineering value-aware AI. The roadmap is structured around three core pillars: (1) learning and representing human values using formal semantics, (2) ensuring the value alignment of both individual agents and multiagent systems, and (3) providing value-based explainability on behaviour. The paper presents a selection of our ongoing work on some of these topics, along with applications to real-life domains.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4ef7\u503c\u611f\u77e5AI\u6982\u5ff5\uff0c\u8d85\u8d8a\u4f20\u7edf\u4ef7\u503c\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u4f9b\u5de5\u7a0b\u5316\u4ef7\u503c\u611f\u77e5AI\u7684\u7b80\u5316\u8def\u7ebf\u56fe\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u652f\u67f1\uff1a\u5b66\u4e60\u8868\u793a\u4eba\u7c7b\u4ef7\u503c\u3001\u786e\u4fdd\u4e2a\u4f53\u4e0e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4ef7\u503c\u5bf9\u9f50\u3001\u63d0\u4f9b\u57fa\u4e8e\u4ef7\u503c\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4f20\u7edfAI\u4ef7\u503c\u5bf9\u9f50\u95ee\u9898\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u4ef7\u503c\u611f\u77e5\u6846\u67b6\u3002\u672c\u6587\u65e8\u5728\u8d85\u8d8a\u5355\u7eaf\u7684\u4ef7\u503c\u5bf9\u9f50\uff0c\u5efa\u7acb\u7cfb\u7edf\u5316\u7684\u4ef7\u503c\u611f\u77e5AI\u5de5\u7a0b\u65b9\u6cd5\uff0c\u4f7fAI\u7cfb\u7edf\u4e0d\u4ec5\u80fd\u5bf9\u9f50\u4eba\u7c7b\u4ef7\u503c\uff0c\u8fd8\u80fd\u7406\u89e3\u3001\u8868\u793a\u548c\u89e3\u91ca\u57fa\u4e8e\u4ef7\u503c\u7684\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u4ef7\u503c\u611f\u77e5AI\u7684\u4e09\u652f\u67f1\u8def\u7ebf\u56fe\uff1a1) \u4f7f\u7528\u5f62\u5f0f\u8bed\u4e49\u5b66\u5b66\u4e60\u548c\u8868\u793a\u4eba\u7c7b\u4ef7\u503c\uff1b2) \u786e\u4fdd\u4e2a\u4f53\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u4ef7\u503c\u5bf9\u9f50\uff1b3) \u63d0\u4f9b\u57fa\u4e8e\u4ef7\u503c\u7684\u884c\u4e3a\u53ef\u89e3\u91ca\u6027\u3002\u5c55\u793a\u4e86\u5728\u8fd9\u4e9b\u4e3b\u9898\u4e0a\u7684\u6301\u7eed\u7814\u7a76\u5de5\u4f5c\uff0c\u5e76\u5e94\u7528\u4e8e\u73b0\u5b9e\u9886\u57df\u3002", "result": "\u5efa\u7acb\u4e86\u4ef7\u503c\u611f\u77e5AI\u7684\u7cfb\u7edf\u5316\u6846\u67b6\uff0c\u63d0\u4f9b\u4e86\u4ece\u7406\u8bba\u5230\u5b9e\u8df5\u7684\u5de5\u7a0b\u8def\u7ebf\u56fe\u3002\u901a\u8fc7\u5f62\u5f0f\u8bed\u4e49\u5b66\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4eba\u7c7b\u4ef7\u503c\u7684\u7cbe\u786e\u8868\u793a\uff0c\u5f00\u53d1\u4e86\u786e\u4fdd\u4ef7\u503c\u5bf9\u9f50\u7684\u6280\u672f\uff0c\u5e76\u5efa\u7acb\u4e86\u57fa\u4e8e\u4ef7\u503c\u7684\u53ef\u89e3\u91ca\u6027\u673a\u5236\uff0c\u5728\u5b9e\u9645\u9886\u57df\u4e2d\u5f97\u5230\u5e94\u7528\u9a8c\u8bc1\u3002", "conclusion": "\u4ef7\u503c\u611f\u77e5AI\u6846\u67b6\u8d85\u8d8a\u4e86\u4f20\u7edf\u4ef7\u503c\u5bf9\u9f50\uff0c\u4e3a\u6784\u5efa\u66f4\u5b89\u5168\u3001\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u5de5\u7a0b\u65b9\u6cd5\u3002\u4e09\u652f\u67f1\u8def\u7ebf\u56fe\u4e3aAI\u4ef7\u503c\u5de5\u7a0b\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u53d1\u5c55\u65b9\u5411\uff0c\u6709\u52a9\u4e8e\u5b9e\u73b0AI\u4e0e\u4eba\u7c7b\u4ef7\u503c\u7684\u6df1\u5ea6\u878d\u5408\u3002"}}
{"id": "2512.12022", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12022", "abs": "https://arxiv.org/abs/2512.12022", "authors": ["Kaichuang Zhang", "Wei Yin", "Jinghao Yang", "Ping Xu"], "title": "DFedReweighting: A Unified Framework for Objective-Oriented Reweighting in Decentralized Federated Learning", "comment": null, "summary": "Decentralized federated learning (DFL) has recently emerged as a promising paradigm that enables multiple clients to collaboratively train machine learning model through iterative rounds of local training, communication, and aggregation without relying on a central server which introduces potential vulnerabilities in conventional Federated Learning. Nevertheless, DFL systems continue to face a range of challenges, including fairness, robustness, etc. To address these challenges, we propose \\textbf{DFedReweighting}, a unified aggregation framework designed to achieve diverse objectives in DFL systems via a objective-oriented reweighting aggregation at the final step of each learning round. Specifically, the framework first computes preliminary weights based on \\textit{target performance metric} obtained from auxiliary dataset constructed using local data. These weights are then refined using \\textit{customized reweighting strategy}, resulting in the final aggregation weights. Our results from the theoretical analysis demonstrate that the appropriate combination of the target performance metric and the customized reweighting strategy ensures linear convergence. Experimental results consistently show that our proposed framework significantly improves fairness and robustness against Byzantine attacks in diverse scenarios. Provided that appropriate target performance metrics and customized reweighting strategy are selected, our framework can achieve a wide range of desired learning objectives.", "AI": {"tldr": "DFedReweighting\u662f\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\u7684\u7edf\u4e00\u805a\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u76ee\u6807\u5bfc\u5411\u7684\u6743\u91cd\u8c03\u6574\u6765\u63d0\u5347\u516c\u5e73\u6027\u548c\u9c81\u68d2\u6027", "motivation": "\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\u867d\u7136\u907f\u514d\u4e86\u4e2d\u5fc3\u670d\u52a1\u5668\u7684\u5355\u70b9\u6545\u969c\u98ce\u9669\uff0c\u4f46\u4ecd\u9762\u4e34\u516c\u5e73\u6027\u3001\u9c81\u68d2\u6027\u7b49\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u7075\u6d3b\u7684\u6846\u67b6\u6765\u540c\u65f6\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898", "method": "\u63d0\u51faDFedReweighting\u6846\u67b6\uff0c\u5728\u6bcf\u8f6e\u5b66\u4e60\u7684\u6700\u540e\u4e00\u6b65\u8fdb\u884c\u76ee\u6807\u5bfc\u5411\u7684\u6743\u91cd\u8c03\u6574\uff1a\u9996\u5148\u57fa\u4e8e\u76ee\u6807\u6027\u80fd\u6307\u6807\u8ba1\u7b97\u521d\u6b65\u6743\u91cd\uff0c\u7136\u540e\u901a\u8fc7\u5b9a\u5236\u5316\u91cd\u52a0\u6743\u7b56\u7565\u8fdb\u884c\u7ec6\u5316\uff0c\u5f97\u5230\u6700\u7ec8\u805a\u5408\u6743\u91cd", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0c\u9002\u5f53\u7684\u76ee\u6807\u6027\u80fd\u6307\u6807\u548c\u91cd\u52a0\u6743\u7b56\u7565\u7ec4\u5408\u80fd\u786e\u4fdd\u7ebf\u6027\u6536\u655b\uff1b\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u8be5\u6846\u67b6\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u516c\u5e73\u6027\u548c\u5bf9\u6297\u62dc\u5360\u5ead\u653b\u51fb\u7684\u9c81\u68d2\u6027", "conclusion": "DFedReweighting\u662f\u4e00\u4e2a\u7075\u6d3b\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u5408\u9002\u7684\u76ee\u6807\u6027\u80fd\u6307\u6807\u548c\u91cd\u52a0\u6743\u7b56\u7565\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5e7f\u6cdb\u7684\u671f\u671b\u5b66\u4e60\u76ee\u6807\uff0c\u6709\u6548\u89e3\u51b3\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u516c\u5e73\u6027\u548c\u9c81\u68d2\u6027\u95ee\u9898"}}
{"id": "2512.12108", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12108", "abs": "https://arxiv.org/abs/2512.12108", "authors": ["Dashti A. Ali", "Aras T. Asaad", "Jacob J. Peoples", "Mohammad Hamghalam", "Alex Robins", "Mane Piliposyan", "Richard K. G. Do", "Natalie Gangai", "Yun S. Chun", "Ahmad Bashir Barekzai", "Jayasree Chakraborty", "Hala Khasawneh", "Camila Vilela", "Natally Horvat", "Jo\u00e3o Miranda", "Alice C. Wei", "Amber L. Simpson"], "title": "A Novel Patch-Based TDA Approach for Computed Tomography", "comment": null, "summary": "The development of machine learning (ML) models based on computed tomography (CT) imaging modality has been a major focus of recent research in the medical imaging domain. Incorporating robust feature engineering approach can highly improve the performance of these models. Topological data analysis (TDA), a recent development based on the mathematical field of algebraic topology, mainly focuses on the data from a topological perspective, extracting deeper insight and higher dimensional structures from the data. Persistent homology (PH), a fundamental tool in the area of TDA, can extract topological features such as connected components, cycles and voids from the data. A popular approach to construct PH from 3D CT images is to utilize the 3D cubical complex filtration, a method adapted for grid-structured data. However, this approach may not always yield the best performance and can suffer from computational complexity with higher resolution CT images. This study introduces a novel patch-based PH construction approach tailored for volumetric medical imaging data, in particular CT modality. A wide range of experiments has been conducted on several datasets of 3D CT images to comprehensively analyze the performance of the proposed method with various parameters and benchmark it against the 3D cubical complex algorithm. Our results highlight the dominance of the patch-based TDA approach in terms of both classification performance and time-efficiency. The proposed approach outperformed the cubical complex method, achieving average improvement of 10.38%, 6.94%, 2.06%, 11.58%, and 8.51% in accuracy, AUC, sensitivity, specificity, and F1 score, respectively, across all datasets. Finally, we provide a convenient python package, Patch-TDA, to facilitate the utilization of the proposed approach.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf93D CT\u533b\u5b66\u5f71\u50cf\u7684\u57fa\u4e8e\u5206\u5757\u7684\u62d3\u6251\u6570\u636e\u5206\u6790\u65b9\u6cd5\uff0c\u76f8\u6bd4\u4f20\u7edf\u76843D\u7acb\u65b9\u4f53\u590d\u5f62\u65b9\u6cd5\uff0c\u5728\u5206\u7c7b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e3D\u7acb\u65b9\u4f53\u590d\u5f62\u8fc7\u6ee4\u7684\u62d3\u6251\u6570\u636e\u5206\u6790\u65b9\u6cd5\u5728\u5904\u7406\u9ad8\u5206\u8fa8\u7387CT\u56fe\u50cf\u65f6\u5b58\u5728\u6027\u80fd\u4e0d\u8db3\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u63d0\u53d6CT\u56fe\u50cf\u4e2d\u7684\u62d3\u6251\u7279\u5f81\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u5206\u5757\u7684\u6301\u4e45\u540c\u8c03\u6784\u5efa\u65b9\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9\u4f53\u79ef\u533b\u5b66\u6210\u50cf\u6570\u636e\uff08\u7279\u522b\u662fCT\u6a21\u6001\uff09\u8bbe\u8ba1\u3002\u8be5\u65b9\u6cd5\u5c063D CT\u56fe\u50cf\u5206\u5272\u6210\u5c0f\u5757\u8fdb\u884c\u5904\u7406\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u7acb\u65b9\u4f53\u590d\u5f62\u65b9\u6cd5\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\u3002", "result": "\u5728\u591a\u4e2a3D CT\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u5206\u5757\u7684TDA\u65b9\u6cd5\u5728\u5206\u7c7b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u90fd\u4f18\u4e8e\u4f20\u7edf\u76843D\u7acb\u65b9\u4f53\u590d\u5f62\u7b97\u6cd5\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5728\u51c6\u786e\u7387\u3001AUC\u3001\u654f\u611f\u6027\u3001\u7279\u5f02\u6027\u548cF1\u5206\u6570\u4e0a\u5206\u522b\u5e73\u5747\u63d0\u5347\u4e8610.38%\u30016.94%\u30012.06%\u300111.58%\u548c8.51%\u3002", "conclusion": "\u57fa\u4e8e\u5206\u5757\u7684\u62d3\u6251\u6570\u636e\u5206\u6790\u65b9\u6cd5\u57283D CT\u56fe\u50cf\u5904\u7406\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u65e2\u63d0\u9ad8\u4e86\u5206\u7c7b\u6027\u80fd\u53c8\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u3002\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u65b9\u4fbf\u7684Python\u8f6f\u4ef6\u5305Patch-TDA\u6765\u4fc3\u8fdb\u8be5\u65b9\u6cd5\u7684\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2512.12187", "categories": ["cs.CY", "cs.HC", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2512.12187", "abs": "https://arxiv.org/abs/2512.12187", "authors": ["David Gamba", "Daniel M. Romero", "Grant Schoenebeck"], "title": "The Ideological Turing Test for Moderation of Outgroup Affective Animosity", "comment": "32 pages", "summary": "Rising animosity toward ideological opponents poses critical societal challenges. We introduce and test the Ideological Turing Test, a gamified framework requiring participants to adopt and defend opposing viewpoints, to reduce affective animosity and affective polarization.\n  We conducted a mixed-design experiment ($N = 203$) with four conditions: modality (debate/writing) x perspective-taking (Own/Opposite side). Participants engaged in structured interactions defending assigned positions, with outcomes judged by peers. We measured changes in affective animosity and ideological position immediately post-intervention and at 2-6 week follow-up.\n  Perspective-taking reduced out-group animosity and ideological polarization. However, effects differed by modality (writing vs. debate) and over time. For affective animosity, writing from the opposite perspective yielded the largest immediate reduction ($\u0394=+0.45$ SD), but the effect was not detectable at the 4-6 week follow-up. In contrast, the debate modality maintained a statistically significant reduction in animosity immediately after and at follow-up ($\u0394=+0.37$ SD). For ideological position, adopting the opposite perspective led to significant immediate movement across modalities (writing: $\u0394=+0.91$ SD; debate: $\u0394=+0.51$ SD), and these changes persisted at follow-up. Judged performance (winning) did not moderate these effects, and willingness to re-participate was similar across conditions (~20-36%).\n  These findings challenge assumptions about adversarial methods, revealing distinct temporal patterns: non-adversarial engagement fosters short-term empathy gains, while cognitive engagement through debate sustains affective benefits. The Ideological Turing Test demonstrates potential as a scalable tool for reducing polarization, particularly when combining perspective-taking with reflective adversarial interactions.", "AI": {"tldr": "\u7814\u7a76\u5f15\u5165\"\u610f\u8bc6\u5f62\u6001\u56fe\u7075\u6d4b\u8bd5\"\u6e38\u620f\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u8ba9\u53c2\u4e0e\u8005\u91c7\u7eb3\u548c\u8fa9\u62a4\u5bf9\u7acb\u89c2\u70b9\u6765\u51cf\u5c11\u60c5\u611f\u654c\u610f\u548c\u6781\u5316\u3002\u5b9e\u9a8c\u53d1\u73b0\u6362\u4f4d\u601d\u8003\u80fd\u964d\u4f4e\u5916\u7fa4\u4f53\u654c\u610f\u548c\u610f\u8bc6\u5f62\u6001\u6781\u5316\uff0c\u4f46\u5199\u4f5c\u548c\u8fa9\u8bba\u4e24\u79cd\u65b9\u5f0f\u7684\u6548\u679c\u5b58\u5728\u65f6\u95f4\u5dee\u5f02\u3002", "motivation": "\u9488\u5bf9\u65e5\u76ca\u4e25\u91cd\u7684\u610f\u8bc6\u5f62\u6001\u5bf9\u7acb\u548c\u60c5\u611f\u654c\u610f\u8fd9\u4e00\u5173\u952e\u793e\u4f1a\u6311\u6218\uff0c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u51cf\u5c11\u60c5\u611f\u6781\u5316\u7684\u6709\u6548\u65b9\u6cd5\u3002\u4f20\u7edf\u7684\u5bf9\u6297\u6027\u65b9\u6cd5\u53ef\u80fd\u52a0\u5267\u5206\u6b67\uff0c\u9700\u8981\u65b0\u7684\u5e72\u9884\u7b56\u7565\u6765\u4fc3\u8fdb\u8de8\u610f\u8bc6\u5f62\u6001\u7406\u89e3\u3002", "method": "\u91c7\u7528\u6df7\u5408\u8bbe\u8ba1\u5b9e\u9a8c\uff08N=203\uff09\uff0c\u8bbe\u7f6e\u56db\u4e2a\u6761\u4ef6\uff1a\u65b9\u5f0f\uff08\u8fa9\u8bba/\u5199\u4f5c\uff09\u00d7\u6362\u4f4d\u601d\u8003\uff08\u5df1\u65b9/\u5bf9\u65b9\u7acb\u573a\uff09\u3002\u53c2\u4e0e\u8005\u8fdb\u884c\u7ed3\u6784\u5316\u4e92\u52a8\uff0c\u4e3a\u6307\u5b9a\u7acb\u573a\u8fa9\u62a4\uff0c\u7ed3\u679c\u7531\u540c\u4f34\u8bc4\u5224\u3002\u6d4b\u91cf\u5e72\u9884\u540e\u7acb\u5373\u548c2-6\u5468\u968f\u8bbf\u65f6\u7684\u60c5\u611f\u654c\u610f\u548c\u610f\u8bc6\u5f62\u6001\u7acb\u573a\u53d8\u5316\u3002", "result": "\u6362\u4f4d\u601d\u8003\u964d\u4f4e\u4e86\u5916\u7fa4\u4f53\u654c\u610f\u548c\u610f\u8bc6\u5f62\u6001\u6781\u5316\uff0c\u4f46\u6548\u679c\u56e0\u65b9\u5f0f\u548c\u65f6\u95f4\u800c\u5f02\uff1a\u5199\u4f5c\u65b9\u5f0f\u5728\u6362\u4f4d\u601d\u8003\u65f6\u4ea7\u751f\u6700\u5927\u7684\u5373\u65f6\u60c5\u611f\u654c\u610f\u51cf\u5c11\uff08\u0394=+0.45 SD\uff09\uff0c\u4f464-6\u5468\u540e\u6548\u679c\u6d88\u5931\uff1b\u8fa9\u8bba\u65b9\u5f0f\u5219\u80fd\u7ef4\u6301\u663e\u8457\u7684\u654c\u610f\u51cf\u5c11\uff08\u0394=+0.37 SD\uff09\u3002\u610f\u8bc6\u5f62\u6001\u7acb\u573a\u65b9\u9762\uff0c\u6362\u4f4d\u601d\u8003\u5bfc\u81f4\u663e\u8457\u7684\u5373\u65f6\u53d8\u5316\uff08\u5199\u4f5c\uff1a\u0394=+0.91 SD\uff1b\u8fa9\u8bba\uff1a\u0394=+0.51 SD\uff09\uff0c\u4e14\u8fd9\u4e9b\u53d8\u5316\u5728\u968f\u8bbf\u4e2d\u6301\u7eed\u3002", "conclusion": "\u7814\u7a76\u6311\u6218\u4e86\u5bf9\u6297\u6027\u65b9\u6cd5\u7684\u5047\u8bbe\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u7684\u65f6\u95f4\u6a21\u5f0f\uff1a\u975e\u5bf9\u6297\u6027\u53c2\u4e0e\u4fc3\u8fdb\u77ed\u671f\u5171\u60c5\u589e\u76ca\uff0c\u800c\u901a\u8fc7\u8fa9\u8bba\u7684\u8ba4\u77e5\u53c2\u4e0e\u5219\u80fd\u7ef4\u6301\u60c5\u611f\u76ca\u5904\u3002\u610f\u8bc6\u5f62\u6001\u56fe\u7075\u6d4b\u8bd5\u5c55\u793a\u4e86\u4f5c\u4e3a\u51cf\u5c11\u6781\u5316\u53ef\u6269\u5c55\u5de5\u5177\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5c06\u6362\u4f4d\u601d\u8003\u4e0e\u53cd\u601d\u6027\u5bf9\u6297\u4e92\u52a8\u76f8\u7ed3\u5408\u65f6\u3002"}}
{"id": "2512.12046", "categories": ["cs.LG", "cs.RO", "eess.SY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.12046", "abs": "https://arxiv.org/abs/2512.12046", "authors": ["Vittorio Giammarino", "Ahmed H. Qureshi"], "title": "Goal Reaching with Eikonal-Constrained Hierarchical Quasimetric Reinforcement Learning", "comment": null, "summary": "Goal-Conditioned Reinforcement Learning (GCRL) mitigates the difficulty of reward design by framing tasks as goal reaching rather than maximizing hand-crafted reward signals. In this setting, the optimal goal-conditioned value function naturally forms a quasimetric, motivating Quasimetric RL (QRL), which constrains value learning to quasimetric mappings and enforces local consistency through discrete, trajectory-based constraints. We propose Eikonal-Constrained Quasimetric RL (Eik-QRL), a continuous-time reformulation of QRL based on the Eikonal Partial Differential Equation (PDE). This PDE-based structure makes Eik-QRL trajectory-free, requiring only sampled states and goals, while improving out-of-distribution generalization. We provide theoretical guarantees for Eik-QRL and identify limitations that arise under complex dynamics. To address these challenges, we introduce Eik-Hierarchical QRL (Eik-HiQRL), which integrates Eik-QRL into a hierarchical decomposition. Empirically, Eik-HiQRL achieves state-of-the-art performance in offline goal-conditioned navigation and yields consistent gains over QRL in manipulation tasks, matching temporal-difference methods.", "AI": {"tldr": "Eik-HiQRL\uff1a\u57fa\u4e8eEikonal PDE\u7684\u8fde\u7eed\u65f6\u95f4\u51c6\u5ea6\u91cf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u5206\u89e3\u89e3\u51b3\u590d\u6742\u52a8\u6001\u95ee\u9898\uff0c\u5728\u79bb\u7ebf\u76ee\u6807\u5bfc\u822a\u548c\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8fbe\u5230SOTA\u6027\u80fd", "motivation": "\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\uff08GCRL\uff09\u901a\u8fc7\u76ee\u6807\u8fbe\u6210\u6846\u67b6\u7b80\u5316\u5956\u52b1\u8bbe\u8ba1\uff0c\u51c6\u5ea6\u91cf\u5f3a\u5316\u5b66\u4e60\uff08QRL\uff09\u5229\u7528\u51c6\u5ea6\u91cf\u7ed3\u6784\u7ea6\u675f\u4ef7\u503c\u51fd\u6570\u5b66\u4e60\uff0c\u4f46\u73b0\u6709QRL\u65b9\u6cd5\u4f9d\u8d56\u79bb\u6563\u8f68\u8ff9\u7ea6\u675f\uff0c\u5b58\u5728\u5c40\u9650\u6027", "method": "\u63d0\u51faEik-QRL\uff1a\u57fa\u4e8eEikonal\u504f\u5fae\u5206\u65b9\u7a0b\u7684\u8fde\u7eed\u65f6\u95f4QRL\u91cd\u6784\uff0c\u5b9e\u73b0\u65e0\u8f68\u8ff9\u5b66\u4e60\uff1b\u8fdb\u4e00\u6b65\u63d0\u51faEik-HiQRL\uff1a\u5c06Eik-QRL\u96c6\u6210\u5230\u5206\u5c42\u5206\u89e3\u4e2d\uff0c\u89e3\u51b3\u590d\u6742\u52a8\u6001\u95ee\u9898", "result": "Eik-HiQRL\u5728\u79bb\u7ebf\u76ee\u6807\u6761\u4ef6\u5bfc\u822a\u4efb\u52a1\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u64cd\u4f5c\u4efb\u52a1\u4e2d\u76f8\u6bd4QRL\u83b7\u5f97\u4e00\u81f4\u63d0\u5347\uff0c\u4e0e\u65f6\u95f4\u5dee\u5206\u65b9\u6cd5\u6027\u80fd\u76f8\u5f53", "conclusion": "Eikonal\u7ea6\u675f\u7684\u51c6\u5ea6\u91cf\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u63d0\u4f9b\u4e86\u8fde\u7eed\u65f6\u95f4\u3001\u65e0\u8f68\u8ff9\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u5206\u89e3\u6709\u6548\u5904\u7406\u590d\u6742\u52a8\u6001\uff0c\u5728GCRL\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd"}}
{"id": "2512.12128", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12128", "abs": "https://arxiv.org/abs/2512.12128", "authors": ["Thomas Manzini", "Priyankari Perali", "Raisa Karnik", "Robin R. Murphy"], "title": "A Benchmark Dataset for Spatially Aligned Road Damage Assessment in Small Uncrewed Aerial Systems Disaster Imagery", "comment": "11 pages, 6 figures, 6 tables. To appear AAAI'26", "summary": "This paper presents the largest known benchmark dataset for road damage assessment and road alignment, and provides 18 baseline models trained on the CRASAR-U-DRIODs dataset's post-disaster small uncrewed aerial systems (sUAS) imagery from 10 federally declared disasters, addressing three challenges within prior post-disaster road damage assessment datasets. While prior disaster road damage assessment datasets exist, there is no current state of practice, as prior public datasets have either been small-scale or reliant on low-resolution imagery insufficient for detecting phenomena of interest to emergency managers. Further, while machine learning (ML) systems have been developed for this task previously, none are known to have been operationally validated. These limitations are overcome in this work through the labeling of 657.25km of roads according to a 10-class labeling schema, followed by training and deploying ML models during the operational response to Hurricanes Debby and Helene in 2024. Motivated by observed road line misalignment in practice, 9,184 road line adjustments were provided for spatial alignment of a priori road lines, as it was found that when the 18 baseline models are deployed against real-world misaligned road lines, model performance degraded on average by 5.596\\% Macro IoU. If spatial alignment is not considered, approximately 8\\% (11km) of adverse conditions on road lines will be labeled incorrectly, with approximately 9\\% (59km) of road lines misaligned off the actual road. These dynamics are gaps that should be addressed by the ML, CV, and robotics communities to enable more effective and informed decision-making during disasters.", "AI": {"tldr": "\u8be5\u7814\u7a76\u521b\u5efa\u4e86\u6700\u5927\u7684\u9053\u8def\u635f\u574f\u8bc4\u4f30\u57fa\u51c6\u6570\u636e\u96c6CRASAR-U-DRIODs\uff0c\u5305\u542b10\u6b21\u8054\u90a6\u707e\u5bb3\u540e\u7684\u65e0\u4eba\u673a\u56fe\u50cf\uff0c\u6807\u6ce8\u4e86657.25\u516c\u91cc\u9053\u8def\uff0c\u5e76\u63d0\u4f9b18\u4e2a\u57fa\u7ebf\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u89c4\u6a21\u5c0f\u3001\u5206\u8fa8\u7387\u4f4e\u3001\u7f3a\u4e4f\u64cd\u4f5c\u9a8c\u8bc1\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u707e\u5bb3\u9053\u8def\u635f\u574f\u8bc4\u4f30\u6570\u636e\u96c6\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u89c4\u6a21\u5c0f\u6216\u4f9d\u8d56\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\uff0c\u65e0\u6cd5\u68c0\u6d4b\u5e94\u6025\u7ba1\u7406\u8005\u5173\u5fc3\u7684\u73b0\u8c61\uff1b2) \u7f3a\u4e4f\u64cd\u4f5c\u9a8c\u8bc1\u7684\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\uff1b3) \u5b9e\u8df5\u4e2d\u53d1\u73b0\u9053\u8def\u7ebf\u9519\u4f4d\u95ee\u9898\u4e25\u91cd\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "method": "1) \u521b\u5efaCRASAR-U-DRIODs\u6570\u636e\u96c6\uff0c\u5305\u542b10\u6b21\u8054\u90a6\u707e\u5bb3\u540e\u7684\u65e0\u4eba\u673a\u56fe\u50cf\uff1b2) \u6309\u716710\u7c7b\u6807\u6ce8\u65b9\u6848\u6807\u6ce8657.25\u516c\u91cc\u9053\u8def\uff1b3) \u63d0\u4f9b9,184\u4e2a\u9053\u8def\u7ebf\u8c03\u6574\u7528\u4e8e\u7a7a\u95f4\u5bf9\u9f50\uff1b4) \u8bad\u7ec318\u4e2a\u57fa\u7ebf\u6a21\u578b\u5e76\u57282024\u5e74\u98d3\u98ceDebby\u548cHelene\u7684\u5b9e\u9645\u5e94\u6025\u54cd\u5e94\u4e2d\u90e8\u7f72\u9a8c\u8bc1\u3002", "result": "1) \u5f5318\u4e2a\u57fa\u7ebf\u6a21\u578b\u90e8\u7f72\u5230\u5b9e\u9645\u9519\u4f4d\u7684\u9053\u8def\u7ebf\u65f6\uff0c\u6a21\u578b\u6027\u80fd\u5e73\u5747\u4e0b\u964d5.596% Macro IoU\uff1b2) \u5982\u679c\u4e0d\u8003\u8651\u7a7a\u95f4\u5bf9\u9f50\uff0c\u7ea68%\uff0811\u516c\u91cc\uff09\u7684\u4e0d\u826f\u9053\u8def\u6761\u4ef6\u4f1a\u88ab\u9519\u8bef\u6807\u6ce8\uff1b3) \u7ea69%\uff0859\u516c\u91cc\uff09\u7684\u9053\u8def\u7ebf\u4e0e\u5b9e\u9645\u9053\u8def\u9519\u4f4d\u3002", "conclusion": "\u9053\u8def\u7ebf\u7a7a\u95f4\u9519\u4f4d\u662f\u5f71\u54cd\u707e\u5bb3\u9053\u8def\u635f\u574f\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\uff0cML\u3001CV\u548c\u673a\u5668\u4eba\u793e\u533a\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\uff0c\u4ee5\u5b9e\u73b0\u707e\u5bb3\u671f\u95f4\u66f4\u6709\u6548\u548c\u660e\u667a\u7684\u51b3\u7b56\u5236\u5b9a\u3002"}}
{"id": "2512.12212", "categories": ["cs.CY", "econ.GN", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.12212", "abs": "https://arxiv.org/abs/2512.12212", "authors": ["Elizabeth Irenne Yuwono", "Dian Tjondronegoro", "Shawn Hunter", "Amber Marshall"], "title": "Anticipatory Governance in Data-Constrained Environments: A Predictive Simulation Framework for Digital Financial Inclusion", "comment": "28 pages, 3 figures", "summary": "Financial exclusion remains a major barrier to digital public service delivery in resource-constrained and archipelagic nations. Traditional policy evaluations rely on retrospective data, limiting the ex-ante intelligence needed for agile resource allocation. This study introduces a predictive simulation framework to support anticipatory governance within government information systems. Using the UNCDF Pacific Digital Economy dataset of 10,108 respondents, we apply a three-stage pipeline: descriptive profiling, interpretable machine learning, and scenario simulation to forecast outcomes of digital financial literacy interventions before deployment. Leveraging cross-sectional structural associations, the framework projects intervention scenarios as prioritization heuristics rather than causal estimates. A transparent linear regression model with R-squared of 95.9 identifies modifiable policy levers. Simulations indicate that foundational digital capabilities such as device access and expense tracking yield the highest projected gains, up to 5.5 percent, outperforming attitudinal nudges. The model enables precision targeting, highlighting young female caregivers as high-leverage responders while flagging non-responders such as urban professionals to prevent resource misallocation. This research demonstrates how static survey data can be repurposed into actionable policy intelligence, offering a scalable and evidence-based blueprint for embedding predictive analytics into public-sector decision-support systems to advance equity-focused digital governance.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u9884\u6d4b\u6a21\u62df\u6846\u67b6\uff0c\u5229\u7528\u673a\u5668\u5b66\u4e60\u5206\u6790\u6570\u5b57\u91d1\u878d\u7d20\u517b\u5e72\u9884\u63aa\u65bd\u7684\u6548\u679c\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u7684\u5c9b\u56fd\u63d0\u4f9b\u524d\u77bb\u6027\u653f\u7b56\u51b3\u7b56\u652f\u6301\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5c9b\u56fd\uff0c\u91d1\u878d\u6392\u65a5\u662f\u6570\u5b57\u516c\u5171\u670d\u52a1\u4ea4\u4ed8\u7684\u4e3b\u8981\u969c\u788d\u3002\u4f20\u7edf\u653f\u7b56\u8bc4\u4f30\u4f9d\u8d56\u56de\u987e\u6027\u6570\u636e\uff0c\u7f3a\u4e4f\u524d\u77bb\u6027\u60c5\u62a5\u6765\u652f\u6301\u654f\u6377\u7684\u8d44\u6e90\u5206\u914d\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u80fd\u591f\u5728\u5e72\u9884\u63aa\u65bd\u90e8\u7f72\u524d\u9884\u6d4b\u5176\u6548\u679c\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u6d41\u7a0b\uff1a1) \u63cf\u8ff0\u6027\u5206\u6790\uff0c2) \u53ef\u89e3\u91ca\u673a\u5668\u5b66\u4e60\uff0c3) \u60c5\u666f\u6a21\u62df\u3002\u4f7f\u7528UNCDF\u592a\u5e73\u6d0b\u6570\u5b57\u7ecf\u6d4e\u6570\u636e\u96c6\uff0810,108\u540d\u53d7\u8bbf\u8005\uff09\uff0c\u901a\u8fc7\u900f\u660e\u7ebf\u6027\u56de\u5f52\u6a21\u578b\uff08R\u5e73\u65b995.9%\uff09\u8bc6\u522b\u53ef\u4fee\u6539\u7684\u653f\u7b56\u6760\u6746\uff0c\u5e76\u6a21\u62df\u4e0d\u540c\u5e72\u9884\u60c5\u666f\u3002", "result": "\u6a21\u578b\u8bc6\u522b\u51fa\u8bbe\u5907\u8bbf\u95ee\u548c\u8d39\u7528\u8ddf\u8e2a\u7b49\u57fa\u7840\u6570\u5b57\u80fd\u529b\u80fd\u5e26\u6765\u6700\u9ad8\u9884\u671f\u6536\u76ca\uff08\u8fbe5.5%\uff09\uff0c\u4f18\u4e8e\u6001\u5ea6\u6fc0\u52b1\u3002\u6a21\u578b\u652f\u6301\u7cbe\u51c6\u5b9a\u4f4d\uff0c\u53d1\u73b0\u5e74\u8f7b\u5973\u6027\u62a4\u7406\u4eba\u5458\u662f\u9ad8\u6760\u6746\u54cd\u5e94\u8005\uff0c\u800c\u57ce\u5e02\u4e13\u4e1a\u4eba\u58eb\u662f\u975e\u54cd\u5e94\u8005\uff0c\u53ef\u9632\u6b62\u8d44\u6e90\u9519\u914d\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u9759\u6001\u8c03\u67e5\u6570\u636e\u8f6c\u5316\u4e3a\u53ef\u64cd\u4f5c\u7684\u653f\u7b56\u60c5\u62a5\uff0c\u4e3a\u516c\u5171\u90e8\u95e8\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u5d4c\u5165\u9884\u6d4b\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u57fa\u4e8e\u8bc1\u636e\u7684\u84dd\u56fe\uff0c\u63a8\u8fdb\u4e86\u4ee5\u516c\u5e73\u4e3a\u91cd\u70b9\u7684\u6570\u5b57\u6cbb\u7406\u3002"}}
{"id": "2512.12692", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12692", "abs": "https://arxiv.org/abs/2512.12692", "authors": ["Mahir Labib Dihan", "Tanzima Hashem", "Mohammed Eunus Ali", "Md Rizwan Parvez"], "title": "WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment", "comment": "Under review at ICLR 2026. Project page: https://kagnlp.github.io/WebOperator/", "summary": "LLM-based agents often operate in a greedy, step-by-step manner, selecting actions solely based on the current observation without considering long-term consequences or alternative paths. This lack of foresight is particularly problematic in web environments, which are only partially observable-limited to browser-visible content (e.g., DOM and UI elements)-where a single misstep often requires complex and brittle navigation to undo. Without an explicit backtracking mechanism, agents struggle to correct errors or systematically explore alternative paths. Tree-search methods provide a principled framework for such structured exploration, but existing approaches lack mechanisms for safe backtracking, making them prone to unintended side effects. They also assume that all actions are reversible, ignoring the presence of irreversible actions-limitations that reduce their effectiveness in realistic web tasks. To address these challenges, we introduce WebOperator, a tree-search framework that enables reliable backtracking and strategic exploration. Our method incorporates a best-first search strategy that ranks actions by both reward estimates and safety considerations, along with a robust backtracking mechanism that verifies the feasibility of previously visited paths before replaying them, preventing unintended side effects. To further guide exploration, WebOperator generates action candidates from multiple, varied reasoning contexts to ensure diverse and robust exploration, and subsequently curates a high-quality action set by filtering out invalid actions pre-execution and merging semantically equivalent ones. Experimental results on WebArena and WebVoyager demonstrate the effectiveness of WebOperator. On WebArena, WebOperator achieves a state-of-the-art 54.6% success rate with gpt-4o, underscoring the critical advantage of integrating strategic foresight with safe execution.", "AI": {"tldr": "WebOperator\u662f\u4e00\u4e2a\u6811\u641c\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u4f73\u4f18\u5148\u641c\u7d22\u3001\u5b89\u5168\u56de\u6eaf\u673a\u5236\u548c\u591a\u63a8\u7406\u4e0a\u4e0b\u6587\u751f\u6210\u52a8\u4f5c\u5019\u9009\uff0c\u89e3\u51b3LLM\u667a\u80fd\u4f53\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u7684Web\u73af\u5883\u4e2d\u7f3a\u4e4f\u8fdc\u89c1\u548c\u65e0\u6cd5\u5b89\u5168\u56de\u6eaf\u7684\u95ee\u9898\u3002", "motivation": "LLM\u667a\u80fd\u4f53\u5728Web\u73af\u5883\u4e2d\u901a\u5e38\u91c7\u7528\u8d2a\u5a6a\u7684\u9010\u6b65\u64cd\u4f5c\u65b9\u5f0f\uff0c\u7f3a\u4e4f\u957f\u8fdc\u8003\u8651\u548c\u66ff\u4ee3\u8def\u5f84\u63a2\u7d22\u80fd\u529b\u3002Web\u73af\u5883\u662f\u90e8\u5206\u53ef\u89c2\u5bdf\u7684\uff08\u4ec5\u9650\u4e8e\u6d4f\u89c8\u5668\u53ef\u89c1\u5185\u5bb9\uff09\uff0c\u5355\u4e2a\u9519\u8bef\u6b65\u9aa4\u5f80\u5f80\u9700\u8981\u590d\u6742\u4e14\u8106\u5f31\u7684\u5bfc\u822a\u6765\u64a4\u9500\u3002\u73b0\u6709\u6811\u641c\u7d22\u65b9\u6cd5\u7f3a\u4e4f\u5b89\u5168\u56de\u6eaf\u673a\u5236\uff0c\u5047\u8bbe\u6240\u6709\u52a8\u4f5c\u90fd\u53ef\u9006\uff0c\u5ffd\u7565\u4e86\u4e0d\u53ef\u9006\u52a8\u4f5c\u7684\u5b58\u5728\uff0c\u964d\u4f4e\u4e86\u5728\u5b9e\u9645Web\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "WebOperator\u5f15\u5165\u4e00\u4e2a\u6811\u641c\u7d22\u6846\u67b6\uff0c\u5305\u542b\uff1a1\uff09\u6700\u4f73\u4f18\u5148\u641c\u7d22\u7b56\u7565\uff0c\u6839\u636e\u5956\u52b1\u4f30\u8ba1\u548c\u5b89\u5168\u8003\u8651\u5bf9\u52a8\u4f5c\u8fdb\u884c\u6392\u5e8f\uff1b2\uff09\u9c81\u68d2\u7684\u56de\u6eaf\u673a\u5236\uff0c\u5728\u91cd\u653e\u4e4b\u524d\u9a8c\u8bc1\u5df2\u8bbf\u95ee\u8def\u5f84\u7684\u53ef\u884c\u6027\uff0c\u9632\u6b62\u610f\u5916\u526f\u4f5c\u7528\uff1b3\uff09\u4ece\u591a\u4e2a\u4e0d\u540c\u63a8\u7406\u4e0a\u4e0b\u6587\u751f\u6210\u52a8\u4f5c\u5019\u9009\uff0c\u786e\u4fdd\u63a2\u7d22\u7684\u591a\u6837\u6027\u548c\u9c81\u68d2\u6027\uff1b4\uff09\u901a\u8fc7\u9884\u6267\u884c\u8fc7\u6ee4\u65e0\u6548\u52a8\u4f5c\u548c\u5408\u5e76\u8bed\u4e49\u7b49\u4ef7\u52a8\u4f5c\u6765\u7b5b\u9009\u9ad8\u8d28\u91cf\u52a8\u4f5c\u96c6\u3002", "result": "\u5728WebArena\u548cWebVoyager\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eWebOperator\u7684\u6709\u6548\u6027\u3002\u5728WebArena\u4e0a\uff0cWebOperator\u4f7f\u7528gpt-4o\u5b9e\u73b0\u4e8654.6%\u7684\u6700\u65b0\u6210\u529f\u7387\uff0c\u8bc1\u660e\u4e86\u5c06\u6218\u7565\u8fdc\u89c1\u4e0e\u5b89\u5168\u6267\u884c\u76f8\u7ed3\u5408\u7684\u5173\u952e\u4f18\u52bf\u3002", "conclusion": "WebOperator\u901a\u8fc7\u96c6\u6210\u6218\u7565\u8fdc\u89c1\u548c\u5b89\u5168\u6267\u884c\uff0c\u89e3\u51b3\u4e86LLM\u667a\u80fd\u4f53\u5728\u90e8\u5206\u53ef\u89c2\u5bdfWeb\u73af\u5883\u4e2d\u7f3a\u4e4f\u8fdc\u89c1\u548c\u65e0\u6cd5\u5b89\u5168\u56de\u6eaf\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\uff0c\u4e3aWeb\u5bfc\u822a\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u6846\u67b6\u3002"}}
{"id": "2512.12066", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12066", "abs": "https://arxiv.org/abs/2512.12066", "authors": ["Erik Larsen"], "title": "The Instability of Safety: How Random Seeds and Temperature Expose Inconsistent LLM Refusal Behavior", "comment": "14 pages, 7 figures, 6 tables. Code and data available at https://github.com/erikl2/safety-refusal-stability", "summary": "Current safety evaluations of large language models rely on single-shot testing, implicitly assuming that model responses are deterministic and representative of the model's safety alignment. We challenge this assumption by investigating the stability of safety refusal decisions across random seeds and temperature settings. Testing four instruction-tuned models from three families (Llama 3.1 8B, Qwen 2.5 7B, Qwen 3 8B, Gemma 3 12B) on 876 harmful prompts across 20 different sampling configurations (4 temperatures x 5 random seeds), we find that 18-28% of prompts exhibit decision flips--the model refuses in some configurations but complies in others--depending on the model. Our Safety Stability Index (SSI) reveals that higher temperatures significantly reduce decision stability (Friedman chi-squared = 44.71, p < 0.001), with mean SSI dropping from 0.951 at temperature 0.0 to 0.896 at temperature 1.0. We validate our findings across all model families using Claude 3.5 Haiku as a unified external judge, achieving 89.0% inter-judge agreement with our primary Llama 70B judge (Cohen's kappa = 0.62). These findings demonstrate that single-shot safety evaluations are insufficient for reliable safety assessment. We show that single-shot evaluation agrees with multi-sample ground truth only 92.4% of the time, and recommend using at least 3 samples per prompt for reliable safety assessment.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u62d2\u7edd\u51b3\u7b56\u5728\u4e0d\u540c\u968f\u673a\u79cd\u5b50\u548c\u6e29\u5ea6\u8bbe\u7f6e\u4e0b\u5b58\u5728\u663e\u8457\u4e0d\u7a33\u5b9a\u6027\uff0c\u5355\u6b21\u6d4b\u8bd5\u65e0\u6cd5\u53ef\u9760\u8bc4\u4f30\u6a21\u578b\u5b89\u5168\u6027", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u8bc4\u4f30\u901a\u5e38\u91c7\u7528\u5355\u6b21\u6d4b\u8bd5\uff0c\u9690\u542b\u5047\u8bbe\u6a21\u578b\u54cd\u5e94\u662f\u786e\u5b9a\u6027\u7684\u4e14\u80fd\u4ee3\u8868\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\u7a0b\u5ea6\u3002\u672c\u6587\u6311\u6218\u8fd9\u4e00\u5047\u8bbe\uff0c\u7814\u7a76\u5b89\u5168\u62d2\u7edd\u51b3\u7b56\u5728\u4e0d\u540c\u968f\u673a\u79cd\u5b50\u548c\u6e29\u5ea6\u8bbe\u7f6e\u4e0b\u7684\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "method": "\u6d4b\u8bd5\u4e86\u6765\u81ea\u4e09\u4e2a\u5bb6\u65cf\u7684\u56db\u4e2a\u6307\u4ee4\u8c03\u4f18\u6a21\u578b\uff08Llama 3.1 8B, Qwen 2.5 7B, Qwen 3 8B, Gemma 3 12B\uff09\uff0c\u4f7f\u7528876\u4e2a\u6709\u5bb3\u63d0\u793a\u548c20\u79cd\u4e0d\u540c\u91c7\u6837\u914d\u7f6e\uff084\u4e2a\u6e29\u5ea6\u00d75\u4e2a\u968f\u673a\u79cd\u5b50\uff09\u3002\u63d0\u51fa\u5b89\u5168\u7a33\u5b9a\u6027\u6307\u6570\uff08SSI\uff09\u6765\u8861\u91cf\u51b3\u7b56\u7a33\u5b9a\u6027\uff0c\u5e76\u4f7f\u7528Claude 3.5 Haiku\u4f5c\u4e3a\u7edf\u4e00\u5916\u90e8\u8bc4\u5224\u8005\u9a8c\u8bc1\u7ed3\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b018-28%\u7684\u63d0\u793a\u5728\u4e0d\u540c\u914d\u7f6e\u4e0b\u51fa\u73b0\u51b3\u7b56\u7ffb\u8f6c\uff08\u6a21\u578b\u5728\u67d0\u4e9b\u914d\u7f6e\u4e0b\u62d2\u7edd\uff0c\u5728\u5176\u4ed6\u914d\u7f6e\u4e0b\u540c\u610f\uff09\u3002\u6e29\u5ea6\u8d8a\u9ad8\uff0c\u51b3\u7b56\u7a33\u5b9a\u6027\u663e\u8457\u964d\u4f4e\uff08Friedman\u5361\u65b9=44.71\uff0cp<0.001\uff09\uff0c\u5e73\u5747SSI\u4ece\u6e29\u5ea60.0\u65f6\u76840.951\u4e0b\u964d\u5230\u6e29\u5ea61.0\u65f6\u76840.896\u3002\u5355\u6b21\u8bc4\u4f30\u4e0e\u591a\u6837\u672c\u771f\u5b9e\u60c5\u51b5\u7684\u4e00\u81f4\u6027\u4ec5\u4e3a92.4%\u3002", "conclusion": "\u5355\u6b21\u5b89\u5168\u8bc4\u4f30\u4e0d\u8db3\u4ee5\u53ef\u9760\u8bc4\u4f30\u6a21\u578b\u5b89\u5168\u6027\u3002\u5efa\u8bae\u6bcf\u4e2a\u63d0\u793a\u81f3\u5c11\u4f7f\u75283\u4e2a\u6837\u672c\u8fdb\u884c\u5b89\u5168\u8bc4\u4f30\uff0c\u4ee5\u786e\u4fdd\u8bc4\u4f30\u7ed3\u679c\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2512.12142", "categories": ["cs.CV", "cs.AI", "cs.LG", "physics.ao-ph", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2512.12142", "abs": "https://arxiv.org/abs/2512.12142", "authors": ["Bj\u00f6rn L\u00fctjens", "Patrick Alexander", "Raf Antwerpen", "Til Widmann", "Guido Cervone", "Marco Tedesco"], "title": "MeltwaterBench: Deep learning for spatiotemporal downscaling of surface meltwater", "comment": null, "summary": "The Greenland ice sheet is melting at an accelerated rate due to processes that are not fully understood and hard to measure. The distribution of surface meltwater can help understand these processes and is observable through remote sensing, but current maps of meltwater face a trade-off: They are either high-resolution in time or space, but not both. We develop a deep learning model that creates gridded surface meltwater maps at daily 100m resolution by fusing data streams from remote sensing observations and physics-based models. In particular, we spatiotemporally downscale regional climate model (RCM) outputs using synthetic aperture radar (SAR), passive microwave (PMW), and a digital elevation model (DEM) over the Helheim Glacier in Eastern Greenland from 2017-2023. Using SAR-derived meltwater as \"ground truth\", we show that a deep learning-based method that fuses all data streams is over 10 percentage points more accurate over our study area than existing non deep learning-based approaches that only rely on a regional climate model (83% vs. 95% Acc.) or passive microwave observations (72% vs. 95% Acc.). Alternatively, creating a gridded product through a running window calculation with SAR data underestimates extreme melt events, but also achieves notable accuracy (90%) and does not rely on deep learning. We evaluate standard deep learning methods (UNet and DeepLabv3+), and publish our spatiotemporally aligned dataset as a benchmark, MeltwaterBench, for intercomparisons with more complex data-driven downscaling methods. The code and data are available at $\\href{https://github.com/blutjens/hrmelt}{github.com/blutjens/hrmelt}$.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u878d\u5408\u9065\u611f\u89c2\u6d4b\u548c\u7269\u7406\u6a21\u578b\u6570\u636e\uff0c\u751f\u6210\u683c\u9675\u5170\u51b0\u76d6\u6bcf\u65e5100\u7c73\u5206\u8fa8\u7387\u7684\u5730\u8868\u878d\u6c34\u5206\u5e03\u56fe\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u878d\u6c34\u5730\u56fe\u5728\u65f6\u95f4\u548c\u7a7a\u95f4\u5206\u8fa8\u7387\u4e0a\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u683c\u9675\u5170\u51b0\u76d6\u52a0\u901f\u878d\u5316\uff0c\u4f46\u76f8\u5173\u8fc7\u7a0b\u5c1a\u672a\u5b8c\u5168\u7406\u89e3\u4e14\u96be\u4ee5\u6d4b\u91cf\u3002\u73b0\u6709\u878d\u6c34\u5730\u56fe\u9762\u4e34\u65f6\u95f4\u5206\u8fa8\u7387\u4e0e\u7a7a\u95f4\u5206\u8fa8\u7387\u7684\u6743\u8861\uff1a\u8981\u4e48\u65f6\u95f4\u5206\u8fa8\u7387\u9ad8\u4f46\u7a7a\u95f4\u5206\u8fa8\u7387\u4f4e\uff0c\u8981\u4e48\u7a7a\u95f4\u5206\u8fa8\u7387\u9ad8\u4f46\u65f6\u95f4\u5206\u8fa8\u7387\u4f4e\u3002", "method": "\u5f00\u53d1\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u878d\u5408\u533a\u57df\u6c14\u5019\u6a21\u578b\uff08RCM\uff09\u3001\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\uff08SAR\uff09\u3001\u88ab\u52a8\u5fae\u6ce2\uff08PMW\uff09\u548c\u6570\u5b57\u9ad8\u7a0b\u6a21\u578b\uff08DEM\uff09\u6570\u636e\uff0c\u5bf92017-2023\u5e74\u4e1c\u683c\u9675\u5170Helheim\u51b0\u5ddd\u533a\u57df\u8fdb\u884c\u65f6\u7a7a\u964d\u5c3a\u5ea6\u5904\u7406\u3002\u4f7f\u7528SAR\u884d\u751f\u7684\u878d\u6c34\u6570\u636e\u4f5c\u4e3a\"\u5730\u9762\u771f\u503c\"\uff0c\u8bc4\u4f30\u4e86UNet\u548cDeepLabv3+\u7b49\u6807\u51c6\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u878d\u5408\u6240\u6709\u6570\u636e\u6d41\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u7814\u7a76\u533a\u57df\u6bd4\u73b0\u6709\u975e\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u51c6\u786e\u7387\u9ad810\u4e2a\u767e\u5206\u70b9\u4ee5\u4e0a\uff1a\u76f8\u6bd4\u4ec5\u4f9d\u8d56\u533a\u57df\u6c14\u5019\u6a21\u578b\u7684\u65b9\u6cd5\uff0883% vs. 95%\uff09\uff0c\u4ee5\u53ca\u4ec5\u4f9d\u8d56\u88ab\u52a8\u5fae\u6ce2\u89c2\u6d4b\u7684\u65b9\u6cd5\uff0872% vs. 95%\uff09\u3002\u57fa\u4e8eSAR\u6570\u636e\u7684\u6ed1\u52a8\u7a97\u53e3\u8ba1\u7b97\u65b9\u6cd5\u867d\u7136\u4f4e\u4f30\u6781\u7aef\u878d\u5316\u4e8b\u4ef6\uff0c\u4f46\u4e5f\u8fbe\u5230\u4e8690%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u878d\u5408\u591a\u6e90\u6570\u636e\u7684\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u9ad8\u5730\u8868\u878d\u6c34\u5730\u56fe\u7684\u65f6\u7a7a\u5206\u8fa8\u7387\uff0c\u4e3a\u7406\u89e3\u683c\u9675\u5170\u51b0\u76d6\u878d\u5316\u8fc7\u7a0b\u63d0\u4f9b\u66f4\u7cbe\u786e\u7684\u5de5\u5177\u3002\u7814\u7a76\u53d1\u5e03\u4e86\u65f6\u7a7a\u5bf9\u9f50\u7684\u6570\u636e\u96c6MeltwaterBench\u4f5c\u4e3a\u57fa\u51c6\uff0c\u4fc3\u8fdb\u66f4\u590d\u6742\u6570\u636e\u9a71\u52a8\u964d\u5c3a\u5ea6\u65b9\u6cd5\u7684\u6bd4\u8f83\u3002"}}
{"id": "2512.12306", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.12306", "abs": "https://arxiv.org/abs/2512.12306", "authors": ["Amir Yunus", "Peng Rend Gay", "Oon Teng Lee"], "title": "From Co-Design to Metacognitive Laziness: Evaluating Generative AI in Vocational Education", "comment": "108 pages", "summary": "This study examines the development and deployment of a Generative AI proof-of-concept (POC) designed to support lecturers in a vocational education setting in Singapore. Employing a user-centred, mixed-methods design process, we co-developed an AI chatbot with lecturers to address recurring instructional challenges during exam preparation, specifically managing repetitive questions and scaling feedback delivery. The POC achieved its primary operational goals: lecturers reported streamlined workflows, reduced cognitive load, and observed improved student confidence in navigating course content. However, the deployment yielded unexpected insights into student learning behaviours. Despite enhanced teaching processes, performance data revealed no significant improvement in overall student assessment outcomes. Deep analysis of interaction logs identified concerning patterns, including self-efficacy-driven dependency, \"metacognitive laziness\" (cognitive offloading), and divergent usage strategies. While high-ability students leveraged the tool for strategic verification, low-ability students frequently used it to bypass cognitive effort, potentially exacerbating performance gaps. These findings suggest that Generative AI's educational influence extends beyond instructional efficiency to shape cognitive engagement, self-regulation, and learner equity. The study raises consequential design questions regarding how AI tools can be engineered to minimise dependency, scaffold metacognitive development, and calibrate support across varying ability levels. We conclude that while Generative AI can substantially enhance the teaching experience, achieving meaningful learning gains requires rigorous attention to learner behaviour and the equitable design of AI-supported environments.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u9762\u5411\u65b0\u52a0\u5761\u804c\u4e1a\u6559\u80b2\u7684\u751f\u6210\u5f0fAI\u804a\u5929\u673a\u5668\u4eba\uff0c\u65e8\u5728\u8f85\u52a9\u6559\u5e08\u5904\u7406\u8003\u8bd5\u51c6\u5907\u4e2d\u7684\u91cd\u590d\u6027\u95ee\u9898\u3002\u867d\u7136\u63d0\u5347\u4e86\u6559\u5b66\u6548\u7387\uff0c\u4f46\u672a\u663e\u8457\u6539\u5584\u5b66\u751f\u6210\u7ee9\uff0c\u53cd\u800c\u63ed\u793a\u4e86AI\u4f7f\u7528\u4e2d\u7684\u8ba4\u77e5\u4f9d\u8d56\u548c\u516c\u5e73\u6027\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u804c\u4e1a\u6559\u80b2\u6559\u5e08\u5728\u8003\u8bd5\u51c6\u5907\u9636\u6bb5\u9762\u4e34\u7684\u91cd\u590d\u6027\u95ee\u9898\u7ba1\u7406\u548c\u89c4\u6a21\u5316\u53cd\u9988\u4ea4\u4ed8\u7684\u6311\u6218\uff0c\u901a\u8fc7AI\u6280\u672f\u51cf\u8f7b\u6559\u5e08\u8ba4\u77e5\u8d1f\u62c5\uff0c\u63d0\u5347\u6559\u5b66\u6548\u7387\u3002", "method": "\u91c7\u7528\u7528\u6237\u4e2d\u5fc3\u7684\u6df7\u5408\u65b9\u6cd5\u8bbe\u8ba1\u8fc7\u7a0b\uff0c\u4e0e\u6559\u5e08\u5171\u540c\u5f00\u53d1AI\u804a\u5929\u673a\u5668\u4eba\u539f\u578b\uff0c\u901a\u8fc7\u7528\u6237\u53cd\u9988\u3001\u5de5\u4f5c\u6d41\u7a0b\u5206\u6790\u548c\u5b66\u751f\u4e92\u52a8\u65e5\u5fd7\u6570\u636e\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "AI\u5de5\u5177\u6210\u529f\u7b80\u5316\u4e86\u6559\u5e08\u5de5\u4f5c\u6d41\u7a0b\u5e76\u964d\u4f4e\u4e86\u8ba4\u77e5\u8d1f\u8377\uff0c\u4f46\u5b66\u751f\u6574\u4f53\u8bc4\u4f30\u6210\u7ee9\u672a\u663e\u8457\u63d0\u5347\u3002\u4e92\u52a8\u65e5\u5fd7\u5206\u6790\u63ed\u793a\u4e86\u4ee4\u4eba\u62c5\u5fe7\u7684\u6a21\u5f0f\uff1a\u9ad8\u80fd\u529b\u5b66\u751f\u5c06\u5de5\u5177\u7528\u4e8e\u7b56\u7565\u6027\u9a8c\u8bc1\uff0c\u800c\u4f4e\u80fd\u529b\u5b66\u751f\u5219\u4f9d\u8d56AI\u7ed5\u8fc7\u8ba4\u77e5\u52aa\u529b\uff0c\u53ef\u80fd\u52a0\u5267\u6210\u7ee9\u5dee\u8ddd\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u80fd\u663e\u8457\u63d0\u5347\u6559\u5b66\u4f53\u9a8c\uff0c\u4f46\u8981\u5b9e\u73b0\u6709\u610f\u4e49\u7684\u5b66\u4e60\u6210\u679c\uff0c\u9700\u8981\u5173\u6ce8AI\u5de5\u5177\u5982\u4f55\u5f71\u54cd\u8ba4\u77e5\u53c2\u4e0e\u3001\u81ea\u6211\u8c03\u8282\u548c\u5b66\u4e60\u516c\u5e73\u6027\u3002\u672a\u6765\u8bbe\u8ba1\u5e94\u6700\u5c0f\u5316\u4f9d\u8d56\u3001\u652f\u6301\u5143\u8ba4\u77e5\u53d1\u5c55\uff0c\u5e76\u9488\u5bf9\u4e0d\u540c\u80fd\u529b\u6c34\u5e73\u6821\u51c6\u652f\u6301\u3002"}}
{"id": "2512.12371", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.12371", "abs": "https://arxiv.org/abs/2512.12371", "authors": ["David M. Berry"], "title": "AI Sprints: Towards a Critical Method for Human-AI Collaboration", "comment": null, "summary": "The emergence of Large Language Models presents a remarkable opportunity for humanities and social science research. I argue these technologies instantiate what I have called the algorithmic condition, whereby computational systems increasingly mediate not just our analytical tools but how we understand nature and society more generally. This article introduces the possibility for new forms of humanistic inquiry through what I term 'AI sprints', as intensive time-boxed research sessions. This is a research method combining the critical reflexivity essential to humanistic inquiry with iterative dialogue with generative AI. Drawing on experimental work in critical code studies, I demonstrate how tight loops of iterative development can adapt data and book sprint methodologies whilst acknowledging the profound transformations generative AI introduces. Through examining the process of human-AI collaboration when undertaken in these intensive research sessions, I seek to outline this approach as a broader research method. The article builds on Rogers' digital methods approach, proposing that we extend methodologies to study digital objects through their native protocols, using AI systems not merely to process digital traces but to analyse materials traditionally requiring manual coding or transcription. I aim to show this by introducing three cognitive modes, cognitive delegation, productive augmentation, and cognitive overhead, explaining how researchers can maintain a strategic overview whilst using LLM capabilities. The paper contributes both a practical methodology for intensive AI-augmented research and a theoretical framework for understanding the epistemological transformations of this hybrid method. A critical methodology must therefore operate in both technical and theoretical registers, sustaining a rigorous ethical-computational engagement with AI systems and outputs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"AI\u51b2\u523a\"\u4f5c\u4e3a\u4eba\u6587\u5b66\u79d1\u7814\u7a76\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u6279\u5224\u6027\u53cd\u601d\u4e0e\u751f\u6210\u5f0fAI\u8fed\u4ee3\u5bf9\u8bdd\uff0c\u63a2\u7d22\u4eba\u673a\u534f\u4f5c\u7814\u7a76\u6a21\u5f0f", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u51fa\u73b0\u4e3a\u4eba\u6587\u793e\u79d1\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u673a\u9047\uff0c\u8fd9\u4e9b\u6280\u672f\u4f53\u73b0\u4e86\"\u7b97\u6cd5\u6761\u4ef6\"\uff0c\u5373\u8ba1\u7b97\u7cfb\u7edf\u4e0d\u4ec5\u4e2d\u4ecb\u5206\u6790\u5de5\u5177\uff0c\u8fd8\u5f71\u54cd\u6211\u4eec\u5bf9\u81ea\u7136\u548c\u793e\u4f1a\u7684\u7406\u89e3\u65b9\u5f0f\u3002\u4f5c\u8005\u65e8\u5728\u63a2\u7d22\u901a\u8fc7AI\u589e\u5f3a\u7814\u7a76\u7684\u65b0\u5f62\u5f0f\u4eba\u6587\u63a2\u7a76", "method": "\u63d0\u51fa\"AI\u51b2\u523a\"\u7814\u7a76\u65b9\u6cd5\uff0c\u7ed3\u5408\u6570\u636e\u51b2\u523a\u548c\u4e66\u7c4d\u51b2\u523a\u65b9\u6cd5\uff0c\u91c7\u7528\u5bc6\u96c6\u65f6\u95f4\u9650\u5236\u7684\u7814\u7a76\u4f1a\u8bae\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8eRogers\u7684\u6570\u5b57\u65b9\u6cd5\uff0c\u6269\u5c55\u65b9\u6cd5\u8bba\u4ee5\u901a\u8fc7AI\u7cfb\u7edf\u539f\u751f\u534f\u8bae\u7814\u7a76\u6570\u5b57\u5bf9\u8c61\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u5904\u7406\u6570\u5b57\u75d5\u8ff9\u3002\u5f15\u5165\u4e09\u79cd\u8ba4\u77e5\u6a21\u5f0f\uff1a\u8ba4\u77e5\u59d4\u6258\u3001\u751f\u4ea7\u6027\u589e\u5f3a\u548c\u8ba4\u77e5\u5f00\u9500", "result": "\u8bba\u6587\u8d21\u732e\u4e86AI\u589e\u5f3a\u7814\u7a76\u7684\u5b9e\u7528\u65b9\u6cd5\u8bba\u548c\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u7406\u89e3\u8fd9\u79cd\u6df7\u5408\u65b9\u6cd5\u7684\u8ba4\u8bc6\u8bba\u8f6c\u53d8\u3002\u5c55\u793a\u4e86\u901a\u8fc7\u8fed\u4ee3\u5f00\u53d1\u7d27\u5bc6\u5faa\u73af\u5982\u4f55\u9002\u5e94\u6570\u636e\u51b2\u523a\u65b9\u6cd5\uff0c\u540c\u65f6\u627f\u8ba4\u751f\u6210\u5f0fAI\u5e26\u6765\u7684\u6df1\u523b\u53d8\u9769", "conclusion": "\u6279\u5224\u6027\u65b9\u6cd5\u8bba\u5fc5\u987b\u5728\u6280\u672f\u548c\u7406\u8bba\u4e24\u4e2a\u5c42\u9762\u8fd0\u4f5c\uff0c\u7ef4\u6301\u4e0eAI\u7cfb\u7edf\u548c\u8f93\u51fa\u7684\u4e25\u683c\u4f26\u7406-\u8ba1\u7b97\u53c2\u4e0e\u3002AI\u51b2\u523a\u65b9\u6cd5\u4e3a\u4eba\u6587\u793e\u79d1\u7814\u7a76\u63d0\u4f9b\u4e86\u7ed3\u5408\u6279\u5224\u6027\u53cd\u601d\u4e0eAI\u80fd\u529b\u7684\u65b0\u7814\u7a76\u8303\u5f0f"}}
{"id": "2512.12076", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.12076", "abs": "https://arxiv.org/abs/2512.12076", "authors": ["Yu-Chia Huang", "Juntong Chen", "Dongyu Liu", "Kwan-Liu Ma"], "title": "SigTime: Learning and Visually Explaining Time Series Signatures", "comment": null, "summary": "Understanding and distinguishing temporal patterns in time series data is essential for scientific discovery and decision-making. For example, in biomedical research, uncovering meaningful patterns in physiological signals can improve diagnosis, risk assessment, and patient outcomes. However, existing methods for time series pattern discovery face major challenges, including high computational complexity, limited interpretability, and difficulty in capturing meaningful temporal structures. To address these gaps, we introduce a novel learning framework that jointly trains two Transformer models using complementary time series representations: shapelet-based representations to capture localized temporal structures and traditional feature engineering to encode statistical properties. The learned shapelets serve as interpretable signatures that differentiate time series across classification labels. Additionally, we develop a visual analytics system -- SigTIme -- with coordinated views to facilitate exploration of time series signatures from multiple perspectives, aiding in useful insights generation. We quantitatively evaluate our learning framework on eight publicly available datasets and one proprietary clinical dataset. Additionally, we demonstrate the effectiveness of our system through two usage scenarios along with the domain experts: one involving public ECG data and the other focused on preterm labor analysis.", "AI": {"tldr": "\u63d0\u51faSigTime\u6846\u67b6\uff0c\u7ed3\u5408Transformer\u6a21\u578b\u3001shapelet\u8868\u793a\u548c\u7279\u5f81\u5de5\u7a0b\uff0c\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u6a21\u5f0f\u53d1\u73b0\u548c\u53ef\u89c6\u5316\u5206\u6790", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u6a21\u5f0f\u53d1\u73b0\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u53ef\u89e3\u91ca\u6027\u6709\u9650\u3001\u96be\u4ee5\u6355\u6349\u6709\u610f\u4e49\u7684\u65f6\u5e8f\u7ed3\u6784\u7b49\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u751f\u7269\u533b\u5b66\u7814\u7a76\u4e2d\uff0c\u53d1\u73b0\u751f\u7406\u4fe1\u53f7\u4e2d\u7684\u6709\u610f\u4e49\u6a21\u5f0f\u5bf9\u8bca\u65ad\u548c\u60a3\u8005\u9884\u540e\u81f3\u5173\u91cd\u8981", "method": "\u63d0\u51fa\u8054\u5408\u8bad\u7ec3\u4e24\u4e2aTransformer\u6a21\u578b\u7684\u5b66\u4e60\u6846\u67b6\uff1a\u4f7f\u7528shapelet\u8868\u793a\u6355\u6349\u5c40\u90e8\u65f6\u5e8f\u7ed3\u6784\uff0c\u7ed3\u5408\u4f20\u7edf\u7279\u5f81\u5de5\u7a0b\u7f16\u7801\u7edf\u8ba1\u7279\u6027\uff1b\u5f00\u53d1SigTime\u53ef\u89c6\u5316\u5206\u6790\u7cfb\u7edf\uff0c\u63d0\u4f9b\u591a\u89c6\u89d2\u534f\u8c03\u89c6\u56fe", "result": "\u57288\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u548c1\u4e2a\u4e34\u5e8a\u4e13\u6709\u6570\u636e\u96c6\u4e0a\u5b9a\u91cf\u8bc4\u4f30\u4e86\u5b66\u4e60\u6846\u67b6\uff1b\u901a\u8fc7\u4e24\u4e2a\u4f7f\u7528\u573a\u666f\uff08\u516c\u5171ECG\u6570\u636e\u548c\u65e9\u4ea7\u5206\u6790\uff09\u4e0e\u9886\u57df\u4e13\u5bb6\u4e00\u8d77\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u6709\u6548\u6027", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u548c\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u53d1\u73b0\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u6709\u610f\u4e49\u6a21\u5f0f\uff0c\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5728\u751f\u7269\u533b\u5b66\u7b49\u9886\u57df\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2512.12165", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12165", "abs": "https://arxiv.org/abs/2512.12165", "authors": ["Daniel Adebi", "Sagnik Majumder", "Kristen Grauman"], "title": "Audio-Visual Camera Pose Estimationn with Passive Scene Sounds and In-the-Wild Video", "comment": null, "summary": "Understanding camera motion is a fundamental problem in embodied perception and 3D scene understanding. While visual methods have advanced rapidly, they often struggle under visually degraded conditions such as motion blur or occlusions. In this work, we show that passive scene sounds provide complementary cues for relative camera pose estimation for in-the-wild videos. We introduce a simple but effective audio-visual framework that integrates direction-ofarrival (DOA) spectra and binauralized embeddings into a state-of-the-art vision-only pose estimation model. Our results on two large datasets show consistent gains over strong visual baselines, plus robustness when the visual information is corrupted. To our knowledge, this represents the first work to successfully leverage audio for relative camera pose estimation in real-world videos, and it establishes incidental, everyday audio as an unexpected but promising signal for a classic spatial challenge. Project: http://vision.cs.utexas.edu/projects/av_camera_pose.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u5229\u7528\u97f3\u9891\u4fe1\u606f\u8fdb\u884c\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u4e2d\u76f8\u5bf9\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u97f3\u9891-\u89c6\u89c9\u6846\u67b6\u5728\u89c6\u89c9\u4fe1\u606f\u9000\u5316\u65f6\u4ecd\u80fd\u4fdd\u6301\u9c81\u68d2\u6027", "motivation": "\u89c6\u89c9\u65b9\u6cd5\u5728\u8fd0\u52a8\u6a21\u7cca\u3001\u906e\u6321\u7b49\u89c6\u89c9\u9000\u5316\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u88ab\u52a8\u573a\u666f\u58f0\u97f3\u63d0\u4f9b\u4e86\u4e92\u8865\u7ebf\u7d22\uff0c\u53ef\u7528\u4e8e\u589e\u5f3a\u76f8\u673a\u59ff\u6001\u4f30\u8ba1", "method": "\u63d0\u51fa\u7b80\u5355\u7684\u97f3\u9891-\u89c6\u89c9\u6846\u67b6\uff0c\u5c06\u5230\u8fbe\u65b9\u5411\u8c31\u548c\u53cc\u8033\u5316\u5d4c\u5165\u96c6\u6210\u5230\u6700\u5148\u8fdb\u7684\u7eaf\u89c6\u89c9\u59ff\u6001\u4f30\u8ba1\u6a21\u578b\u4e2d", "result": "\u5728\u4e24\u4e2a\u5927\u578b\u6570\u636e\u96c6\u4e0a\u76f8\u6bd4\u5f3a\u89c6\u89c9\u57fa\u7ebf\u83b7\u5f97\u4e00\u81f4\u63d0\u5347\uff0c\u5728\u89c6\u89c9\u4fe1\u606f\u53d7\u635f\u65f6\u8868\u73b0\u51fa\u9c81\u68d2\u6027", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u6210\u529f\u5229\u7528\u97f3\u9891\u8fdb\u884c\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u4e2d\u76f8\u5bf9\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u7684\u5de5\u4f5c\uff0c\u786e\u7acb\u4e86\u65e5\u5e38\u97f3\u9891\u4f5c\u4e3a\u7ecf\u5178\u7a7a\u95f4\u6311\u6218\u7684\u610f\u5916\u4f46\u6709\u524d\u666f\u7684\u4fe1\u53f7"}}
{"id": "2512.12592", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.12592", "abs": "https://arxiv.org/abs/2512.12592", "authors": ["Tom Lee", "Sihoon Lee", "Seonghun Kim"], "title": "Beyond Static Scoring: Enhancing Assessment Validity via AI-Generated Interactive Verification", "comment": null, "summary": "Large Language Models (LLMs) challenge the validity of traditional open-ended assessments by blurring the lines of authorship. While recent research has focused on the accuracy of automated scoring (AES), these static approaches fail to capture process evidence or verify genuine student understanding. This paper introduces a novel Human-AI Collaboration framework that enhances assessment integrity by combining rubric-based automated scoring with AI-generated, targeted follow-up questions. In a pilot study with university instructors (N=9), we demonstrate that while Stage 1 (Auto-Scoring) ensures procedural fairness and consistency, Stage 2 (Interactive Verification) is essential for construct validity, effectively diagnosing superficial reasoning or unverified AI use. We report on the systems design, instructor perceptions of fairness versus validity, and the necessity of adaptive difficulty in follow-up questioning. The findings offer a scalable pathway for authentic assessment that moves beyond policing AI to integrating it as a synergistic partner in the evaluation process.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4eba\u673a\u534f\u4f5c\u8bc4\u4f30\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u52a8\u8bc4\u5206\u548cAI\u751f\u6210\u7684\u9488\u5bf9\u6027\u8ffd\u95ee\uff0c\u4ee5\u5e94\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u5bf9\u5f00\u653e\u5f0f\u8bc4\u4f30\u7684\u6311\u6218\uff0c\u786e\u4fdd\u8bc4\u4f30\u7684\u771f\u5b9e\u6027\u548c\u6709\u6548\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u6a21\u7cca\u4e86\u4f5c\u8005\u8eab\u4efd\u754c\u9650\uff0c\u6311\u6218\u4f20\u7edf\u5f00\u653e\u5f0f\u8bc4\u4f30\u7684\u6709\u6548\u6027\u3002\u73b0\u6709\u81ea\u52a8\u8bc4\u5206\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u8fc7\u7a0b\u8bc1\u636e\u6216\u9a8c\u8bc1\u5b66\u751f\u771f\u5b9e\u7406\u89e3\uff0c\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4eba\u673a\u534f\u4f5c\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u57fa\u4e8e\u91cf\u89c4\u7684\u81ea\u52a8\u8bc4\u5206\u786e\u4fdd\u7a0b\u5e8f\u516c\u5e73\u6027\uff1b\u7b2c\u4e8c\u9636\u6bb5AI\u751f\u6210\u9488\u5bf9\u6027\u8ffd\u95ee\u8fdb\u884c\u4ea4\u4e92\u9a8c\u8bc1\uff0c\u8bca\u65ad\u8868\u9762\u63a8\u7406\u6216AI\u4f7f\u7528\u60c5\u51b5\u3002", "result": "\u8bd5\u70b9\u7814\u7a76\u663e\u793a\uff0c\u7b2c\u4e00\u9636\u6bb5\u786e\u4fdd\u516c\u5e73\u6027\u548c\u4e00\u81f4\u6027\uff0c\u7b2c\u4e8c\u9636\u6bb5\u5bf9\u6784\u5ff5\u6548\u5ea6\u81f3\u5173\u91cd\u8981\uff0c\u80fd\u6709\u6548\u8bca\u65ad\u8868\u9762\u63a8\u7406\u6216\u672a\u7ecf\u9a8c\u8bc1\u7684AI\u4f7f\u7528\u3002\u6559\u5e08\u8ba4\u53ef\u516c\u5e73\u6027\u4e0e\u6548\u5ea6\u7684\u5e73\u8861\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u771f\u5b9e\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u8def\u5f84\uff0c\u8d85\u8d8a\u4e86\u5355\u7eaf\u76d1\u7ba1AI\uff0c\u800c\u662f\u5c06\u5176\u6574\u5408\u4e3a\u8bc4\u4f30\u8fc7\u7a0b\u4e2d\u7684\u534f\u540c\u4f19\u4f34\uff0c\u786e\u4fdd\u8bc4\u4f30\u7684\u5b8c\u6574\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2512.12804", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12804", "abs": "https://arxiv.org/abs/2512.12804", "authors": ["Sander Beckers"], "title": "Causal Counterfactuals Reconsidered", "comment": "Preprint: currently under review", "summary": "I develop a novel semantics for probabilities of counterfactuals that generalizes the standard Pearlian semantics: it applies to probabilistic causal models that cannot be extended into realistic structural causal models and are therefore beyond the scope of Pearl's semantics. This generalization is needed because, as I show, such probabilistic causal models arise even in simple settings. My semantics offer a natural compromize in the long-standing debate between Pearl and Dawid over counterfactuals: I agree with Dawid that universal causal determinism and unrealistic variables should be rejected, but I agree with Pearl that a general semantics of counterfactuals is nonetheless possible. I restrict attention to causal models that satisfy the Markov condition, only contain realistic variables, and are causally complete. Although I formulate my proposal using structural causal models, as does Pearl, I refrain from using so-called response variables. Moreover, I prove that my semantics is equivalent to two other recent proposals that do not involve structural causal models, and that it is in line with various comments on stochastic counterfactuals that have appeared in the literature more broadly. Throughout I also reflect on the universality of the Markov condition and explore a novel generalization of causal abstractions", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53cd\u4e8b\u5b9e\u6982\u7387\u8bed\u4e49\u5b66\uff0c\u63a8\u5e7f\u4e86\u6807\u51c6\u7684Pearl\u8bed\u4e49\u5b66\uff0c\u9002\u7528\u4e8e\u65e0\u6cd5\u6269\u5c55\u4e3a\u73b0\u5b9e\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u7684\u6982\u7387\u56e0\u679c\u6a21\u578b", "motivation": "\u9700\u8981\u5904\u7406Pearl\u8bed\u4e49\u5b66\u65e0\u6cd5\u6db5\u76d6\u7684\u6982\u7387\u56e0\u679c\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u5373\u4f7f\u5728\u7b80\u5355\u8bbe\u7f6e\u4e2d\u4e5f\u4f1a\u51fa\u73b0\u3002\u5728Pearl\u548cDawid\u5173\u4e8e\u53cd\u4e8b\u5b9e\u7684\u957f\u671f\u4e89\u8bba\u4e2d\u5bfb\u6c42\u6298\u4e2d\u65b9\u6848", "method": "\u9650\u5236\u5173\u6ce8\u6ee1\u8db3\u9a6c\u5c14\u53ef\u592b\u6761\u4ef6\u3001\u4ec5\u5305\u542b\u73b0\u5b9e\u53d8\u91cf\u4e14\u56e0\u679c\u5b8c\u5907\u7684\u56e0\u679c\u6a21\u578b\u3002\u867d\u7136\u4f7f\u7528\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff0c\u4f46\u907f\u514d\u4f7f\u7528\u6240\u8c13\u7684\u54cd\u5e94\u53d8\u91cf", "result": "\u8bc1\u660e\u4e86\u65b0\u8bed\u4e49\u5b66\u4e0e\u53e6\u5916\u4e24\u4e2a\u4e0d\u6d89\u53ca\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u7684\u8fd1\u671f\u63d0\u6848\u7b49\u4ef7\uff0c\u5e76\u4e14\u4e0e\u6587\u732e\u4e2d\u5173\u4e8e\u968f\u673a\u53cd\u4e8b\u5b9e\u7684\u5404\u79cd\u8bc4\u8bba\u4e00\u81f4", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6298\u4e2d\u65b9\u6848\uff1a\u540c\u610fDawid\u62d2\u7edd\u666e\u904d\u56e0\u679c\u51b3\u5b9a\u8bba\u548c\u4e0d\u73b0\u5b9e\u53d8\u91cf\uff0c\u4f46\u540c\u610fPearl\u8ba4\u4e3a\u4e00\u822c\u53cd\u4e8b\u5b9e\u8bed\u4e49\u5b66\u662f\u53ef\u80fd\u7684\u3002\u540c\u65f6\u63a2\u8ba8\u4e86\u9a6c\u5c14\u53ef\u592b\u6761\u4ef6\u7684\u666e\u904d\u6027\u548c\u56e0\u679c\u62bd\u8c61\u7684\u65b0\u63a8\u5e7f"}}
{"id": "2512.12086", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.12086", "abs": "https://arxiv.org/abs/2512.12086", "authors": ["Xin Yang", "Omid Ardakanian"], "title": "CLOAK: Contrastive Guidance for Latent Diffusion-Based Data Obfuscation", "comment": null, "summary": "Data obfuscation is a promising technique for mitigating attribute inference attacks by semi-trusted parties with access to time-series data emitted by sensors. Recent advances leverage conditional generative models together with adversarial training or mutual information-based regularization to balance data privacy and utility. However, these methods often require modifying the downstream task, struggle to achieve a satisfactory privacy-utility trade-off, or are computationally intensive, making them impractical for deployment on resource-constrained mobile IoT devices. We propose Cloak, a novel data obfuscation framework based on latent diffusion models. In contrast to prior work, we employ contrastive learning to extract disentangled representations, which guide the latent diffusion process to retain useful information while concealing private information. This approach enables users with diverse privacy needs to navigate the privacy-utility trade-off with minimal retraining. Extensive experiments on four public time-series datasets, spanning multiple sensing modalities, and a dataset of facial images demonstrate that Cloak consistently outperforms state-of-the-art obfuscation techniques and is well-suited for deployment in resource-constrained settings.", "AI": {"tldr": "Cloak\u662f\u4e00\u4e2a\u57fa\u4e8e\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u6570\u636e\u6df7\u6dc6\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u63d0\u53d6\u89e3\u8026\u8868\u793a\uff0c\u6307\u5bfc\u6269\u6563\u8fc7\u7a0b\u5728\u4fdd\u7559\u6709\u7528\u4fe1\u606f\u7684\u540c\u65f6\u9690\u85cf\u9690\u79c1\u4fe1\u606f\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u7684\u79fb\u52a8\u7269\u8054\u7f51\u8bbe\u5907\u4e0a\u5b9e\u73b0\u66f4\u597d\u7684\u9690\u79c1-\u6548\u7528\u6743\u8861\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u6df7\u6dc6\u65b9\u6cd5\u5b58\u5728\u4ee5\u4e0b\u95ee\u9898\uff1a\u9700\u8981\u4fee\u6539\u4e0b\u6e38\u4efb\u52a1\u3001\u96be\u4ee5\u8fbe\u5230\u6ee1\u610f\u7684\u9690\u79c1-\u6548\u7528\u6743\u8861\u3001\u8ba1\u7b97\u5bc6\u96c6\u4e0d\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u79fb\u52a8\u7269\u8054\u7f51\u8bbe\u5907\u90e8\u7f72\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u6570\u636e\u6df7\u6dc6\u65b9\u6848\u3002", "method": "\u63d0\u51faCloak\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u63d0\u53d6\u89e3\u8026\u8868\u793a\uff1b2\uff09\u5229\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\u8fdb\u884c\u6570\u636e\u6df7\u6dc6\uff1b3\uff09\u901a\u8fc7\u89e3\u8026\u8868\u793a\u6307\u5bfc\u6269\u6563\u8fc7\u7a0b\uff0c\u5728\u4fdd\u7559\u6709\u7528\u4fe1\u606f\u7684\u540c\u65f6\u9690\u85cf\u9690\u79c1\u4fe1\u606f\uff1b4\uff09\u652f\u6301\u7528\u6237\u6839\u636e\u9690\u79c1\u9700\u6c42\u7075\u6d3b\u8c03\u6574\u9690\u79c1-\u6548\u7528\u6743\u8861\uff0c\u65e0\u9700\u5927\u91cf\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5171\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\uff08\u6db5\u76d6\u591a\u79cd\u4f20\u611f\u6a21\u6001\uff09\u548c\u4e00\u4e2a\u4eba\u8138\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff1aCloak\u5728\u9690\u79c1\u4fdd\u62a4\u6548\u679c\u4e0a\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6df7\u6dc6\u6280\u672f\uff0c\u540c\u65f6\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u8868\u73b0\u826f\u597d\u3002", "conclusion": "Cloak\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u9690\u79c1-\u6548\u7528\u6743\u8861\uff0c\u4e14\u9002\u5408\u5728\u8d44\u6e90\u53d7\u9650\u7684\u79fb\u52a8\u7269\u8054\u7f51\u8bbe\u5907\u4e0a\u90e8\u7f72\uff0c\u4e3a\u534a\u53ef\u4fe1\u65b9\u8bbf\u95ee\u4f20\u611f\u5668\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u65f6\u7684\u5c5e\u6027\u63a8\u7406\u653b\u51fb\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7f13\u89e3\u65b9\u6848\u3002"}}
{"id": "2512.12707", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.12707", "abs": "https://arxiv.org/abs/2512.12707", "authors": ["Hugo Roger Paz"], "title": "From Linear Risk to Emergent Harm: Complexity as the Missing Core of AI Governance", "comment": "White Paper / Policy Brief (Working Paper). Published version available at: https://doi.org/10.5281/zenodo.17929014", "summary": "Risk-based AI regulation has become the dominant paradigm in AI governance, promising proportional controls aligned with anticipated harms. This paper argues that such frameworks often fail for structural reasons: they implicitly assume linear causality, stable system boundaries, and largely predictable responses to regulation. In practice, AI operates within complex adaptive socio-technical systems in which harm is frequently emergent, delayed, redistributed, and amplified through feedback loops and strategic adaptation by system actors. As a result, compliance can increase while harm is displaced or concealed rather than eliminated. We propose a complexity-based framework for AI governance that treats regulation as intervention rather than control, prioritises dynamic system mapping over static classifications, and integrates causal reasoning and simulation for policy design under uncertainty. The aim is not to eliminate uncertainty, but to enable robust system stewardship through monitoring, learning, and iterative revision of governance interventions.", "AI": {"tldr": "\u672c\u6587\u6279\u8bc4\u4e86\u57fa\u4e8e\u98ce\u9669\u7684AI\u76d1\u7ba1\u6846\u67b6\u7684\u7ed3\u6784\u6027\u7f3a\u9677\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u590d\u6742\u6027\u7684AI\u6cbb\u7406\u65b0\u6846\u67b6", "motivation": "\u57fa\u4e8e\u98ce\u9669\u7684AI\u76d1\u7ba1\u5df2\u6210\u4e3aAI\u6cbb\u7406\u7684\u4e3b\u5bfc\u8303\u5f0f\uff0c\u4f46\u8fd9\u7c7b\u6846\u67b6\u5f80\u5f80\u56e0\u7ed3\u6784\u6027\u539f\u56e0\u800c\u5931\u8d25\u3002\u5b83\u4eec\u9690\u542b\u5730\u5047\u8bbe\u7ebf\u6027\u56e0\u679c\u5173\u7cfb\u3001\u7a33\u5b9a\u7684\u7cfb\u7edf\u8fb9\u754c\u548c\u53ef\u9884\u6d4b\u7684\u76d1\u7ba1\u54cd\u5e94\uff0c\u800c\u5b9e\u9645\u4e0aAI\u5728\u590d\u6742\u7684\u81ea\u9002\u5e94\u793e\u4f1a\u6280\u672f\u7cfb\u7edf\u4e2d\u8fd0\u884c\uff0c\u5371\u5bb3\u5e38\u5e38\u662f\u6d8c\u73b0\u7684\u3001\u5ef6\u8fdf\u7684\u3001\u91cd\u65b0\u5206\u914d\u7684\uff0c\u5e76\u901a\u8fc7\u53cd\u9988\u5faa\u73af\u548c\u7cfb\u7edf\u53c2\u4e0e\u8005\u7684\u6218\u7565\u9002\u5e94\u800c\u653e\u5927\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u590d\u6742\u6027\u7684AI\u6cbb\u7406\u6846\u67b6\uff0c\u5c06\u76d1\u7ba1\u89c6\u4e3a\u5e72\u9884\u800c\u975e\u63a7\u5236\uff0c\u4f18\u5148\u8003\u8651\u52a8\u6001\u7cfb\u7edf\u6620\u5c04\u800c\u975e\u9759\u6001\u5206\u7c7b\uff0c\u5e76\u6574\u5408\u56e0\u679c\u63a8\u7406\u548c\u6a21\u62df\u6765\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u653f\u7b56\u8bbe\u8ba1\u3002", "result": "\u5408\u89c4\u6027\u53ef\u80fd\u589e\u52a0\uff0c\u4f46\u5371\u5bb3\u53ea\u662f\u88ab\u8f6c\u79fb\u6216\u9690\u85cf\u800c\u975e\u6d88\u9664\u3002\u65b0\u6846\u67b6\u7684\u76ee\u6807\u4e0d\u662f\u6d88\u9664\u4e0d\u786e\u5b9a\u6027\uff0c\u800c\u662f\u901a\u8fc7\u76d1\u63a7\u3001\u5b66\u4e60\u548c\u8fed\u4ee3\u4fee\u8ba2\u6cbb\u7406\u5e72\u9884\u6765\u5b9e\u73b0\u7a33\u5065\u7684\u7cfb\u7edf\u7ba1\u7406\u3002", "conclusion": "\u9700\u8981\u4ece\u4f20\u7edf\u7684\u57fa\u4e8e\u98ce\u9669\u7684\u76d1\u7ba1\u8303\u5f0f\u8f6c\u5411\u57fa\u4e8e\u590d\u6742\u6027\u7684\u6cbb\u7406\u65b9\u6cd5\uff0c\u4ee5\u5e94\u5bf9AI\u5728\u590d\u6742\u793e\u4f1a\u6280\u672f\u7cfb\u7edf\u4e2d\u4ea7\u751f\u7684\u6d8c\u73b0\u6027\u5371\u5bb3\u548c\u7cfb\u7edf\u6027\u98ce\u9669\u3002"}}
{"id": "2512.12199", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12199", "abs": "https://arxiv.org/abs/2512.12199", "authors": ["Ercan Erkalkan", "Vedat Topuz", "Ay\u00e7a Ak"], "title": "Thermal RGB Fusion for Micro-UAV Wildfire Perimeter Tracking with Minimal Comms", "comment": "Conference paper in 17th International Scientific Studies Congress proceedings. Topic: thermal+RGB rule level fusion, RDP boundary simplification, leader follower guidance, sub 50ms embedded SoC, minimal communications for wildfire perimeter tracking. Thermal RGB Fusion for Micro-UAV", "summary": "This study introduces a lightweight perimeter tracking method designed for micro UAV teams operating over wildfire environments under limited bandwidth conditions. Thermal image frames generate coarse hot region masks through adaptive thresholding and morphological refinement, while RGB frames contribute edge cues and suppress texture related false detections using gradient based filtering. A rule level merging strategy selects boundary candidates and simplifies them via the Ramer Douglas Peucker algorithm. The system incorporates periodic beacons and an inertial feedback loop that maintains trajectory stability in the presence of GPS degradation. The guidance loop targets sub 50 ms latency on embedded System on Chip (SoC) platforms by constraining per frame pixel operations and precomputing gradient tables. Small scale simulations demonstrate reductions in average path length and boundary jitter compared to a pure edge tracking baseline, while maintaining environmental coverage measured through intersection merge analysis. Battery consumption and computational utilization confirm the feasibility of achieving 10, 15 m/s forward motion on standard micro platforms. This approach enables rapid deployment in the field, requiring robust sensing and minimal communications for emergency reconnaissance applications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5468\u754c\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u6709\u9650\u5e26\u5bbd\u6761\u4ef6\u4e0b\u5728\u91ce\u706b\u73af\u5883\u4e2d\u64cd\u4f5c\u7684\u5fae\u578b\u65e0\u4eba\u673a\u7f16\u961f\u3002\u901a\u8fc7\u70ed\u6210\u50cf\u548cRGB\u56fe\u50cf\u878d\u5408\u5904\u7406\uff0c\u7ed3\u5408\u5468\u671f\u6027\u4fe1\u6807\u548c\u60ef\u6027\u53cd\u9988\uff0c\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u3001\u7a33\u5b9a\u7684\u8fb9\u754c\u8ddf\u8e2a\u3002", "motivation": "\u9488\u5bf9\u91ce\u706b\u73af\u5883\u4e0b\u7684\u7d27\u6025\u4fa6\u5bdf\u5e94\u7528\uff0c\u9700\u8981\u5fae\u578b\u65e0\u4eba\u673a\u7f16\u961f\u5728\u6709\u9650\u5e26\u5bbd\u6761\u4ef6\u4e0b\u8fdb\u884c\u5feb\u901f\u90e8\u7f72\uff0c\u8981\u6c42\u9c81\u68d2\u611f\u77e5\u548c\u6700\u5c0f\u5316\u901a\u4fe1\uff0c\u540c\u65f6\u4fdd\u6301\u8f68\u8ff9\u7a33\u5b9a\u6027\u3002", "method": "1. \u70ed\u6210\u50cf\u5e27\u901a\u8fc7\u81ea\u9002\u5e94\u9608\u503c\u548c\u5f62\u6001\u5b66\u7ec6\u5316\u751f\u6210\u7c97\u7565\u7684\u70ed\u533a\u57df\u63a9\u7801\uff1b2. RGB\u5e27\u63d0\u4f9b\u8fb9\u7f18\u7ebf\u7d22\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u6ee4\u6ce2\u6291\u5236\u7eb9\u7406\u76f8\u5173\u7684\u8bef\u68c0\u6d4b\uff1b3. \u89c4\u5219\u7ea7\u878d\u5408\u7b56\u7565\u9009\u62e9\u8fb9\u754c\u5019\u9009\uff0c\u901a\u8fc7Ramer-Douglas-Peucker\u7b97\u6cd5\u7b80\u5316\uff1b4. \u7cfb\u7edf\u5305\u542b\u5468\u671f\u6027\u4fe1\u6807\u548c\u60ef\u6027\u53cd\u9988\u56de\u8def\uff0c\u5728GPS\u964d\u7ea7\u65f6\u4fdd\u6301\u8f68\u8ff9\u7a33\u5b9a\u6027\uff1b5. \u5728\u5d4c\u5165\u5f0fSoC\u5e73\u53f0\u4e0a\u901a\u8fc7\u9650\u5236\u6bcf\u5e27\u50cf\u7d20\u64cd\u4f5c\u548c\u9884\u8ba1\u7b97\u68af\u5ea6\u8868\u5b9e\u73b0\u4e9a50ms\u5ef6\u8fdf\u3002", "result": "\u5c0f\u89c4\u6a21\u6a21\u62df\u663e\u793a\uff1a\u4e0e\u7eaf\u8fb9\u7f18\u8ddf\u8e2a\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5e73\u5747\u8def\u5f84\u957f\u5ea6\u548c\u8fb9\u754c\u6296\u52a8\u51cf\u5c11\uff0c\u540c\u65f6\u901a\u8fc7\u4ea4\u96c6\u5408\u5e76\u5206\u6790\u4fdd\u6301\u73af\u5883\u8986\u76d6\uff1b\u7535\u6c60\u6d88\u8017\u548c\u8ba1\u7b97\u5229\u7528\u7387\u8bc1\u5b9e\u4e86\u5728\u6807\u51c6\u5fae\u578b\u5e73\u53f0\u4e0a\u5b9e\u73b010-15 m/s\u524d\u5411\u8fd0\u52a8\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5feb\u901f\u73b0\u573a\u90e8\u7f72\uff0c\u4e3a\u7d27\u6025\u4fa6\u5bdf\u5e94\u7528\u63d0\u4f9b\u4e86\u9c81\u68d2\u611f\u77e5\u548c\u6700\u5c0f\u5316\u901a\u4fe1\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u6709\u9650\u5e26\u5bbd\u6761\u4ef6\u4e0b\u6709\u6548\u652f\u6301\u5fae\u578b\u65e0\u4eba\u673a\u7f16\u961f\u5728\u91ce\u706b\u73af\u5883\u4e2d\u7684\u5468\u754c\u8ddf\u8e2a\u4efb\u52a1\u3002"}}
{"id": "2512.12837", "categories": ["cs.CY", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.12837", "abs": "https://arxiv.org/abs/2512.12837", "authors": ["Sahibpreet Singh", "Manjit Singh"], "title": "Algorithmic Criminal Liability in Greenwashing: Comparing India, United States, and European Union", "comment": "Published in HPNLU Journal of Law, Business and Economics, Vol. 3, 2024, pp. 51-68. ISSN: 2584-0436", "summary": "AI-powered greenwashing has emerged as an insidious challenge within corporate sustainability governance, exacerbating the opacity of environmental disclosures and subverting regulatory oversight. This study conducts a comparative legal analysis of criminal liability for AI-mediated greenwashing across India, the US, and the EU, exposing doctrinal lacunae in attributing culpability when deceptive claims originate from algorithmic systems. Existing statutes exhibit anthropocentric biases by predicating liability on demonstrable human intent, rendering them ill-equipped to address algorithmic deception. The research identifies a critical gap in jurisprudential adaptation, as prevailing fraud statutes remain antiquated vis-\u00e0-vis AI-generated misrepresentation. Utilising a doctrinal legal methodology, this study systematically dissects judicial precedents and statutory instruments, yielding results regarding the potential expansion of corporate criminal liability. Findings underscore the viability of strict liability models, recalibrated governance frameworks for AI accountability, and algorithmic due diligence mandates under ESG regimes. Comparative insights reveal jurisdictional disparities, with the EU Corporate Sustainability Due Diligence Directive (CSDDD) offering a potential transnational model. This study contributes to AI ethics and environmental jurisprudence by advocating for a hybrid liability framework integrating algorithmic risk assessment with legal personhood constructs, ensuring algorithmic opacity does not preclude liability enforcement.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u5206\u6790\u4e86\u5370\u5ea6\u3001\u7f8e\u56fd\u548c\u6b27\u76df\u5bf9AI\u9a71\u52a8\u7684\"\u7eff\u8272\u6e05\u6d17\"\uff08greenwashing\uff09\u7684\u5211\u4e8b\u8d23\u4efb\u8ba4\u5b9a\uff0c\u53d1\u73b0\u73b0\u6709\u6cd5\u5f8b\u5b58\u5728\u4eba\u7c7b\u4e2d\u5fc3\u4e3b\u4e49\u504f\u89c1\uff0c\u96be\u4ee5\u8ffd\u7a76\u7b97\u6cd5\u7cfb\u7edf\u7684\u6b3a\u9a97\u8d23\u4efb\uff0c\u5e76\u63d0\u51fa\u6df7\u5408\u8d23\u4efb\u6846\u67b6\u5efa\u8bae\u3002", "motivation": "AI\u9a71\u52a8\u7684\u7eff\u8272\u6e05\u6d17\u5df2\u6210\u4e3a\u4f01\u4e1a\u53ef\u6301\u7eed\u53d1\u5c55\u6cbb\u7406\u4e2d\u7684\u9690\u853d\u6311\u6218\uff0c\u52a0\u5267\u4e86\u73af\u5883\u4fe1\u606f\u62ab\u9732\u7684\u4e0d\u900f\u660e\u6027\u5e76\u89c4\u907f\u76d1\u7ba1\u76d1\u7763\u3002\u73b0\u6709\u6cd5\u5f8b\u57fa\u4e8e\u4eba\u7c7b\u610f\u56fe\u7684\u8d23\u4efb\u8ba4\u5b9a\u6a21\u5f0f\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u7b97\u6cd5\u7cfb\u7edf\u7684\u6b3a\u9a97\u884c\u4e3a\u3002", "method": "\u91c7\u7528\u6cd5\u5f8b\u6559\u4e49\u5b66\u65b9\u6cd5\uff0c\u7cfb\u7edf\u5206\u6790\u5370\u5ea6\u3001\u7f8e\u56fd\u548c\u6b27\u76df\u7684\u53f8\u6cd5\u5224\u4f8b\u548c\u6cd5\u89c4\uff0c\u6bd4\u8f83\u4e0d\u540c\u6cd5\u57df\u5bf9AI\u4e2d\u4ecb\u7eff\u8272\u6e05\u6d17\u7684\u5211\u4e8b\u8d23\u4efb\u8ba4\u5b9a\u673a\u5236\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u6b3a\u8bc8\u6cd5\u89c4\u9762\u5bf9AI\u751f\u6210\u7684\u865a\u5047\u9648\u8ff0\u663e\u5f97\u8fc7\u65f6\uff0c\u5b58\u5728\u8d23\u4efb\u8ba4\u5b9a\u7a7a\u767d\u3002\u4e25\u683c\u8d23\u4efb\u6a21\u5f0f\u3001\u91cd\u65b0\u8c03\u6574\u7684AI\u95ee\u8d23\u6cbb\u7406\u6846\u67b6\u4ee5\u53caESG\u5236\u5ea6\u4e0b\u7684\u7b97\u6cd5\u5c3d\u804c\u8c03\u67e5\u8981\u6c42\u5177\u6709\u53ef\u884c\u6027\u3002\u6b27\u76df\u300a\u4f01\u4e1a\u53ef\u6301\u7eed\u53d1\u5c55\u5c3d\u804c\u8c03\u67e5\u6307\u4ee4\u300b\u63d0\u4f9b\u4e86\u6f5c\u5728\u7684\u8de8\u56fd\u6a21\u5f0f\u3002", "conclusion": "\u7814\u7a76\u4e3b\u5f20\u5efa\u7acb\u6df7\u5408\u8d23\u4efb\u6846\u67b6\uff0c\u5c06\u7b97\u6cd5\u98ce\u9669\u8bc4\u4f30\u4e0e\u6cd5\u5f8b\u4eba\u683c\u6982\u5ff5\u76f8\u7ed3\u5408\uff0c\u786e\u4fdd\u7b97\u6cd5\u4e0d\u900f\u660e\u6027\u4e0d\u4f1a\u963b\u788d\u8d23\u4efb\u8ffd\u7a76\uff0c\u4e3aAI\u4f26\u7406\u548c\u73af\u5883\u6cd5\u7406\u5b66\u505a\u51fa\u8d21\u732e\u3002"}}
{"id": "2512.12856", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12856", "abs": "https://arxiv.org/abs/2512.12856", "authors": ["Saad Alqithami"], "title": "Forgetful but Faithful: A Cognitive Memory Architecture and Benchmark for Privacy-Aware Generative Agents", "comment": null, "summary": "As generative agents become increasingly sophisticated and deployed in long-term interactive scenarios, their memory management capabilities emerge as a critical bottleneck for both performance and privacy. Current approaches either maintain unlimited memory stores, leading to computational intractability and privacy concerns, or employ simplistic forgetting mechanisms that compromise agent coherence and functionality. This paper introduces the Memory-Aware Retention Schema (MaRS), a novel framework for human-centered memory management in generative agents, coupled with six theoretically-grounded forgetting policies that balance performance, privacy, and computational efficiency. We present the Forgetful but Faithful Agent (FiFA) benchmark, a comprehensive evaluation framework that assesses agent performance across narrative coherence, goal completion, social recall accuracy, privacy preservation, and cost efficiency. Through extensive experimentation involving 300 evaluation runs across multiple memory budgets and agent configurations, we demonstrate that our hybrid forgetting policy achieves superior performance (composite score: 0.911) while maintaining computational tractability and privacy guarantees. Our work establishes new benchmarks for memory-budgeted agent evaluation and provides practical guidelines for deploying generative agents in resource-constrained, privacy-sensitive environments. The theoretical foundations, implementation framework, and empirical results contribute to the emerging field of human-centered AI by addressing fundamental challenges in agent memory management that directly impact user trust, system scalability, and regulatory compliance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMemory-Aware Retention Schema (MaRS)\u6846\u67b6\u548c\u516d\u79cd\u9057\u5fd8\u7b56\u7565\uff0c\u7528\u4e8e\u89e3\u51b3\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u5728\u957f\u671f\u4ea4\u4e92\u4e2d\u7684\u5185\u5b58\u7ba1\u7406\u548c\u9690\u79c1\u95ee\u9898\uff0c\u5e76\u901a\u8fc7FiFA\u57fa\u51c6\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u6df7\u5408\u9057\u5fd8\u7b56\u7565\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u65e5\u76ca\u590d\u6742\u5e76\u90e8\u7f72\u4e8e\u957f\u671f\u4ea4\u4e92\u573a\u666f\uff0c\u5176\u5185\u5b58\u7ba1\u7406\u80fd\u529b\u6210\u4e3a\u6027\u80fd\u548c\u9690\u79c1\u7684\u5173\u952e\u74f6\u9888\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u7ef4\u62a4\u65e0\u9650\u5185\u5b58\u5b58\u50a8\u5bfc\u81f4\u8ba1\u7b97\u4e0d\u53ef\u884c\u548c\u9690\u79c1\u95ee\u9898\uff0c\u8981\u4e48\u91c7\u7528\u7b80\u5355\u9057\u5fd8\u673a\u5236\u635f\u5bb3\u667a\u80fd\u4f53\u4e00\u81f4\u6027\u548c\u529f\u80fd\u3002", "method": "\u63d0\u51faMemory-Aware Retention Schema (MaRS)\u6846\u67b6\uff0c\u7ed3\u5408\u516d\u79cd\u7406\u8bba\u57fa\u7840\u7684\u9057\u5fd8\u7b56\u7565\uff0c\u5e73\u8861\u6027\u80fd\u3001\u9690\u79c1\u548c\u8ba1\u7b97\u6548\u7387\u3002\u5f00\u53d1Forgetful but Faithful Agent (FiFA)\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u8bc4\u4f30\u667a\u80fd\u4f53\u5728\u53d9\u4e8b\u4e00\u81f4\u6027\u3001\u76ee\u6807\u5b8c\u6210\u3001\u793e\u4ea4\u56de\u5fc6\u51c6\u786e\u6027\u3001\u9690\u79c1\u4fdd\u62a4\u548c\u6210\u672c\u6548\u7387\u7b49\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "\u901a\u8fc7300\u6b21\u8bc4\u4f30\u5b9e\u9a8c\uff0c\u6df7\u5408\u9057\u5fd8\u7b56\u7565\u5728\u591a\u4e2a\u5185\u5b58\u9884\u7b97\u548c\u667a\u80fd\u4f53\u914d\u7f6e\u4e2d\u53d6\u5f97\u6700\u4f73\u6027\u80fd\uff08\u7efc\u5408\u5f97\u5206\uff1a0.911\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u53ef\u884c\u6027\u548c\u9690\u79c1\u4fdd\u8bc1\u3002\u5efa\u7acb\u4e86\u5185\u5b58\u9884\u7b97\u667a\u80fd\u4f53\u8bc4\u4f30\u7684\u65b0\u57fa\u51c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4eba\u672cAI\u9886\u57df\u505a\u51fa\u8d21\u732e\uff0c\u89e3\u51b3\u4e86\u76f4\u63a5\u5f71\u54cd\u7528\u6237\u4fe1\u4efb\u3001\u7cfb\u7edf\u53ef\u6269\u5c55\u6027\u548c\u76d1\u7ba1\u5408\u89c4\u6027\u7684\u667a\u80fd\u4f53\u5185\u5b58\u7ba1\u7406\u57fa\u672c\u6311\u6218\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u3001\u9690\u79c1\u654f\u611f\u73af\u5883\u4e2d\u7684\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\u3002"}}
{"id": "2512.12116", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.12116", "abs": "https://arxiv.org/abs/2512.12116", "authors": ["Muhammad Bilal Shahid", "Prajwal Koirla", "Cody Fleming"], "title": "Neural CDEs as Correctors for Learned Time Series Models", "comment": null, "summary": "Learned time-series models, whether continuous- or discrete-time, are widely used to forecast the states of a dynamical system. Such models generate multi-step forecasts either directly, by predicting the full horizon at once, or iteratively, by feeding back their own predictions at each step. In both cases, the multi-step forecasts are prone to errors. To address this, we propose a Predictor-Corrector mechanism where the Predictor is any learned time-series model and the Corrector is a neural controlled differential equation. The Predictor forecasts, and the Corrector predicts the errors of the forecasts. Adding these errors to the forecasts improves forecast performance. The proposed Corrector works with irregularly sampled time series and continuous- and discrete-time Predictors. Additionally, we introduce two regularization strategies to improve the extrapolation performance of the Corrector with accelerated training. We evaluate our Corrector with diverse Predictors, e.g., neural ordinary differential equations, Contiformer, and DLinear, on synthetic, physics simulation, and real-world forecasting datasets. The experiments demonstrate that the Predictor-Corrector mechanism consistently improves the performance compared to Predictor alone.", "AI": {"tldr": "\u63d0\u51faPredictor-Corrector\u673a\u5236\uff0c\u7528\u795e\u7ecf\u63a7\u5236\u5fae\u5206\u65b9\u7a0b\u4f5c\u4e3a\u4fee\u6b63\u5668\u6765\u9884\u6d4b\u9884\u6d4b\u8bef\u5dee\uff0c\u63d0\u5347\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6027\u80fd", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\uff08\u8fde\u7eed\u6216\u79bb\u6563\u65f6\u95f4\uff09\u7684\u591a\u6b65\u9884\u6d4b\u5b58\u5728\u8bef\u5dee\uff0c\u65e0\u8bba\u662f\u76f4\u63a5\u9884\u6d4b\u6574\u4e2a\u65f6\u95f4\u8303\u56f4\u8fd8\u662f\u8fed\u4ee3\u53cd\u9988\u9884\u6d4b\u90fd\u5b58\u5728\u51c6\u786e\u6027\u95ee\u9898", "method": "\u63d0\u51faPredictor-Corrector\u673a\u5236\uff1aPredictor\u662f\u4efb\u610f\u5b66\u4e60\u7684\u65f6\u95f4\u5e8f\u5217\u6a21\u578b\uff0cCorrector\u662f\u795e\u7ecf\u63a7\u5236\u5fae\u5206\u65b9\u7a0b\uff0c\u7528\u4e8e\u9884\u6d4b\u9884\u6d4b\u8bef\u5dee\uff0c\u5c06\u8bef\u5dee\u52a0\u5230\u9884\u6d4b\u503c\u4e0a\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002Corrector\u652f\u6301\u4e0d\u89c4\u5219\u91c7\u6837\u65f6\u95f4\u5e8f\u5217\u548c\u8fde\u7eed/\u79bb\u6563\u65f6\u95f4Predictor\uff0c\u5e76\u5f15\u5165\u4e24\u79cd\u6b63\u5219\u5316\u7b56\u7565\u63d0\u5347\u5916\u63a8\u6027\u80fd", "result": "\u5728\u5408\u6210\u6570\u636e\u3001\u7269\u7406\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u9884\u6d4b\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u4f7f\u7528\u795e\u7ecfODE\u3001Contiformer\u3001DLinear\u7b49\u4e0d\u540cPredictor\uff0c\u5b9e\u9a8c\u8868\u660ePredictor-Corrector\u673a\u5236\u76f8\u6bd4\u5355\u72ec\u4f7f\u7528Predictor\u80fd\u6301\u7eed\u63d0\u5347\u6027\u80fd", "conclusion": "\u63d0\u51fa\u7684Predictor-Corrector\u673a\u5236\u80fd\u6709\u6548\u51cf\u5c11\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u8bef\u5dee\uff0c\u63d0\u5347\u9884\u6d4b\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u591a\u79cdPredictor\u6a21\u578b\u548c\u4e0d\u540c\u7c7b\u578b\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e"}}
{"id": "2512.12205", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12205", "abs": "https://arxiv.org/abs/2512.12205", "authors": ["Peizheng Li", "Ioannis Mavromatis", "Ajith Sahadevan", "Tim Farnham", "Adnan Aijaz", "Aftab Khan"], "title": "A Multi-Year Urban Streetlight Imagery Dataset for Visual Monitoring and Spatio-Temporal Drift Detection", "comment": "10 pages, 7 figures. Submitted to Data in Brief (Elsevier)", "summary": "We present a large-scale, longitudinal visual dataset of urban streetlights captured by 22 fixed-angle cameras deployed across Bristol, U.K., from 2021 to 2025. The dataset contains over 526,000 images, collected hourly under diverse lighting, weather, and seasonal conditions. Each image is accompanied by rich metadata, including timestamps, GPS coordinates, and device identifiers. This unique real-world dataset enables detailed investigation of visual drift, anomaly detection, and MLOps strategies in smart city deployments. To promtoe seconardary analysis, we additionally provide a self-supervised framework based on convolutional variational autoencoders (CNN-VAEs). Models are trained separately for each camera node and for day/night image sets. We define two per-sample drift metrics: relative centroid drift, capturing latent space deviation from a baseline quarter, and relative reconstruction error, measuring normalized image-domain degradation. This dataset provides a realistic, fine-grained benchmark for evaluating long-term model stability, drift-aware learning, and deployment-ready vision systems. The images and structured metadata are publicly released in JPEG and CSV formats, supporting reproducibility and downstream applications such as streetlight monitoring, weather inference, and urban scene understanding. The dataset can be found at https://doi.org/10.5281/zenodo.17781192 and https://doi.org/10.5281/zenodo.17859120.", "AI": {"tldr": "\u82f1\u56fd\u5e03\u91cc\u65af\u6258\u5c14\u90e8\u7f7222\u4e2a\u56fa\u5b9a\u89d2\u5ea6\u6444\u50cf\u5934\uff0c\u91c7\u96c62021-2025\u5e74\u57ce\u5e02\u8def\u706f\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5305\u542b52.6\u4e07\u5f20\u56fe\u50cf\u53ca\u4e30\u5bcc\u5143\u6570\u636e\uff0c\u7528\u4e8e\u7814\u7a76\u89c6\u89c9\u6f02\u79fb\u3001\u5f02\u5e38\u68c0\u6d4b\u548cMLOps\u7b56\u7565\u3002", "motivation": "\u4e3a\u667a\u80fd\u57ce\u5e02\u90e8\u7f72\u63d0\u4f9b\u771f\u5b9e\u4e16\u754c\u3001\u5927\u89c4\u6a21\u3001\u957f\u671f\u7684\u89c6\u89c9\u6570\u636e\u96c6\uff0c\u652f\u6301\u7814\u7a76\u89c6\u89c9\u6f02\u79fb\u3001\u6a21\u578b\u7a33\u5b9a\u6027\u3001\u5f02\u5e38\u68c0\u6d4b\u7b49\u5b9e\u9645\u95ee\u9898\u3002", "method": "\u4f7f\u752822\u4e2a\u56fa\u5b9a\u6444\u50cf\u5934\u6bcf\u5c0f\u65f6\u91c7\u96c6\u56fe\u50cf\uff0c\u63d0\u4f9b\u4e30\u5bcc\u5143\u6570\u636e\uff08\u65f6\u95f4\u6233\u3001GPS\u5750\u6807\u3001\u8bbe\u5907ID\uff09\u3002\u57fa\u4e8e\u5377\u79ef\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08CNN-VAEs\uff09\u6784\u5efa\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u4e3a\u6bcf\u4e2a\u6444\u50cf\u5934\u8282\u70b9\u548c\u65e5/\u591c\u56fe\u50cf\u96c6\u5206\u522b\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b52.6\u4e07\u5f20\u56fe\u50cf\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e0d\u540c\u5149\u7167\u3001\u5929\u6c14\u548c\u5b63\u8282\u6761\u4ef6\u3002\u5b9a\u4e49\u4e86\u4e24\u4e2a\u6f02\u79fb\u5ea6\u91cf\u6307\u6807\uff1a\u76f8\u5bf9\u8d28\u5fc3\u6f02\u79fb\uff08\u6355\u6349\u6f5c\u5728\u7a7a\u95f4\u504f\u79bb\uff09\u548c\u76f8\u5bf9\u91cd\u5efa\u8bef\u5dee\uff08\u6d4b\u91cf\u56fe\u50cf\u57df\u9000\u5316\uff09\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u8bc4\u4f30\u957f\u671f\u6a21\u578b\u7a33\u5b9a\u6027\u3001\u6f02\u79fb\u611f\u77e5\u5b66\u4e60\u548c\u90e8\u7f72\u5c31\u7eea\u7684\u89c6\u89c9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u771f\u5b9e\u3001\u7ec6\u7c92\u5ea6\u7684\u57fa\u51c6\uff0c\u652f\u6301\u8857\u9053\u7167\u660e\u76d1\u63a7\u3001\u5929\u6c14\u63a8\u65ad\u548c\u57ce\u5e02\u573a\u666f\u7406\u89e3\u7b49\u4e0b\u6e38\u5e94\u7528\u3002"}}
{"id": "2512.12919", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.12919", "abs": "https://arxiv.org/abs/2512.12919", "authors": ["Maria Y. Rodriguez", "Ehren Dohler", "Jon Phillips", "Melissa Villodas", "Voltaire Vegara", "Kenny Joseph", "Amy Wilson"], "title": "Open Source Software and Data for Human Service Development: A Case Study on Predicting Housing Instability", "comment": "24 pages, 5 tables, 3 figures, 4 appendices", "summary": "Open-source data and tools are lauded as essential for replicable and usable social science, though little is known about their use in resource constrained human service provision. This paper examines the challenges and opportunities of open-source tools and data in human service development by using both to forecast failure to pay eviction filings in Bronx County, NY. We use zip code level data from the Housing Data Coalition, the American Community Survey 5-year estimates, and DeepMaps Model of the Labor Force to forecast rates through July 2021. We employ multilevel (MLM) and exponential smoothing (ETS) models using the R project for Statistical Computing, an oft used open-source statistical software. We compare our results to what happened during the same period, to illustrate the efficacy of the open-source tools and techniques employed. We argue open-source data and software may facilitate rapid analysis of public data - a much-needed ability in human service intervention development under increasingly constrained resources - but find public data are limited by the information they reliably capture, limiting their utility by a non-trivial margin of error. The manuscript concludes by considering lessons for human service organizations with limited analytical resources and a vested interest in low-resourced communities.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5f00\u6e90\u6570\u636e\u548c\u5de5\u5177\u5728\u8d44\u6e90\u53d7\u9650\u7684\u4eba\u7c7b\u670d\u52a1\u63d0\u4f9b\u4e2d\u7684\u6311\u6218\u4e0e\u673a\u9047\uff0c\u901a\u8fc7\u4f7f\u7528\u5f00\u6e90\u5de5\u5177\u9884\u6d4b\u7ebd\u7ea6\u5e03\u6717\u514b\u65af\u53bf\u7684\u9a71\u9010\u7533\u8bf7\u672a\u652f\u4ed8\u7387\uff0c\u53d1\u73b0\u5f00\u6e90\u5de5\u5177\u80fd\u4fc3\u8fdb\u5feb\u901f\u5206\u6790\u4f46\u516c\u5171\u6570\u636e\u5b58\u5728\u5c40\u9650\u6027\u3002", "motivation": "\u867d\u7136\u5f00\u6e90\u6570\u636e\u548c\u5de5\u5177\u88ab\u8d5e\u8a89\u4e3a\u53ef\u590d\u5236\u548c\u53ef\u7528\u7684\u793e\u4f1a\u79d1\u5b66\u7814\u7a76\u7684\u57fa\u7840\uff0c\u4f46\u5b83\u4eec\u5728\u8d44\u6e90\u53d7\u9650\u7684\u4eba\u7c7b\u670d\u52a1\u63d0\u4f9b\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u60c5\u51b5\u5c1a\u4e0d\u6e05\u695a\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5f00\u6e90\u5de5\u5177\u548c\u6570\u636e\u5728\u4eba\u7c7b\u670d\u52a1\u53d1\u5c55\u4e2d\u7684\u6311\u6218\u4e0e\u673a\u9047\u3002", "method": "\u4f7f\u7528\u4f4f\u623f\u6570\u636e\u8054\u76df\u7684\u90ae\u653f\u7f16\u7801\u7ea7\u522b\u6570\u636e\u3001\u7f8e\u56fd\u793e\u533a\u8c03\u67e55\u5e74\u4f30\u8ba1\u6570\u636e\u548cDeepMaps\u52b3\u52a8\u529b\u6a21\u578b\uff0c\u901a\u8fc7R\u7edf\u8ba1\u8ba1\u7b97\u9879\u76ee\uff08\u5f00\u6e90\u7edf\u8ba1\u8f6f\u4ef6\uff09\u5e94\u7528\u591a\u5c42\u6b21\u6a21\u578b\uff08MLM\uff09\u548c\u6307\u6570\u5e73\u6ed1\u6a21\u578b\uff08ETS\uff09\u9884\u6d4b2021\u5e747\u6708\u524d\u7684\u9a71\u9010\u7533\u8bf7\u672a\u652f\u4ed8\u7387\u3002", "result": "\u5c06\u9884\u6d4b\u7ed3\u679c\u4e0e\u540c\u671f\u5b9e\u9645\u53d1\u751f\u60c5\u51b5\u8fdb\u884c\u6bd4\u8f83\uff0c\u8bc4\u4f30\u5f00\u6e90\u5de5\u5177\u548c\u6280\u672f\u7684\u6709\u6548\u6027\u3002\u7814\u7a76\u53d1\u73b0\u5f00\u6e90\u6570\u636e\u548c\u8f6f\u4ef6\u53ef\u4ee5\u4fc3\u8fdb\u516c\u5171\u6570\u636e\u7684\u5feb\u901f\u5206\u6790\uff0c\u4f46\u516c\u5171\u6570\u636e\u53d7\u9650\u4e8e\u5176\u53ef\u9760\u6355\u83b7\u7684\u4fe1\u606f\u8303\u56f4\uff0c\u5b58\u5728\u4e0d\u53ef\u5ffd\u89c6\u7684\u8bef\u5dee\u5e45\u5ea6\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u7528\u6027\u3002", "conclusion": "\u5f00\u6e90\u5de5\u5177\u5728\u8d44\u6e90\u6709\u9650\u7684\u4eba\u7c7b\u670d\u52a1\u7ec4\u7ec7\u4e2d\u5177\u6709\u5e94\u7528\u4ef7\u503c\uff0c\u7279\u522b\u662f\u5728\u8d44\u6e90\u65e5\u76ca\u53d7\u9650\u7684\u73af\u5883\u4e0b\u8fdb\u884c\u5e72\u9884\u5f00\u53d1\u65f6\u3002\u4f46\u9700\u8981\u8003\u8651\u516c\u5171\u6570\u636e\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5173\u6ce8\u4f4e\u8d44\u6e90\u793e\u533a\u7684\u4eba\u7c7b\u670d\u52a1\u7ec4\u7ec7\u63d0\u4f9b\u5b9e\u8df5\u542f\u793a\u3002"}}
{"id": "2512.12206", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12206", "abs": "https://arxiv.org/abs/2512.12206", "authors": ["Jeongjun Park", "Sunwook Hwang", "Hyeonho Noh", "Jin Mo Yang", "Hyun Jong Yang", "Saewoong Bahk"], "title": "ALERT Open Dataset and Input-Size-Agnostic Vision Transformer for Driver Activity Recognition using IR-UWB", "comment": null, "summary": "Distracted driving contributes to fatal crashes worldwide. To address this, researchers are using driver activity recognition (DAR) with impulse radio ultra-wideband (IR-UWB) radar, which offers advantages such as interference resistance, low power consumption, and privacy preservation. However, two challenges limit its adoption: the lack of large-scale real-world UWB datasets covering diverse distracted driving behaviors, and the difficulty of adapting fixed-input Vision Transformers (ViTs) to UWB radar data with non-standard dimensions.\n  This work addresses both challenges. We present the ALERT dataset, which contains 10,220 radar samples of seven distracted driving activities collected in real driving conditions. We also propose the input-size-agnostic Vision Transformer (ISA-ViT), a framework designed for radar-based DAR. The proposed method resizes UWB data to meet ViT input requirements while preserving radar-specific information such as Doppler shifts and phase characteristics. By adjusting patch configurations and leveraging pre-trained positional embedding vectors (PEVs), ISA-ViT overcomes the limitations of naive resizing approaches. In addition, a domain fusion strategy combines range- and frequency-domain features to further improve classification performance.\n  Comprehensive experiments demonstrate that ISA-ViT achieves a 22.68% accuracy improvement over an existing ViT-based approach for UWB-based DAR. By publicly releasing the ALERT dataset and detailing our input-size-agnostic strategy, this work facilitates the development of more robust and scalable distracted driving detection systems for real-world deployment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faISA-ViT\u6846\u67b6\u548cALERT\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u57fa\u4e8eUWB\u96f7\u8fbe\u7684\u9a7e\u9a76\u5458\u5206\u5fc3\u884c\u4e3a\u8bc6\u522b\u4e2d\u7f3a\u4e4f\u5927\u89c4\u6a21\u771f\u5b9e\u6570\u636e\u96c6\u548cViT\u6a21\u578b\u8f93\u5165\u5c3a\u5bf8\u56fa\u5b9a\u7684\u95ee\u9898\u3002", "motivation": "\u5206\u5fc3\u9a7e\u9a76\u5bfc\u81f4\u5168\u7403\u81f4\u547d\u4e8b\u6545\uff0c\u57fa\u4e8eIR-UWB\u96f7\u8fbe\u7684\u9a7e\u9a76\u5458\u6d3b\u52a8\u8bc6\u522b\u5177\u6709\u6297\u5e72\u6270\u3001\u4f4e\u529f\u8017\u548c\u9690\u79c1\u4fdd\u62a4\u4f18\u52bf\uff0c\u4f46\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u7f3a\u4e4f\u8986\u76d6\u591a\u79cd\u5206\u5fc3\u9a7e\u9a76\u884c\u4e3a\u7684\u5927\u89c4\u6a21\u771f\u5b9eUWB\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u56fa\u5b9a\u8f93\u5165\u5c3a\u5bf8\u7684Vision Transformers\u96be\u4ee5\u9002\u5e94\u975e\u6807\u51c6\u7ef4\u5ea6\u7684UWB\u96f7\u8fbe\u6570\u636e\u3002", "method": "\u63d0\u51fa\u8f93\u5165\u5c3a\u5bf8\u65e0\u5173\u7684Vision Transformer\uff08ISA-ViT\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u8c03\u6574\u8865\u4e01\u914d\u7f6e\u548c\u5229\u7528\u9884\u8bad\u7ec3\u4f4d\u7f6e\u5d4c\u5165\u5411\u91cf\uff0c\u5728\u6ee1\u8db3ViT\u8f93\u5165\u8981\u6c42\u7684\u540c\u65f6\u4fdd\u7559\u96f7\u8fbe\u7279\u5b9a\u4fe1\u606f\uff08\u591a\u666e\u52d2\u9891\u79fb\u548c\u76f8\u4f4d\u7279\u5f81\uff09\u3002\u91c7\u7528\u57df\u878d\u5408\u7b56\u7565\u7ed3\u5408\u8ddd\u79bb\u57df\u548c\u9891\u57df\u7279\u5f81\u63d0\u5347\u5206\u7c7b\u6027\u80fd\uff0c\u5e76\u53d1\u5e03\u5305\u542b10,220\u4e2a\u96f7\u8fbe\u6837\u672c\u7684ALERT\u6570\u636e\u96c6\u3002", "result": "ISA-ViT\u5728\u57fa\u4e8eUWB\u7684\u9a7e\u9a76\u5458\u6d3b\u52a8\u8bc6\u522b\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4\u73b0\u6709ViT\u65b9\u6cd5\u5b9e\u73b0\u4e8622.68%\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002ALERT\u6570\u636e\u96c6\u4e3a\u771f\u5b9e\u9a7e\u9a76\u6761\u4ef6\u4e0b\u4e03\u79cd\u5206\u5fc3\u9a7e\u9a76\u6d3b\u52a8\u63d0\u4f9b\u4e86\u5927\u89c4\u6a21\u96f7\u8fbe\u6837\u672c\u3002", "conclusion": "\u901a\u8fc7\u516c\u5f00ALERT\u6570\u636e\u96c6\u548c\u8be6\u7ec6\u63cf\u8ff0\u8f93\u5165\u5c3a\u5bf8\u65e0\u5173\u7b56\u7565\uff0c\u8fd9\u9879\u5de5\u4f5c\u4fc3\u8fdb\u4e86\u66f4\u9c81\u68d2\u3001\u53ef\u6269\u5c55\u7684\u5206\u5fc3\u9a7e\u9a76\u68c0\u6d4b\u7cfb\u7edf\u7684\u5f00\u53d1\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.13061", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.13061", "abs": "https://arxiv.org/abs/2512.13061", "authors": ["Jianjun Xiao", "Cixiao Wang", "Wenmei Zhang"], "title": "Modeling Collaborative Problem Solving Dynamics from Group Discourse: A Text-Mining Approach with Synergy Degree Model", "comment": "16 pages, 2 figures", "summary": "Measuring collaborative problem solving (CPS) synergy remains challenging in learning analytics, as classical manual coding cannot capture emergent system-level dynamics. This study introduces a computational framework that integrates automated discourse analysis with the Synergy Degree Model (SDM) to quantify CPS synergy from group communication. Data were collected from 52 learners in 12 groups during a 5-week connectivist MOOC (cMOOC) activity. Nine classification models were applied to automatically identify ten CPS behaviors across four interaction levels: operation, wayfinding, sense-making, and creation. While BERT achieved the highest accuracy, GPT models demonstrated superior precision suitable for human-AI collaborative coding. Within the SDM framework, each interaction level was treated as a subsystem to compute group-level order parameters and derive synergy degrees. Permutation tests showed automated measures preserve construct validity, despite systematic biases at the subsystem level. Statistical analyses revealed significant task-type differences: survey study groups exhibited higher creation-order than mode study groups, suggesting \"controlled disorder\" may benefit complex problem solving. Importantly, synergy degree distinguished collaborative quality, ranging from excellent to failing groups. Findings establish synergy degree as a sensitive indicator of collaboration and demonstrate the feasibility of scaling fine-grained CPS analytics through AI-in-the-loop approaches.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u8ba1\u7b97\u6846\u67b6\uff0c\u6574\u5408\u81ea\u52a8\u8bdd\u8bed\u5206\u6790\u548c\u534f\u540c\u5ea6\u6a21\u578b\u6765\u91cf\u5316\u534f\u4f5c\u95ee\u9898\u89e3\u51b3\u4e2d\u7684\u534f\u540c\u6548\u5e94\uff0c\u901a\u8fc7AI\u8f85\u52a9\u65b9\u6cd5\u5b9e\u73b0\u7ec6\u7c92\u5ea6CPS\u5206\u6790\u7684\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u534f\u4f5c\u95ee\u9898\u89e3\u51b3\u7684\u534f\u540c\u6548\u5e94\u6d4b\u91cf\u5728\u5206\u6790\u5b66\u4e60\u4e2d\u5177\u6709\u6311\u6218\u6027\uff0c\u4f20\u7edf\u624b\u52a8\u7f16\u7801\u65e0\u6cd5\u6355\u6349\u7cfb\u7edf\u5c42\u9762\u7684\u6d8c\u73b0\u52a8\u6001\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u91cf\u5316CPS\u534f\u540c\u6548\u5e94\u7684\u8ba1\u7b97\u6846\u67b6\u3002", "method": "\u6574\u5408\u81ea\u52a8\u8bdd\u8bed\u5206\u6790\u4e0e\u534f\u540c\u5ea6\u6a21\u578b\uff0c\u6536\u96c652\u540d\u5b66\u4e60\u8005\u572812\u4e2a\u5c0f\u7ec4\u4e2d\u76845\u5468cMOOC\u6d3b\u52a8\u6570\u636e\uff0c\u5e94\u75289\u4e2a\u5206\u7c7b\u6a21\u578b\u81ea\u52a8\u8bc6\u522b\u56db\u4e2a\u4ea4\u4e92\u5c42\u6b21\uff08\u64cd\u4f5c\u3001\u5bfb\u8def\u3001\u610f\u4e49\u5efa\u6784\u3001\u521b\u9020\uff09\u7684\u5341\u79cdCPS\u884c\u4e3a\uff0c\u5c06\u6bcf\u4e2a\u4ea4\u4e92\u5c42\u6b21\u4f5c\u4e3a\u5b50\u7cfb\u7edf\u8ba1\u7b97\u7fa4\u4f53\u5c42\u9762\u7684\u5e8f\u53c2\u6570\u5e76\u63a8\u5bfc\u534f\u540c\u5ea6\u3002", "result": "BERT\u6a21\u578b\u83b7\u5f97\u6700\u9ad8\u51c6\u786e\u7387\uff0cGPT\u6a21\u578b\u5728\u7cbe\u5ea6\u4e0a\u8868\u73b0\u66f4\u4f18\u9002\u5408\u4eba\u673a\u534f\u4f5c\u7f16\u7801\uff1b\u7f6e\u6362\u6d4b\u8bd5\u663e\u793a\u81ea\u52a8\u5316\u6d4b\u91cf\u4fdd\u6301\u6784\u5ff5\u6548\u5ea6\uff1b\u7edf\u8ba1\u5206\u6790\u663e\u793a\u4efb\u52a1\u7c7b\u578b\u5dee\u5f02\u663e\u8457\uff1a\u8c03\u67e5\u7814\u7a76\u5c0f\u7ec4\u6bd4\u6a21\u5f0f\u7814\u7a76\u5c0f\u7ec4\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u521b\u9020\u5e8f\u53c2\u6570\uff1b\u534f\u540c\u5ea6\u80fd\u591f\u533a\u5206\u4ece\u4f18\u79c0\u5230\u5931\u8d25\u7684\u4e0d\u540c\u534f\u4f5c\u8d28\u91cf\u3002", "conclusion": "\u534f\u540c\u5ea6\u4f5c\u4e3a\u534f\u4f5c\u8d28\u91cf\u7684\u654f\u611f\u6307\u6807\uff0c\u901a\u8fc7AI\u5728\u73af\u65b9\u6cd5\u5b9e\u73b0\u7ec6\u7c92\u5ea6CPS\u5206\u6790\u7684\u53ef\u6269\u5c55\u6027\u662f\u53ef\u884c\u7684\uff0c\"\u53d7\u63a7\u65e0\u5e8f\"\u53ef\u80fd\u6709\u5229\u4e8e\u590d\u6742\u95ee\u9898\u89e3\u51b3\u3002"}}
{"id": "2512.12970", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.12970", "abs": "https://arxiv.org/abs/2512.12970", "authors": ["Paola Di Maio"], "title": "Towards Open Standards for Systemic Complexity in Digital Forensics", "comment": null, "summary": "The intersection of artificial intelligence (AI) and digital forensics (DF) is becoming increasingly complex, ubiquitous, and pervasive, with overlapping techniques and technologies being adopted in all types of scientific and technical inquiry. Despite incredible advances, forensic sciences are not exempt from errors and remain vulnerable to fallibility. To mitigate the limitations of errors in DF, the systemic complexity is identified and addressed with the adoption of human-readable artifacts and open standards. A DF AI model schema based on the state of the art is outlined.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u57fa\u4e8e\u5f00\u653e\u6807\u51c6\u548c\u4eba\u7c7b\u53ef\u8bfb\u5de5\u4ef6\u7684\u6570\u5b57\u53d6\u8bc1AI\u6a21\u578b\u67b6\u6784\uff0c\u4ee5\u89e3\u51b3AI\u4e0e\u6570\u5b57\u53d6\u8bc1\u4ea4\u53c9\u9886\u57df\u4e2d\u7684\u7cfb\u7edf\u590d\u6742\u6027\u548c\u9519\u8bef\u95ee\u9898", "motivation": "AI\u4e0e\u6570\u5b57\u53d6\u8bc1\u6280\u672f\u4ea4\u53c9\u65e5\u76ca\u590d\u6742\u666e\u904d\uff0c\u4f46\u53d6\u8bc1\u79d1\u5b66\u4ecd\u5b58\u5728\u9519\u8bef\u548c\u8106\u5f31\u6027\uff0c\u9700\u8981\u89e3\u51b3\u7cfb\u7edf\u590d\u6742\u6027\u4ee5\u964d\u4f4e\u9519\u8bef\u9650\u5236", "method": "\u91c7\u7528\u4eba\u7c7b\u53ef\u8bfb\u5de5\u4ef6\u548c\u5f00\u653e\u6807\u51c6\uff0c\u57fa\u4e8e\u6700\u65b0\u6280\u672f\u63d0\u51fa\u6570\u5b57\u53d6\u8bc1AI\u6a21\u578b\u67b6\u6784", "result": "\u5efa\u7acb\u4e86\u57fa\u4e8e\u5f00\u653e\u6807\u51c6\u548c\u4eba\u7c7b\u53ef\u8bfb\u5de5\u4ef6\u7684\u6570\u5b57\u53d6\u8bc1AI\u6a21\u578b\u67b6\u6784\u6846\u67b6", "conclusion": "\u901a\u8fc7\u91c7\u7528\u4eba\u7c7b\u53ef\u8bfb\u5de5\u4ef6\u548c\u5f00\u653e\u6807\u51c6\uff0c\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u6570\u5b57\u53d6\u8bc1AI\u4ea4\u53c9\u9886\u57df\u7684\u7cfb\u7edf\u590d\u6742\u6027\u548c\u9519\u8bef\u95ee\u9898"}}
{"id": "2512.13260", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.13260", "abs": "https://arxiv.org/abs/2512.13260", "authors": ["Hugo Roger Paz"], "title": "From Educational Analytics to AI Governance: Transferable Lessons from Complex Systems Interventions", "comment": "36 pages, 2 tables", "summary": "Both student retention in higher education and artificial intelligence governance face a common structural challenge: the application of linear regulatory frameworks to complex adaptive systems. Risk-based approaches dominate both domains, yet systematically fail because they assume stable causal pathways, predictable actor responses, and controllable system boundaries. This paper extracts transferable methodological principles from CAPIRE (Curriculum, Archetypes, Policies, Interventions & Research Environment), an empirically validated framework for educational analytics that treats student dropout as an emergent property of curricular structures, institutional rules, and macroeconomic shocks. Drawing on longitudinal data from engineering programmes and causal inference methods, CAPIRE demonstrates that well-intentioned interventions routinely generate unintended consequences when system complexity is ignored. We argue that five core principles developed within CAPIRE - temporal observation discipline, structural mapping over categorical classification, archetype-based heterogeneity analysis, causal mechanism identification, and simulation-based policy design - transfer directly to the challenge of governing AI systems. The isomorphism is not merely analogical: both domains exhibit non-linearity, emergence, feedback loops, strategic adaptation, and path dependence. We propose Complex Systems AI Governance (CSAIG) as an integrated framework that operationalises these principles for regulatory design, shifting the central question from \"how risky is this AI system?\" to \"how does this intervention reshape system dynamics?\" The contribution is twofold: demonstrating that empirical lessons from one complex systems domain can accelerate governance design in another, and offering a concrete methodological architecture for complexity-aware AI regulation.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u6559\u80b2\u5206\u6790\u6846\u67b6CAPIRE\u7684\u65b9\u6cd5\u8bba\u539f\u5219\u5e94\u7528\u4e8eAI\u6cbb\u7406\uff0c\u5efa\u7acb\u590d\u6742\u7cfb\u7edfAI\u6cbb\u7406\uff08CSAIG\uff09\u6846\u67b6\uff0c\u4ece\u98ce\u9669\u5bfc\u5411\u8f6c\u5411\u7cfb\u7edf\u52a8\u6001\u5206\u6790\u3002", "motivation": "\u9ad8\u7b49\u6559\u80b2\u5b66\u751f\u4fdd\u7559\u7387\u548c\u4eba\u5de5\u667a\u80fd\u6cbb\u7406\u9762\u4e34\u5171\u540c\u7684\u7ed3\u6784\u6027\u6311\u6218\uff1a\u5c06\u7ebf\u6027\u76d1\u7ba1\u6846\u67b6\u5e94\u7528\u4e8e\u590d\u6742\u9002\u5e94\u7cfb\u7edf\u3002\u98ce\u9669\u5bfc\u5411\u65b9\u6cd5\u5728\u8fd9\u4e24\u4e2a\u9886\u57df\u90fd\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u4f46\u7531\u4e8e\u5047\u8bbe\u7a33\u5b9a\u7684\u56e0\u679c\u8def\u5f84\u3001\u53ef\u9884\u6d4b\u7684\u884c\u4e3a\u8005\u54cd\u5e94\u548c\u53ef\u63a7\u7684\u7cfb\u7edf\u8fb9\u754c\u800c\u7cfb\u7edf\u6027\u5730\u5931\u8d25\u3002", "method": "\u4eceCAPIRE\u6846\u67b6\uff08\u8bfe\u7a0b\u3001\u539f\u578b\u3001\u653f\u7b56\u3001\u5e72\u9884\u548c\u7814\u7a76\u73af\u5883\uff09\u4e2d\u63d0\u53d6\u53ef\u8f6c\u79fb\u7684\u65b9\u6cd5\u8bba\u539f\u5219\uff0c\u8be5\u6846\u67b6\u57fa\u4e8e\u5de5\u7a0b\u9879\u76ee\u7684\u7eb5\u5411\u6570\u636e\u548c\u56e0\u679c\u63a8\u65ad\u65b9\u6cd5\uff0c\u5c06\u5b66\u751f\u8f8d\u5b66\u89c6\u4e3a\u8bfe\u7a0b\u7ed3\u6784\u3001\u5236\u5ea6\u89c4\u5219\u548c\u5b8f\u89c2\u7ecf\u6d4e\u51b2\u51fb\u7684\u6d8c\u73b0\u5c5e\u6027\u3002\u63d0\u51fa\u590d\u6742\u7cfb\u7edfAI\u6cbb\u7406\uff08CSAIG\uff09\u6846\u67b6\uff0c\u5c06\u4e94\u4e2a\u6838\u5fc3\u539f\u5219\uff08\u65f6\u95f4\u89c2\u5bdf\u7eaa\u5f8b\u3001\u7ed3\u6784\u6620\u5c04\u800c\u975e\u5206\u7c7b\u5206\u7c7b\u3001\u57fa\u4e8e\u539f\u578b\u7684\u5f02\u8d28\u6027\u5206\u6790\u3001\u56e0\u679c\u673a\u5236\u8bc6\u522b\u3001\u57fa\u4e8e\u6a21\u62df\u7684\u653f\u7b56\u8bbe\u8ba1\uff09\u5e94\u7528\u4e8eAI\u7cfb\u7edf\u6cbb\u7406\u3002", "result": "CAPIRE\u6846\u67b6\u8bc1\u660e\uff0c\u5f53\u5ffd\u89c6\u7cfb\u7edf\u590d\u6742\u6027\u65f6\uff0c\u5584\u610f\u7684\u5e72\u9884\u63aa\u65bd\u901a\u5e38\u4f1a\u5e26\u6765\u610f\u5916\u540e\u679c\u3002\u8bba\u6587\u5c55\u793a\u4e86\u4ece\u4e00\u4e2a\u590d\u6742\u7cfb\u7edf\u9886\u57df\uff08\u6559\u80b2\uff09\u83b7\u5f97\u7684\u7ecf\u9a8c\u6559\u8bad\u53ef\u4ee5\u52a0\u901f\u53e6\u4e00\u4e2a\u9886\u57df\uff08AI\u6cbb\u7406\uff09\u7684\u6cbb\u7406\u8bbe\u8ba1\uff0c\u5e76\u4e3a\u590d\u6742\u6027\u611f\u77e5\u7684AI\u76d1\u7ba1\u63d0\u4f9b\u4e86\u5177\u4f53\u7684\u65b9\u6cd5\u8bba\u67b6\u6784\u3002", "conclusion": "\u9ad8\u7b49\u6559\u80b2\u548cAI\u6cbb\u7406\u5728\u975e\u7ebf\u6027\u3001\u6d8c\u73b0\u6027\u3001\u53cd\u9988\u5faa\u73af\u3001\u6218\u7565\u9002\u5e94\u548c\u8def\u5f84\u4f9d\u8d56\u65b9\u9762\u5b58\u5728\u540c\u6784\u6027\u3002CSAIG\u6846\u67b6\u5c06\u76d1\u7ba1\u8bbe\u8ba1\u7684\u6838\u5fc3\u95ee\u9898\u4ece\"\u8fd9\u4e2aAI\u7cfb\u7edf\u6709\u591a\u5371\u9669\uff1f\"\u8f6c\u53d8\u4e3a\"\u8fd9\u4e2a\u5e72\u9884\u5982\u4f55\u91cd\u5851\u7cfb\u7edf\u52a8\u6001\uff1f\"\uff0c\u4e3a\u590d\u6742\u6027\u611f\u77e5\u7684AI\u76d1\u7ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6cd5\u8bba\u3002"}}
{"id": "2512.13070", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13070", "abs": "https://arxiv.org/abs/2512.13070", "authors": ["Bizhe Bai", "Hongming Wu", "Peng Ye", "Tao Chen"], "title": "M-GRPO: Stabilizing Self-Supervised Reinforcement Learning for Large Language Models with Momentum-Anchored Policy Optimization", "comment": "7 pages, 5 figures,Accepted NeurIPS 2025 Workshop on Efficient Reasoning", "summary": "Self-supervised reinforcement learning (RL) presents a promising approach for enhancing the reasoning capabilities of Large Language Models (LLMs) without reliance on expensive human-annotated data. However, we find that existing methods suffer from a critical failure mode under long-horizon training: a \"policy collapse\" where performance precipitously degrades. We diagnose this instability and demonstrate that simply scaling the number of rollouts -- a common strategy to improve performance -- only delays, but does not prevent, this collapse. To counteract this instability, we first introduce M-GRPO (Momentum-Anchored Group Relative Policy Optimization), a framework that leverages a slowly evolving momentum model to provide a stable training target. In addition, we identify that this process is often accompanied by a rapid collapse in policy entropy, resulting in a prematurely confident and suboptimal policy. To specifically address this issue, we propose a second contribution: an adaptive filtering method based on the interquartile range (IQR) that dynamically prunes low-entropy trajectories, preserving essential policy diversity. Our extensive experiments on multiple reasoning benchmarks demonstrate that M-GRPO stabilizes the training process while the IQR filter prevents premature convergence. The combination of these two innovations leads to superior training stability and state-of-the-art performance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faM-GRPO\u6846\u67b6\u548cIQR\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u89e3\u51b3\u81ea\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u4e2d\u957f\u671f\u8bad\u7ec3\u65f6\u7684\u7b56\u7565\u5d29\u6e83\u95ee\u9898\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u81ea\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u957f\u671f\u8bad\u7ec3\u4e2d\u5b58\u5728\"\u7b56\u7565\u5d29\u6e83\"\u95ee\u9898\uff0c\u6027\u80fd\u6025\u5267\u4e0b\u964d\u3002\u5373\u4f7f\u589e\u52a0rollout\u6570\u91cf\u4e5f\u53ea\u80fd\u5ef6\u8fdf\u800c\u975e\u9632\u6b62\u5d29\u6e83\uff0c\u9700\u8981\u65b0\u7684\u7a33\u5b9a\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e24\u4e2a\u521b\u65b0\uff1a1) M-GRPO\u6846\u67b6\uff0c\u5229\u7528\u7f13\u6162\u6f14\u53d8\u7684\u52a8\u91cf\u6a21\u578b\u63d0\u4f9b\u7a33\u5b9a\u8bad\u7ec3\u76ee\u6807\uff1b2) \u57fa\u4e8e\u56db\u5206\u4f4d\u8ddd\u7684\u81ea\u9002\u5e94\u8fc7\u6ee4\u65b9\u6cd5\uff0c\u52a8\u6001\u4fee\u526a\u4f4e\u71b5\u8f68\u8ff9\uff0c\u4fdd\u6301\u7b56\u7565\u591a\u6837\u6027\u3002", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cM-GRPO\u7a33\u5b9a\u4e86\u8bad\u7ec3\u8fc7\u7a0b\uff0cIQR\u8fc7\u6ee4\u5668\u9632\u6b62\u4e86\u8fc7\u65e9\u6536\u655b\uff0c\u7ec4\u5408\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7M-GRPO\u7684\u7a33\u5b9a\u8bad\u7ec3\u76ee\u6807\u548cIQR\u8fc7\u6ee4\u5668\u7684\u71b5\u4fdd\u6301\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u81ea\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7b56\u7565\u5d29\u6e83\u95ee\u9898\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u7a33\u5b9a\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2512.12131", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.12131", "abs": "https://arxiv.org/abs/2512.12131", "authors": ["Zhengyang Wang", "Ziyue Liu", "Ruijie Zhang", "Avinash Maurya", "Paul Hovland", "Bogdan Nicolae", "Franck Cappello", "Zheng Zhang"], "title": "BOOST: BOttleneck-Optimized Scalable Training Framework for Low-Rank Large Language Models", "comment": null, "summary": "The scale of transformer model pre-training is constrained by the increasing computation and communication cost. Low-rank bottleneck architectures offer a promising solution to significantly reduce the training time and memory footprint with minimum impact on accuracy. Despite algorithmic efficiency, bottleneck architectures scale poorly under standard tensor parallelism. Simply applying 3D parallelism designed for full-rank methods leads to excessive communication and poor GPU utilization. To address this limitation, we propose BOOST, an efficient training framework tailored for large-scale low-rank bottleneck architectures. BOOST introduces a novel Bottleneck-aware Tensor Parallelism, and combines optimizations such as online-RMSNorm, linear layer grouping, and low-rank activation checkpointing to achieve end-to-end training speedup. Evaluations on different low-rank bottleneck architectures demonstrate that BOOST achieves 1.46-1.91$\\times$ speedup over full-rank model baselines and 1.87-2.27$\\times$ speedup over low-rank model with naively integrated 3D parallelism, with improved GPU utilization and reduced communication overhead.", "AI": {"tldr": "BOOST\u6846\u67b6\u9488\u5bf9\u4f4e\u79e9\u74f6\u9888\u67b6\u6784\u7684\u5927\u89c4\u6a21\u8bad\u7ec3\uff0c\u63d0\u51fa\u74f6\u9888\u611f\u77e5\u5f20\u91cf\u5e76\u884c\u7b49\u4f18\u5316\u6280\u672f\uff0c\u76f8\u6bd4\u5168\u79e9\u6a21\u578b\u57fa\u7ebf\u83b7\u5f971.46-1.91\u500d\u52a0\u901f\uff0c\u76f8\u6bd4\u7b80\u5355\u96c6\u62103D\u5e76\u884c\u7684\u4f4e\u79e9\u6a21\u578b\u83b7\u5f971.87-2.27\u500d\u52a0\u901f\u3002", "motivation": "Transformer\u6a21\u578b\u9884\u8bad\u7ec3\u7684\u89c4\u6a21\u53d7\u8ba1\u7b97\u548c\u901a\u4fe1\u6210\u672c\u589e\u52a0\u7684\u9650\u5236\uff0c\u4f4e\u79e9\u74f6\u9888\u67b6\u6784\u80fd\u663e\u8457\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u548c\u5185\u5b58\u5360\u7528\uff0c\u4f46\u5bf9\u6807\u51c6\u5f20\u91cf\u5e76\u884c\u6269\u5c55\u6027\u5dee\uff0c\u7b80\u5355\u5e94\u75283D\u5e76\u884c\u4f1a\u5bfc\u81f4\u8fc7\u5ea6\u901a\u4fe1\u548cGPU\u5229\u7528\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faBOOST\u8bad\u7ec3\u6846\u67b6\uff0c\u5305\u542b\u74f6\u9888\u611f\u77e5\u5f20\u91cf\u5e76\u884c\u3001\u5728\u7ebfRMSNorm\u3001\u7ebf\u6027\u5c42\u5206\u7ec4\u548c\u4f4e\u79e9\u6fc0\u6d3b\u68c0\u67e5\u70b9\u7b49\u4f18\u5316\u6280\u672f\uff0c\u4e13\u95e8\u9488\u5bf9\u5927\u89c4\u6a21\u4f4e\u79e9\u74f6\u9888\u67b6\u6784\u8bbe\u8ba1\u3002", "result": "\u5728\u4e0d\u540c\u4f4e\u79e9\u74f6\u9888\u67b6\u6784\u4e0a\u8bc4\u4f30\uff0cBOOST\u76f8\u6bd4\u5168\u79e9\u6a21\u578b\u57fa\u7ebf\u83b7\u5f971.46-1.91\u500d\u52a0\u901f\uff0c\u76f8\u6bd4\u7b80\u5355\u96c6\u62103D\u5e76\u884c\u7684\u4f4e\u79e9\u6a21\u578b\u83b7\u5f971.87-2.27\u500d\u52a0\u901f\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86GPU\u5229\u7528\u7387\u5e76\u51cf\u5c11\u4e86\u901a\u4fe1\u5f00\u9500\u3002", "conclusion": "BOOST\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4f4e\u79e9\u74f6\u9888\u67b6\u6784\u5728\u5927\u89c4\u6a21\u8bad\u7ec3\u4e2d\u7684\u5e76\u884c\u6269\u5c55\u95ee\u9898\uff0c\u901a\u8fc7\u4e13\u95e8\u4f18\u5316\u7684\u5e76\u884c\u7b56\u7565\u548c\u591a\u9879\u6280\u672f\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u8bad\u7ec3\u52a0\u901f\u548c\u8d44\u6e90\u5229\u7528\u6548\u7387\u63d0\u5347\u3002"}}
{"id": "2512.12209", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12209", "abs": "https://arxiv.org/abs/2512.12209", "authors": ["Zahra Dehghanian", "Morteza Abolghasemi", "Hamid Beigy", "Hamid R. Rabiee"], "title": "CineLOG: A Training Free Approach for Cinematic Long Video Generation", "comment": null, "summary": "Controllable video synthesis is a central challenge in computer vision, yet current models struggle with fine grained control beyond textual prompts, particularly for cinematic attributes like camera trajectory and genre. Existing datasets often suffer from severe data imbalance, noisy labels, or a significant simulation to real gap. To address this, we introduce CineLOG, a new dataset of 5,000 high quality, balanced, and uncut video clips. Each entry is annotated with a detailed scene description, explicit camera instructions based on a standard cinematic taxonomy, and genre label, ensuring balanced coverage across 17 diverse camera movements and 15 film genres. We also present our novel pipeline designed to create this dataset, which decouples the complex text to video (T2V) generation task into four easier stages with more mature technology. To enable coherent, multi shot sequences, we introduce a novel Trajectory Guided Transition Module that generates smooth spatio-temporal interpolation. Extensive human evaluations show that our pipeline significantly outperforms SOTA end to end T2V models in adhering to specific camera and screenplay instructions, while maintaining professional visual quality. All codes and data are available at https://cine-log.pages.dev.", "AI": {"tldr": "CineLOG\u662f\u4e00\u4e2a\u5305\u542b5000\u4e2a\u9ad8\u8d28\u91cf\u5e73\u8861\u89c6\u9891\u7247\u6bb5\u7684\u6570\u636e\u96c6\uff0c\u5e26\u6709\u8be6\u7ec6\u573a\u666f\u63cf\u8ff0\u3001\u57fa\u4e8e\u6807\u51c6\u7535\u5f71\u5206\u7c7b\u7684\u660e\u786e\u6444\u50cf\u673a\u6307\u4ee4\u548c\u7c7b\u578b\u6807\u7b7e\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u7684\u6570\u636e\u4e0d\u5e73\u8861\u3001\u566a\u58f0\u6807\u7b7e\u548c\u6a21\u62df\u5230\u771f\u5b9e\u5dee\u8ddd\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u53ef\u63a7\u89c6\u9891\u5408\u6210\u6a21\u578b\u5728\u8d85\u8d8a\u6587\u672c\u63d0\u793a\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u6444\u50cf\u673a\u8f68\u8ff9\u548c\u7535\u5f71\u7c7b\u578b\u7b49\u7535\u5f71\u5c5e\u6027\u65b9\u9762\u3002\u73b0\u6709\u6570\u636e\u96c6\u901a\u5e38\u5b58\u5728\u4e25\u91cd\u7684\u6570\u636e\u4e0d\u5e73\u8861\u3001\u566a\u58f0\u6807\u7b7e\u6216\u663e\u8457\u7684\u6a21\u62df\u5230\u771f\u5b9e\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u4e86CineLOG\u6570\u636e\u96c6\u521b\u5efa\u7684\u65b0\u6d41\u7a0b\uff0c\u5c06\u590d\u6742\u7684\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u4efb\u52a1\u89e3\u8026\u4e3a\u56db\u4e2a\u66f4\u7b80\u5355\u7684\u9636\u6bb5\uff0c\u4f7f\u7528\u66f4\u6210\u719f\u7684\u6280\u672f\u3002\u5f15\u5165\u4e86\u8f68\u8ff9\u5f15\u5bfc\u8fc7\u6e21\u6a21\u5757\u6765\u751f\u6210\u5e73\u6ed1\u7684\u65f6\u7a7a\u63d2\u503c\uff0c\u5b9e\u73b0\u8fde\u8d2f\u7684\u591a\u955c\u5934\u5e8f\u5217\u3002", "result": "\u5e7f\u6cdb\u7684\u4eba\u7c7b\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6d41\u7a0b\u5728\u9075\u5faa\u7279\u5b9a\u6444\u50cf\u673a\u548c\u5267\u672c\u6307\u4ee4\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u7aef\u5230\u7aef\u6587\u672c\u5230\u89c6\u9891\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u4e13\u4e1a\u7684\u89c6\u89c9\u8d28\u91cf\u3002", "conclusion": "CineLOG\u6570\u636e\u96c6\u548c\u63d0\u51fa\u7684\u6d41\u7a0b\u4e3a\u53ef\u63a7\u89c6\u9891\u5408\u6210\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u3001\u5e73\u8861\u7684\u6570\u636e\u8d44\u6e90\u548c\u6709\u6548\u7684\u6280\u672f\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2512.13404", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.13404", "abs": "https://arxiv.org/abs/2512.13404", "authors": ["Stefan Kulk", "Frederik Zuiderveen Borgesius"], "title": "Google Spain v. Gonz\u00e1les: Did the Court forget about freedom of expression?", "comment": null, "summary": "When reviewing a job application letter, going on a first date, or considering doing business with someone, the first thing many people do is entering the person's name in a search engine. A search engine can point searchers to information that would otherwise have remained obscure. If somebody searched for the name of Spanish lawyer Mario Costeja Gonz\u00e1lez, Google showed search results that included a link to a 1998 newspaper announcement implying he had financial troubles at the time. Gonz\u00e1lez wanted Google to stop showing those links and started a procedure in Spain. After some legal wrangling, the Spanish Audiencia Nacional (National High Court) asked the Court of Justice of the European Union (CJEU) for advice on the application of the Data Protection Directive, which led to the controversial judgment in Google Spain. In its judgment, the CJEU holds that people, under certain conditions, have the right to have search results for their name delisted. This right can also extend to lawfully published information.", "AI": {"tldr": "\u6b27\u76df\u6cd5\u9662\u5728Google Spain\u6848\u4e2d\u786e\u7acb\uff0c\u4e2a\u4eba\u5728\u4e00\u5b9a\u6761\u4ef6\u4e0b\u6709\u6743\u8981\u6c42\u641c\u7d22\u5f15\u64ce\u5220\u9664\u4e0e\u5176\u59d3\u540d\u76f8\u5173\u7684\u641c\u7d22\u7ed3\u679c\uff0c\u5373\u4f7f\u76f8\u5173\u4fe1\u606f\u662f\u5408\u6cd5\u53d1\u5e03\u7684\u3002", "motivation": "\u968f\u7740\u641c\u7d22\u5f15\u64ce\u7684\u666e\u53ca\uff0c\u4e2a\u4eba\u9690\u79c1\u9762\u4e34\u65b0\u6311\u6218\u3002\u5f53\u4eba\u4eec\u641c\u7d22\u4ed6\u4eba\u59d3\u540d\u65f6\uff0c\u53ef\u80fd\u627e\u5230\u591a\u5e74\u524d\u7684\u8d1f\u9762\u4fe1\u606f\uff0c\u8fd9\u4e9b\u4fe1\u606f\u867d\u7136\u5408\u6cd5\u53d1\u5e03\uff0c\u4f46\u53ef\u80fd\u5bf9\u4e2a\u4eba\u5f53\u524d\u751f\u6d3b\u9020\u6210\u4e0d\u516c\u6b63\u5f71\u54cd\u3002\u897f\u73ed\u7259\u5f8b\u5e08Mario Costeja Gonz\u00e1lez\u7684\u6848\u4f8b\u51f8\u663e\u4e86\u6570\u5b57\u65f6\u4ee3\u4e2a\u4eba\u6570\u636e\u4fdd\u62a4\u4e0e\u4fe1\u606f\u81ea\u7531\u4e4b\u95f4\u7684\u51b2\u7a81\u3002", "method": "\u901a\u8fc7\u6cd5\u5f8b\u8bc9\u8bbc\u9014\u5f84\uff0c\u897f\u73ed\u7259\u56fd\u5bb6\u6cd5\u9662\u5c06\u6848\u4ef6\u63d0\u4ea4\u7ed9\u6b27\u76df\u6cd5\u9662\uff08CJEU\uff09\uff0c\u8bf7\u6c42\u5c31\u300a\u6570\u636e\u4fdd\u62a4\u6307\u4ee4\u300b\u7684\u9002\u7528\u63d0\u4f9b\u6307\u5bfc\u3002\u6b27\u76df\u6cd5\u9662\u901a\u8fc7\u53f8\u6cd5\u89e3\u91ca\uff0c\u786e\u7acb\u4e86\"\u88ab\u9057\u5fd8\u6743\"\u7684\u6cd5\u5f8b\u539f\u5219\u3002", "result": "\u6b27\u76df\u6cd5\u9662\u88c1\u5b9a\uff0c\u4e2a\u4eba\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u6709\u6743\u8981\u6c42\u641c\u7d22\u5f15\u64ce\u5220\u9664\u4e0e\u5176\u59d3\u540d\u76f8\u5173\u7684\u641c\u7d22\u7ed3\u679c\uff0c\u5373\u4f7f\u539f\u59cb\u4fe1\u606f\u662f\u5408\u6cd5\u53d1\u5e03\u7684\u3002\u8fd9\u4e00\u5224\u51b3\u786e\u7acb\u4e86\u6570\u5b57\u65f6\u4ee3\u7684\"\u88ab\u9057\u5fd8\u6743\"\uff0c\u5bf9\u641c\u7d22\u5f15\u64ce\u8fd0\u8425\u548c\u4e2a\u4eba\u6570\u636e\u4fdd\u62a4\u4ea7\u751f\u4e86\u6df1\u8fdc\u5f71\u54cd\u3002", "conclusion": "Google Spain\u6848\u786e\u7acb\u4e86\u4e2a\u4eba\u5728\u6570\u5b57\u65f6\u4ee3\u4fdd\u62a4\u9690\u79c1\u7684\u91cd\u8981\u6743\u5229\u2014\u2014\"\u88ab\u9057\u5fd8\u6743\"\uff0c\u5e73\u8861\u4e86\u4e2a\u4eba\u4fe1\u606f\u4fdd\u62a4\u4e0e\u4fe1\u606f\u81ea\u7531\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4e3a\u6b27\u76df\u6570\u636e\u4fdd\u62a4\u6cd5\u5f8b\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.13102", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13102", "abs": "https://arxiv.org/abs/2512.13102", "authors": ["Rajeev Bhatt Ambati", "Tianyi Niu", "Aashu Singh", "Shlok Mishra", "Shashank Srivastava", "Snigdha Chaturvedi"], "title": "Socratic Students: Teaching Language Models to Learn by Asking Questions", "comment": null, "summary": "Large Language Models (LLMs) excel at static interactions, where they answer user queries by retrieving knowledge encoded in their parameters. However, in many real-world settings, such as educational tutoring or medical assistance, relevant information is not directly available and must be actively acquired through dynamic interactions. An interactive agent would recognize its own uncertainty, ask targeted questions, and retain new knowledge efficiently. Prior work has primarily explored effective ways for a teacher to instruct the student, where the teacher identifies student gaps and provides guidance. In this work, we shift the focus to the student and investigate effective strategies to actively query the teacher in seeking useful information. Across math and coding benchmarks, where baseline student models begin with near-zero performance, we show that student-led approaches consistently yield absolute Pass@k improvements of at least 0.5 over static baselines. To improve question quality, we train students using Direct Preference Optimization (DPO) with guidance from either self or stronger students. We find that this guided training enables smaller models to learn how to ask better questions, further enhancing learning efficiency.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u5982\u4f55\u8ba9\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u5b66\u751f\u4e3b\u52a8\u5411\u6559\u5e08\u63d0\u95ee\u4ee5\u83b7\u53d6\u77e5\u8bc6\uff0c\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u4efb\u52a1\u4e0a\u663e\u8457\u63d0\u5347\u5b66\u4e60\u6548\u679c", "motivation": "\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u64c5\u957f\u9759\u6001\u4ea4\u4e92\uff0c\u4f46\u5728\u73b0\u5b9e\u573a\u666f\u5982\u6559\u80b2\u8f85\u5bfc\u6216\u533b\u7597\u534f\u52a9\u4e2d\uff0c\u76f8\u5173\u4fe1\u606f\u9700\u8981\u52a8\u6001\u4ea4\u4e92\u4e3b\u52a8\u83b7\u53d6\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6559\u5e08\u5982\u4f55\u6709\u6548\u6307\u5bfc\u5b66\u751f\uff0c\u800c\u672c\u6587\u8f6c\u5411\u7814\u7a76\u5b66\u751f\u5982\u4f55\u4e3b\u52a8\u5411\u6559\u5e08\u63d0\u95ee\u7684\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u5b66\u751f\u4e3b\u5bfc\u7684\u4e3b\u52a8\u63d0\u95ee\u65b9\u6cd5\uff0c\u4f7f\u7528\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u8bad\u7ec3\u5b66\u751f\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u6211\u6307\u5bfc\u6216\u66f4\u5f3a\u5b66\u751f\u7684\u6307\u5bfc\u6765\u63d0\u5347\u63d0\u95ee\u8d28\u91cf\u3002", "result": "\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5b66\u751f\u4e3b\u5bfc\u65b9\u6cd5\u76f8\u6bd4\u9759\u6001\u57fa\u7ebf\u81f3\u5c11\u83b7\u5f970.5\u7684\u7edd\u5bf9Pass@k\u63d0\u5347\u3002\u6307\u5bfc\u8bad\u7ec3\u4f7f\u8f83\u5c0f\u6a21\u578b\u5b66\u4f1a\u63d0\u51fa\u66f4\u597d\u7684\u95ee\u9898\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u3002", "conclusion": "\u5b66\u751f\u4e3b\u52a8\u63d0\u95ee\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5728\u52a8\u6001\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u5b66\u4e60\u80fd\u529b\uff0c\u6307\u5bfc\u8bad\u7ec3\u65b9\u6cd5\u6709\u52a9\u4e8e\u6a21\u578b\u5b66\u4e60\u5982\u4f55\u63d0\u51fa\u66f4\u4f18\u8d28\u7684\u95ee\u9898\u3002"}}
{"id": "2512.12218", "categories": ["cs.CV", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12218", "abs": "https://arxiv.org/abs/2512.12218", "authors": ["Rheeya Uppaal", "Phu Mon Htut", "Min Bai", "Nikolaos Pappas", "Zheng Qi"], "title": "Journey Before Destination: On the importance of Visual Faithfulness in Slow Thinking", "comment": "Preprint", "summary": "Reasoning-augmented vision language models (VLMs) generate explicit chains of thought that promise greater capability and transparency but also introduce new failure modes: models may reach correct answers via visually unfaithful intermediate steps, or reason faithfully yet fail on the final prediction. Standard evaluations that only measure final-answer accuracy cannot distinguish these behaviors. We introduce the visual faithfulness of reasoning chains as a distinct evaluation dimension, focusing on whether the perception steps of a reasoning chain are grounded in the image. We propose a training- and reference-free framework that decomposes chains into perception versus reasoning steps and uses off-the-shelf VLM judges for step-level faithfulness, additionally verifying this approach through a human meta-evaluation. Building on this metric, we present a lightweight self-reflection procedure that detects and locally regenerates unfaithful perception steps without any training. Across multiple reasoning-trained VLMs and perception-heavy benchmarks, our method reduces Unfaithful Perception Rate while preserving final-answer accuracy, improving the reliability of multimodal reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u94fe\u89c6\u89c9\u5fe0\u5b9e\u6027\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u89e3\u611f\u77e5\u4e0e\u63a8\u7406\u6b65\u9aa4\uff0c\u4f7f\u7528\u73b0\u6210VLM\u8bc4\u4f30\u6b65\u9aa4\u7ea7\u5fe0\u5b9e\u6027\uff0c\u5e76\u63d0\u51fa\u8f7b\u91cf\u7ea7\u81ea\u53cd\u601d\u7a0b\u5e8f\u68c0\u6d4b\u548c\u4fee\u590d\u4e0d\u5fe0\u5b9e\u7684\u611f\u77e5\u6b65\u9aa4\u3002", "motivation": "\u5f53\u524d\u63a8\u7406\u589e\u5f3a\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u867d\u7136\u751f\u6210\u663e\u5f0f\u601d\u7ef4\u94fe\uff0c\u4f46\u5b58\u5728\u65b0\u5931\u8d25\u6a21\u5f0f\uff1a\u6a21\u578b\u53ef\u80fd\u901a\u8fc7\u89c6\u89c9\u4e0d\u5fe0\u5b9e\u7684\u4e2d\u95f4\u6b65\u9aa4\u5f97\u51fa\u6b63\u786e\u7b54\u6848\uff0c\u6216\u5fe0\u5b9e\u63a8\u7406\u5374\u6700\u7ec8\u9884\u6d4b\u5931\u8d25\u3002\u4ec5\u8bc4\u4f30\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u6027\u7684\u6807\u51c6\u65b9\u6cd5\u65e0\u6cd5\u533a\u5206\u8fd9\u4e9b\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u65e0\u9700\u8bad\u7ec3\u548c\u53c2\u8003\u7684\u6846\u67b6\uff0c\u5c06\u63a8\u7406\u94fe\u5206\u89e3\u4e3a\u611f\u77e5\u4e0e\u63a8\u7406\u6b65\u9aa4\uff0c\u4f7f\u7528\u73b0\u6210VLM\u8bc4\u4f30\u6b65\u9aa4\u7ea7\u89c6\u89c9\u5fe0\u5b9e\u6027\uff1b\u63d0\u51fa\u8f7b\u91cf\u7ea7\u81ea\u53cd\u601d\u7a0b\u5e8f\uff0c\u68c0\u6d4b\u5e76\u5c40\u90e8\u91cd\u65b0\u751f\u6210\u4e0d\u5fe0\u5b9e\u7684\u611f\u77e5\u6b65\u9aa4\u3002", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u8bad\u7ec3VLM\u548c\u611f\u77e5\u5bc6\u96c6\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u964d\u4f4e\u4e86\u4e0d\u5fe0\u5b9e\u611f\u77e5\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u6027\uff0c\u63d0\u9ad8\u4e86\u591a\u6a21\u6001\u63a8\u7406\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u89c6\u89c9\u63a8\u7406\u94fe\u7684\u5fe0\u5b9e\u6027\u5e94\u4f5c\u4e3a\u72ec\u7acb\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u63d0\u51fa\u7684\u6846\u67b6\u548c\u81ea\u53cd\u601d\u7a0b\u5e8f\u80fd\u6709\u6548\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u53ef\u9760\u6027\u548c\u900f\u660e\u5ea6\u3002"}}
{"id": "2512.13405", "categories": ["cs.CY"], "pdf": "https://arxiv.org/pdf/2512.13405", "abs": "https://arxiv.org/abs/2512.13405", "authors": ["Frederik Johannes Zuiderveen Borgesius"], "title": "Improving Privacy Protection in the area of Behavioural Targeting", "comment": null, "summary": "This PhD thesis discusses how European law could improve privacy protection in the area of behavioural targeting. Behavioural targeting, also referred to as online profiling, involves monitoring people's online behaviour, and using the collected information to show people individually targeted advertisements. To protect privacy in the area of behavioural targeting, the EU lawmaker mainly relies on the consent requirement for the use of tracking technologies in the e-Privacy Directive, and on general data protection law. With informed consent requirements, the law aims to empower people to make choices in their best interests. But behavioural studies cast doubt on the effectiveness of the empowerment approach as a privacy protection measure. Many people click \"I agree\" to any statement that is presented to them. Therefore, to mitigate privacy problems such as chilling effects, this study argues for a combined approach of protecting and empowering the individual. Compared to the current approach, the lawmaker should focus more on protecting people. The PhD thesis is a legal study, but it also incorporates insights from other disciplines, such as computer science, behavioural economics, and media studies. This study is among the first to discuss the implications of behavioural research for European data protection policy. The topic of whether data protection law should apply to pseudonymous data is discussed in depth. The study contains a detailed analysis of the role of informed consent in data protection law, and gives much attention to the tension between protecting and empowering the individual within data protection law.", "AI": {"tldr": "\u8be5\u535a\u58eb\u8bba\u6587\u63a2\u8ba8\u6b27\u76df\u6cd5\u5f8b\u5982\u4f55\u901a\u8fc7\u7ed3\u5408\u4fdd\u62a4\u4e0e\u8d4b\u6743\u7684\u65b9\u6cd5\u6765\u6539\u8fdb\u884c\u4e3a\u5b9a\u5411\u5e7f\u544a\u4e2d\u7684\u9690\u79c1\u4fdd\u62a4\uff0c\u6311\u6218\u4e86\u5f53\u524d\u4e3b\u8981\u4f9d\u8d56\u77e5\u60c5\u540c\u610f\u7684\u8d4b\u6743\u65b9\u6cd5\u3002", "motivation": "\u884c\u4e3a\u5b9a\u5411\uff08\u5728\u7ebf\u753b\u50cf\uff09\u901a\u8fc7\u76d1\u63a7\u7528\u6237\u5728\u7ebf\u884c\u4e3a\u6765\u5c55\u793a\u4e2a\u6027\u5316\u5e7f\u544a\uff0c\u5bf9\u9690\u79c1\u6784\u6210\u5a01\u80c1\u3002\u6b27\u76df\u76ee\u524d\u4e3b\u8981\u4f9d\u8d56e-Privacy\u6307\u4ee4\u4e2d\u7684\u540c\u610f\u8981\u6c42\u548c\u4e00\u822c\u6570\u636e\u4fdd\u62a4\u6cd5\uff0c\u4f46\u884c\u4e3a\u7814\u7a76\u8868\u660e\u8fd9\u79cd\u8d4b\u6743\u65b9\u6cd5\u6548\u679c\u6709\u9650\uff0c\u8bb8\u591a\u4eba\u4f1a\u76f2\u76ee\u70b9\u51fb\"\u540c\u610f\"\u3002\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u66f4\u6709\u6548\u7684\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u3002", "method": "\u8fd9\u662f\u4e00\u9879\u6cd5\u5f8b\u7814\u7a76\uff0c\u4f46\u878d\u5408\u4e86\u8ba1\u7b97\u673a\u79d1\u5b66\u3001\u884c\u4e3a\u7ecf\u6d4e\u5b66\u548c\u5a92\u4f53\u7814\u7a76\u7b49\u591a\u5b66\u79d1\u89c1\u89e3\u3002\u91c7\u7528\u6cd5\u5f8b\u5206\u6790\u65b9\u6cd5\uff0c\u6df1\u5165\u63a2\u8ba8\u4e86\u6570\u636e\u4fdd\u62a4\u6cd5\u662f\u5426\u5e94\u9002\u7528\u4e8e\u5047\u540d\u6570\u636e\uff0c\u5206\u6790\u4e86\u77e5\u60c5\u540c\u610f\u5728\u6570\u636e\u4fdd\u62a4\u6cd5\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u7814\u7a76\u4e86\u4fdd\u62a4\u4e0e\u8d4b\u6743\u4e4b\u95f4\u7684\u5f20\u529b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u4e3b\u8981\u4f9d\u8d56\u77e5\u60c5\u540c\u610f\u7684\u8d4b\u6743\u65b9\u6cd5\u4e0d\u8db3\u4ee5\u6709\u6548\u4fdd\u62a4\u9690\u79c1\u3002\u8bba\u6587\u4e3b\u5f20\u91c7\u7528\u4fdd\u62a4\u4e0e\u8d4b\u6743\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u5efa\u8bae\u7acb\u6cd5\u8005\u5e94\u66f4\u4fa7\u91cd\u4e8e\u4fdd\u62a4\u4e2a\u4eba\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u8d4b\u4e88\u9009\u62e9\u6743\u3002\u8fd9\u662f\u9996\u6279\u8ba8\u8bba\u884c\u4e3a\u7814\u7a76\u5bf9\u6b27\u6d32\u6570\u636e\u4fdd\u62a4\u653f\u7b56\u5f71\u54cd\u7684\u5b66\u672f\u7814\u7a76\u4e4b\u4e00\u3002", "conclusion": "\u4e3a\u7f13\u89e3\u9690\u79c1\u95ee\u9898\uff08\u5982\u5bd2\u8749\u6548\u5e94\uff09\uff0c\u6b27\u76df\u6cd5\u5f8b\u9700\u8981\u4ece\u5f53\u524d\u4e3b\u8981\u4f9d\u8d56\u8d4b\u6743\u7684\u6a21\u5f0f\u8f6c\u5411\u4fdd\u62a4\u4e0e\u8d4b\u6743\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\u3002\u7acb\u6cd5\u8005\u5e94\u66f4\u6ce8\u91cd\u4fdd\u62a4\u4e2a\u4eba\uff0c\u540c\u65f6\u4fdd\u6301\u9002\u5ea6\u7684\u8d4b\u6743\u5143\u7d20\u3002\u8fd9\u79cd\u7efc\u5408\u65b9\u6cd5\u80fd\u66f4\u597d\u5730\u5e94\u5bf9\u884c\u4e3a\u5b9a\u5411\u5e7f\u544a\u5e26\u6765\u7684\u9690\u79c1\u6311\u6218\u3002"}}
{"id": "2512.13131", "categories": ["cs.AI", "cs.CV", "cs.GR", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.13131", "abs": "https://arxiv.org/abs/2512.13131", "authors": ["Xin Guo", "Yifan Zhao", "Jia Li"], "title": "Towards Unified Co-Speech Gesture Generation via Hierarchical Implicit Periodicity Learning", "comment": "IEEE Transactions on Image Processing", "summary": "Generating 3D-based body movements from speech shows great potential in extensive downstream applications, while it still suffers challenges in imitating realistic human movements. Predominant research efforts focus on end-to-end generation schemes to generate co-speech gestures, spanning GANs, VQ-VAE, and recent diffusion models. As an ill-posed problem, in this paper, we argue that these prevailing learning schemes fail to model crucial inter- and intra-correlations across different motion units, i.e. head, body, and hands, thus leading to unnatural movements and poor coordination. To delve into these intrinsic correlations, we propose a unified Hierarchical Implicit Periodicity (HIP) learning approach for audio-inspired 3D gesture generation. Different from predominant research, our approach models this multi-modal implicit relationship by two explicit technique insights: i) To disentangle the complicated gesture movements, we first explore the gesture motion phase manifolds with periodic autoencoders to imitate human natures from realistic distributions while incorporating non-period ones from current latent states for instance-level diversities. ii) To model the hierarchical relationship of face motions, body gestures, and hand movements, driving the animation with cascaded guidance during learning. We exhibit our proposed approach on 3D avatars and extensive experiments show our method outperforms the state-of-the-art co-speech gesture generation methods by both quantitative and qualitative evaluations. Code and models will be publicly available.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u9690\u5f0f\u5468\u671f\u6027\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u8bed\u97f3\u751f\u6210\u66f4\u81ea\u7136\u534f\u8c03\u76843D\u8eab\u4f53\u52a8\u4f5c\uff0c\u901a\u8fc7\u5efa\u6a21\u4e0d\u540c\u8fd0\u52a8\u5355\u5143\u4e4b\u95f4\u7684\u5185\u5728\u76f8\u5173\u6027\u6765\u6539\u5584\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u4ece\u8bed\u97f3\u751f\u62103D\u8eab\u4f53\u52a8\u4f5c\u7684\u65b9\u6cd5\u867d\u7136\u5728\u4e0b\u6e38\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5b58\u5728\u751f\u6210\u52a8\u4f5c\u4e0d\u81ea\u7136\u7684\u95ee\u9898\u3002\u4e3b\u6d41\u7814\u7a76\u91c7\u7528\u7aef\u5230\u7aef\u751f\u6210\u65b9\u6848\uff08\u5982GANs\u3001VQ-VAE\u3001\u6269\u6563\u6a21\u578b\uff09\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5efa\u6a21\u4e0d\u540c\u8fd0\u52a8\u5355\u5143\uff08\u5934\u3001\u8eab\u4f53\u3001\u624b\uff09\u4e4b\u95f4\u7684\u5173\u952e\u76f8\u4e92\u5173\u8054\u548c\u5185\u90e8\u5173\u8054\uff0c\u5bfc\u81f4\u52a8\u4f5c\u4e0d\u81ea\u7136\u548c\u534f\u8c03\u6027\u5dee\u3002", "method": "\u63d0\u51fa\u4e86\u7edf\u4e00\u7684\u5206\u5c42\u9690\u5f0f\u5468\u671f\u6027\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6280\u672f\u6d1e\u5bdf\uff1a1\uff09\u4f7f\u7528\u5468\u671f\u6027\u81ea\u7f16\u7801\u5668\u63a2\u7d22\u624b\u52bf\u8fd0\u52a8\u76f8\u4f4d\u6d41\u5f62\uff0c\u4ece\u771f\u5b9e\u5206\u5e03\u4e2d\u6a21\u4eff\u4eba\u7c7b\u81ea\u7136\u7279\u6027\uff0c\u540c\u65f6\u7ed3\u5408\u5f53\u524d\u6f5c\u5728\u72b6\u6001\u7684\u975e\u5468\u671f\u6027\u7279\u5f81\u4ee5\u5b9e\u73b0\u5b9e\u4f8b\u7ea7\u591a\u6837\u6027\uff1b2\uff09\u901a\u8fc7\u7ea7\u8054\u5f15\u5bfc\u5efa\u6a21\u9762\u90e8\u52a8\u4f5c\u3001\u8eab\u4f53\u59ff\u52bf\u548c\u624b\u90e8\u8fd0\u52a8\u4e4b\u95f4\u7684\u5206\u5c42\u5173\u7cfb\u3002", "result": "\u57283D\u865a\u62df\u5f62\u8c61\u4e0a\u5c55\u793a\u4e86\u6240\u63d0\u65b9\u6cd5\uff0c\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e0a\u90fd\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u8bed\u97f3\u4f34\u968f\u624b\u52bf\u751f\u6210\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5efa\u6a21\u8fd0\u52a8\u5355\u5143\u4e4b\u95f4\u7684\u5185\u5728\u76f8\u5173\u6027\uff0c\u63d0\u51fa\u7684\u5206\u5c42\u9690\u5f0f\u5468\u671f\u6027\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u66f4\u81ea\u7136\u534f\u8c03\u76843D\u8eab\u4f53\u52a8\u4f5c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u6a21\u4eff\u771f\u5b9e\u4eba\u7c7b\u52a8\u4f5c\u65b9\u9762\u7684\u4e0d\u8db3\u3002"}}
{"id": "2512.12219", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12219", "abs": "https://arxiv.org/abs/2512.12219", "authors": ["Zhi Chen", "Jingcai Guo", "Taotao Cai", "Yuxiang Cai"], "title": "Fine-Grained Zero-Shot Learning with Attribute-Centric Representations", "comment": "Preprint", "summary": "Recognizing unseen fine-grained categories demands a model that can distinguish subtle visual differences. This is typically achieved by transferring visual-attribute relationships from seen classes to unseen classes. The core challenge is attribute entanglement, where conventional models collapse distinct attributes like color, shape, and texture into a single visual embedding. This causes interference that masks these critical distinctions. The post-hoc solutions of previous work are insufficient, as they operate on representations that are already mixed. We propose a zero-shot learning framework that learns AttributeCentric Representations (ACR) to tackle this problem by imposing attribute disentanglement during representation learning. ACR is achieved with two mixture-of-experts components, including Mixture of Patch Experts (MoPE) and Mixture of Attribute Experts (MoAE). First, MoPE is inserted into the transformer using a dual-level routing mechanism to conditionally dispatch image patches to specialized experts. This ensures coherent attribute families are processed by dedicated experts. Finally, the MoAE head projects these expert-refined features into sparse, partaware attribute maps for robust zero-shot classification. On zero-shot learning benchmark datasets CUB, AwA2, and SUN, our ACR achieves consistent state-of-the-art results.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c5e\u6027\u4e2d\u5fc3\u8868\u793a\uff08ACR\uff09\u7684\u96f6\u6837\u672c\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u4e2a\u4e13\u5bb6\u6df7\u5408\u7ec4\u4ef6\uff08MoPE\u548cMoAE\uff09\u5b9e\u73b0\u5c5e\u6027\u89e3\u7f20\uff0c\u4ee5\u89e3\u51b3\u7ec6\u7c92\u5ea6\u7c7b\u522b\u8bc6\u522b\u4e2d\u7684\u5c5e\u6027\u7ea0\u7f20\u95ee\u9898\u3002", "motivation": "\u8bc6\u522b\u672a\u89c1\u8fc7\u7684\u7ec6\u7c92\u5ea6\u7c7b\u522b\u9700\u8981\u6a21\u578b\u80fd\u591f\u533a\u5206\u7ec6\u5fae\u7684\u89c6\u89c9\u5dee\u5f02\u3002\u4f20\u7edf\u65b9\u6cd5\u5c06\u989c\u8272\u3001\u5f62\u72b6\u3001\u7eb9\u7406\u7b49\u4e0d\u540c\u5c5e\u6027\u538b\u7f29\u5230\u5355\u4e00\u89c6\u89c9\u5d4c\u5165\u4e2d\uff0c\u5bfc\u81f4\u5c5e\u6027\u7ea0\u7f20\u95ee\u9898\uff0c\u63a9\u76d6\u4e86\u5173\u952e\u533a\u522b\u3002\u73b0\u6709\u7684\u4e8b\u540e\u89e3\u51b3\u65b9\u6848\u4e0d\u8db3\uff0c\u56e0\u4e3a\u5b83\u4eec\u5904\u7406\u7684\u662f\u5df2\u7ecf\u6df7\u5408\u7684\u8868\u5f81\u3002", "method": "\u63d0\u51faAttributeCentric Representations\uff08ACR\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u4e2a\u4e13\u5bb6\u6df7\u5408\u7ec4\u4ef6\u5b9e\u73b0\u5c5e\u6027\u89e3\u7f20\uff1a1\uff09Mixture of Patch Experts\uff08MoPE\uff09\uff1a\u4f7f\u7528\u53cc\u7ea7\u8def\u7531\u673a\u5236\u5c06\u56fe\u50cf\u5757\u6709\u6761\u4ef6\u5730\u5206\u914d\u5230\u4e13\u95e8\u7684\u4e13\u5bb6\u8fdb\u884c\u5904\u7406\uff1b2\uff09Mixture of Attribute Experts\uff08MoAE\uff09\uff1a\u5c06\u4e13\u5bb6\u7cbe\u70bc\u7684\u7279\u5f81\u6295\u5f71\u5230\u7a00\u758f\u7684\u3001\u90e8\u5206\u611f\u77e5\u7684\u5c5e\u6027\u6620\u5c04\u4e2d\uff0c\u7528\u4e8e\u9c81\u68d2\u7684\u96f6\u6837\u672c\u5206\u7c7b\u3002", "result": "\u5728\u96f6\u6837\u672c\u5b66\u4e60\u57fa\u51c6\u6570\u636e\u96c6CUB\u3001AwA2\u548cSUN\u4e0a\uff0cACR\u6846\u67b6\u53d6\u5f97\u4e86\u6301\u7eed\u7684\u6700\u5148\u8fdb\u7ed3\u679c\u3002", "conclusion": "\u901a\u8fc7\u5728\u8868\u793a\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u65bd\u52a0\u5c5e\u6027\u89e3\u7f20\uff0c\u63d0\u51fa\u7684ACR\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5c5e\u6027\u7ea0\u7f20\u95ee\u9898\uff0c\u5728\u7ec6\u7c92\u5ea6\u96f6\u6837\u672c\u5b66\u4e60\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2512.13658", "categories": ["cs.CY", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13658", "abs": "https://arxiv.org/abs/2512.13658", "authors": ["Mohammadreza Molavi", "Mohammad Moein", "Mohammadreza Tavakoli", "Abdolali Faraji", "Stefan T. Mol", "G\u00e1bor Kismih\u00f3k"], "title": "Embedding-Based Rankings of Educational Resources based on Learning Outcome Alignment: Benchmarking, Expert Validation, and Learner Performance", "comment": "Accepted for publication at the 16th International Conference on Learning Analytics & Knowledge (LAK 2026)", "summary": "As the online learning landscape evolves, the need for personalization is increasingly evident. Although educational resources are burgeoning, educators face challenges selecting materials that both align with intended learning outcomes and address diverse learner needs. Large Language Models (LLMs) are attracting growing interest for their potential to create learning resources that better support personalization, but verifying coverage of intended outcomes still requires human alignment review, which is costly and limits scalability. We propose a framework that supports the cost-effective automation of evaluating alignment between educational resources and intended learning outcomes. Using human-generated materials, we benchmarked LLM-based text-embedding models and found that the most accurate model (Voyage) achieved 79% accuracy in detecting alignment. We then applied the optimal model to LLM-generated resources and, via expert evaluation, confirmed that it reliably assessed correspondence to intended outcomes (83% accuracy). Finally, in a three-group experiment with 360 learners, higher alignment scores were positively related to greater learning performance, chi-squared(2, N = 360) = 15.39, p < 0.001. These findings show that embedding-based alignment scores can facilitate scalable personalization by confirming alignment with learning outcomes, which allows teachers to focus on tailoring content to diverse learner needs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u6587\u672c\u5d4c\u5165\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u8bc4\u4f30\u6559\u80b2\u8d44\u6e90\u4e0e\u9884\u671f\u5b66\u4e60\u6210\u679c\u4e4b\u95f4\u7684\u5bf9\u9f50\u5ea6\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4b\u5bf9\u9f50\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5e76\u8bc1\u660e\u4e86\u5bf9\u9f50\u5ea6\u5206\u6570\u4e0e\u5b66\u4e60\u8868\u73b0\u6b63\u76f8\u5173\u3002", "motivation": "\u968f\u7740\u5728\u7ebf\u6559\u80b2\u7684\u53d1\u5c55\uff0c\u4e2a\u6027\u5316\u9700\u6c42\u65e5\u76ca\u589e\u957f\u3002\u867d\u7136\u6559\u80b2\u8d44\u6e90\u4e30\u5bcc\uff0c\u4f46\u6559\u5e08\u96be\u4ee5\u9009\u62e9\u65e2\u7b26\u5408\u5b66\u4e60\u76ee\u6807\u53c8\u80fd\u6ee1\u8db3\u591a\u6837\u5316\u5b66\u4e60\u8005\u9700\u6c42\u7684\u6750\u6599\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6709\u6f5c\u529b\u521b\u5efa\u4e2a\u6027\u5316\u5b66\u4e60\u8d44\u6e90\uff0c\u4f46\u9a8c\u8bc1\u5176\u4e0e\u9884\u671f\u6210\u679c\u7684\u5bf9\u9f50\u4ecd\u9700\u8981\u6602\u8d35\u7684\u4eba\u5de5\u5ba1\u6838\uff0c\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u652f\u6301\u7ecf\u6d4e\u9ad8\u6548\u81ea\u52a8\u5316\u8bc4\u4f30\u6559\u80b2\u8d44\u6e90\u4e0e\u5b66\u4e60\u6210\u679c\u5bf9\u9f50\u5ea6\u7684\u6846\u67b6\u3002\u9996\u5148\u4f7f\u7528\u4eba\u5de5\u751f\u6210\u7684\u6750\u6599\u5bf9\u57fa\u4e8eLLM\u7684\u6587\u672c\u5d4c\u5165\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u627e\u5230\u6700\u51c6\u786e\u7684\u6a21\u578b\uff08Voyage\uff0c79%\u51c6\u786e\u7387\uff09\u3002\u7136\u540e\u5c06\u6700\u4f18\u6a21\u578b\u5e94\u7528\u4e8eLLM\u751f\u6210\u7684\u8d44\u6e90\uff0c\u5e76\u901a\u8fc7\u4e13\u5bb6\u8bc4\u4f30\u9a8c\u8bc1\u5176\u53ef\u9760\u6027\uff0883%\u51c6\u786e\u7387\uff09\u3002\u6700\u540e\u5728360\u540d\u5b66\u4e60\u8005\u7684\u4e09\u7ec4\u5b9e\u9a8c\u4e2d\u68c0\u9a8c\u5bf9\u9f50\u5ea6\u5206\u6570\u4e0e\u5b66\u4e60\u8868\u73b0\u7684\u5173\u7cfb\u3002", "result": "\u6700\u51c6\u786e\u7684\u6587\u672c\u5d4c\u5165\u6a21\u578b\uff08Voyage\uff09\u5728\u68c0\u6d4b\u5bf9\u9f50\u6027\u65b9\u9762\u8fbe\u523079%\u51c6\u786e\u7387\u3002\u5e94\u7528\u4e8eLLM\u751f\u6210\u8d44\u6e90\u65f6\uff0c\u6a21\u578b\u53ef\u9760\u5730\u8bc4\u4f30\u4e86\u4e0e\u9884\u671f\u6210\u679c\u7684\u5bf9\u5e94\u5173\u7cfb\uff0883%\u51c6\u786e\u7387\uff09\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u66f4\u9ad8\u7684\u5bf9\u9f50\u5ea6\u5206\u6570\u4e0e\u66f4\u597d\u7684\u5b66\u4e60\u8868\u73b0\u6b63\u76f8\u5173\uff0c\u5361\u65b9\u68c0\u9a8c\u7ed3\u679c\u663e\u8457\uff08\u03c7\u00b2(2, N=360)=15.39, p<0.001\uff09\u3002", "conclusion": "\u57fa\u4e8e\u5d4c\u5165\u7684\u5bf9\u9f50\u5ea6\u5206\u6570\u53ef\u4ee5\u901a\u8fc7\u786e\u8ba4\u6559\u80b2\u8d44\u6e90\u4e0e\u5b66\u4e60\u6210\u679c\u7684\u5bf9\u9f50\u6765\u4fc3\u8fdb\u53ef\u6269\u5c55\u7684\u4e2a\u6027\u5316\u6559\u80b2\uff0c\u4f7f\u6559\u5e08\u80fd\u591f\u4e13\u6ce8\u4e8e\u6839\u636e\u591a\u6837\u5316\u5b66\u4e60\u8005\u9700\u6c42\u5b9a\u5236\u5185\u5bb9\uff0c\u51cf\u5c11\u4eba\u5de5\u5ba1\u6838\u6210\u672c\u3002"}}
{"id": "2512.13142", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.13142", "abs": "https://arxiv.org/abs/2512.13142", "authors": ["Anika Sharma", "Malavika Mampally", "Chidaksh Ravuru", "Kandyce Brennan", "Neil Gaikwad"], "title": "Can AI Understand What We Cannot Say? Measuring Multilevel Alignment Through Abortion Stigma Across Cognitive, Interpersonal, and Structural Levels", "comment": null, "summary": "As large language models increasingly mediate stigmatized health decisions, their capacity to genuinely understand complex psychological and physiological phenomena remains poorly evaluated. Can AI understand what we cannot say? We investigate whether LLMs coherently represent abortion stigma across the cognitive, interpersonal, and structural levels where it operates. We systematically tested 627 demographically diverse personas across five leading LLMs using the validated Individual Level Abortion Stigma Scale (ILAS). Our multilevel analysis examined whether models coherently represent stigma at the cognitive level (self-judgment), interpersonal level (anticipated judgment and isolation), and structural level (community condemnation and disclosure patterns), as well as overall stigma. Models fail tests of genuine understanding across all levels. They overestimate interpersonal stigma while underestimating cognitive stigma, assume uniform community condemnation, introduce demographic biases absent from human validation data, miss the empirically validated stigma-secrecy relationship, and contradict themselves within theoretical constructs. These patterns reveal that current alignment approaches ensure appropriate language but not coherent multilevel understanding. This work provides empirical evidence that current LLMs lack coherent multilevel understanding of psychological and physiological constructs. AI safety in high-stakes contexts demands new approaches to design (multilevel coherence), evaluation (continuous auditing), governance and regulation (mandatory audits, accountability, deployment restrictions), and AI literacy in domains where understanding what people cannot say determines whether support helps or harms.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u7f3a\u4e4f\u5bf9\u5815\u80ce\u803b\u8fb1\u7684\u591a\u5c42\u6b21\u8fde\u8d2f\u7406\u89e3\uff0c\u65e0\u6cd5\u771f\u6b63\u7406\u89e3\u590d\u6742\u7684\u5fc3\u7406\u751f\u7406\u73b0\u8c61\uff0c\u5b58\u5728\u7cfb\u7edf\u6027\u504f\u5dee\u548c\u77db\u76fe", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u4ecb\u5165\u6c61\u540d\u5316\u7684\u5065\u5eb7\u51b3\u7b56\uff0c\u8bc4\u4f30\u5b83\u4eec\u662f\u5426\u771f\u6b63\u7406\u89e3\u590d\u6742\u7684\u5fc3\u7406\u548c\u751f\u7406\u73b0\u8c61\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7a76LLMs\u662f\u5426\u80fd\u5728\u8ba4\u77e5\u3001\u4eba\u9645\u548c\u7ed3\u6784\u5c42\u9762\u8fde\u8d2f\u5730\u8868\u793a\u5815\u80ce\u803b\u8fb1", "method": "\u4f7f\u7528\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u4e2a\u4f53\u5c42\u9762\u5815\u80ce\u803b\u8fb1\u91cf\u8868\uff08ILAS\uff09\uff0c\u5728\u4e94\u4e2a\u9886\u5148\u7684LLMs\u4e0a\u7cfb\u7edf\u6d4b\u8bd5\u4e86627\u4e2a\u4e0d\u540c\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\u7684\u89d2\u8272\u3002\u8fdb\u884c\u591a\u5c42\u6b21\u5206\u6790\uff0c\u8003\u5bdf\u6a21\u578b\u5728\u8ba4\u77e5\u5c42\u9762\uff08\u81ea\u6211\u5224\u65ad\uff09\u3001\u4eba\u9645\u5c42\u9762\uff08\u9884\u671f\u5224\u65ad\u548c\u5b64\u7acb\uff09\u548c\u7ed3\u6784\u5c42\u9762\uff08\u793e\u533a\u8c34\u8d23\u548c\u62ab\u9732\u6a21\u5f0f\uff09\u4ee5\u53ca\u6574\u4f53\u803b\u8fb1\u7684\u8868\u793a\u80fd\u529b", "result": "\u6a21\u578b\u5728\u6240\u6709\u5c42\u9762\u90fd\u672a\u80fd\u901a\u8fc7\u771f\u6b63\u7406\u89e3\u7684\u6d4b\u8bd5\uff1a\u9ad8\u4f30\u4eba\u9645\u803b\u8fb1\u540c\u65f6\u4f4e\u4f30\u8ba4\u77e5\u803b\u8fb1\uff1b\u5047\u8bbe\u7edf\u4e00\u7684\u793e\u533a\u8c34\u8d23\uff1b\u5f15\u5165\u4eba\u7c7b\u9a8c\u8bc1\u6570\u636e\u4e2d\u4e0d\u5b58\u5728\u7684\u4eba\u53e3\u7edf\u8ba1\u504f\u5dee\uff1b\u9519\u8fc7\u7ecf\u9a8c\u9a8c\u8bc1\u7684\u803b\u8fb1-\u4fdd\u5bc6\u5173\u7cfb\uff1b\u5728\u7406\u8bba\u5efa\u6784\u4e2d\u81ea\u76f8\u77db\u76fe", "conclusion": "\u5f53\u524d\u7684\u5bf9\u9f50\u65b9\u6cd5\u53ea\u80fd\u786e\u4fdd\u9002\u5f53\u7684\u8bed\u8a00\u4f7f\u7528\uff0c\u4f46\u4e0d\u80fd\u4fdd\u8bc1\u591a\u5c42\u6b21\u8fde\u8d2f\u7406\u89e3\u3002\u5728\u9ad8\u98ce\u9669\u60c5\u5883\u4e2d\uff0cAI\u5b89\u5168\u9700\u8981\u65b0\u7684\u8bbe\u8ba1\u65b9\u6cd5\uff08\u591a\u5c42\u6b21\u8fde\u8d2f\u6027\uff09\u3001\u8bc4\u4f30\u65b9\u6cd5\uff08\u6301\u7eed\u5ba1\u8ba1\uff09\u3001\u6cbb\u7406\u548c\u76d1\u7ba1\uff08\u5f3a\u5236\u5ba1\u8ba1\u3001\u95ee\u8d23\u5236\u3001\u90e8\u7f72\u9650\u5236\uff09\uff0c\u4ee5\u53ca\u5728\u7406\u89e3\u4eba\u4eec\u65e0\u6cd5\u8a00\u8bf4\u7684\u9886\u57df\u63d0\u9ad8AI\u7d20\u517b"}}
{"id": "2512.12220", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12220", "abs": "https://arxiv.org/abs/2512.12220", "authors": ["Minheng Ni", "Zhengyuan Yang", "Yaowen Zhang", "Linjie Li", "Chung-Ching Lin", "Kevin Lin", "Zhendong Wang", "Xiaofei Wang", "Shujie Liu", "Lei Zhang", "Wangmeng Zuo", "Lijuan Wang"], "title": "ProImage-Bench: Rubric-Based Evaluation for Professional Image Generation", "comment": null, "summary": "We study professional image generation, where a model must synthesize information-dense, scientifically precise illustrations from technical descriptions rather than merely produce visually plausible pictures. To quantify the progress, we introduce ProImage-Bench, a rubric-based benchmark that targets biology schematics, engineering/patent drawings, and general scientific diagrams. For 654 figures collected from real textbooks and technical reports, we construct detailed image instructions and a hierarchy of rubrics that decompose correctness into 6,076 criteria and 44,131 binary checks. Rubrics are derived from surrounding text and reference figures using large multimodal models, and are evaluated by an automated LMM-based judge with a principled penalty scheme that aggregates sub-question outcomes into interpretable criterion scores. We benchmark several representative text-to-image models on ProImage-Bench and find that, despite strong open-domain performance, the best base model reaches only 0.791 rubric accuracy and 0.553 criterion score overall, revealing substantial gaps in fine-grained scientific fidelity. Finally, we show that the same rubrics provide actionable supervision: feeding failed checks back into an editing model for iterative refinement boosts a strong generator from 0.653 to 0.865 in rubric accuracy and from 0.388 to 0.697 in criterion score. ProImage-Bench thus offers both a rigorous diagnostic for professional image generation and a scalable signal for improving specification-faithful scientific illustrations.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86ProImage-Bench\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e13\u4e1a\u56fe\u50cf\u751f\u6210\u6a21\u578b\u5728\u79d1\u5b66\u7cbe\u786e\u56fe\u793a\u65b9\u9762\u7684\u80fd\u529b\uff0c\u901a\u8fc7\u57fa\u4e8e\u89c4\u5219\u7684\u8bc4\u4f30\u548c\u8fed\u4ee3\u7f16\u8f91\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u5f00\u653e\u9886\u57df\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u751f\u6210\u4fe1\u606f\u5bc6\u96c6\u3001\u79d1\u5b66\u7cbe\u786e\u7684\u4e13\u4e1a\u56fe\u793a\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e13\u95e8\u7684\u8bc4\u4f30\u57fa\u51c6\u6765\u91cf\u5316\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u6784\u5efaProImage-Bench\u57fa\u51c6\uff0c\u5305\u542b654\u4e2a\u771f\u5b9e\u6559\u79d1\u4e66\u548c\u6280\u672f\u62a5\u544a\u4e2d\u7684\u56fe\u793a\uff0c\u521b\u5efa\u8be6\u7ec6\u56fe\u50cf\u6307\u4ee4\u548c\u5c42\u6b21\u5316\u8bc4\u4f30\u89c4\u5219\uff086,076\u4e2a\u6807\u51c6\uff0c44,131\u4e2a\u4e8c\u5143\u68c0\u67e5\uff09\u3002\u4f7f\u7528\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u4ece\u6587\u672c\u548c\u53c2\u8003\u56fe\u50cf\u4e2d\u63a8\u5bfc\u89c4\u5219\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u5316LMM\u8bc4\u4f30\u5668\u8fdb\u884c\u8bc4\u5206\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0c\u5373\u4f7f\u5728\u5f00\u653e\u9886\u57df\u8868\u73b0\u4f18\u79c0\u7684\u6a21\u578b\uff0c\u5728ProImage-Bench\u4e0a\u4e5f\u53ea\u80fd\u8fbe\u52300.791\u7684\u89c4\u5219\u51c6\u786e\u7387\u548c0.553\u7684\u6807\u51c6\u5206\u6570\uff0c\u8868\u660e\u5728\u79d1\u5b66\u4fdd\u771f\u5ea6\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u8ddd\u3002\u901a\u8fc7\u5c06\u5931\u8d25\u7684\u68c0\u67e5\u53cd\u9988\u7ed9\u7f16\u8f91\u6a21\u578b\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\uff0c\u53ef\u4ee5\u5c06\u89c4\u5219\u51c6\u786e\u7387\u4ece0.653\u63d0\u5347\u52300.865\uff0c\u6807\u51c6\u5206\u6570\u4ece0.388\u63d0\u5347\u52300.697\u3002", "conclusion": "ProImage-Bench\u4e3a\u4e13\u4e1a\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u5e76\u4e3a\u6539\u8fdb\u89c4\u8303\u5fe0\u5b9e\u7684\u79d1\u5b66\u56fe\u793a\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u76d1\u7763\u4fe1\u53f7\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u6a21\u578b\u5728\u4e13\u4e1a\u9886\u57df\u7684\u5e94\u7528\u3002"}}
{"id": "2512.13154", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13154", "abs": "https://arxiv.org/abs/2512.13154", "authors": ["Emre Can Acikgoz", "Jinoh Oh", "Joo Hyuk Jeon", "Jie Hao", "Heng Ji", "Dilek Hakkani-T\u00fcr", "Gokhan Tur", "Xiang Li", "Chengyuan Ma", "Xing Fan"], "title": "MAC: A Multi-Agent Framework for Interactive User Clarification in Multi-turn Conversations", "comment": null, "summary": "Conversational agents often encounter ambiguous user requests, requiring an effective clarification to successfully complete tasks. While recent advancements in real-world applications favor multi-agent architectures to manage complex conversational scenarios efficiently, ambiguity resolution remains a critical and underexplored challenge--particularly due to the difficulty of determining which agent should initiate a clarification and how agents should coordinate their actions when faced with uncertain or incomplete user input. The fundamental questions of when to interrupt a user and how to formulate the optimal clarification query within the most optimal multi-agent settings remain open. In this paper, we propose MAC (Multi-Agent Clarification), an interactive multi-agent framework specifically optimized to resolve user ambiguities by strategically managing clarification dialogues. We first introduce a novel taxonomy categorizing user ambiguities to systematically guide clarification strategies. Then, we present MAC that autonomously coordinates multiple agents to interact synergistically with users. Empirical evaluations on MultiWOZ 2.4 demonstrate that enabling clarification at both levels increases task success rate 7.8\\% (54.5 to 62.3) and reduces the average number of dialogue turns (6.53 to 4.86) by eliciting all required user information up front and minimizing repetition. Our findings highlight the importance of active user interaction and role-aware clarification for more reliable human-agent communication.", "AI": {"tldr": "\u63d0\u51faMAC\u591a\u667a\u80fd\u4f53\u6f84\u6e05\u6846\u67b6\uff0c\u901a\u8fc7\u667a\u80fd\u534f\u8c03\u591a\u4e2a\u667a\u80fd\u4f53\u4e0e\u7528\u6237\u4ea4\u4e92\uff0c\u89e3\u51b3\u5bf9\u8bdd\u4e2d\u7684\u6b67\u4e49\u95ee\u9898\uff0c\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u5e76\u51cf\u5c11\u5bf9\u8bdd\u8f6e\u6b21\u3002", "motivation": "\u5bf9\u8bdd\u7cfb\u7edf\u5e38\u9047\u5230\u6a21\u7cca\u7684\u7528\u6237\u8bf7\u6c42\uff0c\u9700\u8981\u6709\u6548\u7684\u6f84\u6e05\u673a\u5236\u3002\u867d\u7136\u591a\u667a\u80fd\u4f53\u67b6\u6784\u5728\u590d\u6742\u5bf9\u8bdd\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u6b67\u4e49\u89e3\u51b3\u4ecd\u662f\u4e00\u4e2a\u5173\u952e\u4e14\u672a\u88ab\u5145\u5206\u63a2\u7d22\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u786e\u5b9a\u54ea\u4e2a\u667a\u80fd\u4f53\u5e94\u53d1\u8d77\u6f84\u6e05\u4ee5\u53ca\u667a\u80fd\u4f53\u5982\u4f55\u534f\u8c03\u884c\u52a8\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "method": "\u63d0\u51faMAC\uff08\u591a\u667a\u80fd\u4f53\u6f84\u6e05\uff09\u6846\u67b6\uff0c\u9996\u5148\u5f15\u5165\u65b0\u7684\u7528\u6237\u6b67\u4e49\u5206\u7c7b\u6cd5\u6765\u7cfb\u7edf\u6307\u5bfc\u6f84\u6e05\u7b56\u7565\uff0c\u7136\u540e\u8bbe\u8ba1\u80fd\u591f\u81ea\u4e3b\u534f\u8c03\u591a\u4e2a\u667a\u80fd\u4f53\u4e0e\u7528\u6237\u534f\u540c\u4ea4\u4e92\u7684\u7cfb\u7edf\u3002", "result": "\u5728MultiWOZ 2.4\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793a\uff0c\u5728\u4e24\u4e2a\u5c42\u9762\u542f\u7528\u6f84\u6e05\u529f\u80fd\u53ef\u5c06\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad87.8%\uff08\u4ece54.5%\u523062.3%\uff09\uff0c\u5e76\u5c06\u5e73\u5747\u5bf9\u8bdd\u8f6e\u6b21\u4ece6.53\u51cf\u5c11\u52304.86\uff0c\u901a\u8fc7\u63d0\u524d\u83b7\u53d6\u6240\u6709\u5fc5\u8981\u7528\u6237\u4fe1\u606f\u5e76\u6700\u5c0f\u5316\u91cd\u590d\u6765\u5b9e\u73b0\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u4e3b\u52a8\u7528\u6237\u4ea4\u4e92\u548c\u89d2\u8272\u611f\u77e5\u6f84\u6e05\u5bf9\u4e8e\u66f4\u53ef\u9760\u7684\u4eba\u673a\u901a\u4fe1\u7684\u91cd\u8981\u6027\uff0cMAC\u6846\u67b6\u901a\u8fc7\u667a\u80fd\u534f\u8c03\u591a\u667a\u80fd\u4f53\u6f84\u6e05\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u5bf9\u8bdd\u6b67\u4e49\u95ee\u9898\u3002"}}
{"id": "2512.12198", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2512.12198", "abs": "https://arxiv.org/abs/2512.12198", "authors": ["Jirui Jin", "Cheng Zeng", "Pawan Prakash", "Ellad B. Tadmor", "Adrian Roitberg", "Richard G. Hennig", "Stefano Martiniani", "Mingjie Liu"], "title": "MolGuidance: Advanced Guidance Strategies for Conditional Molecular Generation with Flow Matching", "comment": "19 pages, 5 figures, code: https://github.com/Liu-Group-UF/MolGuidance", "summary": "Key objectives in conditional molecular generation include ensuring chemical validity, aligning generated molecules with target properties, promoting structural diversity, and enabling efficient sampling for discovery. Recent advances in computer vision introduced a range of new guidance strategies for generative models, many of which can be adapted to support these goals. In this work, we integrate state-of-the-art guidance methods -- including classifier-free guidance, autoguidance, and model guidance -- in a leading molecule generation framework built on an SE(3)-equivariant flow matching process. We propose a hybrid guidance strategy that separately guides continuous and discrete molecular modalities -- operating on velocity fields and predicted logits, respectively -- while jointly optimizing their guidance scales via Bayesian optimization. Our implementation, benchmarked on the QM9 and QMe14S datasets, achieves new state-of-the-art performance in property alignment for de novo molecular generation. The generated molecules also exhibit high structural validity. Furthermore, we systematically compare the strengths and limitations of various guidance methods, offering insights into their broader applicability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u5f15\u5bfc\u7b56\u7565\uff0c\u5c06\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u5148\u8fdb\u5f15\u5bfc\u65b9\u6cd5\uff08\u5982\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u3001\u81ea\u52a8\u5f15\u5bfc\u548c\u6a21\u578b\u5f15\u5bfc\uff09\u6574\u5408\u5230SE(3)-\u7b49\u53d8\u6d41\u5339\u914d\u5206\u5b50\u751f\u6210\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u4f18\u5316\u8054\u5408\u4f18\u5316\u8fde\u7eed\u548c\u79bb\u6563\u6a21\u6001\u7684\u5f15\u5bfc\u5c3a\u5ea6\uff0c\u5728QM9\u548cQMe14S\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u6761\u4ef6\u5206\u5b50\u751f\u6210\u7684\u5173\u952e\u76ee\u6807\u5305\u62ec\u786e\u4fdd\u5316\u5b66\u6709\u6548\u6027\u3001\u4f7f\u751f\u6210\u5206\u5b50\u4e0e\u76ee\u6807\u5c5e\u6027\u5bf9\u9f50\u3001\u4fc3\u8fdb\u7ed3\u6784\u591a\u6837\u6027\u4ee5\u53ca\u5b9e\u73b0\u9ad8\u6548\u91c7\u6837\u3002\u867d\u7136\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u5df2\u5f00\u53d1\u51fa\u591a\u79cd\u65b0\u7684\u5f15\u5bfc\u7b56\u7565\uff0c\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5728\u5206\u5b50\u751f\u6210\u9886\u57df\u7684\u5e94\u7528\u548c\u6574\u5408\u4ecd\u6709\u5f85\u63a2\u7d22\u3002", "method": "1. \u5c06\u6700\u5148\u8fdb\u7684\u5f15\u5bfc\u65b9\u6cd5\uff08\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\u3001\u81ea\u52a8\u5f15\u5bfc\u3001\u6a21\u578b\u5f15\u5bfc\uff09\u6574\u5408\u5230\u57fa\u4e8eSE(3)-\u7b49\u53d8\u6d41\u5339\u914d\u7684\u5206\u5b50\u751f\u6210\u6846\u67b6\u4e2d\n2. \u63d0\u51fa\u6df7\u5408\u5f15\u5bfc\u7b56\u7565\uff0c\u5206\u522b\u5f15\u5bfc\u8fde\u7eed\u5206\u5b50\u6a21\u6001\uff08\u901f\u5ea6\u573a\uff09\u548c\u79bb\u6563\u5206\u5b50\u6a21\u6001\uff08\u9884\u6d4blogits\uff09\n3. \u4f7f\u7528\u8d1d\u53f6\u65af\u4f18\u5316\u8054\u5408\u4f18\u5316\u8fde\u7eed\u548c\u79bb\u6563\u6a21\u6001\u7684\u5f15\u5bfc\u5c3a\u5ea6\n4. \u5728QM9\u548cQMe14S\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5", "result": "1. \u5728\u4ece\u5934\u5206\u5b50\u751f\u6210\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5c5e\u6027\u5bf9\u9f50\u6027\u80fd\n2. \u751f\u6210\u7684\u5206\u5b50\u5177\u6709\u9ad8\u7ed3\u6784\u6709\u6548\u6027\n3. \u7cfb\u7edf\u6bd4\u8f83\u4e86\u5404\u79cd\u5f15\u5bfc\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u4e3a\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u89c1\u89e3", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5730\u5c06\u8ba1\u7b97\u673a\u89c6\u89c9\u4e2d\u7684\u5148\u8fdb\u5f15\u5bfc\u7b56\u7565\u6574\u5408\u5230\u5206\u5b50\u751f\u6210\u6846\u67b6\u4e2d\uff0c\u63d0\u51fa\u7684\u6df7\u5408\u5f15\u5bfc\u65b9\u6cd5\u5728\u5c5e\u6027\u5bf9\u9f50\u548c\u7ed3\u6784\u6709\u6548\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u4e3a\u6761\u4ef6\u5206\u5b50\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e0d\u540c\u5f15\u5bfc\u65b9\u6cd5\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2512.12222", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12222", "abs": "https://arxiv.org/abs/2512.12222", "authors": ["Nathalie Alexander", "Arnaud Gucciardi", "Umberto Michelucci"], "title": "Comparison of different segmentation algorithms on brain volume and fractal dimension in infant brain MRIs", "comment": null, "summary": "Accurate segmentation of infant brain MRI is essential for quantifying developmental changes in structure and complexity. However, ongoing myelination and reduced tissue contrast make automated segmentation particularly challenging. This study systematically compared segmentation accuracy and its impact on volumetric and fractal dimension (FD) estimates in infant brain MRI using the Baby Open Brains (BOB) dataset (71 scans, 1-9 months). Two methods, SynthSeg and SamSeg, were evaluated against expert annotations using Dice, Intersection over Union, 95th-percentile Hausdorff distance, and Normalised Mutual Information. SynthSeg outperformed SamSeg across all quality metrics (mean Dice > 0.8 for major regions) and provided volumetric estimates closely matching the manual reference (mean +4% [-28% - 71%]). SamSeg systematically overestimated ventricular and whole-brain volumes (mean +76% [-12% - 190%]). Segmentation accuracy improved with age, consistent with increasing tissue contrast during myelination. Fractal dimension a(FD) nalyses revealed significant regional differences between SynthSeg and expert segmentations, and Bland-Altman limits of agreement indicated that segmentation-related FD variability exceeded most group differences reported in developmental cohorts. Volume and FD deviations were positively correlated across structures, indicating that segmentation bias directly affects FD estimation. Overall, SynthSeg provided the most reliable volumetric and FD results for paediatric MRI, yet small morphological differences in volume and FD should be interpreted with caution due to segmentation-related uncertainty.", "AI": {"tldr": "\u8be5\u7814\u7a76\u7cfb\u7edf\u6bd4\u8f83\u4e86SynthSeg\u548cSamSeg\u4e24\u79cd\u65b9\u6cd5\u5728\u5a74\u513f\u8111MRI\u5206\u5272\u4e2d\u7684\u51c6\u786e\u6027\uff0c\u4ee5\u53ca\u5bf9\u4f53\u79ef\u548c\u5206\u5f62\u7ef4\u5ea6\u4f30\u8ba1\u7684\u5f71\u54cd\u3002SynthSeg\u5728\u6240\u6709\u8d28\u91cf\u6307\u6807\u4e0a\u4f18\u4e8eSamSeg\uff0c\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u4f53\u79ef\u548c\u5206\u5f62\u7ef4\u5ea6\u7ed3\u679c\u3002", "motivation": "\u5a74\u513f\u8111MRI\u7684\u51c6\u786e\u5206\u5272\u5bf9\u4e8e\u91cf\u5316\u53d1\u80b2\u8fc7\u7a0b\u4e2d\u7684\u7ed3\u6784\u548c\u590d\u6742\u6027\u53d8\u5316\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u9ad3\u9798\u5316\u8fc7\u7a0b\u4e2d\u7ec4\u7ec7\u5bf9\u6bd4\u5ea6\u964d\u4f4e\uff0c\u81ea\u52a8\u5206\u5272\u7279\u522b\u5177\u6709\u6311\u6218\u6027\u3002\u9700\u8981\u8bc4\u4f30\u4e0d\u540c\u5206\u5272\u65b9\u6cd5\u5bf9\u4f53\u79ef\u548c\u5206\u5f62\u7ef4\u5ea6\u4f30\u8ba1\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528Baby Open Brains\u6570\u636e\u96c6\uff0871\u6b21\u626b\u63cf\uff0c1-9\u4e2a\u6708\u5a74\u513f\uff09\uff0c\u6bd4\u8f83SynthSeg\u548cSamSeg\u4e24\u79cd\u5206\u5272\u65b9\u6cd5\u4e0e\u4e13\u5bb6\u6807\u6ce8\u7684\u51c6\u786e\u6027\u3002\u8bc4\u4f30\u6307\u6807\u5305\u62ecDice\u7cfb\u6570\u3001\u4ea4\u5e76\u6bd4\u300195%\u767e\u5206\u4f4dHausdorff\u8ddd\u79bb\u548c\u5f52\u4e00\u5316\u4e92\u4fe1\u606f\u3002\u5206\u6790\u5206\u5272\u51c6\u786e\u6027\u5bf9\u4f53\u79ef\u548c\u5206\u5f62\u7ef4\u5ea6\u4f30\u8ba1\u7684\u5f71\u54cd\u3002", "result": "SynthSeg\u5728\u6240\u6709\u8d28\u91cf\u6307\u6807\u4e0a\u4f18\u4e8eSamSeg\uff08\u4e3b\u8981\u533a\u57df\u5e73\u5747Dice > 0.8\uff09\uff0c\u4f53\u79ef\u4f30\u8ba1\u4e0e\u624b\u52a8\u53c2\u8003\u63a5\u8fd1\uff08\u5e73\u5747+4%\uff09\u3002SamSeg\u7cfb\u7edf\u6027\u5730\u9ad8\u4f30\u8111\u5ba4\u548c\u5168\u8111\u4f53\u79ef\uff08\u5e73\u5747+76%\uff09\u3002\u5206\u5272\u51c6\u786e\u6027\u968f\u5e74\u9f84\u589e\u957f\u800c\u63d0\u9ad8\u3002\u5206\u5f62\u7ef4\u5ea6\u5206\u6790\u663e\u793aSynthSeg\u4e0e\u4e13\u5bb6\u5206\u5272\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u533a\u57df\u5dee\u5f02\uff0c\u5206\u5272\u76f8\u5173\u7684FD\u53d8\u5f02\u6027\u8d85\u8fc7\u4e86\u5927\u591a\u6570\u53d1\u80b2\u961f\u5217\u62a5\u544a\u7684\u7ec4\u95f4\u5dee\u5f02\u3002", "conclusion": "SynthSeg\u4e3a\u513f\u79d1MRI\u63d0\u4f9b\u4e86\u6700\u53ef\u9760\u7684\u4f53\u79ef\u548c\u5206\u5f62\u7ef4\u5ea6\u7ed3\u679c\uff0c\u4f46\u7531\u4e8e\u5206\u5272\u76f8\u5173\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5bf9\u4f53\u79ef\u548c\u5206\u5f62\u7ef4\u5ea6\u7684\u5fae\u5c0f\u5f62\u6001\u5dee\u5f02\u5e94\u8c28\u614e\u89e3\u91ca\u3002\u5206\u5272\u504f\u5dee\u76f4\u63a5\u5f71\u54cd\u5206\u5f62\u7ef4\u5ea6\u4f30\u8ba1\u3002"}}
{"id": "2512.12210", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12210", "abs": "https://arxiv.org/abs/2512.12210", "authors": ["Yuting Tang", "Weibang Jiang", "Shanglin Li", "Yong Li", "Chenyu Liu", "Xinliang Zhou", "Yi Ding", "Cuntai Guan"], "title": "EEG-DLite: Dataset Distillation for Efficient Large EEG Model Training", "comment": "Accepted by AAAI-2026", "summary": "Large-scale EEG foundation models have shown strong generalization across a range of downstream tasks, but their training remains resource-intensive due to the volume and variable quality of EEG data. In this work, we introduce EEG-DLite, a data distillation framework that enables more efficient pre-training by selectively removing noisy and redundant samples from large EEG datasets. EEG-DLite begins by encoding EEG segments into compact latent representations using a self-supervised autoencoder, allowing sample selection to be performed efficiently and with reduced sensitivity to noise. Based on these representations, EEG-DLite filters out outliers and minimizes redundancy, resulting in a smaller yet informative subset that retains the diversity essential for effective foundation model training. Through extensive experiments, we demonstrate that training on only 5 percent of a 2,500-hour dataset curated with EEG-DLite yields performance comparable to, and in some cases better than, training on the full dataset across multiple downstream tasks. To our knowledge, this is the first systematic study of pre-training data distillation in the context of EEG foundation models. EEG-DLite provides a scalable and practical path toward more effective and efficient physiological foundation modeling. The code is available at https://github.com/t170815518/EEG-DLite.", "AI": {"tldr": "EEG-DLite\u901a\u8fc7\u6570\u636e\u84b8\u998f\u6846\u67b6\uff0c\u4ece\u5927\u89c4\u6a21EEG\u6570\u636e\u96c6\u4e2d\u9009\u62e9\u6027\u53bb\u9664\u566a\u58f0\u548c\u5197\u4f59\u6837\u672c\uff0c\u4ec5\u75285%\u7684\u6570\u636e\u5c31\u80fd\u8fbe\u5230\u4e0e\u5b8c\u6574\u6570\u636e\u96c6\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u7684\u6027\u80fd\uff0c\u663e\u8457\u63d0\u9ad8EEG\u57fa\u7840\u6a21\u578b\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u5927\u89c4\u6a21EEG\u57fa\u7840\u6a21\u578b\u867d\u7136\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u7531\u4e8eEEG\u6570\u636e\u91cf\u5927\u4e14\u8d28\u91cf\u53c2\u5dee\u4e0d\u9f50\uff0c\u8bad\u7ec3\u8fc7\u7a0b\u8d44\u6e90\u6d88\u8017\u5de8\u5927\u3002\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "EEG-DLite\u91c7\u7528\u6570\u636e\u84b8\u998f\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u81ea\u76d1\u7763\u81ea\u52a8\u7f16\u7801\u5668\u5c06EEG\u7247\u6bb5\u7f16\u7801\u4e3a\u7d27\u51d1\u7684\u6f5c\u5728\u8868\u793a\uff1b2\uff09\u57fa\u4e8e\u8fd9\u4e9b\u8868\u793a\u8fc7\u6ee4\u5f02\u5e38\u503c\u5e76\u6700\u5c0f\u5316\u5197\u4f59\uff1b3\uff09\u751f\u6210\u4e00\u4e2a\u66f4\u5c0f\u4f46\u4fe1\u606f\u4e30\u5bcc\u7684\u5b50\u96c6\uff0c\u4fdd\u7559\u57fa\u7840\u6a21\u578b\u8bad\u7ec3\u6240\u9700\u7684\u591a\u6837\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u4ec5\u4f7f\u7528EEG-DLite\u4ece2500\u5c0f\u65f6\u6570\u636e\u96c6\u4e2d\u7cbe\u9009\u76845%\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u80fd\u8fbe\u5230\u4e0e\u5b8c\u6574\u6570\u636e\u96c6\u8bad\u7ec3\u76f8\u5f53\u751a\u81f3\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "EEG-DLite\u662f\u9996\u4e2a\u9488\u5bf9EEG\u57fa\u7840\u6a21\u578b\u7684\u9884\u8bad\u7ec3\u6570\u636e\u84b8\u998f\u7cfb\u7edf\u7814\u7a76\uff0c\u4e3a\u66f4\u6709\u6548\u548c\u9ad8\u6548\u7684\u751f\u7406\u57fa\u7840\u5efa\u6a21\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u5b9e\u7528\u7684\u8def\u5f84\u3002"}}
{"id": "2512.12252", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12252", "abs": "https://arxiv.org/abs/2512.12252", "authors": ["Kyosuke Nishishita", "Atsuki Sato", "Yusuke Matsui"], "title": "Optimized Learned Count-Min Sketch", "comment": "4 pages, 3 figures. Accepted at NeurIPS 2025 Workshop on Machine Learning for Systems", "summary": "Count-Min Sketch (CMS) is a memory-efficient data structure for estimating the frequency of elements in a multiset. Learned Count-Min Sketch (LCMS) enhances CMS with a machine learning model to reduce estimation error under the same memory usage, but suffers from slow construction due to empirical parameter tuning and lacks theoretical guarantees on intolerable error probability. We propose Optimized Learned Count-Min Sketch (OptLCMS), which partitions the input domain and assigns each partition to its own CMS instance, with CMS parameters analytically derived for fixed thresholds, and thresholds optimized via dynamic programming with approximate feasibility checks. This reduces the need for empirical validation, enabling faster construction while providing theoretical guarantees under these assumptions. OptLCMS also allows explicit control of the allowable error threshold, improving flexibility in practice. Experiments show that OptLCMS builds faster, achieves lower intolerable error probability, and matches the estimation accuracy of LCMS.", "AI": {"tldr": "OptLCMS\u662f\u4e00\u79cd\u4f18\u5316\u7684\u5b66\u4e60\u578bCount-Min Sketch\uff0c\u901a\u8fc7\u5206\u533a\u548c\u52a8\u6001\u89c4\u5212\u4f18\u5316\u53c2\u6570\uff0c\u5728\u4fdd\u6301\u76f8\u540c\u5185\u5b58\u4f7f\u7528\u4e0b\u6bd4LCMS\u6784\u5efa\u66f4\u5feb\u3001\u7406\u8bba\u4fdd\u8bc1\u66f4\u597d\u3001\u9519\u8bef\u6982\u7387\u66f4\u4f4e\u3002", "motivation": "\u4f20\u7edf\u5b66\u4e60\u578bCount-Min Sketch\uff08LCMS\uff09\u867d\u7136\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6a21\u578b\u51cf\u5c11\u4e86\u4f30\u8ba1\u8bef\u5dee\uff0c\u4f46\u5b58\u5728\u6784\u5efa\u901f\u5ea6\u6162\uff08\u9700\u8981\u7ecf\u9a8c\u53c2\u6570\u8c03\u4f18\uff09\u548c\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1\uff08\u7279\u522b\u662f\u4e0d\u53ef\u5bb9\u5fcd\u9519\u8bef\u6982\u7387\uff09\u7684\u95ee\u9898\u3002", "method": "1. \u5c06\u8f93\u5165\u57df\u5206\u533a\uff0c\u6bcf\u4e2a\u5206\u533a\u5206\u914d\u72ec\u7acb\u7684CMS\u5b9e\u4f8b\uff1b2. \u9488\u5bf9\u56fa\u5b9a\u9608\u503c\u5206\u6790\u63a8\u5bfcCMS\u53c2\u6570\uff1b3. \u901a\u8fc7\u5e26\u8fd1\u4f3c\u53ef\u884c\u6027\u68c0\u67e5\u7684\u52a8\u6001\u89c4\u5212\u4f18\u5316\u9608\u503c\uff1b4. \u63d0\u4f9b\u5bf9\u5141\u8bb8\u8bef\u5dee\u9608\u503c\u7684\u663e\u5f0f\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660eOptLCMS\u6784\u5efa\u901f\u5ea6\u66f4\u5feb\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u4e0d\u53ef\u5bb9\u5fcd\u9519\u8bef\u6982\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0eLCMS\u76f8\u5f53\u7684\u4f30\u8ba1\u7cbe\u5ea6\u3002", "conclusion": "OptLCMS\u89e3\u51b3\u4e86LCMS\u7684\u4e3b\u8981\u9650\u5236\uff0c\u63d0\u4f9b\u4e86\u66f4\u5feb\u7684\u6784\u5efa\u901f\u5ea6\u3001\u7406\u8bba\u4fdd\u8bc1\u548c\u66f4\u597d\u7684\u9519\u8bef\u63a7\u5236\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5185\u5b58\u6548\u7387\u548c\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u66f4\u5177\u7075\u6d3b\u6027\u3002"}}
{"id": "2512.12246", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12246", "abs": "https://arxiv.org/abs/2512.12246", "authors": ["I Putu Andika Bagas Jiwanta", "Ayu Purwarianti"], "title": "Moment and Highlight Detection via MLLM Frame Segmentation", "comment": null, "summary": "Detecting video moments and highlights from natural-language queries have been unified by transformer-based methods. Other works use generative Multimodal LLM (MLLM) to predict moments and/or highlights as text timestamps, utilizing its reasoning capability. While effective, text-based generation cannot provide direct gradients for frame-level predictions because the model only emits language tokens. Although recent Reinforcement Learning (RL) methods attempt to address the issue, we propose a novel approach by applying segmentation objectives directly on the LLM's output tokens. The LLM is fed with a fixed number of frames alongside a prompt that enforces it to output a sequence of continuous \"0\" and/or \"1\" characters, with one character per frame. The \"0\"/\"1\" characters benefit from the LLM's inherent language capability while also acting as background and foreground probabilities, respectively. Training employs segmentation losses on the probabilities alongside a normal causal LM loss. At inference, beam search generates sequence and logits, acting as moments and saliency scores, respectively. Despite sampling only 25 frames -- less than half of comparable methods -- our method achieved strong highlight detection (56.74 HIT@1) on QVHighlights. Additionally, our efficient method scores above the baseline (35.28 MAP) for moment retrieval. Empirically, segmentation losses provide a stable complementary learning signal even when the causal LM loss plateaus.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728LLM\u8f93\u51fatoken\u4e0a\u76f4\u63a5\u5e94\u7528\u5206\u5272\u76ee\u6807\u6765\u68c0\u6d4b\u89c6\u9891\u65f6\u523b\u548c\u9ad8\u5149\u7247\u6bb5\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u6587\u672c\u751f\u6210\u65b9\u6cd5\u65e0\u6cd5\u63d0\u4f9b\u5e27\u7ea7\u9884\u6d4b\u68af\u5ea6\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u751f\u6210\u5f0f\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\u867d\u7136\u6709\u6548\uff0c\u4f46\u6587\u672c\u751f\u6210\u65e0\u6cd5\u4e3a\u5e27\u7ea7\u9884\u6d4b\u63d0\u4f9b\u76f4\u63a5\u68af\u5ea6\u3002\u867d\u7136\u6700\u8fd1\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5c1d\u8bd5\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u4f46\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u76f4\u63a5\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u56fa\u5b9a\u6570\u91cf\u7684\u5e27\u8f93\u5165LLM\uff0c\u5e76\u8bbe\u8ba1\u63d0\u793a\u4f7f\u5176\u8f93\u51fa\u8fde\u7eed\u7684\"0\"\u548c/\u6216\"1\"\u5b57\u7b26\u5e8f\u5217\uff0c\u6bcf\u4e2a\u5b57\u7b26\u5bf9\u5e94\u4e00\u5e27\u3002\"0\"/\"1\"\u5b57\u7b26\u65e2\u5229\u7528LLM\u7684\u8bed\u8a00\u80fd\u529b\uff0c\u53c8\u5206\u522b\u4f5c\u4e3a\u80cc\u666f\u548c\u524d\u666f\u6982\u7387\u3002\u8bad\u7ec3\u65f6\u7ed3\u5408\u5206\u5272\u635f\u5931\u548c\u56e0\u679c\u8bed\u8a00\u6a21\u578b\u635f\u5931\u3002", "result": "\u5728QVHighlights\u6570\u636e\u96c6\u4e0a\uff0c\u4ec5\u91c7\u683725\u5e27\uff08\u5c11\u4e8e\u540c\u7c7b\u65b9\u6cd5\u4e00\u534a\uff09\u5c31\u5b9e\u73b0\u4e8656.74 HIT@1\u7684\u9ad8\u5149\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u5728\u65f6\u523b\u68c0\u7d22\u4efb\u52a1\u4e0a\u8fbe\u523035.28 MAP\uff0c\u8d85\u8fc7\u4e86\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5728LLM\u8f93\u51fatoken\u4e0a\u76f4\u63a5\u5e94\u7528\u5206\u5272\u76ee\u6807\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6587\u672c\u751f\u6210\u65b9\u6cd5\u65e0\u6cd5\u63d0\u4f9b\u5e27\u7ea7\u9884\u6d4b\u68af\u5ea6\u7684\u95ee\u9898\uff0c\u5206\u5272\u635f\u5931\u5373\u4f7f\u5728\u56e0\u679cLM\u635f\u5931\u5e73\u53f0\u671f\u4e5f\u80fd\u63d0\u4f9b\u7a33\u5b9a\u7684\u8865\u5145\u5b66\u4e60\u4fe1\u53f7\u3002"}}
{"id": "2512.13481", "categories": ["cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.13481", "abs": "https://arxiv.org/abs/2512.13481", "authors": ["Ojas Pungalia", "Rashi Upadhyay", "Abhishek Mishra", "Abhiram H", "Tejasvi Alladi", "Sujan Yenuganti", "Dhruv Kumar"], "title": "neuralFOMO: Can LLMs Handle Being Second Best? Measuring Envy-Like Preferences in Multi-Agent Settings", "comment": "Under Review", "summary": "Envy is a common human behavior that shapes competitiveness and can alter outcomes in team settings. As large language models (LLMs) increasingly act on behalf of humans in collaborative and competitive workflows, there is a pressing need to evaluate whether and under what conditions they exhibit envy-like preferences. In this paper, we test whether LLMs show envy-like behavior toward each other. We considered two scenarios: (1) A point allocation game that tests whether a model tries to win over its peer. (2) A workplace setting observing behaviour when recognition is unfair. Our findings reveal consistent evidence of envy-like patterns in certain LLMs, with large variation across models and contexts. For instance, GPT-5-mini and Claude-3.7-Sonnet show a clear tendency to pull down the peer model to equalize outcomes, whereas Mistral-Small-3.2-24B instead focuses on maximizing its own individual gains. These results highlight the need to consider competitive dispositions as a safety and design factor in LLM-based multi-agent systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6d4b\u8bd5\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u662f\u5426\u8868\u73b0\u51fa\u7c7b\u4f3c\u5ac9\u5992\u7684\u884c\u4e3a\uff0c\u53d1\u73b0\u5728\u67d0\u4e9b\u6a21\u578b\u548c\u60c5\u5883\u4e2d\u5b58\u5728\u5ac9\u5992\u6a21\u5f0f\uff0c\u4e0d\u540c\u6a21\u578b\u8868\u73b0\u5dee\u5f02\u663e\u8457\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u5728\u534f\u4f5c\u548c\u7ade\u4e89\u5de5\u4f5c\u6d41\u4e2d\u4ee3\u8868\u4eba\u7c7b\u884c\u52a8\uff0c\u9700\u8981\u8bc4\u4f30\u5b83\u4eec\u662f\u5426\u4ee5\u53ca\u5728\u4f55\u79cd\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u7c7b\u4f3c\u5ac9\u5992\u7684\u504f\u597d\uff0c\u8fd9\u5bf9\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5b89\u5168\u548c\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u5b9e\u9a8c\u573a\u666f\uff1a(1) \u70b9\u6570\u5206\u914d\u6e38\u620f\uff0c\u6d4b\u8bd5\u6a21\u578b\u662f\u5426\u8bd5\u56fe\u80dc\u8fc7\u540c\u4f34\uff1b(2) \u5de5\u4f5c\u573a\u6240\u8bbe\u7f6e\uff0c\u89c2\u5bdf\u5728\u8ba4\u53ef\u4e0d\u516c\u5e73\u60c5\u51b5\u4e0b\u7684\u884c\u4e3a\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u67d0\u4e9bLLMs\u5b58\u5728\u4e00\u81f4\u7684\u5ac9\u5992\u6a21\u5f0f\uff0c\u4f46\u4e0d\u540c\u6a21\u578b\u548c\u60c5\u5883\u95f4\u5dee\u5f02\u5f88\u5927\u3002GPT-5-mini\u548cClaude-3.7-Sonnet\u503e\u5411\u4e8e\u62c9\u4f4e\u540c\u4f34\u4ee5\u5e73\u8861\u7ed3\u679c\uff0c\u800cMistral-Small-3.2-24B\u5219\u4e13\u6ce8\u4e8e\u6700\u5927\u5316\u81ea\u8eab\u6536\u76ca\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\uff0c\u9700\u8981\u8003\u8651\u7ade\u4e89\u503e\u5411\u4f5c\u4e3a\u5b89\u5168\u548c\u8bbe\u8ba1\u56e0\u7d20\uff0c\u6a21\u578b\u53ef\u80fd\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u5ac9\u5992\u884c\u4e3a\u3002"}}
{"id": "2512.13240", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13240", "abs": "https://arxiv.org/abs/2512.13240", "authors": ["Zihui Zhao", "Zechang Li"], "title": "Reflective Preference Optimization (RPO): Enhancing On-Policy Alignment via Hint-Guided Reflection", "comment": null, "summary": "Direct Preference Optimization (DPO) has emerged as a lightweight and effective alternative to Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with AI Feedback (RLAIF) for aligning large language and vision-language models. However, the standard DPO formulation, in which both the chosen and rejected responses are generated by the same policy, suffers from a weak learning signal because the two responses often share similar errors and exhibit small Kullback-Leibler (KL) divergence. This leads to slow and unstable convergence. To address this limitation, we introduce Reflective Preference Optimization (RPO), a new framework that incorporates hint-guided reflection into the DPO paradigm. RPO uses external models to identify hallucination sources and generate concise reflective hints, enabling the construction of on-policy preference pairs with stronger contrastiveness and clearer preference signals. We theoretically show that conditioning on hints increases the expected preference margin through mutual information and improves sample efficiency while remaining within the policy distribution family. Empirically, RPO achieves superior alignment with fewer training samples and iterations, substantially reducing hallucination rates and delivering state-of-the-art performance across multimodal benchmarks.", "AI": {"tldr": "RPO\uff08Reflective Preference Optimization\uff09\u662f\u4e00\u79cd\u6539\u8fdb\u7684\u504f\u597d\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5916\u90e8\u6a21\u578b\u751f\u6210\u53cd\u601d\u63d0\u793a\u6765\u589e\u5f3aDPO\u7684\u5b66\u4e60\u4fe1\u53f7\uff0c\u51cf\u5c11\u5e7b\u89c9\u5e76\u63d0\u9ad8\u591a\u6a21\u6001\u5bf9\u9f50\u6548\u7387\u3002", "motivation": "\u6807\u51c6DPO\u65b9\u6cd5\u4e2d\uff0c\u9009\u62e9\u548c\u62d2\u7edd\u7684\u54cd\u5e94\u6765\u81ea\u540c\u4e00\u7b56\u7565\uff0c\u4e24\u8005\u5e38\u5305\u542b\u76f8\u4f3c\u9519\u8bef\u4e14KL\u6563\u5ea6\u5c0f\uff0c\u5bfc\u81f4\u5b66\u4e60\u4fe1\u53f7\u5f31\u3001\u6536\u655b\u6162\u4e14\u4e0d\u7a33\u5b9a\u3002", "method": "RPO\u5f15\u5165\u63d0\u793a\u5f15\u5bfc\u7684\u53cd\u601d\u673a\u5236\uff0c\u4f7f\u7528\u5916\u90e8\u6a21\u578b\u8bc6\u522b\u5e7b\u89c9\u6765\u6e90\u5e76\u751f\u6210\u7b80\u6d01\u53cd\u601d\u63d0\u793a\uff0c\u6784\u5efa\u5177\u6709\u66f4\u5f3a\u5bf9\u6bd4\u6027\u548c\u66f4\u6e05\u6670\u504f\u597d\u4fe1\u53f7\u7684\u7b56\u7565\u5185\u504f\u597d\u5bf9\u3002", "result": "RPO\u5728\u66f4\u5c11\u7684\u8bad\u7ec3\u6837\u672c\u548c\u8fed\u4ee3\u6b21\u6570\u4e0b\u5b9e\u73b0\u66f4\u597d\u7684\u5bf9\u9f50\u6548\u679c\uff0c\u663e\u8457\u964d\u4f4e\u5e7b\u89c9\u7387\uff0c\u5728\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "RPO\u901a\u8fc7\u6574\u5408\u53cd\u601d\u63d0\u793a\u589e\u5f3aDPO\u6846\u67b6\uff0c\u7406\u8bba\u4e0a\u63d0\u9ad8\u4e86\u504f\u597d\u8fb9\u9645\u7684\u671f\u671b\u503c\uff0c\u5b9e\u8bc1\u4e0a\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u3001\u7a33\u5b9a\u7684\u591a\u6a21\u6001\u6a21\u578b\u5bf9\u9f50\u3002"}}
{"id": "2512.12273", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12273", "abs": "https://arxiv.org/abs/2512.12273", "authors": ["Bihao You", "Jiping Cui"], "title": "GRC-Net: Gram Residual Co-attention Net for epilepsy prediction", "comment": null, "summary": "Prediction of epilepsy based on electroencephalogram (EEG) signals is a rapidly evolving field. Previous studies have traditionally applied 1D processing to the entire EEG signal. However, we have adopted the Gram Matrix method to transform the signals into a 3D representation, enabling modeling of signal relationships across dimensions while preserving the temporal dependencies of the one-dimensional signals. Additionally, we observed an imbalance between local and global signals within the EEG data. Therefore, we introduced multi-level feature extraction, utilizing coattention for capturing global signal characteristics and an inception structure for processing local signals, achieving multi-granular feature extraction. Our experiments on the BONN dataset demonstrate that for the most challenging five-class classification task, GRC-Net achieved an accuracy of 93.66%, outperforming existing methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faGRC-Net\u6a21\u578b\uff0c\u901a\u8fc7Gram\u77e9\u9635\u5c06EEG\u4fe1\u53f7\u8f6c\u6362\u4e3a3D\u8868\u793a\uff0c\u7ed3\u5408\u5171\u6ce8\u610f\u529b\u673a\u5236\u548cInception\u7ed3\u6784\u8fdb\u884c\u591a\u7c92\u5ea6\u7279\u5f81\u63d0\u53d6\uff0c\u5728\u766b\u75eb\u9884\u6d4b\u4efb\u52a1\u4e0a\u53d6\u5f9793.66%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edf\u766b\u75eb\u9884\u6d4b\u65b9\u6cd5\u901a\u5e38\u5bf9EEG\u4fe1\u53f7\u8fdb\u884c\u4e00\u7ef4\u5904\u7406\uff0c\u65e0\u6cd5\u5145\u5206\u5efa\u6a21\u4fe1\u53f7\u95f4\u7684\u591a\u7ef4\u5173\u7cfb\u3002\u540c\u65f6\uff0cEEG\u6570\u636e\u4e2d\u5b58\u5728\u5c40\u90e8\u4e0e\u5168\u5c40\u4fe1\u53f7\u4e0d\u5e73\u8861\u7684\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u3002", "method": "1. \u4f7f\u7528Gram\u77e9\u9635\u5c06\u4e00\u7ef4EEG\u4fe1\u53f7\u8f6c\u6362\u4e3a3D\u8868\u793a\uff0c\u4fdd\u7559\u65f6\u95f4\u4f9d\u8d56\u5173\u7cfb\uff1b2. \u63d0\u51fa\u591a\u7ea7\u7279\u5f81\u63d0\u53d6\uff1a\u5171\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u5168\u5c40\u7279\u5f81\uff0cInception\u7ed3\u6784\u5904\u7406\u5c40\u90e8\u4fe1\u53f7\uff1b3. \u6784\u5efaGRC-Net\u6a21\u578b\u5b9e\u73b0\u591a\u7c92\u5ea6\u7279\u5f81\u63d0\u53d6\u3002", "result": "\u5728BONN\u6570\u636e\u96c6\u4e0a\uff0cGRC-Net\u5728\u6700\u5177\u6311\u6218\u6027\u7684\u4e94\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fbe\u523093.66%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc73D\u4fe1\u53f7\u8868\u793a\u548c\u591a\u7ea7\u7279\u5f81\u63d0\u53d6\u7b56\u7565\uff0cGRC-Net\u80fd\u6709\u6548\u5904\u7406EEG\u4fe1\u53f7\u7684\u5c40\u90e8-\u5168\u5c40\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u5728\u766b\u75eb\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2512.12268", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12268", "abs": "https://arxiv.org/abs/2512.12268", "authors": ["Yuqing Lei", "Yingjun Du", "Yawen Huang", "Xiantong Zhen", "Ling Shao"], "title": "MetaTPT: Meta Test-time Prompt Tuning for Vision-Language Models", "comment": "NeurIPS 2025 Workshop", "summary": "Vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization but remain sensitive to domain shifts at test time. Test-time prompt tuning (TPT) mitigates this issue by adapting prompts with fixed augmentations, which may falter in more challenging settings. In this work, we propose Meta Test-Time Prompt Tuning (MetaTPT), a meta-learning framework that learns a self-supervised auxiliary task to guide test-time prompt tuning. The auxiliary task dynamically learns parameterized augmentations for each sample, enabling more expressive transformations that capture essential features in target domains. MetaTPT adopts a dual-loop optimization paradigm: an inner loop learns a self-supervised task that generates informative views, while the outer loop performs prompt tuning by enforcing consistency across these views. By coupling augmentation learning with prompt tuning, MetaTPT improves test-time adaptation under domain shifts. Extensive experiments demonstrate that MetaTPT achieves state-of-the-art performance on domain generalization and cross-dataset benchmarks.", "AI": {"tldr": "MetaTPT\uff1a\u4e00\u79cd\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u8f85\u52a9\u4efb\u52a1\u5b66\u4e60\u53c2\u6570\u5316\u589e\u5f3a\u6765\u6307\u5bfc\u6d4b\u8bd5\u65f6\u63d0\u793a\u8c03\u4f18\uff0c\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u57df\u504f\u79fb\u4e0b\u7684\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982CLIP\uff09\u5177\u6709\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5728\u6d4b\u8bd5\u65f6\u5bf9\u57df\u504f\u79fb\u4ecd\u7136\u654f\u611f\u3002\u73b0\u6709\u7684\u6d4b\u8bd5\u65f6\u63d0\u793a\u8c03\u4f18\u65b9\u6cd5\u4f7f\u7528\u56fa\u5b9a\u589e\u5f3a\uff0c\u5728\u66f4\u5177\u6311\u6218\u6027\u7684\u573a\u666f\u4e2d\u53ef\u80fd\u5931\u6548\u3002", "method": "\u63d0\u51faMetaTPT\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u5faa\u73af\u4f18\u5316\u8303\u5f0f\uff1a\u5185\u5faa\u73af\u5b66\u4e60\u81ea\u76d1\u7763\u4efb\u52a1\u751f\u6210\u4fe1\u606f\u4e30\u5bcc\u7684\u89c6\u56fe\uff0c\u5916\u5faa\u73af\u901a\u8fc7\u5728\u8fd9\u4e9b\u89c6\u56fe\u95f4\u5f3a\u5236\u4e00\u81f4\u6027\u6765\u6267\u884c\u63d0\u793a\u8c03\u4f18\u3002\u6846\u67b6\u52a8\u6001\u5b66\u4e60\u6bcf\u4e2a\u6837\u672c\u7684\u53c2\u6570\u5316\u589e\u5f3a\uff0c\u5b9e\u73b0\u66f4\u5177\u8868\u8fbe\u529b\u7684\u53d8\u6362\u3002", "result": "\u5728\u57df\u6cdb\u5316\u548c\u8de8\u6570\u636e\u96c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMetaTPT\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u901a\u8fc7\u5c06\u589e\u5f3a\u5b66\u4e60\u4e0e\u63d0\u793a\u8c03\u4f18\u76f8\u7ed3\u5408\uff0cMetaTPT\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6d4b\u8bd5\u65f6\u9762\u5bf9\u57df\u504f\u79fb\u7684\u9002\u5e94\u80fd\u529b\u3002"}}
{"id": "2512.12277", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12277", "abs": "https://arxiv.org/abs/2512.12277", "authors": ["Thibault Geoffroy", "Myriam Maumy", "Lionel Prevost"], "title": "Feature Aggregation for Efficient Continual Learning of Complex Facial Expressions", "comment": "28 pages, 8 figures, chapter for \"Emotion and Facial Recognition in Artificial Intelligence: Sustainable Multidisciplinary Perspectives and Applications\" (2026)", "summary": "As artificial intelligence (AI) systems become increasingly embedded in our daily life, the ability to recognize and adapt to human emotions is essential for effective human-computer interaction. Facial expression recognition (FER) provides a primary channel for inferring affective states, but the dynamic and culturally nuanced nature of emotions requires models that can learn continuously without forgetting prior knowledge. In this work, we propose a hybrid framework for FER in a continual learning setting that mitigates catastrophic forgetting. Our approach integrates two complementary modalities: deep convolutional features and facial Action Units (AUs) derived from the Facial Action Coding System (FACS). The combined representation is modelled through Bayesian Gaussian Mixture Models (BGMMs), which provide a lightweight, probabilistic solution that avoids retraining while offering strong discriminative power. Using the Compound Facial Expression of Emotion (CFEE) dataset, we show that our model can first learn basic expressions and then progressively recognize compound expressions. Experiments demonstrate improved accuracy, stronger knowledge retention, and reduced forgetting. This framework contributes to the development of emotionally intelligent AI systems with applications in education, healthcare, and adaptive user interfaces.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u8fde\u7eed\u5b66\u4e60\u573a\u666f\u7684\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u6df1\u5ea6\u5377\u79ef\u7279\u5f81\u548c\u9762\u90e8\u52a8\u4f5c\u5355\u5143\uff0c\u4f7f\u7528\u8d1d\u53f6\u65af\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u8bc6\u522b\u548c\u9002\u5e94\u4eba\u7c7b\u60c5\u611f\u5bf9\u4e8e\u6709\u6548\u7684\u4eba\u673a\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\u3002\u9762\u90e8\u8868\u60c5\u8bc6\u522b\u662f\u63a8\u65ad\u60c5\u611f\u72b6\u6001\u7684\u4e3b\u8981\u6e20\u9053\uff0c\u4f46\u60c5\u611f\u5177\u6709\u52a8\u6001\u6027\u548c\u6587\u5316\u7ec6\u5fae\u5dee\u522b\uff0c\u9700\u8981\u80fd\u591f\u6301\u7eed\u5b66\u4e60\u800c\u4e0d\u9057\u5fd8\u5148\u524d\u77e5\u8bc6\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u6846\u67b6\uff0c\u6574\u5408\u4e24\u79cd\u4e92\u8865\u6a21\u6001\uff1a\u6df1\u5ea6\u5377\u79ef\u7279\u5f81\u548c\u57fa\u4e8e\u9762\u90e8\u52a8\u4f5c\u7f16\u7801\u7cfb\u7edf\u7684\u9762\u90e8\u52a8\u4f5c\u5355\u5143\u3002\u901a\u8fc7\u8d1d\u53f6\u65af\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u5bf9\u7ec4\u5408\u8868\u793a\u8fdb\u884c\u5efa\u6a21\uff0c\u63d0\u4f9b\u8f7b\u91cf\u7ea7\u6982\u7387\u89e3\u51b3\u65b9\u6848\uff0c\u907f\u514d\u91cd\u65b0\u8bad\u7ec3\u540c\u65f6\u4fdd\u6301\u5f3a\u5224\u522b\u80fd\u529b\u3002", "result": "\u4f7f\u7528\u590d\u5408\u9762\u90e8\u8868\u60c5\u6570\u636e\u96c6\uff0c\u6a21\u578b\u80fd\u591f\u5148\u5b66\u4e60\u57fa\u672c\u8868\u60c5\uff0c\u7136\u540e\u9010\u6b65\u8bc6\u522b\u590d\u5408\u8868\u60c5\u3002\u5b9e\u9a8c\u663e\u793a\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3001\u589e\u5f3a\u4e86\u77e5\u8bc6\u4fdd\u7559\u80fd\u529b\u5e76\u51cf\u5c11\u4e86\u9057\u5fd8\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u52a9\u4e8e\u5f00\u53d1\u5177\u6709\u60c5\u611f\u667a\u80fd\u7684AI\u7cfb\u7edf\uff0c\u53ef\u5e94\u7528\u4e8e\u6559\u80b2\u3001\u533b\u7597\u4fdd\u5065\u548c\u81ea\u9002\u5e94\u7528\u6237\u754c\u9762\u7b49\u9886\u57df\u3002"}}
{"id": "2512.13374", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13374", "abs": "https://arxiv.org/abs/2512.13374", "authors": ["Francesca Da Ros", "Luca Di Gaspero", "Kevin Roitero"], "title": "Behavior and Representation in Large Language Models for Combinatorial Optimization: From Feature Extraction to Algorithm Selection", "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have opened new perspectives for automation in optimization. While several studies have explored how LLMs can generate or solve optimization models, far less is understood about what these models actually learn regarding problem structure or algorithmic behavior. This study investigates how LLMs internally represent combinatorial optimization problems and whether such representations can support downstream decision tasks. We adopt a twofold methodology combining direct querying, which assesses LLM capacity to explicitly extract instance features, with probing analyses that examine whether such information is implicitly encoded within their hidden layers. The probing framework is further extended to a per-instance algorithm selection task, evaluating whether LLM-derived representations can predict the best-performing solver. Experiments span four benchmark problems and three instance representations. Results show that LLMs exhibit moderate ability to recover feature information from problem instances, either through direct querying or probing. Notably, the predictive power of LLM hidden-layer representations proves comparable to that achieved through traditional feature extraction, suggesting that LLMs capture meaningful structural information relevant to optimization performance.", "AI": {"tldr": "LLMs\u80fd\u591f\u4ece\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e2d\u5b66\u4e60\u7ed3\u6784\u4fe1\u606f\uff0c\u5176\u9690\u85cf\u5c42\u8868\u793a\u5728\u7b97\u6cd5\u9009\u62e9\u4efb\u52a1\u4e2d\u4e0e\u4f20\u7edf\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u8868\u73b0\u76f8\u5f53", "motivation": "\u867d\u7136\u5df2\u6709\u7814\u7a76\u63a2\u7d22LLMs\u751f\u6210\u6216\u6c42\u89e3\u4f18\u5316\u6a21\u578b\uff0c\u4f46\u5bf9\u5176\u5b66\u4e60\u95ee\u9898\u7ed3\u6784\u6216\u7b97\u6cd5\u884c\u4e3a\u7684\u80fd\u529b\u4e86\u89e3\u4e0d\u8db3\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76LLMs\u5982\u4f55\u5185\u90e8\u8868\u793a\u7ec4\u5408\u4f18\u5316\u95ee\u9898\uff0c\u4ee5\u53ca\u8fd9\u4e9b\u8868\u793a\u662f\u5426\u80fd\u652f\u6301\u4e0b\u6e38\u51b3\u7b56\u4efb\u52a1", "method": "\u91c7\u7528\u53cc\u91cd\u65b9\u6cd5\uff1a1) \u76f4\u63a5\u67e5\u8be2\u8bc4\u4f30LLMs\u663e\u5f0f\u63d0\u53d6\u5b9e\u4f8b\u7279\u5f81\u7684\u80fd\u529b\uff1b2) \u63a2\u6d4b\u5206\u6790\u68c0\u67e5\u8fd9\u4e9b\u4fe1\u606f\u662f\u5426\u9690\u5f0f\u7f16\u7801\u5728\u9690\u85cf\u5c42\u4e2d\u3002\u63a2\u6d4b\u6846\u67b6\u8fdb\u4e00\u6b65\u6269\u5c55\u5230\u6bcf\u4e2a\u5b9e\u4f8b\u7684\u7b97\u6cd5\u9009\u62e9\u4efb\u52a1\uff0c\u8bc4\u4f30LLM\u6d3e\u751f\u8868\u793a\u662f\u5426\u80fd\u9884\u6d4b\u6700\u4f73\u6c42\u89e3\u5668", "result": "\u5b9e\u9a8c\u6db5\u76d6\u56db\u4e2a\u57fa\u51c6\u95ee\u9898\u548c\u4e09\u79cd\u5b9e\u4f8b\u8868\u793a\u3002\u7ed3\u679c\u663e\u793aLLMs\u5728\u901a\u8fc7\u76f4\u63a5\u67e5\u8be2\u6216\u63a2\u6d4b\u6062\u590d\u7279\u5f81\u4fe1\u606f\u65b9\u9762\u8868\u73b0\u51fa\u4e2d\u7b49\u80fd\u529b\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cLLM\u9690\u85cf\u5c42\u8868\u793a\u7684\u9884\u6d4b\u80fd\u529b\u4e0e\u4f20\u7edf\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u76f8\u5f53\uff0c\u8868\u660eLLMs\u6355\u83b7\u4e86\u4e0e\u4f18\u5316\u6027\u80fd\u76f8\u5173\u7684\u6709\u610f\u4e49\u7ed3\u6784\u4fe1\u606f", "conclusion": "LLMs\u80fd\u591f\u4ece\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u4e2d\u5b66\u4e60\u7ed3\u6784\u4fe1\u606f\uff0c\u5176\u9690\u85cf\u5c42\u8868\u793a\u5728\u7b97\u6cd5\u9009\u62e9\u4efb\u52a1\u4e2d\u4e0e\u4f20\u7edf\u7279\u5f81\u63d0\u53d6\u65b9\u6cd5\u8868\u73b0\u76f8\u5f53\uff0c\u4e3a\u4f18\u5316\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2"}}
{"id": "2512.12301", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12301", "abs": "https://arxiv.org/abs/2512.12301", "authors": ["Mahima Kumavat", "Aditya Maheshwari"], "title": "TwinFormer: A Dual-Level Transformer for Long-Sequence Time-Series Forecasting", "comment": "14 pages, 4 figures", "summary": "TwinFormer is a hierarchical Transformer for long-sequence time-series forecasting. It divides the input into non-overlapping temporal patches and processes them in two stages: (1) a Local Informer with top-$k$ Sparse Attention models intra-patch dynamics, followed by mean pooling; (2) a Global Informer captures long-range inter-patch dependencies using the same top-$k$ attention. A lightweight GRU aggregates the globally contextualized patch tokens for direct multi-horizon prediction. The resulting architecture achieves linear $O(kLd)$ time and memory complexity. On eight real-world benchmarking datasets from six different domains, including weather, stock price, temperature, power consumption, electricity, and disease, and forecasting horizons $96-720$, TwinFormer secures $27$ positions in the top two out of $34$. Out of the $27$, it achieves the best performance on MAE and RMSE at $17$ places and $10$ at the second-best place on MAE and RMSE. This consistently outperforms PatchTST, iTransformer, FEDformer, Informer, and vanilla Transformers. Ablations confirm the superiority of top-$k$ Sparse Attention over ProbSparse and the effectiveness of GRU-based aggregation. Code is available at this repository: https://github.com/Mahimakumavat1205/TwinFormer.", "AI": {"tldr": "TwinFormer\u662f\u4e00\u4e2a\u7528\u4e8e\u957f\u5e8f\u5217\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u5206\u5c42Transformer\uff0c\u901a\u8fc7\u5c40\u90e8-\u5168\u5c40\u53cc\u9636\u6bb5\u5904\u7406\u548c\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u957f\u5e8f\u5217\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u4f20\u7edfTransformer\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u96be\u4ee5\u6355\u6349\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\u7684\u95ee\u9898\uff0c\u9700\u8981\u8bbe\u8ba1\u4e00\u79cd\u65e2\u80fd\u6709\u6548\u5efa\u6a21\u5c40\u90e8\u52a8\u6001\u53c8\u80fd\u6355\u83b7\u5168\u5c40\u4f9d\u8d56\u7684\u9ad8\u6548\u67b6\u6784\u3002", "method": "1. \u5c06\u8f93\u5165\u5212\u5206\u4e3a\u975e\u91cd\u53e0\u7684\u65f6\u95f4\u7247\u6bb5\uff1b2. \u5c40\u90e8\u4fe1\u606f\u5668\u4f7f\u7528top-k\u7a00\u758f\u6ce8\u610f\u529b\u5efa\u6a21\u7247\u6bb5\u5185\u52a8\u6001\uff0c\u7136\u540e\u8fdb\u884c\u5e73\u5747\u6c60\u5316\uff1b3. \u5168\u5c40\u4fe1\u606f\u5668\u4f7f\u7528\u76f8\u540c\u7684top-k\u6ce8\u610f\u529b\u6355\u83b7\u7247\u6bb5\u95f4\u7684\u957f\u7a0b\u4f9d\u8d56\uff1b4. \u4f7f\u7528\u8f7b\u91cf\u7ea7GRU\u805a\u5408\u5168\u5c40\u4e0a\u4e0b\u6587\u5316\u7684\u7247\u6bb5\u6807\u8bb0\u8fdb\u884c\u591a\u6b65\u9884\u6d4b\u3002", "result": "\u57288\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\uff08\u5929\u6c14\u3001\u80a1\u4ef7\u3001\u6e29\u5ea6\u3001\u7535\u529b\u6d88\u8017\u3001\u7535\u529b\u3001\u75be\u75c5\u7b49\uff09\u4e0a\uff0c\u9884\u6d4b\u8303\u56f496-720\u6b65\uff0cTwinFormer\u572834\u4e2a\u8bc4\u4f30\u4e2d\u83b7\u5f97\u4e8627\u4e2a\u524d\u4e24\u540d\u4f4d\u7f6e\uff0c\u5176\u4e2d17\u4e2a\u6700\u4f73MAE\u548cRMSE\uff0c10\u4e2a\u6b21\u4f73\u3002\u6027\u80fd\u4f18\u4e8ePatchTST\u3001iTransformer\u3001FEDformer\u3001Informer\u548c\u539f\u59cbTransformer\u3002", "conclusion": "TwinFormer\u901a\u8fc7\u5206\u5c42\u7ed3\u6784\u548c\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u4e86\u7ebf\u6027\u590d\u6742\u5ea6\uff0c\u5728\u957f\u5e8f\u5217\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0ctop-k\u7a00\u758f\u6ce8\u610f\u529b\u4f18\u4e8eProbSparse\uff0cGRU\u805a\u5408\u6709\u6548\u3002"}}
{"id": "2512.12287", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12287", "abs": "https://arxiv.org/abs/2512.12287", "authors": ["Ahmad Zafarani", "Zahra Dehghanian", "Mohammadreza Davoodi", "Mohsen Shadroo", "MohammadAmin Fazli", "Hamid R. Rabiee"], "title": "RealDrag: The First Dragging Benchmark with Real Target Image", "comment": null, "summary": "The evaluation of drag based image editing models is unreliable due to a lack of standardized benchmarks and metrics. This ambiguity stems from inconsistent evaluation protocols and, critically, the absence of datasets containing ground truth target images, making objective comparisons between competing methods difficult. To address this, we introduce \\textbf{RealDrag}, the first comprehensive benchmark for point based image editing that includes paired ground truth target images. Our dataset contains over 400 human annotated samples from diverse video sources, providing source/target images, handle/target points, editable region masks, and descriptive captions for both the image and the editing action.\n  We also propose four novel, task specific metrics: Semantical Distance (SeD), Outer Mask Preserving Score (OMPS), Inner Patch Preserving Score (IPPS), and Directional Similarity (DiS). These metrics are designed to quantify pixel level matching fidelity, check preservation of non edited (out of mask) regions, and measure semantic alignment with the desired task. Using this benchmark, we conduct the first large scale systematic analysis of the field, evaluating 17 SOTA models. Our results reveal clear trade offs among current approaches and establish a robust, reproducible baseline to guide future research. Our dataset and evaluation toolkit will be made publicly available.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86RealDrag\u2014\u2014\u9996\u4e2a\u5305\u542b\u771f\u5b9e\u76ee\u6807\u56fe\u50cf\u7684\u57fa\u4e8e\u70b9\u7f16\u8f91\u56fe\u50cf\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b400\u591a\u4e2a\u6807\u6ce8\u6837\u672c\uff0c\u5e76\u8bbe\u8ba1\u4e86\u56db\u4e2a\u65b0\u6307\u6807\u6765\u5ba2\u89c2\u8bc4\u4f30\u62d6\u62fd\u5f0f\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u62d6\u62fd\u5f0f\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u7684\u8bc4\u4f30\u4e0d\u53ef\u9760\uff0c\u7f3a\u4e4f\u6807\u51c6\u5316\u57fa\u51c6\u548c\u6307\u6807\u3002\u4e3b\u8981\u95ee\u9898\u5305\u62ec\u8bc4\u4f30\u534f\u8bae\u4e0d\u4e00\u81f4\uff0c\u4ee5\u53ca\u7f3a\u5c11\u5305\u542b\u771f\u5b9e\u76ee\u6807\u56fe\u50cf\u7684\u6570\u636e\u96c6\uff0c\u5bfc\u81f4\u96be\u4ee5\u5ba2\u89c2\u6bd4\u8f83\u4e0d\u540c\u65b9\u6cd5\u3002", "method": "1. \u6784\u5efaRealDrag\u6570\u636e\u96c6\uff1a\u5305\u542b400\u591a\u4e2a\u6765\u81ea\u591a\u6837\u5316\u89c6\u9891\u6e90\u7684\u4eba\u5de5\u6807\u6ce8\u6837\u672c\uff0c\u63d0\u4f9b\u6e90/\u76ee\u6807\u56fe\u50cf\u3001\u5904\u7406/\u76ee\u6807\u70b9\u3001\u53ef\u7f16\u8f91\u533a\u57df\u63a9\u7801\u4ee5\u53ca\u56fe\u50cf\u548c\u7f16\u8f91\u52a8\u4f5c\u7684\u63cf\u8ff0\u6027\u6807\u6ce8\u3002\n2. \u63d0\u51fa\u56db\u4e2a\u4efb\u52a1\u7279\u5b9a\u6307\u6807\uff1a\u8bed\u4e49\u8ddd\u79bb(SeD)\u3001\u5916\u90e8\u63a9\u7801\u4fdd\u6301\u5206\u6570(OMPS)\u3001\u5185\u90e8\u8865\u4e01\u4fdd\u6301\u5206\u6570(IPPS)\u548c\u65b9\u5411\u76f8\u4f3c\u6027(DiS)\uff0c\u5206\u522b\u91cf\u5316\u50cf\u7d20\u7ea7\u5339\u914d\u4fdd\u771f\u5ea6\u3001\u975e\u7f16\u8f91\u533a\u57df\u4fdd\u6301\u548c\u8bed\u4e49\u5bf9\u9f50\u3002", "result": "\u4f7f\u7528\u8be5\u57fa\u51c6\u5bf917\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\u8fdb\u884c\u4e86\u9996\u6b21\u5927\u89c4\u6a21\u7cfb\u7edf\u5206\u6790\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u4e4b\u95f4\u7684\u660e\u786e\u6743\u8861\uff0c\u5e76\u5efa\u7acb\u4e86\u7a33\u5065\u3001\u53ef\u590d\u73b0\u7684\u57fa\u7ebf\u6765\u6307\u5bfc\u672a\u6765\u7814\u7a76\u3002", "conclusion": "RealDrag\u662f\u9996\u4e2a\u5305\u542b\u771f\u5b9e\u76ee\u6807\u56fe\u50cf\u7684\u57fa\u4e8e\u70b9\u7f16\u8f91\u56fe\u50cf\u7efc\u5408\u57fa\u51c6\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u6570\u636e\u96c6\u548c\u4e13\u95e8\u8bbe\u8ba1\u7684\u6307\u6807\uff0c\u89e3\u51b3\u4e86\u8be5\u9886\u57df\u8bc4\u4f30\u4e0d\u53ef\u9760\u7684\u95ee\u9898\uff0c\u4e3a\u5ba2\u89c2\u6bd4\u8f83\u6a21\u578b\u6027\u80fd\u63d0\u4f9b\u4e86\u53ef\u9760\u6846\u67b6\u3002"}}
{"id": "2512.12325", "categories": ["cs.LG", "math.ST", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.12325", "abs": "https://arxiv.org/abs/2512.12325", "authors": ["Shubhada Agrawal", "Aaditya Ramdas"], "title": "Eventually LIL Regret: Almost Sure $\\ln\\ln T$ Regret for a sub-Gaussian Mixture on Unbounded Data", "comment": "24 pages", "summary": "We prove that a classic sub-Gaussian mixture proposed by Robbins in a stochastic setting actually satisfies a path-wise (deterministic) regret bound. For every path in a natural ``Ville event'' $E_\u03b1$, this regret till time $T$ is bounded by $\\ln^2(1/\u03b1)/V_T + \\ln (1/\u03b1) + \\ln \\ln V_T$ up to universal constants, where $V_T$ is a nonnegative, nondecreasing, cumulative variance process. (The bound reduces to $\\ln(1/\u03b1) + \\ln \\ln V_T$ if $V_T \\geq \\ln(1/\u03b1)$.) If the data were stochastic, then one can show that $E_\u03b1$ has probability at least $1-\u03b1$ under a wide class of distributions (eg: sub-Gaussian, symmetric, variance-bounded, etc.). In fact, we show that on the Ville event $E_0$ of probability one, the regret on every path in $E_0$ is eventually bounded by $\\ln \\ln V_T$ (up to constants). We explain how this work helps bridge the world of adversarial online learning (which usually deals with regret bounds for bounded data), with game-theoretic statistics (which can handle unbounded data, albeit using stochastic assumptions). In short, conditional regret bounds serve as a bridge between stochastic and adversarial betting.", "AI": {"tldr": "\u8be5\u8bba\u6587\u8bc1\u660e\u4e86Robbins\u63d0\u51fa\u7684\u7ecf\u5178\u6b21\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u6ee1\u8db3\u8def\u5f84\u5f0f\uff08\u786e\u5b9a\u6027\uff09\u9057\u61be\u754c\uff0c\u5728Ville\u4e8b\u4ef6E_\u03b1\u4e2d\uff0c\u9057\u61be\u754c\u4e3aln\u00b2(1/\u03b1)/V_T + ln(1/\u03b1) + ln ln V_T\uff0c\u5176\u4e2dV_T\u662f\u7d2f\u79ef\u65b9\u5dee\u8fc7\u7a0b\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u6865\u63a5\u5bf9\u6297\u6027\u5728\u7ebf\u5b66\u4e60\uff08\u901a\u5e38\u5904\u7406\u6709\u754c\u6570\u636e\u7684\u9057\u61be\u754c\uff09\u548c\u535a\u5f08\u8bba\u7edf\u8ba1\uff08\u53ef\u4ee5\u5904\u7406\u65e0\u754c\u6570\u636e\uff0c\u4f46\u9700\u8981\u968f\u673a\u5047\u8bbe\uff09\u4e24\u4e2a\u9886\u57df\u3002\u901a\u8fc7\u6761\u4ef6\u9057\u61be\u754c\u4f5c\u4e3a\u968f\u673a\u548c\u5bf9\u6297\u6027\u6295\u6ce8\u4e4b\u95f4\u7684\u6865\u6881\u3002", "method": "\u5206\u6790Robbins\u63d0\u51fa\u7684\u7ecf\u5178\u6b21\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff0c\u8bc1\u660e\u5176\u5728Ville\u4e8b\u4ef6E_\u03b1\u4e2d\u6ee1\u8db3\u8def\u5f84\u5f0f\u786e\u5b9a\u6027\u9057\u61be\u754c\u3002\u4f7f\u7528\u7d2f\u79ef\u65b9\u5dee\u8fc7\u7a0bV_T\u4f5c\u4e3a\u5173\u952e\u53c2\u6570\uff0c\u7814\u7a76\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u9057\u61be\u754c\u53d8\u5316\u3002", "result": "\u8bc1\u660e\u4e86\u5728Ville\u4e8b\u4ef6E_\u03b1\u4e2d\uff0c\u9057\u61be\u754c\u4e3aln\u00b2(1/\u03b1)/V_T + ln(1/\u03b1) + ln ln V_T\uff1b\u5f53V_T \u2265 ln(1/\u03b1)\u65f6\u7b80\u5316\u4e3aln(1/\u03b1) + ln ln V_T\uff1b\u5728\u6982\u7387\u4e3a1\u7684\u4e8b\u4ef6E_0\u4e2d\uff0c\u9057\u61be\u6700\u7ec8\u88abln ln V_T\uff08\u4e58\u4ee5\u5e38\u6570\uff09\u6240\u754c\u5b9a\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u901a\u8fc7\u6761\u4ef6\u9057\u61be\u754c\u6210\u529f\u6865\u63a5\u4e86\u5bf9\u6297\u6027\u5728\u7ebf\u5b66\u4e60\u548c\u535a\u5f08\u8bba\u7edf\u8ba1\u4e24\u4e2a\u9886\u57df\uff0c\u4e3a\u5904\u7406\u6709\u754c\u548c\u65e0\u754c\u6570\u636e\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u5c55\u793a\u4e86\u968f\u673a\u548c\u5bf9\u6297\u6027\u6295\u6ce8\u4e4b\u95f4\u7684\u7406\u8bba\u8054\u7cfb\u3002"}}
{"id": "2512.12296", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12296", "abs": "https://arxiv.org/abs/2512.12296", "authors": ["Hyunju Lee", "Youngmin Oh", "Jeimin Jeon", "Donghyeon Baek", "Bumsub Ham"], "title": "GrowTAS: Progressive Expansion from Small to Large Subnets for Efficient ViT Architecture Search", "comment": "Accepted to WACV 2026", "summary": "Transformer architecture search (TAS) aims to automatically discover efficient vision transformers (ViTs), reducing the need for manual design. Existing TAS methods typically train an over-parameterized network (i.e., a supernet) that encompasses all candidate architectures (i.e., subnets). However, all subnets share the same set of weights, which leads to interference that degrades the smaller subnets severely. We have found that well-trained small subnets can serve as a good foundation for training larger ones. Motivated by this, we propose a progressive training framework, dubbed GrowTAS, that begins with training small subnets and incorporate larger ones gradually. This enables reducing the interference and stabilizing a training process. We also introduce GrowTAS+ that fine-tunes a subset of weights only to further enhance the performance of large subnets. Extensive experiments on ImageNet and several transfer learning benchmarks, including CIFAR-10/100, Flowers, CARS, and INAT-19, demonstrate the effectiveness of our approach over current TAS methods", "AI": {"tldr": "GrowTAS\u662f\u4e00\u79cd\u6e10\u8fdb\u5f0fTransformer\u67b6\u6784\u641c\u7d22\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u5c0f\u578b\u5b50\u7f51\u5f00\u59cb\u8bad\u7ec3\u5e76\u9010\u6b65\u52a0\u5165\u5927\u578b\u5b50\u7f51\uff0c\u51cf\u5c11\u6743\u91cd\u5171\u4eab\u5e26\u6765\u7684\u5e72\u6270\uff0c\u63d0\u5347\u641c\u7d22\u6548\u679c", "motivation": "\u73b0\u6709\u7684Transformer\u67b6\u6784\u641c\u7d22\u65b9\u6cd5\u4e2d\uff0c\u6240\u6709\u5019\u9009\u67b6\u6784\u5171\u4eab\u540c\u4e00\u7ec4\u6743\u91cd\uff0c\u8fd9\u5bfc\u81f4\u4e25\u91cd\u5e72\u6270\uff0c\u7279\u522b\u662f\u5bf9\u5c0f\u89c4\u6a21\u5b50\u7f51\u9020\u6210\u6027\u80fd\u4e0b\u964d\u3002\u7814\u7a76\u53d1\u73b0\u8bad\u7ec3\u826f\u597d\u7684\u5c0f\u89c4\u6a21\u5b50\u7f51\u53ef\u4ee5\u4f5c\u4e3a\u8bad\u7ec3\u66f4\u5927\u5b50\u7f51\u7684\u826f\u597d\u57fa\u7840", "method": "\u63d0\u51faGrowTAS\u6e10\u8fdb\u8bad\u7ec3\u6846\u67b6\uff1a1\uff09\u4ece\u5c0f\u578b\u5b50\u7f51\u5f00\u59cb\u8bad\u7ec3\uff1b2\uff09\u9010\u6b65\u52a0\u5165\u66f4\u5927\u7684\u5b50\u7f51\uff1b3\uff09\u51cf\u5c11\u5e72\u6270\u5e76\u7a33\u5b9a\u8bad\u7ec3\u8fc7\u7a0b\u3002\u8fd8\u63d0\u51faGrowTAS+\uff0c\u4ec5\u5fae\u8c03\u90e8\u5206\u6743\u91cd\u4ee5\u8fdb\u4e00\u6b65\u63d0\u5347\u5927\u89c4\u6a21\u5b50\u7f51\u6027\u80fd", "result": "\u5728ImageNet\u548c\u591a\u4e2a\u8fc1\u79fb\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\uff08CIFAR-10/100\u3001Flowers\u3001CARS\u3001INAT-19\uff09\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u5f53\u524d\u7684Transformer\u67b6\u6784\u641c\u7d22\u65b9\u6cd5", "conclusion": "\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u6846\u67b6GrowTAS\u901a\u8fc7\u4ece\u5c0f\u578b\u5b50\u7f51\u5f00\u59cb\u5e76\u9010\u6b65\u6269\u5c55\uff0c\u6709\u6548\u51cf\u5c11\u4e86\u6743\u91cd\u5171\u4eab\u5e26\u6765\u7684\u5e72\u6270\uff0c\u63d0\u9ad8\u4e86Transformer\u67b6\u6784\u641c\u7d22\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027"}}
{"id": "2512.12341", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.12341", "abs": "https://arxiv.org/abs/2512.12341", "authors": ["Paul Hofman", "Yusuf Sale", "Eyke H\u00fcllermeier"], "title": "Uncertainty Quantification for Machine Learning: One Size Does Not Fit All", "comment": null, "summary": "Proper quantification of predictive uncertainty is essential for the use of machine learning in safety-critical applications. Various uncertainty measures have been proposed for this purpose, typically claiming superiority over other measures. In this paper, we argue that there is no single best measure. Instead, uncertainty quantification should be tailored to the specific application. To this end, we use a flexible family of uncertainty measures that distinguishes between total, aleatoric, and epistemic uncertainty of second-order distributions. These measures can be instantiated with specific loss functions, so-called proper scoring rules, to control their characteristics, and we show that different characteristics are useful for different tasks. In particular, we show that, for the task of selective prediction, the scoring rule should ideally match the task loss. On the other hand, for out-of-distribution detection, our results confirm that mutual information, a widely used measure of epistemic uncertainty, performs best. Furthermore, in an active learning setting, epistemic uncertainty based on zero-one loss is shown to consistently outperform other uncertainty measures.", "AI": {"tldr": "\u672c\u6587\u8ba4\u4e3a\u4e0d\u5b58\u5728\u5355\u4e00\u7684\"\u6700\u4f73\"\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\uff0c\u5e94\u6839\u636e\u5177\u4f53\u5e94\u7528\u573a\u666f\u5b9a\u5236\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u4e8c\u9636\u5206\u5e03\u7684\u7075\u6d3b\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u6846\u67b6\uff0c\u5e76\u5c55\u793a\u4e86\u4e0d\u540c\u4efb\u52a1\u9700\u8981\u4e0d\u540c\u7279\u6027\u7684\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\uff0c\u51c6\u786e\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7814\u7a76\u901a\u5e38\u58f0\u79f0\u67d0\u79cd\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u4f46\u672c\u6587\u8ba4\u4e3a\u4e0d\u5b58\u5728\u5355\u4e00\u6700\u4f73\u5ea6\u91cf\uff0c\u5e94\u6839\u636e\u5177\u4f53\u5e94\u7528\u573a\u666f\u5b9a\u5236\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7075\u6d3b\u7684\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u6846\u67b6\uff0c\u57fa\u4e8e\u4e8c\u9636\u5206\u5e03\u533a\u5206\u603b\u4e0d\u786e\u5b9a\u6027\u3001\u5076\u7136\u4e0d\u786e\u5b9a\u6027\u548c\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u3002\u8fd9\u4e9b\u5ea6\u91cf\u53ef\u4ee5\u901a\u8fc7\u9002\u5f53\u7684\u8bc4\u5206\u89c4\u5219\uff08proper scoring rules\uff09\u8fdb\u884c\u5b9e\u4f8b\u5316\uff0c\u4ee5\u63a7\u5236\u5176\u7279\u6027\u3002", "result": "\u7814\u7a76\u8868\u660e\uff1a1\uff09\u5bf9\u4e8e\u9009\u62e9\u6027\u9884\u6d4b\u4efb\u52a1\uff0c\u8bc4\u5206\u89c4\u5219\u5e94\u4e0e\u4efb\u52a1\u635f\u5931\u5339\u914d\uff1b2\uff09\u5bf9\u4e8e\u5206\u5e03\u5916\u68c0\u6d4b\uff0c\u4e92\u4fe1\u606f\uff08\u5e7f\u6cdb\u4f7f\u7528\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\uff09\u8868\u73b0\u6700\u4f73\uff1b3\uff09\u5728\u4e3b\u52a8\u5b66\u4e60\u573a\u666f\u4e2d\uff0c\u57fa\u4e8e0-1\u635f\u5931\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u59cb\u7ec8\u4f18\u4e8e\u5176\u4ed6\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u3002", "conclusion": "\u4e0d\u5b58\u5728\u5355\u4e00\u7684\u6700\u4f73\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u65b9\u6cd5\uff0c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u5e94\u6839\u636e\u5177\u4f53\u5e94\u7528\u9700\u6c42\u8fdb\u884c\u5b9a\u5236\u3002\u901a\u8fc7\u7075\u6d3b\u7684\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u6846\u67b6\u548c\u9002\u5f53\u7684\u8bc4\u5206\u89c4\u5219\uff0c\u53ef\u4ee5\u4e3a\u4e0d\u540c\u4efb\u52a1\u9009\u62e9\u6700\u5408\u9002\u7684\u4e0d\u786e\u5b9a\u6027\u5ea6\u91cf\u65b9\u6cd5\u3002"}}
{"id": "2512.12302", "categories": ["cs.CV", "cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12302", "abs": "https://arxiv.org/abs/2512.12302", "authors": ["Huan Zheng", "Yucheng Zhou", "Tianyi Yan", "Jiayi Su", "Hongjun Chen", "Dubing Chen", "Wencheng Han", "Runzhou Tao", "Zhongying Qiu", "Jianfei Yang", "Jianbing Shen"], "title": "From Human Intention to Action Prediction: A Comprehensive Benchmark for Intention-driven End-to-End Autonomous Driving", "comment": null, "summary": "Current end-to-end autonomous driving systems operate at a level of intelligence akin to following simple steering commands. However, achieving genuinely intelligent autonomy requires a paradigm shift: moving from merely executing low-level instructions to understanding and fulfilling high-level, abstract human intentions. This leap from a command-follower to an intention-fulfiller, as illustrated in our conceptual framework, is hindered by a fundamental challenge: the absence of a standardized benchmark to measure and drive progress on this complex task. To address this critical gap, we introduce Intention-Drive, the first comprehensive benchmark designed to evaluate the ability to translate high-level human intent into safe and precise driving actions. Intention-Drive features two core contributions: (1) a new dataset of complex scenarios paired with corresponding natural language intentions, and (2) a novel evaluation protocol centered on the Intent Success Rate (ISR), which assesses the semantic fulfillment of the human's goal beyond simple geometric accuracy. Through an extensive evaluation of a spectrum of baseline models on Intention-Drive, we reveal a significant performance deficit, showing that the baseline model struggle to achieve the comprehensive scene and intention understanding required for this advanced task.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86Intention-Drive\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4ece\u9ad8\u7ea7\u4eba\u7c7b\u610f\u56fe\u5230\u5b89\u5168\u7cbe\u786e\u9a7e\u9a76\u52a8\u4f5c\u7684\u8f6c\u5316\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u610f\u56fe\u7406\u89e3\u65b9\u9762\u7684\u663e\u8457\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4ec5\u80fd\u6267\u884c\u4f4e\u7ea7\u8f6c\u5411\u6307\u4ee4\uff0c\u7f3a\u4e4f\u7406\u89e3\u5e76\u5b9e\u73b0\u9ad8\u7ea7\u62bd\u8c61\u4eba\u7c7b\u610f\u56fe\u7684\u80fd\u529b\u3002\u5b9e\u73b0\u771f\u6b63\u667a\u80fd\u81ea\u4e3b\u9a7e\u9a76\u9700\u8981\u4ece\u547d\u4ee4\u8ddf\u968f\u8005\u8f6c\u53d8\u4e3a\u610f\u56fe\u5b9e\u73b0\u8005\uff0c\u4f46\u7f3a\u4e4f\u6807\u51c6\u5316\u57fa\u51c6\u6765\u8861\u91cf\u548c\u63a8\u52a8\u8fd9\u4e00\u590d\u6742\u4efb\u52a1\u7684\u8fdb\u5c55\u3002", "method": "\u63d0\u51faIntention-Drive\u57fa\u51c6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u8d21\u732e\uff1a(1) \u5305\u542b\u590d\u6742\u573a\u666f\u53ca\u5176\u5bf9\u5e94\u81ea\u7136\u8bed\u8a00\u610f\u56fe\u7684\u65b0\u6570\u636e\u96c6\uff1b(2) \u4ee5\u610f\u56fe\u6210\u529f\u7387(ISR)\u4e3a\u4e2d\u5fc3\u7684\u65b0\u8bc4\u4f30\u534f\u8bae\uff0c\u8bc4\u4f30\u4eba\u7c7b\u76ee\u6807\u7684\u8bed\u4e49\u5b9e\u73b0\u7a0b\u5ea6\uff0c\u8d85\u8d8a\u7b80\u5355\u7684\u51e0\u4f55\u7cbe\u5ea6\u3002", "result": "\u901a\u8fc7\u5bf9\u4e00\u7cfb\u5217\u57fa\u7ebf\u6a21\u578b\u5728Intention-Drive\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u663e\u8457\u7684\u6027\u80fd\u7f3a\u9677\uff0c\u663e\u793a\u57fa\u7ebf\u6a21\u578b\u96be\u4ee5\u8fbe\u5230\u8fd9\u4e00\u9ad8\u7ea7\u4efb\u52a1\u6240\u9700\u7684\u5168\u9762\u573a\u666f\u548c\u610f\u56fe\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "Intention-Drive\u586b\u8865\u4e86\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4ece\u9ad8\u7ea7\u4eba\u7c7b\u610f\u56fe\u5230\u9a7e\u9a76\u52a8\u4f5c\u8f6c\u5316\u80fd\u529b\u7684\u5173\u952e\u7a7a\u767d\uff0c\u4e3a\u4ece\u547d\u4ee4\u8ddf\u968f\u8005\u5230\u610f\u56fe\u5b9e\u73b0\u8005\u7684\u8303\u5f0f\u8f6c\u53d8\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u610f\u56fe\u7406\u89e3\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2512.13505", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13505", "abs": "https://arxiv.org/abs/2512.13505", "authors": ["Henry Prakken", "Wijnand van Woerkom"], "title": "Defending the Hierarchical Result Models of Precedential Constraint", "comment": "This is the long version of a paper with the same title presented at the 38th International Conference on Legal Knowledge and Information Systems", "summary": "In recent years, hierarchical case-based-reasoning models of precedential constraint have been proposed. In various papers, Trevor Bench-Capon criticised these models on the grounds that they would give incorrect outcomes in some cases. In particular, the models would not account for the possibility that intermediate factors are established with different strengths by different base-level factors. In this paper we respond to these criticisms for van Woerkom's result-based hierarchical models. We argue that in some examples Bench-Capon seems to interpret intermediate factors as dimensions, and that applying van Woerkom's dimension-based version of the hierarchical result model to these examples avoids Bench-Capon's criticisms.", "AI": {"tldr": "\u672c\u6587\u56de\u5e94Bench-Capon\u5bf9\u5c42\u6b21\u6848\u4f8b\u63a8\u7406\u6a21\u578b\u7684\u6279\u8bc4\uff0c\u6307\u51fa\u5176\u8bef\u89e3\u4e86\u4e2d\u95f4\u56e0\u7d20\u4e0e\u7ef4\u5ea6\u7684\u533a\u522b\uff0c\u5e76\u8bc1\u660evan Woerkom\u7684\u7ef4\u5ea6\u5c42\u6b21\u7ed3\u679c\u6a21\u578b\u80fd\u907f\u514d\u8fd9\u4e9b\u6279\u8bc4\u3002", "motivation": "\u8fd1\u5e74\u6765\u63d0\u51fa\u7684\u5c42\u6b21\u6848\u4f8b\u63a8\u7406\u6a21\u578b\u53d7\u5230Bench-Capon\u7684\u6279\u8bc4\uff0c\u8ba4\u4e3a\u8fd9\u4e9b\u6a21\u578b\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4f1a\u4ea7\u751f\u9519\u8bef\u7ed3\u679c\uff0c\u7279\u522b\u662f\u65e0\u6cd5\u5904\u7406\u4e2d\u95f4\u56e0\u7d20\u88ab\u4e0d\u540c\u57fa\u7840\u56e0\u7d20\u4ee5\u4e0d\u540c\u5f3a\u5ea6\u786e\u7acb\u7684\u60c5\u51b5\u3002\u672c\u6587\u65e8\u5728\u56de\u5e94\u8fd9\u4e9b\u6279\u8bc4\u3002", "method": "\u901a\u8fc7\u5206\u6790Bench-Capon\u7684\u6279\u8bc4\u6848\u4f8b\uff0c\u6307\u51fa\u5176\u5c06\u4e2d\u95f4\u56e0\u7d20\u8bef\u89e3\u4e3a\u7ef4\u5ea6\uff0c\u7136\u540e\u5e94\u7528van Woerkom\u7684\u7ef4\u5ea6\u5c42\u6b21\u7ed3\u679c\u6a21\u578b\u6765\u91cd\u65b0\u5206\u6790\u8fd9\u4e9b\u6848\u4f8b\u3002", "result": "\u8bc1\u660e\u5f53\u6b63\u786e\u533a\u5206\u4e2d\u95f4\u56e0\u7d20\u548c\u7ef4\u5ea6\u65f6\uff0cvan Woerkom\u7684\u7ef4\u5ea6\u5c42\u6b21\u7ed3\u679c\u6a21\u578b\u80fd\u591f\u907f\u514dBench-Capon\u63d0\u51fa\u7684\u6279\u8bc4\uff0c\u6b63\u786e\u5904\u7406\u4e2d\u95f4\u56e0\u7d20\u88ab\u4e0d\u540c\u57fa\u7840\u56e0\u7d20\u4ee5\u4e0d\u540c\u5f3a\u5ea6\u786e\u7acb\u7684\u60c5\u51b5\u3002", "conclusion": "Bench-Capon\u7684\u6279\u8bc4\u6e90\u4e8e\u5bf9\u4e2d\u95f4\u56e0\u7d20\u548c\u7ef4\u5ea6\u7684\u8bef\u89e3\uff0cvan Woerkom\u7684\u7ef4\u5ea6\u5c42\u6b21\u7ed3\u679c\u6a21\u578b\u662f\u6709\u6548\u7684\uff0c\u80fd\u591f\u6b63\u786e\u5904\u7406\u5148\u524d\u7ea6\u675f\u7684\u5c42\u6b21\u6848\u4f8b\u63a8\u7406\u5efa\u6a21\u95ee\u9898\u3002"}}
{"id": "2512.12365", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12365", "abs": "https://arxiv.org/abs/2512.12365", "authors": ["Thai-Duy Dinh", "Minh-Luan Vo", "Cuong Tuan Nguyen", "Bich-Hien Vo"], "title": "Synthetic Swarm Mosquito Dataset for Acoustic Classification: A Proof of Concept", "comment": "Accepted at RIVF 2025", "summary": "Mosquito-borne diseases pose a serious global health threat, causing over 700,000 deaths annually. This work introduces a proof-of-concept Synthetic Swarm Mosquito Dataset for Acoustic Classification, created to simulate realistic multi-species and noisy swarm conditions. Unlike conventional datasets that require labor-intensive recording of individual mosquitoes, the synthetic approach enables scalable data generation while reducing human resource demands. Using log-mel spectrograms, we evaluated lightweight deep learning architectures for the classification of mosquito species. Experiments show that these models can effectively identify six major mosquito vectors and are suitable for deployment on embedded low-power devices. The study demonstrates the potential of synthetic swarm audio datasets to accelerate acoustic mosquito research and enable scalable real-time surveillance solutions.", "AI": {"tldr": "\u5f00\u53d1\u5408\u6210\u868a\u7fa4\u58f0\u5b66\u5206\u7c7b\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u6a21\u62df\u591a\u7269\u79cd\u5608\u6742\u73af\u5883\u4e0b\u7684\u868a\u5b50\u8bc6\u522b\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u9ad8\u6548\u53ef\u6269\u5c55", "motivation": "\u868a\u5a92\u75be\u75c5\u6bcf\u5e74\u5bfc\u81f4\u8d85\u8fc770\u4e07\u4eba\u6b7b\u4ea1\uff0c\u73b0\u6709\u6570\u636e\u96c6\u9700\u8981\u5927\u91cf\u4eba\u5de5\u91c7\u96c6\u4e2a\u4f53\u868a\u5b50\u5f55\u97f3\uff0c\u8d44\u6e90\u6d88\u8017\u5927\u4e14\u96be\u4ee5\u6269\u5c55", "method": "\u521b\u5efa\u5408\u6210\u868a\u7fa4\u868a\u5b50\u58f0\u5b66\u5206\u7c7b\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u5bf9\u6570\u6885\u5c14\u9891\u8c31\u56fe\uff0c\u8bc4\u4f30\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u8fdb\u884c\u868a\u5b50\u7269\u79cd\u5206\u7c7b", "result": "\u6a21\u578b\u80fd\u6709\u6548\u8bc6\u522b\u516d\u79cd\u4e3b\u8981\u868a\u5b50\u5a92\u4ecb\uff0c\u9002\u5408\u90e8\u7f72\u5728\u5d4c\u5165\u5f0f\u4f4e\u529f\u8017\u8bbe\u5907\u4e0a\uff0c\u5408\u6210\u6570\u636e\u96c6\u80fd\u52a0\u901f\u58f0\u5b66\u868a\u5b50\u7814\u7a76", "conclusion": "\u5408\u6210\u868a\u7fa4\u97f3\u9891\u6570\u636e\u96c6\u5177\u6709\u6f5c\u529b\u52a0\u901f\u868a\u5b50\u58f0\u5b66\u7814\u7a76\uff0c\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u5b9e\u65f6\u76d1\u6d4b\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.12303", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12303", "abs": "https://arxiv.org/abs/2512.12303", "authors": ["Yang Ou", "Xiongwei Zhao", "Xinye Yang", "Yihan Wang", "Yicheng Di", "Rong Yuan", "Xieyuanli Chen", "Xu Zhu"], "title": "OMUDA: Omni-level Masking for Unsupervised Domain Adaptation in Semantic Segmentation", "comment": "Submitted to TMM", "summary": "Unsupervised domain adaptation (UDA) enables semantic segmentation models to generalize from a labeled source domain to an unlabeled target domain. However, existing UDA methods still struggle to bridge the domain gap due to cross-domain contextual ambiguity, inconsistent feature representations, and class-wise pseudo-label noise. To address these challenges, we propose Omni-level Masking for Unsupervised Domain Adaptation (OMUDA), a unified framework that introduces hierarchical masking strategies across distinct representation levels. Specifically, OMUDA comprises: 1) a Context-Aware Masking (CAM) strategy that adaptively distinguishes foreground from background to balance global context and local details; 2) a Feature Distillation Masking (FDM) strategy that enhances robust and consistent feature learning through knowledge transfer from pre-trained models; and 3) a Class Decoupling Masking (CDM) strategy that mitigates the impact of noisy pseudo-labels by explicitly modeling class-wise uncertainty. This hierarchical masking paradigm effectively reduces the domain shift at the contextual, representational, and categorical levels, providing a unified solution beyond existing approaches. Extensive experiments on multiple challenging cross-domain semantic segmentation benchmarks validate the effectiveness of OMUDA. Notably, on the SYNTHIA->Cityscapes and GTA5->Cityscapes tasks, OMUDA can be seamlessly integrated into existing UDA methods and consistently achieving state-of-the-art results with an average improvement of 7%.", "AI": {"tldr": "OMUDA\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u8bed\u4e49\u5206\u5272\u7684\u5c42\u6b21\u5316\u63a9\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u63a9\u7801\u3001\u7279\u5f81\u84b8\u998f\u63a9\u7801\u548c\u7c7b\u522b\u89e3\u8026\u63a9\u7801\u4e09\u4e2a\u7b56\u7565\uff0c\u5728\u4e0a\u4e0b\u6587\u3001\u7279\u5f81\u8868\u793a\u548c\u7c7b\u522b\u5c42\u9762\u51cf\u5c11\u57df\u5dee\u5f02\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97SOTA\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u65b9\u6cd5\u5728\u5904\u7406\u8de8\u57df\u8bed\u4e49\u5206\u5272\u65f6\u9762\u4e34\u4e09\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u8de8\u57df\u4e0a\u4e0b\u6587\u6a21\u7cca\u6027\u3001\u4e0d\u4e00\u81f4\u7684\u7279\u5f81\u8868\u793a\u4ee5\u53ca\u7c7b\u522b\u7ea7\u4f2a\u6807\u7b7e\u566a\u58f0\u3002\u8fd9\u4e9b\u56e0\u7d20\u5bfc\u81f4\u6a21\u578b\u96be\u4ee5\u6709\u6548\u5f25\u5408\u6e90\u57df\u548c\u76ee\u6807\u57df\u4e4b\u95f4\u7684\u5dee\u5f02\u3002", "method": "OMUDA\u63d0\u51fa\u7edf\u4e00\u7684\u5c42\u6b21\u5316\u63a9\u7801\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7b56\u7565\uff1a1) \u4e0a\u4e0b\u6587\u611f\u77e5\u63a9\u7801(CAM)\uff1a\u81ea\u9002\u5e94\u533a\u5206\u524d\u666f\u4e0e\u80cc\u666f\uff0c\u5e73\u8861\u5168\u5c40\u4e0a\u4e0b\u6587\u548c\u5c40\u90e8\u7ec6\u8282\uff1b2) \u7279\u5f81\u84b8\u998f\u63a9\u7801(FDM)\uff1a\u901a\u8fc7\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u77e5\u8bc6\u8f6c\u79fb\u589e\u5f3a\u9c81\u68d2\u4e14\u4e00\u81f4\u7684\u7279\u5f81\u5b66\u4e60\uff1b3) \u7c7b\u522b\u89e3\u8026\u63a9\u7801(CDM)\uff1a\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u7c7b\u522b\u7ea7\u4e0d\u786e\u5b9a\u6027\u6765\u51cf\u8f7b\u566a\u58f0\u4f2a\u6807\u7b7e\u7684\u5f71\u54cd\u3002", "result": "\u5728\u591a\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u8de8\u57df\u8bed\u4e49\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86OMUDA\u7684\u6709\u6548\u6027\u3002\u7279\u522b\u662f\u5728SYNTHIA->Cityscapes\u548cGTA5->Cityscapes\u4efb\u52a1\u4e2d\uff0cOMUDA\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709UDA\u65b9\u6cd5\u4e2d\uff0c\u5e73\u5747\u63d0\u53477%\uff0c\u6301\u7eed\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "OMUDA\u901a\u8fc7\u5c42\u6b21\u5316\u63a9\u7801\u7b56\u7565\u5728\u4e0a\u4e0b\u6587\u3001\u8868\u793a\u548c\u7c7b\u522b\u5c42\u9762\u7cfb\u7edf\u6027\u5730\u51cf\u5c11\u57df\u504f\u79fb\uff0c\u4e3a\u65e0\u76d1\u7763\u57df\u81ea\u9002\u5e94\u8bed\u4e49\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u4e2a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u7684\u7edf\u4e00\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2512.13510", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13510", "abs": "https://arxiv.org/abs/2512.13510", "authors": ["Linjie Mu", "Yannian Gu", "Zhongzhen Huang", "Yakun Zhu", "Shaoting Zhang", "Xiaofan Zhang"], "title": "MedCEG: Reinforcing Verifiable Medical Reasoning with Critical Evidence Graph", "comment": null, "summary": "Large language models with reasoning capabilities have demonstrated impressive performance across a wide range of domains. In clinical applications, a transparent, step-by-step reasoning process provides physicians with strong evidence to support decision-making. While reinforcement learning has effectively enhanced reasoning performance in medical contexts, the clinical reliability of these reasoning processes remains limited because their accuracy and validity are often overlooked during training. To address this gap, we propose MedCEG, a framework that augments medical language models with clinically valid reasoning pathways by explicitly supervising the reasoning process through a Critical Evidence Graph (CEG). We curate a dataset of challenging clinical cases and algorithmically construct a CEG for each sample to represent a high-quality verifiable reasoning pathway. To guide the reasoning process, we introduce a Clinical Reasoning Procedure Reward, which evaluates Node Coverage, Structural Correctness, and Chain Completeness, thereby providing a holistic assessment of reasoning quality. Experimental results show that MedCEG surpasses existing methods in performance while producing clinically valid reasoning chains, representing a solid advancement in reliable medical AI reasoning. The code and models are available at https://github.com/LinjieMu/MedCEG.", "AI": {"tldr": "MedCEG\u6846\u67b6\u901a\u8fc7\u5173\u952e\u8bc1\u636e\u56fe\u76d1\u7763\u533b\u5b66\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u63d0\u5347\u4e34\u5e8a\u63a8\u7406\u7684\u53ef\u9760\u6027\u548c\u6709\u6548\u6027", "motivation": "\u73b0\u6709\u533b\u5b66\u63a8\u7406\u6a21\u578b\u867d\u7136\u6027\u80fd\u6709\u6240\u63d0\u5347\uff0c\u4f46\u5176\u63a8\u7406\u8fc7\u7a0b\u7684\u4e34\u5e8a\u53ef\u9760\u6027\u6709\u9650\uff0c\u56e0\u4e3a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5f80\u5f80\u5ffd\u7565\u4e86\u63a8\u7406\u7684\u51c6\u786e\u6027\u548c\u6709\u6548\u6027\u9a8c\u8bc1", "method": "\u63d0\u51faMedCEG\u6846\u67b6\uff0c\u901a\u8fc7\u5173\u952e\u8bc1\u636e\u56fe(CEG)\u663e\u5f0f\u76d1\u7763\u63a8\u7406\u8fc7\u7a0b\uff1b\u6784\u5efa\u6311\u6218\u6027\u4e34\u5e8a\u6848\u4f8b\u6570\u636e\u96c6\uff0c\u4e3a\u6bcf\u4e2a\u6837\u672c\u7b97\u6cd5\u6784\u5efaCEG\u8868\u793a\u9ad8\u8d28\u91cf\u53ef\u9a8c\u8bc1\u63a8\u7406\u8def\u5f84\uff1b\u5f15\u5165\u4e34\u5e8a\u63a8\u7406\u8fc7\u7a0b\u5956\u52b1\uff0c\u8bc4\u4f30\u8282\u70b9\u8986\u76d6\u3001\u7ed3\u6784\u6b63\u786e\u6027\u548c\u94fe\u5b8c\u6574\u6027", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eMedCEG\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4ea7\u751f\u4e34\u5e8a\u6709\u6548\u7684\u63a8\u7406\u94fe\uff0c\u4ee3\u8868\u4e86\u53ef\u9760\u533b\u5b66AI\u63a8\u7406\u7684\u5b9e\u8d28\u6027\u8fdb\u5c55", "conclusion": "MedCEG\u901a\u8fc7\u5173\u952e\u8bc1\u636e\u56fe\u76d1\u7763\u548c\u4e34\u5e8a\u63a8\u7406\u8fc7\u7a0b\u5956\u52b1\uff0c\u6709\u6548\u63d0\u5347\u4e86\u533b\u5b66\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u53ef\u9760\u6027\u548c\u4e34\u5e8a\u6709\u6548\u6027\uff0c\u4e3a\u53ef\u9760\u533b\u5b66AI\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u6846\u67b6"}}
{"id": "2512.12384", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12384", "abs": "https://arxiv.org/abs/2512.12384", "authors": ["Jesse Ponnock"], "title": "The Data Efficiency Frontier of Financial Foundation Models: Scaling Laws from Continued Pretraining", "comment": "8 pages, 4 figures, 1 table", "summary": "Domain-adaptive pretraining (DAPT) offers a practical path to specializing large language models for high-value domains without full retraining. We conduct an early-stage scaling-law analysis of continued pretraining on U.S. SEC filings, training 1B and 3B-parameter Llama-3.2 models on a 400M-token financial corpus with validation checkpoints at 50M, 100M, 200M, and 400M tokens. Results show consistent improvements in SEC-domain validation loss for both models, with the largest gains occurring within the first 200M tokens and diminishing returns thereafter. Power-law fits reveal shallow exponents, indicating that financial language is highly regular and efficiently learnable under continued pretraining. General-domain validation loss remains effectively unchanged across all token budgets, suggesting minimal drift and no signs of catastrophic forgetting. A data-efficiency frontier further shows that both models move toward improved specialization with negligible mixed-domain degradation. Together, these findings provide early empirical guidance for scaling financial foundation models, suggesting that meaningful domain adaptation can be achieved with comparatively modest token budgets and that larger model scales (7B-70B) remain tractable under projected data requirements.", "AI": {"tldr": "\u5bf91B\u548c3B\u53c2\u6570\u7684Llama-3.2\u6a21\u578b\u5728400M\u91d1\u878d\u8bed\u6599\u4e0a\u8fdb\u884c\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\uff0c\u7ed3\u679c\u663e\u793a\u524d200M token\u6548\u679c\u63d0\u5347\u6700\u660e\u663e\uff0c\u91d1\u878d\u8bed\u8a00\u9ad8\u5ea6\u89c4\u5f8b\u4e14\u6613\u5b66\uff0c\u901a\u7528\u9886\u57df\u6027\u80fd\u4fdd\u6301\u7a33\u5b9a\u3002", "motivation": "\u63a2\u7d22\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\uff08DAPT\uff09\u4f5c\u4e3a\u4e13\u95e8\u5316\u5927\u8bed\u8a00\u6a21\u578b\u5230\u9ad8\u4ef7\u503c\u9886\u57df\uff08\u5982\u91d1\u878d\uff09\u7684\u5b9e\u7528\u8def\u5f84\uff0c\u907f\u514d\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u7684\u6210\u672c\uff0c\u4e3a\u91d1\u878d\u57fa\u7840\u6a21\u578b\u7684\u89c4\u6a21\u5316\u63d0\u4f9b\u65e9\u671f\u7ecf\u9a8c\u6307\u5bfc\u3002", "method": "\u4f7f\u75281B\u548c3B\u53c2\u6570\u7684Llama-3.2\u6a21\u578b\uff0c\u5728\u7f8e\u56fdSEC\u6587\u4ef6\u6784\u6210\u7684400M token\u91d1\u878d\u8bed\u6599\u4e0a\u8fdb\u884c\u6301\u7eed\u9884\u8bad\u7ec3\uff0c\u572850M\u3001100M\u3001200M\u548c400M token\u5904\u8bbe\u7f6e\u9a8c\u8bc1\u68c0\u67e5\u70b9\uff0c\u5206\u6790\u9886\u57df\u9002\u5e94\u6548\u679c\u548c\u7f29\u653e\u89c4\u5f8b\u3002", "result": "\u4e24\u4e2a\u6a21\u578b\u5728SEC\u9886\u57df\u9a8c\u8bc1\u635f\u5931\u5747\u6301\u7eed\u6539\u5584\uff0c\u6700\u5927\u63d0\u5347\u51fa\u73b0\u5728\u524d200M token\uff0c\u4e4b\u540e\u6536\u76ca\u9012\u51cf\uff1b\u5e42\u5f8b\u62df\u5408\u663e\u793a\u6307\u6570\u8f83\u6d45\uff0c\u8868\u660e\u91d1\u878d\u8bed\u8a00\u9ad8\u5ea6\u89c4\u5f8b\u4e14\u6613\u5b66\uff1b\u901a\u7528\u9886\u57df\u9a8c\u8bc1\u635f\u5931\u57fa\u672c\u4e0d\u53d8\uff0c\u65e0\u707e\u96be\u6027\u9057\u5fd8\u8ff9\u8c61\uff1b\u6570\u636e\u6548\u7387\u524d\u6cbf\u663e\u793a\u6a21\u578b\u5411\u4e13\u4e1a\u5316\u6539\u8fdb\uff0c\u6df7\u5408\u9886\u57df\u9000\u5316\u53ef\u5ffd\u7565\u3002", "conclusion": "\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\u80fd\u4ee5\u76f8\u5bf9\u9002\u4e2d\u7684token\u9884\u7b97\u5b9e\u73b0\u6709\u610f\u4e49\u7684\u9886\u57df\u9002\u5e94\uff0c\u91d1\u878d\u8bed\u8a00\u7684\u9ad8\u5ea6\u89c4\u5f8b\u6027\u4f7f\u5176\u5b66\u4e60\u6548\u7387\u9ad8\uff0c\u66f4\u5927\u7684\u6a21\u578b\u89c4\u6a21\uff087B-70B\uff09\u5728\u9884\u8ba1\u6570\u636e\u9700\u6c42\u4e0b\u4ecd\u53ef\u5904\u7406\uff0c\u4e3a\u91d1\u878d\u57fa\u7840\u6a21\u578b\u7684\u89c4\u6a21\u5316\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u6307\u5bfc\u3002"}}
{"id": "2512.12307", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.12307", "abs": "https://arxiv.org/abs/2512.12307", "authors": ["Benjamin Beilharz", "Thomas S. A. Wallis"], "title": "MRD: Using Physically Based Differentiable Rendering to Probe Vision Models for 3D Scene Understanding", "comment": "18 pages, 6 figures. Supplementary material and code will be provided at the end of January", "summary": "While deep learning methods have achieved impressive success in many vision benchmarks, it remains difficult to understand and explain the representations and decisions of these models. Though vision models are typically trained on 2D inputs, they are often assumed to develop an implicit representation of the underlying 3D scene (for example, showing tolerance to partial occlusion, or the ability to reason about relative depth). Here, we introduce MRD (metamers rendered differentiably), an approach that uses physically based differentiable rendering to probe vision models' implicit understanding of generative 3D scene properties, by finding 3D scene parameters that are physically different but produce the same model activation (i.e. are model metamers). Unlike previous pixel-based methods for evaluating model representations, these reconstruction results are always grounded in physical scene descriptions. This means we can, for example, probe a model's sensitivity to object shape while holding material and lighting constant. As a proof-of-principle, we assess multiple models in their ability to recover scene parameters of geometry (shape) and bidirectional reflectance distribution function (material). The results show high similarity in model activation between target and optimized scenes, with varying visual results. Qualitatively, these reconstructions help investigate the physical scene attributes to which models are sensitive or invariant. MRD holds promise for advancing our understanding of both computer and human vision by enabling analysis of how physical scene parameters drive changes in model responses.", "AI": {"tldr": "MRD\u65b9\u6cd5\u901a\u8fc7\u53ef\u5fae\u5206\u6e32\u67d3\u5bfb\u627e\u7269\u7406\u4e0a\u4e0d\u540c\u4f46\u4ea7\u751f\u76f8\u540c\u6a21\u578b\u6fc0\u6d3b\u76843D\u573a\u666f\u53c2\u6570\uff0c\u7528\u4e8e\u63a2\u7a76\u89c6\u89c9\u6a21\u578b\u5bf9\u751f\u6210\u60273D\u573a\u666f\u5c5e\u6027\u7684\u9690\u5f0f\u7406\u89e3", "motivation": "\u5c3d\u7ba1\u6df1\u5ea6\u5b66\u4e60\u5728\u89c6\u89c9\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u6a21\u578b\u8868\u793a\u548c\u51b3\u7b56\u96be\u4ee5\u89e3\u91ca\u3002\u867d\u7136\u89c6\u89c9\u6a21\u578b\u901a\u5e38\u57282D\u8f93\u5165\u4e0a\u8bad\u7ec3\uff0c\u4f46\u5e38\u88ab\u5047\u8bbe\u80fd\u53d1\u5c55\u5bf9\u5e95\u5c423D\u573a\u666f\u7684\u9690\u5f0f\u7406\u89e3\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u63a2\u7a76\u6a21\u578b\u5bf9\u751f\u6210\u60273D\u573a\u666f\u5c5e\u6027\u7684\u7406\u89e3", "method": "\u63d0\u51faMRD\u65b9\u6cd5\uff0c\u4f7f\u7528\u57fa\u4e8e\u7269\u7406\u7684\u53ef\u5fae\u5206\u6e32\u67d3\uff0c\u5bfb\u627e\u7269\u7406\u4e0a\u4e0d\u540c\u4f46\u4ea7\u751f\u76f8\u540c\u6a21\u578b\u6fc0\u6d3b\u76843D\u573a\u666f\u53c2\u6570\uff08\u6a21\u578b\u5143\u76f8\u4f3c\u4f53\uff09\u3002\u4e0e\u4e4b\u524d\u57fa\u4e8e\u50cf\u7d20\u7684\u65b9\u6cd5\u4e0d\u540c\uff0c\u8fd9\u4e9b\u91cd\u5efa\u7ed3\u679c\u59cb\u7ec8\u57fa\u4e8e\u7269\u7406\u573a\u666f\u63cf\u8ff0\uff0c\u53ef\u4ee5\u5206\u79bb\u63a2\u7a76\u6a21\u578b\u5bf9\u4e0d\u540c\u573a\u666f\u5c5e\u6027\u7684\u654f\u611f\u6027", "result": "\u8bc4\u4f30\u4e86\u591a\u4e2a\u6a21\u578b\u5728\u6062\u590d\u573a\u666f\u51e0\u4f55\u5f62\u72b6\u548c\u6750\u6599BRDF\u53c2\u6570\u65b9\u9762\u7684\u80fd\u529b\u3002\u7ed3\u679c\u663e\u793a\u76ee\u6807\u548c\u4f18\u5316\u573a\u666f\u4e4b\u95f4\u7684\u6a21\u578b\u6fc0\u6d3b\u9ad8\u5ea6\u76f8\u4f3c\uff0c\u4f46\u89c6\u89c9\u7ed3\u679c\u5404\u5f02\u3002\u91cd\u5efa\u7ed3\u679c\u6709\u52a9\u4e8e\u5b9a\u6027\u5730\u7814\u7a76\u6a21\u578b\u5bf9\u54ea\u4e9b\u7269\u7406\u573a\u666f\u5c5e\u6027\u654f\u611f\u6216\u4e0d\u654f\u611f", "conclusion": "MRD\u65b9\u6cd5\u901a\u8fc7\u5206\u6790\u7269\u7406\u573a\u666f\u53c2\u6570\u5982\u4f55\u9a71\u52a8\u6a21\u578b\u54cd\u5e94\u53d8\u5316\uff0c\u6709\u671b\u63a8\u8fdb\u5bf9\u8ba1\u7b97\u673a\u548c\u4eba\u7c7b\u89c6\u89c9\u7684\u7406\u89e3\u3002\u8be5\u65b9\u6cd5\u4e3a\u63a2\u7a76\u89c6\u89c9\u6a21\u578b\u7684\u9690\u5f0f3D\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u7684\u5206\u6790\u5de5\u5177"}}
{"id": "2512.12387", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12387", "abs": "https://arxiv.org/abs/2512.12387", "authors": ["Yawen Shao", "Jie Xiao", "Kai Zhu", "Yu Liu", "Wei Zhai", "Yang Cao", "Zheng-Jun Zha"], "title": "Anchoring Values in Temporal and Group Dimensions for Flow Matching Model Alignment", "comment": null, "summary": "Group Relative Policy Optimization (GRPO) has proven highly effective in enhancing the alignment capabilities of Large Language Models (LLMs). However, current adaptations of GRPO for the flow matching-based image generation neglect a foundational conflict between its core principles and the distinct dynamics of the visual synthesis process. This mismatch leads to two key limitations: (i) Uniformly applying a sparse terminal reward across all timesteps impairs temporal credit assignment, ignoring the differing criticality of generation phases from early structure formation to late-stage tuning. (ii) Exclusive reliance on relative, intra-group rewards causes the optimization signal to fade as training converges, leading to the optimization stagnation when reward diversity is entirely depleted. To address these limitations, we propose Value-Anchored Group Policy Optimization (VGPO), a framework that redefines value estimation across both temporal and group dimensions. Specifically, VGPO transforms the sparse terminal reward into dense, process-aware value estimates, enabling precise credit assignment by modeling the expected cumulative reward at each generative stage. Furthermore, VGPO replaces standard group normalization with a novel process enhanced by absolute values to maintain a stable optimization signal even as reward diversity declines. Extensive experiments on three benchmarks demonstrate that VGPO achieves state-of-the-art image quality while simultaneously improving task-specific accuracy, effectively mitigating reward hacking. Project webpage: https://yawen-shao.github.io/VGPO/.", "AI": {"tldr": "VGPO\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3GRPO\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u65f6\u95f4\u7ef4\u5ea6\u548c\u7fa4\u4f53\u7ef4\u5ea6\u7684\u4ef7\u503c\u4f30\u8ba1\u4f18\u5316\uff0c\u63d0\u5347\u56fe\u50cf\u8d28\u91cf\u548c\u4efb\u52a1\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524dGRPO\u65b9\u6cd5\u5728\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u56fe\u50cf\u751f\u6210\u4e2d\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a1\uff09\u7a00\u758f\u7684\u7ec8\u7aef\u5956\u52b1\u5728\u6240\u6709\u65f6\u95f4\u6b65\u5747\u5300\u5e94\u7528\uff0c\u635f\u5bb3\u4e86\u65f6\u95f4\u4fe1\u7528\u5206\u914d\uff0c\u5ffd\u7565\u4e86\u4ece\u65e9\u671f\u7ed3\u6784\u5f62\u6210\u5230\u540e\u671f\u8c03\u6574\u7684\u4e0d\u540c\u5173\u952e\u9636\u6bb5\uff1b2\uff09\u4ec5\u4f9d\u8d56\u76f8\u5bf9\u7fa4\u4f53\u5185\u5956\u52b1\u5bfc\u81f4\u4f18\u5316\u4fe1\u53f7\u968f\u7740\u8bad\u7ec3\u6536\u655b\u800c\u51cf\u5f31\uff0c\u5f53\u5956\u52b1\u591a\u6837\u6027\u8017\u5c3d\u65f6\u51fa\u73b0\u4f18\u5316\u505c\u6ede\u3002", "method": "VGPO\u6846\u67b6\u91cd\u65b0\u5b9a\u4e49\u4e86\u65f6\u95f4\u548c\u7fa4\u4f53\u7ef4\u5ea6\u7684\u4ef7\u503c\u4f30\u8ba1\uff1a1\uff09\u5c06\u7a00\u758f\u7ec8\u7aef\u5956\u52b1\u8f6c\u5316\u4e3a\u5bc6\u96c6\u7684\u3001\u8fc7\u7a0b\u611f\u77e5\u7684\u4ef7\u503c\u4f30\u8ba1\uff0c\u901a\u8fc7\u5efa\u6a21\u6bcf\u4e2a\u751f\u6210\u9636\u6bb5\u7684\u9884\u671f\u7d2f\u79ef\u5956\u52b1\u5b9e\u73b0\u7cbe\u786e\u4fe1\u7528\u5206\u914d\uff1b2\uff09\u7528\u7edd\u5bf9\u4ef7\u503c\u589e\u5f3a\u7684\u65b0\u8fc7\u7a0b\u66ff\u4ee3\u6807\u51c6\u7fa4\u4f53\u5f52\u4e00\u5316\uff0c\u5373\u4f7f\u5728\u5956\u52b1\u591a\u6837\u6027\u4e0b\u964d\u65f6\u4e5f\u80fd\u4fdd\u6301\u7a33\u5b9a\u7684\u4f18\u5316\u4fe1\u53f7\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cVGPO\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u56fe\u50cf\u8d28\u91cf\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u4efb\u52a1\u7279\u5b9a\u51c6\u786e\u6027\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u5956\u52b1\u9ed1\u5ba2\u95ee\u9898\u3002", "conclusion": "VGPO\u901a\u8fc7\u91cd\u65b0\u8bbe\u8ba1\u4ef7\u503c\u4f30\u8ba1\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86GRPO\u5728\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u4f18\u5316\u6846\u67b6\u3002"}}
{"id": "2512.12309", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12309", "abs": "https://arxiv.org/abs/2512.12309", "authors": ["Shenghao Fu", "Yukun Su", "Fengyun Rao", "Jing Lyu", "Xiaohua Xie", "Wei-Shi Zheng"], "title": "WeDetect: Fast Open-Vocabulary Object Detection as Retrieval", "comment": null, "summary": "Open-vocabulary object detection aims to detect arbitrary classes via text prompts. Methods without cross-modal fusion layers (non-fusion) offer faster inference by treating recognition as a retrieval problem, \\ie, matching regions to text queries in a shared embedding space. In this work, we fully explore this retrieval philosophy and demonstrate its unique advantages in efficiency and versatility through a model family named WeDetect: (1) State-of-the-art performance. WeDetect is a real-time detector with a dual-tower architecture. We show that, with well-curated data and full training, the non-fusion WeDetect surpasses other fusion models and establishes a strong open-vocabulary foundation. (2) Fast backtrack of historical data. WeDetect-Uni is a universal proposal generator based on WeDetect. We freeze the entire detector and only finetune an objectness prompt to retrieve generic object proposals across categories. Importantly, the proposal embeddings are class-specific and enable a new application, object retrieval, supporting retrieval objects in historical data. (3) Integration with LMMs for referring expression comprehension (REC). We further propose WeDetect-Ref, an LMM-based object classifier to handle complex referring expressions, which retrieves target objects from the proposal list extracted by WeDetect-Uni. It discards next-token prediction and classifies objects in a single forward pass. Together, the WeDetect family unifies detection, proposal generation, object retrieval, and REC under a coherent retrieval framework, achieving state-of-the-art performance across 15 benchmarks with high inference efficiency.", "AI": {"tldr": "WeDetect\u662f\u4e00\u4e2a\u57fa\u4e8e\u68c0\u7d22\u54f2\u5b66\u7684\u5f00\u96c6\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u5bb6\u65cf\uff0c\u901a\u8fc7\u53cc\u5854\u67b6\u6784\u5b9e\u73b0\u5b9e\u65f6\u68c0\u6d4b\uff0c\u572815\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u5e76\u652f\u6301\u5386\u53f2\u6570\u636e\u56de\u6eaf\u548c\u4e0eLMMs\u96c6\u6210", "motivation": "\u63a2\u7d22\u65e0\u8de8\u6a21\u6001\u878d\u5408\u5c42\u7684\u68c0\u7d22\u5f0f\u5f00\u96c6\u76ee\u6807\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5145\u5206\u53d1\u6325\u5176\u5728\u63a8\u7406\u6548\u7387\u548c\u591a\u529f\u80fd\u6027\u65b9\u9762\u7684\u72ec\u7279\u4f18\u52bf\uff0c\u5efa\u7acb\u7edf\u4e00\u7684\u5f00\u96c6\u76ee\u6807\u68c0\u6d4b\u57fa\u7840", "method": "\u63d0\u51faWeDetect\u6a21\u578b\u5bb6\u65cf\uff1a1) WeDetect\u91c7\u7528\u53cc\u5854\u67b6\u6784\uff0c\u5728\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\u4e2d\u5c06\u533a\u57df\u4e0e\u6587\u672c\u67e5\u8be2\u5339\u914d\uff1b2) WeDetect-Uni\u4f5c\u4e3a\u901a\u7528\u5efa\u8bae\u751f\u6210\u5668\uff0c\u51bb\u7ed3\u68c0\u6d4b\u5668\u4ec5\u5fae\u8c03\u76ee\u6807\u6027\u63d0\u793a\uff1b3) WeDetect-Ref\u57fa\u4e8eLMM\u7684\u5bf9\u8c61\u5206\u7c7b\u5668\uff0c\u5904\u7406\u590d\u6742\u6307\u4ee3\u8868\u8fbe\u5f0f", "result": "\u572815\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u652f\u6301\u5b9e\u65f6\u68c0\u6d4b\u3001\u5386\u53f2\u6570\u636e\u5bf9\u8c61\u68c0\u7d22\u3001\u590d\u6742\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\u7b49\u591a\u79cd\u5e94\u7528\uff0c\u5177\u6709\u9ad8\u63a8\u7406\u6548\u7387", "conclusion": "WeDetect\u5bb6\u65cf\u5728\u7edf\u4e00\u7684\u68c0\u7d22\u6846\u67b6\u4e0b\u6574\u5408\u4e86\u68c0\u6d4b\u3001\u5efa\u8bae\u751f\u6210\u3001\u5bf9\u8c61\u68c0\u7d22\u548c\u6307\u4ee3\u8868\u8fbe\u7406\u89e3\uff0c\u5c55\u793a\u4e86\u68c0\u7d22\u5f0f\u65b9\u6cd5\u5728\u5f00\u96c6\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b\u548c\u591a\u529f\u80fd\u6027"}}
{"id": "2512.12339", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12339", "abs": "https://arxiv.org/abs/2512.12339", "authors": ["Maurya Goyal", "Anuj Singh", "Hadi Jamali-Rad"], "title": "Unified Control for Inference-Time Guidance of Denoising Diffusion Models", "comment": null, "summary": "Aligning diffusion model outputs with downstream objectives is essential for improving task-specific performance. Broadly, inference-time training-free approaches for aligning diffusion models can be categorized into two main strategies: sampling-based methods, which explore multiple candidate outputs and select those with higher reward signals, and gradient-guided methods, which use differentiable reward approximations to directly steer the generation process. In this work, we propose a universal algorithm, UniCoDe, which brings together the strengths of sampling and gradient-based guidance into a unified framework. UniCoDe integrates local gradient signals during sampling, thereby addressing the sampling inefficiency inherent in complex reward-based sampling approaches. By cohesively combining these two paradigms, UniCoDe enables more efficient sampling while offering better trade-offs between reward alignment and divergence from the diffusion unconditional prior. Empirical results demonstrate that UniCoDe remains competitive with state-of-the-art baselines across a range of tasks. The code is available at https://github.com/maurya-goyal10/UniCoDe", "AI": {"tldr": "UniCoDe\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u7b97\u6cd5\uff0c\u5c06\u91c7\u6837\u65b9\u6cd5\u548c\u68af\u5ea6\u5f15\u5bfc\u65b9\u6cd5\u7ed3\u5408\uff0c\u901a\u8fc7\u6574\u5408\u5c40\u90e8\u68af\u5ea6\u4fe1\u53f7\u63d0\u9ad8\u91c7\u6837\u6548\u7387\uff0c\u5728\u5956\u52b1\u5bf9\u9f50\u548c\u6269\u6563\u65e0\u6761\u4ef6\u5148\u9a8c\u504f\u79bb\u4e4b\u95f4\u5b9e\u73b0\u66f4\u597d\u7684\u6743\u8861\u3002", "motivation": "\u5bf9\u9f50\u6269\u6563\u6a21\u578b\u8f93\u51fa\u4e0e\u4e0b\u6e38\u76ee\u6807\u5bf9\u63d0\u9ad8\u4efb\u52a1\u7279\u5b9a\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u63a8\u7406\u65f6\u65e0\u8bad\u7ec3\u65b9\u6cd5\u4e3b\u8981\u5206\u4e3a\u91c7\u6837\u65b9\u6cd5\u548c\u68af\u5ea6\u5f15\u5bfc\u65b9\u6cd5\uff0c\u4f46\u5404\u6709\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u7edf\u4e00\u6846\u67b6\u6765\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\u3002", "method": "\u63d0\u51faUniCoDe\u7b97\u6cd5\uff0c\u5c06\u91c7\u6837\u65b9\u6cd5\u548c\u68af\u5ea6\u5f15\u5bfc\u65b9\u6cd5\u7edf\u4e00\u5230\u4e00\u4e2a\u6846\u67b6\u4e2d\u3002\u5728\u91c7\u6837\u8fc7\u7a0b\u4e2d\u6574\u5408\u5c40\u90e8\u68af\u5ea6\u4fe1\u53f7\uff0c\u89e3\u51b3\u590d\u6742\u5956\u52b1\u91c7\u6837\u65b9\u6cd5\u7684\u6548\u7387\u95ee\u9898\uff0c\u540c\u65f6\u7ed3\u5408\u4e24\u79cd\u8303\u5f0f\u7684\u4f18\u52bf\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cUniCoDe\u5728\u4e00\u7cfb\u5217\u4efb\u52a1\u4e2d\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u4fdd\u6301\u7ade\u4e89\u529b\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u91c7\u6837\uff0c\u5e76\u5728\u5956\u52b1\u5bf9\u9f50\u548c\u6269\u6563\u65e0\u6761\u4ef6\u5148\u9a8c\u504f\u79bb\u4e4b\u95f4\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u6743\u8861\u3002", "conclusion": "UniCoDe\u6210\u529f\u5730\u5c06\u91c7\u6837\u548c\u68af\u5ea6\u5f15\u5bfc\u65b9\u6cd5\u7edf\u4e00\u5230\u4e00\u4e2a\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7\u6574\u5408\u5c40\u90e8\u68af\u5ea6\u4fe1\u53f7\u63d0\u9ad8\u4e86\u91c7\u6837\u6548\u7387\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u5bf9\u9f50\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12357", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12357", "abs": "https://arxiv.org/abs/2512.12357", "authors": ["Zishen Song", "Yongjian Zhu", "Dong Wang", "Hongzhan Liu", "Lingyu Jiang", "Yongxing Duan", "Zehua Zhang", "Sihan Li", "Jiarui Li"], "title": "TCLeaf-Net: a transformer-convolution framework with global-local attention for robust in-field lesion-level plant leaf disease detection", "comment": null, "summary": "Timely and accurate detection of foliar diseases is vital for safeguarding crop growth and reducing yield losses. Yet, in real-field conditions, cluttered backgrounds, domain shifts, and limited lesion-level datasets hinder robust modeling. To address these challenges, we release Daylily-Leaf, a paired lesion-level dataset comprising 1,746 RGB images and 7,839 lesions captured under both ideal and in-field conditions, and propose TCLeaf-Net, a transformer-convolution hybrid detector optimized for real-field use. TCLeaf-Net is designed to tackle three major challenges. To mitigate interference from complex backgrounds, the transformer-convolution module (TCM) couples global context with locality-preserving convolution to suppress non-leaf regions. To reduce information loss during downsampling, the raw-scale feature recalling and sampling (RSFRS) block combines bilinear resampling and convolution to preserve fine spatial detail. To handle variations in lesion scale and feature shifts, the deformable alignment block with FPN (DFPN) employs offset-based alignment and multi-receptive-field perception to strengthen multi-scale fusion. Experimental results show that on the in-field split of the Daylily-Leaf dataset, TCLeaf-Net improves mAP@50 by 5.4 percentage points over the baseline model, reaching 78.2\\%, while reducing computation by 7.5 GFLOPs and GPU memory usage by 8.7\\%. Moreover, the model outperforms recent YOLO and RT-DETR series in both precision and recall, and demonstrates strong performance on the PlantDoc, Tomato-Leaf, and Rice-Leaf datasets, validating its robustness and generalizability to other plant disease detection scenarios.", "AI": {"tldr": "\u63d0\u51faTCLeaf-Net\u7528\u4e8e\u7530\u95f4\u53f6\u7247\u75c5\u5bb3\u68c0\u6d4b\uff0c\u7ed3\u5408Transformer\u548cCNN\u5904\u7406\u590d\u6742\u80cc\u666f\uff0c\u901a\u8fc7\u7279\u5f81\u4fdd\u7559\u548c\u53ef\u53d8\u5f62\u5bf9\u9f50\u63d0\u5347\u591a\u5c3a\u5ea6\u878d\u5408\uff0c\u5728Daylily-Leaf\u6570\u636e\u96c6\u4e0amAP@50\u8fbe78.2%\uff0c\u4f18\u4e8e\u73b0\u6709YOLO\u548cRT-DETR\u6a21\u578b\u3002", "motivation": "\u7530\u95f4\u53f6\u7247\u75c5\u5bb3\u68c0\u6d4b\u9762\u4e34\u590d\u6742\u80cc\u666f\u5e72\u6270\u3001\u57df\u504f\u79fb\u548c\u75c5\u53d8\u7ea7\u522b\u6570\u636e\u96c6\u6709\u9650\u7b49\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u9c81\u68d2\u4e14\u5b9e\u7528\u7684\u68c0\u6d4b\u6a21\u578b\u3002", "method": "1) \u53d1\u5e03Daylily-Leaf\u914d\u5bf9\u75c5\u53d8\u7ea7\u522b\u6570\u636e\u96c6\uff1b2) \u63d0\u51faTCLeaf-Net\u6df7\u5408\u68c0\u6d4b\u5668\uff1aTransformer-\u5377\u79ef\u6a21\u5757(TCM)\u6291\u5236\u975e\u53f6\u7247\u533a\u57df\uff0c\u539f\u59cb\u5c3a\u5ea6\u7279\u5f81\u53ec\u56de\u91c7\u6837(RSFRS)\u4fdd\u7559\u7a7a\u95f4\u7ec6\u8282\uff0c\u53ef\u53d8\u5f62\u5bf9\u9f50FPN(DFPN)\u589e\u5f3a\u591a\u5c3a\u5ea6\u878d\u5408\u3002", "result": "\u5728Daylily-Leaf\u7530\u95f4\u6570\u636e\u96c6\u4e0a\uff0cmAP@50\u63d0\u53475.4\u4e2a\u767e\u5206\u70b9\u81f378.2%\uff0c\u8ba1\u7b97\u91cf\u51cf\u5c117.5 GFLOPs\uff0cGPU\u5185\u5b58\u4f7f\u7528\u964d\u4f4e8.7%\uff0c\u4f18\u4e8eYOLO\u548cRT-DETR\u7cfb\u5217\uff0c\u5728PlantDoc\u7b49\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u826f\u597d\u3002", "conclusion": "TCLeaf-Net\u80fd\u6709\u6548\u5904\u7406\u7530\u95f4\u590d\u6742\u80cc\u666f\u4e0b\u7684\u53f6\u7247\u75c5\u5bb3\u68c0\u6d4b\uff0c\u5177\u6709\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u5b9e\u9645\u519c\u4e1a\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12436", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12436", "abs": "https://arxiv.org/abs/2512.12436", "authors": ["Bart\u0142omiej Starosta", "S\u0142awomir T. Wierzcho\u0144", "Piotr Borkowski", "Dariusz Czerski", "Marcin Sydow", "Eryk Laskowski", "Mieczys\u0142aw A. K\u0142opotek"], "title": "Rough Sets for Explainability of Spectral Graph Clustering", "comment": "24 figures, 21tables", "summary": "Graph Spectral Clustering methods (GSC) allow representing clusters of diverse shapes, densities, etc. However, the results of such algorithms, when applied e.g. to text documents, are hard to explain to the user, especially due to embedding in the spectral space which has no obvious relation to document contents. Furthermore, the presence of documents without clear content meaning and the stochastic nature of the clustering algorithms deteriorate explainability. This paper proposes an enhancement to the explanation methodology, proposed in an earlier research of our team. It allows us to overcome the latter problems by taking inspiration from rough set theory.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7c97\u7cd9\u96c6\u7406\u8bba\u7684\u56fe\u8c31\u805a\u7c7b\u89e3\u91ca\u65b9\u6cd5\u589e\u5f3a\u65b9\u6848\uff0c\u65e8\u5728\u89e3\u51b3\u6587\u6863\u805a\u7c7b\u7ed3\u679c\u96be\u4ee5\u89e3\u91ca\u7684\u95ee\u9898\u3002", "motivation": "\u56fe\u8c31\u805a\u7c7b\u65b9\u6cd5\u5728\u5904\u7406\u6587\u6863\u7b49\u6570\u636e\u65f6\uff0c\u7531\u4e8e\u5d4c\u5165\u5230\u8c31\u7a7a\u95f4\u4e14\u4e0e\u6587\u6863\u5185\u5bb9\u6ca1\u6709\u660e\u663e\u5173\u8054\uff0c\u805a\u7c7b\u7ed3\u679c\u96be\u4ee5\u5411\u7528\u6237\u89e3\u91ca\u3002\u6b64\u5916\uff0c\u6587\u6863\u5185\u5bb9\u4e0d\u660e\u786e\u548c\u805a\u7c7b\u7b97\u6cd5\u7684\u968f\u673a\u6027\u8fdb\u4e00\u6b65\u964d\u4f4e\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u5728\u56e2\u961f\u5148\u524d\u7814\u7a76\u7684\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7684\u89e3\u91ca\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4ece\u7c97\u7cd9\u96c6\u7406\u8bba\u4e2d\u6c72\u53d6\u7075\u611f\uff0c\u4ee5\u89e3\u51b3\u4e0a\u8ff0\u95ee\u9898\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u514b\u670d\u6587\u6863\u5185\u5bb9\u4e0d\u660e\u786e\u548c\u7b97\u6cd5\u968f\u673a\u6027\u5e26\u6765\u7684\u89e3\u91ca\u56f0\u96be\uff0c\u63d0\u9ad8\u805a\u7c7b\u7ed3\u679c\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u57fa\u4e8e\u7c97\u7cd9\u96c6\u7406\u8bba\u7684\u589e\u5f3a\u89e3\u91ca\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u6539\u5584\u56fe\u8c31\u805a\u7c7b\u5728\u6587\u6863\u5206\u6790\u4e2d\u7684\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u4f7f\u805a\u7c7b\u7ed3\u679c\u66f4\u5bb9\u6613\u88ab\u7528\u6237\u7406\u89e3\u548c\u63a5\u53d7\u3002"}}
{"id": "2512.12372", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12372", "abs": "https://arxiv.org/abs/2512.12372", "authors": ["Peixuan Zhang", "Zijian Jia", "Kaiqi Liu", "Shuchen Weng", "Si Li", "Boxin Shi"], "title": "STAGE: Storyboard-Anchored Generation for Cinematic Multi-shot Narrative", "comment": null, "summary": "While recent advancements in generative models have achieved remarkable visual fidelity in video synthesis, creating coherent multi-shot narratives remains a significant challenge. To address this, keyframe-based approaches have emerged as a promising alternative to computationally intensive end-to-end methods, offering the advantages of fine-grained control and greater efficiency. However, these methods often fail to maintain cross-shot consistency and capture cinematic language. In this paper, we introduce STAGE, a SToryboard-Anchored GEneration workflow to reformulate the keyframe-based multi-shot video generation task. Instead of using sparse keyframes, we propose STEP2 to predict a structural storyboard composed of start-end frame pairs for each shot. We introduce the multi-shot memory pack to ensure long-range entity consistency, the dual-encoding strategy for intra-shot coherence, and the two-stage training scheme to learn cinematic inter-shot transition. We also contribute the large-scale ConStoryBoard dataset, including high-quality movie clips with fine-grained annotations for story progression, cinematic attributes, and human preferences. Extensive experiments demonstrate that STAGE achieves superior performance in structured narrative control and cross-shot coherence.", "AI": {"tldr": "STAGE\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6545\u4e8b\u677f\u7684\u591a\u955c\u5934\u89c6\u9891\u751f\u6210\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7\u9884\u6d4b\u7ed3\u6784\u5316\u6545\u4e8b\u677f\u3001\u5f15\u5165\u591a\u955c\u5934\u8bb0\u5fc6\u5305\u548c\u53cc\u7f16\u7801\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5173\u952e\u5e27\u65b9\u6cd5\u5728\u8de8\u955c\u5934\u4e00\u81f4\u6027\u548c\u7535\u5f71\u8bed\u8a00\u6355\u6349\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524d\u751f\u6210\u6a21\u578b\u5728\u89c6\u9891\u5408\u6210\u4e2d\u867d\u7136\u53d6\u5f97\u4e86\u89c6\u89c9\u4fdd\u771f\u5ea6\u7684\u663e\u8457\u8fdb\u6b65\uff0c\u4f46\u521b\u5efa\u8fde\u8d2f\u7684\u591a\u955c\u5934\u53d9\u4e8b\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u57fa\u4e8e\u5173\u952e\u5e27\u7684\u65b9\u6cd5\u867d\u7136\u63d0\u4f9b\u4e86\u7ec6\u7c92\u5ea6\u63a7\u5236\u548c\u66f4\u9ad8\u6548\u7387\uff0c\u4f46\u5f80\u5f80\u65e0\u6cd5\u4fdd\u6301\u8de8\u955c\u5934\u4e00\u81f4\u6027\uff0c\u4e5f\u96be\u4ee5\u6355\u6349\u7535\u5f71\u8bed\u8a00\u3002", "method": "1. \u63d0\u51faSTAGE\u5de5\u4f5c\u6d41\uff0c\u5c06\u57fa\u4e8e\u5173\u952e\u5e27\u7684\u591a\u955c\u5934\u89c6\u9891\u751f\u6210\u4efb\u52a1\u91cd\u65b0\u8868\u8ff0\u4e3a\u6545\u4e8b\u677f\u951a\u5b9a\u751f\u6210\uff1b2. \u5f00\u53d1STEP2\u9884\u6d4b\u6bcf\u4e2a\u955c\u5934\u7684\u8d77\u59cb-\u7ed3\u675f\u5e27\u5bf9\u7ec4\u6210\u7684\u7ed3\u6784\u5316\u6545\u4e8b\u677f\uff1b3. \u5f15\u5165\u591a\u955c\u5934\u8bb0\u5fc6\u5305\u786e\u4fdd\u957f\u8ddd\u79bb\u5b9e\u4f53\u4e00\u81f4\u6027\uff1b4. \u91c7\u7528\u53cc\u7f16\u7801\u7b56\u7565\u4fdd\u8bc1\u955c\u5934\u5185\u8fde\u8d2f\u6027\uff1b5. \u4f7f\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6848\u5b66\u4e60\u7535\u5f71\u5316\u7684\u955c\u5934\u95f4\u8fc7\u6e21\uff1b6. \u6784\u5efa\u5927\u89c4\u6a21ConStoryBoard\u6570\u636e\u96c6\uff0c\u5305\u542b\u9ad8\u8d28\u91cf\u7535\u5f71\u7247\u6bb5\u53ca\u7ec6\u7c92\u5ea6\u6807\u6ce8\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cSTAGE\u5728\u7ed3\u6784\u5316\u53d9\u4e8b\u63a7\u5236\u548c\u8de8\u955c\u5934\u8fde\u8d2f\u6027\u65b9\u9762\u5b9e\u73b0\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "STAGE\u901a\u8fc7\u6545\u4e8b\u677f\u951a\u5b9a\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u955c\u5934\u89c6\u9891\u751f\u6210\u4e2d\u7684\u8de8\u955c\u5934\u4e00\u81f4\u6027\u548c\u7535\u5f71\u8bed\u8a00\u8868\u8fbe\u95ee\u9898\uff0c\u4e3a\u521b\u5efa\u8fde\u8d2f\u7684\u53d9\u4e8b\u89c6\u9891\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12375", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12375", "abs": "https://arxiv.org/abs/2512.12375", "authors": ["Hyunkoo Lee", "Wooseok Jang", "Jini Yang", "Taehwan Kim", "Sangoh Kim", "Sangwon Jung", "Seungryong Kim"], "title": "V-Warper: Appearance-Consistent Video Diffusion Personalization via Value Warping", "comment": "Project Page: https://cvlab-kaist.github.io/V-Warper", "summary": "Video personalization aims to generate videos that faithfully reflect a user-provided subject while following a text prompt. However, existing approaches often rely on heavy video-based finetuning or large-scale video datasets, which impose substantial computational cost and are difficult to scale. Furthermore, they still struggle to maintain fine-grained appearance consistency across frames. To address these limitations, we introduce V-Warper, a training-free coarse-to-fine personalization framework for transformer-based video diffusion models. The framework enhances fine-grained identity fidelity without requiring any additional video training. (1) A lightweight coarse appearance adaptation stage leverages only a small set of reference images, which are already required for the task. This step encodes global subject identity through image-only LoRA and subject-embedding adaptation. (2) A inference-time fine appearance injection stage refines visual fidelity by computing semantic correspondences from RoPE-free mid-layer query--key features. These correspondences guide the warping of appearance-rich value representations into semantically aligned regions of the generation process, with masking ensuring spatial reliability. V-Warper significantly improves appearance fidelity while preserving prompt alignment and motion dynamics, and it achieves these gains efficiently without large-scale video finetuning.", "AI": {"tldr": "V-Warper\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u7684\u89c6\u9891\u4e2a\u6027\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u7c97\u7c92\u5ea6\u5916\u89c2\u9002\u5e94\u548c\u7ec6\u7c92\u5ea6\u5916\u89c2\u6ce8\u5165\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u6587\u672c\u5bf9\u9f50\u548c\u8fd0\u52a8\u52a8\u6001\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u5916\u89c2\u4fdd\u771f\u5ea6\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u89c6\u9891\u5fae\u8c03\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u4e2a\u6027\u5316\u65b9\u6cd5\u4f9d\u8d56\u7e41\u91cd\u7684\u89c6\u9891\u5fae\u8c03\u6216\u5927\u89c4\u6a21\u89c6\u9891\u6570\u636e\u96c6\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u540c\u65f6\u5728\u5e27\u95f4\u4fdd\u6301\u7ec6\u7c92\u5ea6\u5916\u89c2\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u8bad\u7ec3\u514d\u8d39\u6846\u67b6\uff1a1) \u8f7b\u91cf\u7ea7\u7c97\u7c92\u5ea6\u5916\u89c2\u9002\u5e94\u9636\u6bb5\uff0c\u4ec5\u4f7f\u7528\u5c11\u91cf\u53c2\u8003\u56fe\u50cf\uff0c\u901a\u8fc7\u56fe\u50cfLoRA\u548c\u4e3b\u9898\u5d4c\u5165\u9002\u5e94\u7f16\u7801\u5168\u5c40\u4e3b\u9898\u8eab\u4efd\uff1b2) \u63a8\u7406\u65f6\u7ec6\u7c92\u5ea6\u5916\u89c2\u6ce8\u5165\u9636\u6bb5\uff0c\u901a\u8fc7\u8ba1\u7b97RoPE-free\u4e2d\u95f4\u5c42\u67e5\u8be2-\u952e\u7279\u5f81\u7684\u8bed\u4e49\u5bf9\u5e94\u5173\u7cfb\uff0c\u5f15\u5bfc\u5916\u89c2\u4e30\u5bcc\u7684\u503c\u8868\u793a\u5230\u751f\u6210\u8fc7\u7a0b\u7684\u8bed\u4e49\u5bf9\u9f50\u533a\u57df\u3002", "result": "V-Warper\u663e\u8457\u63d0\u9ad8\u4e86\u5916\u89c2\u4fdd\u771f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6587\u672c\u63d0\u793a\u5bf9\u9f50\u548c\u8fd0\u52a8\u52a8\u6001\uff0c\u4e14\u65e0\u9700\u5927\u89c4\u6a21\u89c6\u9891\u5fae\u8c03\u5373\u53ef\u9ad8\u6548\u5b9e\u73b0\u8fd9\u4e9b\u6539\u8fdb\u3002", "conclusion": "V-Warper\u901a\u8fc7\u521b\u65b0\u7684\u7c97\u5230\u7ec6\u6846\u67b6\u89e3\u51b3\u4e86\u73b0\u6709\u89c6\u9891\u4e2a\u6027\u5316\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5728\u65e0\u9700\u989d\u5916\u89c6\u9891\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5916\u89c2\u4e00\u81f4\u6027\uff0c\u4e3a\u57fa\u4e8etransformer\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u4e2a\u6027\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12448", "categories": ["cs.LG", "cs.NE", "physics.data-an", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.12448", "abs": "https://arxiv.org/abs/2512.12448", "authors": ["James Bagrow", "Josh Bongard"], "title": "Optimized Architectures for Kolmogorov-Arnold Networks", "comment": "12 pages, 1 figure, 3 tables", "summary": "Efforts to improve Kolmogorov-Arnold networks (KANs) with architectural enhancements have been stymied by the complexity those enhancements bring, undermining the interpretability that makes KANs attractive in the first place. Here we study overprovisioned architectures combined with sparsification to learn compact, interpretable KANs without sacrificing accuracy. Crucially, we focus on differentiable sparsification, turning architecture search into an end-to-end optimization problem. Across function approximation benchmarks, dynamical systems forecasting, and real-world prediction tasks, we demonstrate competitive or superior accuracy while discovering substantially smaller models. Overprovisioning and sparsification are synergistic, with the combination outperforming either alone. The result is a principled path toward models that are both more expressive and more interpretable, addressing a key tension in scientific machine learning.", "AI": {"tldr": "\u901a\u8fc7\u8fc7\u53c2\u6570\u5316\u67b6\u6784\u7ed3\u5408\u53ef\u5fae\u7a00\u758f\u5316\uff0c\u5728\u4fdd\u6301KANs\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u63d0\u5347\u6027\u80fd\uff0c\u5b9e\u73b0\u66f4\u7d27\u51d1\u3001\u66f4\u51c6\u786e\u7684\u6a21\u578b", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u901a\u8fc7\u67b6\u6784\u589e\u5f3a\u6539\u8fdbKANs\u4f1a\u5f15\u5165\u590d\u6742\u6027\uff0c\u7834\u574f\u5176\u53ef\u89e3\u91ca\u6027\u4f18\u52bf\u3002\u9700\u8981\u5728\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u7684\u540c\u65f6\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "\u91c7\u7528\u8fc7\u53c2\u6570\u5316\u67b6\u6784\u7ed3\u5408\u53ef\u5fae\u7a00\u758f\u5316\u6280\u672f\uff0c\u5c06\u67b6\u6784\u641c\u7d22\u8f6c\u5316\u4e3a\u7aef\u5230\u7aef\u4f18\u5316\u95ee\u9898\uff0c\u5b66\u4e60\u7d27\u51d1\u4e14\u53ef\u89e3\u91ca\u7684KANs\u3002", "result": "\u5728\u51fd\u6570\u903c\u8fd1\u57fa\u51c6\u6d4b\u8bd5\u3001\u52a8\u6001\u7cfb\u7edf\u9884\u6d4b\u548c\u771f\u5b9e\u4e16\u754c\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u5b9e\u73b0\u4e86\u7ade\u4e89\u6027\u6216\u66f4\u4f18\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u53d1\u73b0\u4e86\u663e\u8457\u66f4\u5c0f\u7684\u6a21\u578b\u3002\u8fc7\u53c2\u6570\u5316\u4e0e\u7a00\u758f\u5316\u5177\u6709\u534f\u540c\u6548\u5e94\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u65b9\u6cd5\uff0c\u65e2\u80fd\u83b7\u5f97\u66f4\u5177\u8868\u8fbe\u80fd\u529b\u7684\u6a21\u578b\uff0c\u53c8\u80fd\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\uff0c\u89e3\u51b3\u4e86\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u5173\u952e\u77db\u76fe\u3002"}}
{"id": "2512.12378", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12378", "abs": "https://arxiv.org/abs/2512.12378", "authors": ["Junqiao Fan", "Yunjiao Zhou", "Yizhuo Yang", "Xinyuan Cui", "Jiarui Zhang", "Lihua Xie", "Jianfei Yang", "Chris Xiaoxuan Lu", "Fangqiang Ding"], "title": "M4Human: A Large-Scale Multimodal mmWave Radar Benchmark for Human Mesh Reconstruction", "comment": null, "summary": "Human mesh reconstruction (HMR) provides direct insights into body-environment interaction, which enables various immersive applications. While existing large-scale HMR datasets rely heavily on line-of-sight RGB input, vision-based sensing is limited by occlusion, lighting variation, and privacy concerns. To overcome these limitations, recent efforts have explored radio-frequency (RF) mmWave radar for privacy-preserving indoor human sensing. However, current radar datasets are constrained by sparse skeleton labels, limited scale, and simple in-place actions. To advance the HMR research community, we introduce M4Human, the current largest-scale (661K-frame) ($9\\times$ prior largest) multimodal benchmark, featuring high-resolution mmWave radar, RGB, and depth data. M4Human provides both raw radar tensors (RT) and processed radar point clouds (RPC) to enable research across different levels of RF signal granularity. M4Human includes high-quality motion capture (MoCap) annotations with 3D meshes and global trajectories, and spans 20 subjects and 50 diverse actions, including in-place, sit-in-place, and free-space sports or rehabilitation movements. We establish benchmarks on both RT and RPC modalities, as well as multimodal fusion with RGB-D modalities. Extensive results highlight the significance of M4Human for radar-based human modeling while revealing persistent challenges under fast, unconstrained motion. The dataset and code will be released after the paper publication.", "AI": {"tldr": "M4Human\u662f\u76ee\u524d\u6700\u5927\u89c4\u6a21\u7684\u591a\u6a21\u6001\u4eba\u4f53\u7f51\u683c\u91cd\u5efa\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b66.1\u4e07\u5e27\u9ad8\u5206\u8fa8\u7387\u6beb\u7c73\u6ce2\u96f7\u8fbe\u3001RGB\u548c\u6df1\u5ea6\u6570\u636e\uff0c\u63d0\u4f9b\u539f\u59cb\u96f7\u8fbe\u5f20\u91cf\u548c\u5904\u7406\u540e\u7684\u96f7\u8fbe\u70b9\u4e91\uff0c\u652f\u6301\u4e0d\u540c\u7c92\u5ea6RF\u4fe1\u53f7\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u5927\u89c4\u6a21\u4eba\u4f53\u7f51\u683c\u91cd\u5efa\u6570\u636e\u96c6\u4e3b\u8981\u4f9d\u8d56\u53ef\u89c1\u5149RGB\u8f93\u5165\uff0c\u4f46\u89c6\u89c9\u4f20\u611f\u5b58\u5728\u906e\u6321\u3001\u5149\u7167\u53d8\u5316\u548c\u9690\u79c1\u95ee\u9898\u3002\u6beb\u7c73\u6ce2\u96f7\u8fbe\u80fd\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u7684\u5ba4\u5185\u4eba\u4f53\u611f\u77e5\uff0c\u4f46\u73b0\u6709\u96f7\u8fbe\u6570\u636e\u96c6\u5b58\u5728\u9aa8\u67b6\u6807\u7b7e\u7a00\u758f\u3001\u89c4\u6a21\u6709\u9650\u3001\u52a8\u4f5c\u7b80\u5355\u7b49\u95ee\u9898\u3002", "method": "\u6784\u5efaM4Human\u591a\u6a21\u6001\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5305\u542b661K\u5e27\u6570\u636e\uff08\u6bd4\u73b0\u6709\u6700\u5927\u6570\u636e\u96c6\u59279\u500d\uff09\uff0c\u63d0\u4f9b\u9ad8\u5206\u8fa8\u7387\u6beb\u7c73\u6ce2\u96f7\u8fbe\u3001RGB\u548c\u6df1\u5ea6\u6570\u636e\u3002\u6570\u636e\u96c6\u5305\u542b\u539f\u59cb\u96f7\u8fbe\u5f20\u91cf\u548c\u5904\u7406\u540e\u7684\u96f7\u8fbe\u70b9\u4e91\uff0c\u8986\u76d620\u540d\u53d7\u8bd5\u8005\u548c50\u79cd\u591a\u6837\u5316\u52a8\u4f5c\uff0c\u5305\u62ec\u539f\u5730\u3001\u5750\u59ff\u3001\u81ea\u7531\u7a7a\u95f4\u8fd0\u52a8\u548c\u5eb7\u590d\u52a8\u4f5c\uff0c\u5e76\u63d0\u4f9b\u9ad8\u8d28\u91cf\u8fd0\u52a8\u6355\u6349\u6807\u6ce8\uff083D\u7f51\u683c\u548c\u5168\u5c40\u8f68\u8ff9\uff09\u3002", "result": "\u5efa\u7acb\u4e86\u96f7\u8fbe\u5f20\u91cf\u548c\u96f7\u8fbe\u70b9\u4e91\u4e24\u79cd\u6a21\u6001\u7684\u57fa\u51c6\uff0c\u4ee5\u53ca\u591a\u6a21\u6001\u878d\u5408\uff08RGB-D\uff09\u57fa\u51c6\u3002\u5b9e\u9a8c\u7ed3\u679c\u7a81\u663e\u4e86M4Human\u5bf9\u96f7\u8fbe\u4eba\u4f53\u5efa\u6a21\u7684\u91cd\u8981\u6027\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u5728\u5feb\u901f\u3001\u65e0\u7ea6\u675f\u8fd0\u52a8\u4e0b\u7684\u6301\u7eed\u6311\u6218\u3002", "conclusion": "M4Human\u662f\u76ee\u524d\u6700\u5927\u89c4\u6a21\u7684\u591a\u6a21\u6001\u4eba\u4f53\u7f51\u683c\u91cd\u5efa\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u4e3a\u96f7\u8fbe\u4eba\u4f53\u5efa\u6a21\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\uff0c\u63a8\u52a8\u4e86\u9690\u79c1\u4fdd\u62a4\u4eba\u4f53\u611f\u77e5\u6280\u672f\u7684\u53d1\u5c55\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5c06\u5728\u8bba\u6587\u53d1\u8868\u540e\u516c\u5f00\u3002"}}
{"id": "2512.12461", "categories": ["cs.LG", "cs.AI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2512.12461", "abs": "https://arxiv.org/abs/2512.12461", "authors": ["Eray Erturk", "Saba Hashemi", "Maryam M. Shanechi"], "title": "Cross-Modal Representational Knowledge Distillation for Enhanced Spike-Informed LFP Modeling", "comment": "Published at the 39th Annual Conference on Neural Information Processing Systems 2025. Code is available at https://github.com/ShanechiLab/CrossModalDistillation", "summary": "Local field potentials (LFPs) can be routinely recorded alongside spiking activity in intracortical neural experiments, measure a larger complementary spatiotemporal scale of brain activity for scientific inquiry, and can offer practical advantages over spikes, including greater long-term stability, robustness to electrode degradation, and lower power requirements. Despite these advantages, recent neural modeling frameworks have largely focused on spiking activity since LFP signals pose inherent modeling challenges due to their aggregate, population-level nature, often leading to lower predictive power for downstream task variables such as motor behavior. To address this challenge, we introduce a cross-modal knowledge distillation framework that transfers high-fidelity representational knowledge from pretrained multi-session spike transformer models to LFP transformer models. Specifically, we first train a teacher spike model across multiple recording sessions using a masked autoencoding objective with a session-specific neural tokenization strategy. We then align the latent representations of the student LFP model to those of the teacher spike model. Our results show that the Distilled LFP models consistently outperform single- and multi-session LFP baselines in both fully unsupervised and supervised settings, and can generalize to other sessions without additional distillation while maintaining superior performance. These findings demonstrate that cross-modal knowledge distillation is a powerful and scalable approach for leveraging high-performing spike models to develop more accurate LFP models.", "AI": {"tldr": "\u901a\u8fc7\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\uff0c\u5c06\u9884\u8bad\u7ec3\u7684\u591a\u4f1a\u8bdd\u5c16\u5cf0Transformer\u6a21\u578b\u7684\u77e5\u8bc6\u8fc1\u79fb\u5230LFP Transformer\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86LFP\u6a21\u578b\u5728\u65e0\u76d1\u7763\u548c\u76d1\u7763\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u5c40\u90e8\u573a\u7535\u4f4d\uff08LFPs\uff09\u5728\u795e\u7ecf\u5b9e\u9a8c\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4f46\u56e0\u5176\u7fa4\u4f53\u6c34\u5e73\u7684\u805a\u5408\u7279\u6027\uff0c\u5efa\u6a21\u96be\u5ea6\u5927\uff0c\u9884\u6d4b\u4e0b\u6e38\u4efb\u52a1\u53d8\u91cf\u7684\u80fd\u529b\u901a\u5e38\u4f4e\u4e8e\u5c16\u5cf0\u4fe1\u53f7\u3002\u73b0\u6709\u795e\u7ecf\u5efa\u6a21\u6846\u67b6\u4e3b\u8981\u5173\u6ce8\u5c16\u5cf0\u6d3b\u52a8\uff0c\u800cLFPs\u7684\u5efa\u6a21\u6311\u6218\u9650\u5236\u4e86\u5176\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff1a1\uff09\u9996\u5148\u4f7f\u7528\u63a9\u7801\u81ea\u7f16\u7801\u76ee\u6807\u8bad\u7ec3\u591a\u4f1a\u8bdd\u5c16\u5cf0Transformer\u6559\u5e08\u6a21\u578b\uff0c\u91c7\u7528\u4f1a\u8bdd\u7279\u5b9a\u7684\u795e\u7ecf\u6807\u8bb0\u5316\u7b56\u7565\uff1b2\uff09\u7136\u540e\u5c06\u5b66\u751fLFP\u6a21\u578b\u7684\u6f5c\u5728\u8868\u793a\u4e0e\u6559\u5e08\u5c16\u5cf0\u6a21\u578b\u7684\u8868\u793a\u5bf9\u9f50\uff0c\u5b9e\u73b0\u77e5\u8bc6\u8fc1\u79fb\u3002", "result": "\u84b8\u998f\u540e\u7684LFP\u6a21\u578b\u5728\u5b8c\u5168\u65e0\u76d1\u7763\u548c\u76d1\u7763\u8bbe\u7f6e\u4e2d\u5747\u4f18\u4e8e\u5355\u4f1a\u8bdd\u548c\u591a\u4f1a\u8bddLFP\u57fa\u7ebf\u6a21\u578b\uff0c\u80fd\u591f\u6cdb\u5316\u5230\u5176\u4ed6\u4f1a\u8bdd\u800c\u65e0\u9700\u989d\u5916\u84b8\u998f\uff0c\u540c\u65f6\u4fdd\u6301\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u662f\u4e00\u79cd\u5f3a\u5927\u4e14\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u5229\u7528\u9ad8\u6027\u80fd\u5c16\u5cf0\u6a21\u578b\u5f00\u53d1\u66f4\u51c6\u786e\u7684LFP\u6a21\u578b\uff0c\u4e3a\u795e\u7ecf\u4fe1\u53f7\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2512.12386", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12386", "abs": "https://arxiv.org/abs/2512.12386", "authors": ["Swayam Bhanded"], "title": "Speedrunning ImageNet Diffusion", "comment": null, "summary": "Recent advances have significantly improved the training efficiency of diffusion transformers. However, these techniques have largely been studied in isolation, leaving unexplored the potential synergies from combining multiple approaches. We present SR-DiT (Speedrun Diffusion Transformer), a framework that systematically integrates token routing, architectural improvements, and training modifications on top of representation alignment. Our approach achieves FID 3.49 and KDD 0.319 on ImageNet-256 using only a 140M parameter model at 400K iterations without classifier-free guidance - comparable to results from 685M parameter models trained significantly longer. To our knowledge, this is a state-of the-art result at this model size. Through extensive ablation studies, we identify which technique combinations are most effective and document both synergies and incompatibilities. We release our framework as a computationally accessible baseline for future research.", "AI": {"tldr": "SR-DiT\u6846\u67b6\u6574\u5408\u4e86\u591a\u79cd\u6269\u6563transformer\u8bad\u7ec3\u4f18\u5316\u6280\u672f\uff0c\u5728ImageNet-256\u4e0a\u4ec5\u7528140M\u53c2\u6570\u6a21\u578b\u5c31\u8fbe\u5230\u4e86\u4e0e\u66f4\u5927\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u521b\u9020\u4e86\u8be5\u89c4\u6a21\u4e0b\u7684\u6700\u4f73\u7ed3\u679c\u3002", "motivation": "\u867d\u7136\u6269\u6563transformer\u7684\u8bad\u7ec3\u6548\u7387\u5df2\u6709\u663e\u8457\u63d0\u5347\uff0c\u4f46\u73b0\u6709\u6280\u672f\u591a\u88ab\u5b64\u7acb\u7814\u7a76\uff0c\u7f3a\u4e4f\u5bf9\u591a\u79cd\u65b9\u6cd5\u534f\u540c\u6548\u5e94\u7684\u63a2\u7d22\u3002\u672c\u7814\u7a76\u65e8\u5728\u7cfb\u7edf\u6574\u5408\u4e0d\u540c\u4f18\u5316\u6280\u672f\uff0c\u53d1\u6398\u5b83\u4eec\u7684\u534f\u540c\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e86SR-DiT\u6846\u67b6\uff0c\u5728\u8868\u793a\u5bf9\u9f50\u7684\u57fa\u7840\u4e0a\u7cfb\u7edf\u6574\u5408\u4e86\u4e09\u79cd\u5173\u952e\u6280\u672f\uff1atoken\u8def\u7531\u3001\u67b6\u6784\u6539\u8fdb\u548c\u8bad\u7ec3\u4fee\u6539\uff0c\u901a\u8fc7\u7cfb\u7edf\u7ec4\u5408\u8fd9\u4e9b\u6280\u672f\u6765\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002", "result": "\u5728ImageNet-256\u4e0a\u4ec5\u7528140M\u53c2\u6570\u6a21\u578b\uff08400K\u8fed\u4ee3\uff0c\u65e0\u5206\u7c7b\u5668\u5f15\u5bfc\uff09\u5c31\u8fbe\u5230\u4e86FID 3.49\u548cKDD 0.319\uff0c\u6027\u80fd\u4e0e\u9700\u8981\u66f4\u5927\u53c2\u6570\uff08685M\uff09\u548c\u66f4\u957f\u8bad\u7ec3\u65f6\u95f4\u7684\u6a21\u578b\u76f8\u5f53\uff0c\u521b\u9020\u4e86\u8be5\u89c4\u6a21\u4e0b\u7684\u6700\u5148\u8fdb\u7ed3\u679c\u3002", "conclusion": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u6d88\u878d\u7814\u7a76\u786e\u5b9a\u4e86\u6700\u6709\u6548\u7684\u6280\u672f\u7ec4\u5408\uff0c\u53d1\u73b0\u4e86\u534f\u540c\u6548\u5e94\u548c\u4e0d\u517c\u5bb9\u6027\uff0c\u5e76\u5c06\u6846\u67b6\u5f00\u6e90\u4f5c\u4e3a\u672a\u6765\u7814\u7a76\u7684\u8ba1\u7b97\u53ef\u8bbf\u95ee\u57fa\u7ebf\u3002"}}
{"id": "2512.12395", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12395", "abs": "https://arxiv.org/abs/2512.12395", "authors": ["Haowen Wang", "Xiaoping Yuan", "Fugang Zhang", "Rui Jian", "Yuanwei Zhu", "Xiuquan Qiao", "Yakun Huang"], "title": "ArtGen: Conditional Generative Modeling of Articulated Objects in Arbitrary Part-Level States", "comment": null, "summary": "Generating articulated assets is crucial for robotics, digital twins, and embodied intelligence. Existing generative models often rely on single-view inputs representing closed states, resulting in ambiguous or unrealistic kinematic structures due to the entanglement between geometric shape and joint dynamics. To address these challenges, we introduce ArtGen, a conditional diffusion-based framework capable of generating articulated 3D objects with accurate geometry and coherent kinematics from single-view images or text descriptions at arbitrary part-level states. Specifically, ArtGen employs cross-state Monte Carlo sampling to explicitly enforce global kinematic consistency, reducing structural-motion entanglement. Additionally, we integrate a Chain-of-Thought reasoning module to infer robust structural priors, such as part semantics, joint types, and connectivity, guiding a sparse-expert Diffusion Transformer to specialize in diverse kinematic interactions. Furthermore, a compositional 3D-VAE latent prior enhanced with local-global attention effectively captures fine-grained geometry and global part-level relationships. Extensive experiments on the PartNet-Mobility benchmark demonstrate that ArtGen significantly outperforms state-of-the-art methods.", "AI": {"tldr": "ArtGen\u662f\u4e00\u4e2a\u57fa\u4e8e\u6761\u4ef6\u6269\u6563\u7684\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u5355\u89c6\u56fe\u56fe\u50cf\u6216\u6587\u672c\u63cf\u8ff0\u751f\u6210\u5177\u6709\u51c6\u786e\u51e0\u4f55\u548c\u8fde\u8d2f\u8fd0\u52a8\u5b66\u7684\u94f0\u63a5\u5f0f3D\u7269\u4f53\uff0c\u901a\u8fc7\u4ea4\u53c9\u72b6\u6001\u8499\u7279\u5361\u6d1b\u91c7\u6837\u548c\u601d\u7ef4\u94fe\u63a8\u7406\u89e3\u51b3\u51e0\u4f55\u5f62\u72b6\u4e0e\u5173\u8282\u52a8\u6001\u7ea0\u7f20\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\u901a\u5e38\u4f9d\u8d56\u8868\u793a\u95ed\u5408\u72b6\u6001\u7684\u5355\u89c6\u56fe\u8f93\u5165\uff0c\u5bfc\u81f4\u51e0\u4f55\u5f62\u72b6\u4e0e\u5173\u8282\u52a8\u6001\u7ea0\u7f20\uff0c\u4ea7\u751f\u6a21\u7cca\u6216\u4e0d\u73b0\u5b9e\u7684\u8fd0\u52a8\u5b66\u7ed3\u6784\u3002\u94f0\u63a5\u5f0f\u8d44\u4ea7\u751f\u6210\u5bf9\u673a\u5668\u4eba\u3001\u6570\u5b57\u5b6a\u751f\u548c\u5177\u8eab\u667a\u80fd\u81f3\u5173\u91cd\u8981\u3002", "method": "1) \u91c7\u7528\u4ea4\u53c9\u72b6\u6001\u8499\u7279\u5361\u6d1b\u91c7\u6837\u663e\u5f0f\u5f3a\u5236\u6267\u884c\u5168\u5c40\u8fd0\u52a8\u5b66\u4e00\u81f4\u6027\uff1b2) \u96c6\u6210\u601d\u7ef4\u94fe\u63a8\u7406\u6a21\u5757\u63a8\u65ad\u7ed3\u6784\u5148\u9a8c\uff08\u90e8\u4ef6\u8bed\u4e49\u3001\u5173\u8282\u7c7b\u578b\u3001\u8fde\u63a5\u6027\uff09\uff1b3) \u4f7f\u7528\u7a00\u758f\u4e13\u5bb6\u6269\u6563\u53d8\u6362\u5668\u4e13\u95e8\u5904\u7406\u591a\u6837\u5316\u8fd0\u52a8\u5b66\u4ea4\u4e92\uff1b4) \u91c7\u7528\u5c40\u90e8-\u5168\u5c40\u6ce8\u610f\u529b\u589e\u5f3a\u7684\u7ec4\u5408\u5f0f3D-VAE\u6f5c\u5728\u5148\u9a8c\u6355\u83b7\u7ec6\u7c92\u5ea6\u51e0\u4f55\u548c\u5168\u5c40\u90e8\u4ef6\u7ea7\u5173\u7cfb\u3002", "result": "\u5728PartNet-Mobility\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cArtGen\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "ArtGen\u80fd\u591f\u4ece\u5355\u89c6\u56fe\u56fe\u50cf\u6216\u6587\u672c\u63cf\u8ff0\u751f\u6210\u5177\u6709\u51c6\u786e\u51e0\u4f55\u548c\u8fde\u8d2f\u8fd0\u52a8\u5b66\u7684\u94f0\u63a5\u5f0f3D\u7269\u4f53\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7ed3\u6784-\u8fd0\u52a8\u7ea0\u7f20\u95ee\u9898\uff0c\u4e3a\u673a\u5668\u4eba\u3001\u6570\u5b57\u5b6a\u751f\u548c\u5177\u8eab\u667a\u80fd\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u94f0\u63a5\u5f0f\u8d44\u4ea7\u751f\u6210\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12465", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12465", "abs": "https://arxiv.org/abs/2512.12465", "authors": ["Uriel Singer", "Yaron Lipman"], "title": "Exploring the Design Space of Transition Matching", "comment": null, "summary": "Transition Matching (TM) is an emerging paradigm for generative modeling that generalizes diffusion and flow-matching models as well as continuous-state autoregressive models. TM, similar to previous paradigms, gradually transforms noise samples to data samples, however it uses a second ``internal'' generative model to implement the transition steps, making the transitions more expressive compared to diffusion and flow models. To make this paradigm tractable, TM employs a large backbone network and a smaller \"head\" module to efficiently execute the generative transition step. In this work, we present a large-scale, systematic investigation into the design, training and sampling of the head in TM frameworks, focusing on its time-continuous bidirectional variant. Through comprehensive ablations and experimentation involving training 56 different 1.7B text-to-image models (resulting in 549 unique evaluations) we evaluate the affect of the head module architecture and modeling during training as-well as a useful family of stochastic TM samplers. We analyze the impact on generation quality, training, and inference efficiency. We find that TM with an MLP head, trained with a particular time weighting and sampled with high frequency sampler provides best ranking across all metrics reaching state-of-the-art among all tested baselines, while Transformer head with sequence scaling and low frequency sampling is a runner up excelling at image aesthetics. Lastly, we believe the experiments presented highlight the design aspects that are likely to provide most quality and efficiency gains, while at the same time indicate what design choices are not likely to provide further gains.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86Transition Matching\uff08TM\uff09\u751f\u6210\u6a21\u578b\u4e2d\u5934\u90e8\u6a21\u5757\u7684\u8bbe\u8ba1\u3001\u8bad\u7ec3\u548c\u91c7\u6837\u7b56\u7565\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u9a8c\u53d1\u73b0MLP\u5934\u90e8\u914d\u5408\u7279\u5b9a\u65f6\u95f4\u52a0\u6743\u548c\u9ad8\u9891\u91c7\u6837\u5668\u5728\u7efc\u5408\u6307\u6807\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u8fbe\u5230SOTA\u6c34\u5e73\u3002", "motivation": "TM\u4f5c\u4e3a\u4e00\u79cd\u65b0\u5174\u7684\u751f\u6210\u5efa\u6a21\u8303\u5f0f\uff0c\u6bd4\u6269\u6563\u548c\u6d41\u5339\u914d\u6a21\u578b\u66f4\u5177\u8868\u8fbe\u529b\uff0c\u4f46\u5176\u5934\u90e8\u6a21\u5757\u7684\u8bbe\u8ba1\u3001\u8bad\u7ec3\u548c\u91c7\u6837\u7b56\u7565\u5c1a\u672a\u5f97\u5230\u7cfb\u7edf\u7814\u7a76\uff0c\u9700\u8981\u63a2\u7d22\u5982\u4f55\u4f18\u5316TM\u6846\u67b6\u4ee5\u83b7\u5f97\u6700\u4f73\u751f\u6210\u8d28\u91cf\u548c\u6548\u7387\u3002", "method": "\u91c7\u7528\u65f6\u95f4\u8fde\u7eed\u53cc\u5411\u53d8\u4f53\u7684TM\u6846\u67b6\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u7cfb\u7edf\u6027\u5b9e\u9a8c\uff08\u8bad\u7ec356\u4e2a\u4e0d\u540c\u768417\u4ebf\u53c2\u6570\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\uff0c\u8fdb\u884c549\u6b21\u72ec\u7279\u8bc4\u4f30\uff09\uff0c\u5bf9\u6bd4\u4e0d\u540c\u5934\u90e8\u6a21\u5757\u67b6\u6784\uff08MLP vs Transformer\uff09\u3001\u8bad\u7ec3\u5efa\u6a21\u7b56\u7565\u4ee5\u53ca\u968f\u673aTM\u91c7\u6837\u5668\u5bb6\u65cf\u7684\u6548\u679c\u3002", "result": "MLP\u5934\u90e8\u914d\u5408\u7279\u5b9a\u65f6\u95f4\u52a0\u6743\u548c\u9ad8\u9891\u91c7\u6837\u5668\u5728\u6240\u6709\u6307\u6807\u4e0a\u6392\u540d\u6700\u4f73\uff0c\u8fbe\u5230SOTA\u6c34\u5e73\uff1bTransformer\u5934\u90e8\u914d\u5408\u5e8f\u5217\u7f29\u653e\u548c\u4f4e\u9891\u91c7\u6837\u5728\u56fe\u50cf\u7f8e\u5b66\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002\u5b9e\u9a8c\u63ed\u793a\u4e86\u54ea\u4e9b\u8bbe\u8ba1\u9009\u62e9\u80fd\u5e26\u6765\u6700\u5927\u8d28\u91cf\u548c\u6548\u7387\u63d0\u5347\uff0c\u54ea\u4e9b\u9009\u62e9\u4e0d\u592a\u53ef\u80fd\u63d0\u4f9b\u8fdb\u4e00\u6b65\u589e\u76ca\u3002", "conclusion": "TM\u6846\u67b6\u4e2d\u5934\u90e8\u6a21\u5757\u7684\u8bbe\u8ba1\u5bf9\u751f\u6210\u8d28\u91cf\u548c\u6548\u7387\u6709\u663e\u8457\u5f71\u54cd\uff0cMLP\u5934\u90e8\u914d\u5408\u4f18\u5316\u8bad\u7ec3\u7b56\u7565\u548c\u91c7\u6837\u65b9\u6cd5\u662f\u6700\u4f73\u9009\u62e9\uff0c\u4e3aTM\u6a21\u578b\u7684\u5b9e\u8df5\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\uff0c\u540c\u65f6\u660e\u786e\u4e86\u672a\u6765\u4f18\u5316\u7684\u65b9\u5411\u3002"}}
{"id": "2512.12469", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12469", "abs": "https://arxiv.org/abs/2512.12469", "authors": ["Sandy Fraser", "Patryk Wielopolski"], "title": "Sparse Concept Anchoring for Interpretable and Controllable Neural Representations", "comment": null, "summary": "We introduce Sparse Concept Anchoring, a method that biases latent space to position a targeted subset of concepts while allowing others to self-organize, using only minimal supervision (labels for <0.1% of examples per anchored concept). Training combines activation normalization, a separation regularizer, and anchor or subspace regularizers that attract rare labeled examples to predefined directions or axis-aligned subspaces. The anchored geometry enables two practical interventions: reversible behavioral steering that projects out a concept's latent component at inference, and permanent removal via targeted weight ablation of anchored dimensions. Experiments on structured autoencoders show selective attenuation of targeted concepts with negligible impact on orthogonal features, and complete elimination with reconstruction error approaching theoretical bounds. Sparse Concept Anchoring therefore provides a practical pathway to interpretable, steerable behavior in learned representations.", "AI": {"tldr": "\u63d0\u51fa\u7a00\u758f\u6982\u5ff5\u951a\u5b9a\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u76d1\u7763\uff08\u6bcf\u4e2a\u951a\u5b9a\u6982\u5ff5<0.1%\u7684\u6807\u6ce8\u6837\u672c\uff09\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5b9a\u4f4d\u76ee\u6807\u6982\u5ff5\u5b50\u96c6\uff0c\u540c\u65f6\u5141\u8bb8\u5176\u4ed6\u6982\u5ff5\u81ea\u7ec4\u7ec7\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u3001\u53ef\u64cd\u63a7\u7684\u8868\u793a\u5b66\u4e60\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5b66\u4e60\u8868\u793a\u4e2d\u7279\u5b9a\u6982\u5ff5\u7684\u53ef\u63a7\u6027\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u800c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6700\u5c0f\u76d1\u7763\u5b9e\u73b0\u7279\u5b9a\u6982\u5ff5\u7684\u7cbe\u51c6\u5b9a\u4f4d\u548c\u64cd\u63a7\u3002", "method": "\u7ed3\u5408\u6fc0\u6d3b\u5f52\u4e00\u5316\u3001\u5206\u79bb\u6b63\u5219\u5316\u5668\u4ee5\u53ca\u951a\u5b9a\u6216\u5b50\u7a7a\u95f4\u6b63\u5219\u5316\u5668\uff0c\u5c06\u7a00\u6709\u6807\u6ce8\u6837\u672c\u5438\u5f15\u5230\u9884\u5b9a\u4e49\u65b9\u5411\u6216\u8f74\u5bf9\u9f50\u5b50\u7a7a\u95f4\uff0c\u5f62\u6210\u951a\u5b9a\u51e0\u4f55\u7ed3\u6784\u3002", "result": "\u5728\u7ed3\u6784\u5316\u81ea\u7f16\u7801\u5668\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u80fd\u9009\u62e9\u6027\u8870\u51cf\u76ee\u6807\u6982\u5ff5\u800c\u5bf9\u6b63\u4ea4\u7279\u5f81\u5f71\u54cd\u53ef\u5ffd\u7565\uff0c\u5b8c\u5168\u6d88\u9664\u6982\u5ff5\u65f6\u91cd\u5efa\u8bef\u5dee\u63a5\u8fd1\u7406\u8bba\u754c\u9650\u3002", "conclusion": "\u7a00\u758f\u6982\u5ff5\u951a\u5b9a\u4e3a\u5b66\u4e60\u8868\u793a\u4e2d\u7684\u53ef\u89e3\u91ca\u3001\u53ef\u64cd\u63a7\u884c\u4e3a\u63d0\u4f9b\u4e86\u5b9e\u7528\u9014\u5f84\uff0c\u5b9e\u73b0\u4e86\u6982\u5ff5\u7ea7\u522b\u7684\u7cbe\u786e\u5e72\u9884\u3002"}}
{"id": "2512.12424", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12424", "abs": "https://arxiv.org/abs/2512.12424", "authors": ["Tue-Thu Van-Dinh", "Hoang-Duy Tran", "Truong-Binh Duong", "Mai-Hanh Pham", "Binh-Nam Le-Nguyen", "Quoc-Thai Nguyen"], "title": "ViInfographicVQA: A Benchmark for Single and Multi-image Visual Question Answering on Vietnamese Infographics", "comment": "10 pages, 4 figures, Accepted to AI4Research @ AAAI", "summary": "Infographic Visual Question Answering (InfographicVQA) evaluates a model's ability to read and reason over data-rich, layout-heavy visuals that combine text, charts, icons, and design elements. Compared with scene-text or natural-image VQA, infographics require stronger integration of OCR, layout understanding, and numerical and semantic reasoning. We introduce ViInfographicVQA, the first benchmark for Vietnamese InfographicVQA, comprising over 6747 real-world infographics and 20409 human-verified question-answer pairs across economics, healthcare, education, and more. The benchmark includes two evaluation settings. The Single-image task follows the traditional setup in which each question is answered using a single infographic. The Multi-image task requires synthesizing evidence across multiple semantically related infographics and is, to our knowledge, the first Vietnamese evaluation of cross-image reasoning in VQA. We evaluate a range of recent vision-language models on this benchmark, revealing substantial performance disparities, with the most significant errors occurring on Multi-image questions that involve cross-image integration and non-span reasoning. ViInfographicVQA contributes benchmark results for Vietnamese InfographicVQA and sheds light on the limitations of current multimodal models in low-resource contexts, encouraging future exploration of layout-aware and cross-image reasoning methods.", "AI": {"tldr": "ViInfographicVQA\u662f\u9996\u4e2a\u8d8a\u5357\u8bed\u4fe1\u606f\u56fe\u8868\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\uff0c\u5305\u542b6747\u4e2a\u771f\u5b9e\u4fe1\u606f\u56fe\u8868\u548c20409\u4e2a\u4eba\u5de5\u9a8c\u8bc1\u95ee\u7b54\u5bf9\uff0c\u6db5\u76d6\u7ecf\u6d4e\u3001\u533b\u7597\u3001\u6559\u80b2\u7b49\u9886\u57df\uff0c\u5305\u542b\u5355\u56fe\u548c\u591a\u56fe\u4e24\u79cd\u8bc4\u4f30\u4efb\u52a1\u3002", "motivation": "\u4fe1\u606f\u56fe\u8868\u89c6\u89c9\u95ee\u7b54\u9700\u8981\u6a21\u578b\u5728\u6570\u636e\u4e30\u5bcc\u3001\u5e03\u5c40\u590d\u6742\u7684\u89c6\u89c9\u5185\u5bb9\u4e2d\u8bfb\u53d6\u548c\u63a8\u7406\uff0c\u76f8\u6bd4\u573a\u666f\u6587\u672c\u6216\u81ea\u7136\u56fe\u50cfVQA\uff0c\u9700\u8981\u66f4\u5f3a\u7684OCR\u3001\u5e03\u5c40\u7406\u89e3\u3001\u6570\u503c\u548c\u8bed\u4e49\u63a8\u7406\u80fd\u529b\u3002\u76ee\u524d\u7f3a\u4e4f\u8d8a\u5357\u8bed\u7684\u4fe1\u606f\u56fe\u8868VQA\u57fa\u51c6\uff0c\u7279\u522b\u662f\u9700\u8981\u8de8\u56fe\u50cf\u63a8\u7406\u7684\u591a\u56fe\u4efb\u52a1\u8bc4\u4f30\u3002", "method": "\u521b\u5efa\u4e86ViInfographicVQA\u57fa\u51c6\uff0c\u5305\u542b6747\u4e2a\u771f\u5b9e\u4e16\u754c\u4fe1\u606f\u56fe\u8868\u548c20409\u4e2a\u4eba\u5de5\u9a8c\u8bc1\u7684\u95ee\u7b54\u5bf9\uff0c\u6db5\u76d6\u591a\u4e2a\u9886\u57df\u3002\u8bbe\u8ba1\u4e86\u4e24\u79cd\u8bc4\u4f30\u8bbe\u7f6e\uff1a\u5355\u56fe\u4efb\u52a1\uff08\u4f20\u7edf\u8bbe\u7f6e\uff09\u548c\u591a\u56fe\u4efb\u52a1\uff08\u9700\u8981\u8de8\u591a\u4e2a\u8bed\u4e49\u76f8\u5173\u4fe1\u606f\u56fe\u8868\u5408\u6210\u8bc1\u636e\uff09\u3002\u8bc4\u4f30\u4e86\u591a\u79cd\u6700\u65b0\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u8be5\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u73b0\u6709\u6a21\u578b\u5728\u8be5\u57fa\u51c6\u4e0a\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u8ddd\uff0c\u6700\u4e25\u91cd\u7684\u9519\u8bef\u51fa\u73b0\u5728\u591a\u56fe\u95ee\u9898\u4e0a\uff0c\u7279\u522b\u662f\u6d89\u53ca\u8de8\u56fe\u50cf\u6574\u5408\u548c\u975e\u8de8\u5ea6\u63a8\u7406\u7684\u4efb\u52a1\u3002\u8fd9\u63ed\u793a\u4e86\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e0b\u7684\u5c40\u9650\u6027\u3002", "conclusion": "ViInfographicVQA\u4e3a\u8d8a\u5357\u8bed\u4fe1\u606f\u56fe\u8868VQA\u63d0\u4f9b\u4e86\u57fa\u51c6\u7ed3\u679c\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u591a\u6a21\u6001\u6a21\u578b\u5728\u5e03\u5c40\u611f\u77e5\u548c\u8de8\u56fe\u50cf\u63a8\u7406\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u9f13\u52b1\u672a\u6765\u63a2\u7d22\u5e03\u5c40\u611f\u77e5\u548c\u8de8\u56fe\u50cf\u63a8\u7406\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u73af\u5883\u4e2d\u3002"}}
{"id": "2512.12425", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12425", "abs": "https://arxiv.org/abs/2512.12425", "authors": ["Hangwei Zhang", "Armando Teles Fortes", "Tianyi Wei", "Xingang Pan"], "title": "BokehDepth: Enhancing Monocular Depth Estimation through Bokeh Generation", "comment": null, "summary": "Bokeh and monocular depth estimation are tightly coupled through the same lens imaging geometry, yet current methods exploit this connection in incomplete ways. High-quality bokeh rendering pipelines typically depend on noisy depth maps, which amplify estimation errors into visible artifacts, while modern monocular metric depth models still struggle on weakly textured, distant and geometrically ambiguous regions where defocus cues are most informative. We introduce BokehDepth, a two-stage framework that decouples bokeh synthesis from depth prediction and treats defocus as an auxiliary supervision-free geometric cue. In Stage-1, a physically guided controllable bokeh generator, built on a powerful pretrained image editing backbone, produces depth-free bokeh stacks with calibrated bokeh strength from a single sharp input. In Stage-2, a lightweight defocus-aware aggregation module plugs into existing monocular depth encoders, fuses features along the defocus dimension, and exposes stable depth-sensitive variations while leaving downstream decoder unchanged. Across challenging benchmarks, BokehDepth improves visual fidelity over depth-map-based bokeh baselines and consistently boosts the metric accuracy and robustness of strong monocular depth foundation models.", "AI": {"tldr": "BokehDepth\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u5c06\u6563\u666f\u5408\u6210\u4e0e\u6df1\u5ea6\u4f30\u8ba1\u89e3\u8026\uff0c\u5229\u7528\u6563\u7126\u4f5c\u4e3a\u65e0\u76d1\u7763\u7684\u51e0\u4f55\u7ebf\u7d22\uff0c\u65e2\u63d0\u5347\u6563\u666f\u6e32\u67d3\u8d28\u91cf\u53c8\u6539\u5584\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u6563\u666f\u4e0e\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u4e4b\u95f4\u7684\u7d27\u5bc6\u8054\u7cfb\uff1a\u9ad8\u8d28\u91cf\u7684\u6563\u666f\u6e32\u67d3\u4f9d\u8d56\u6709\u566a\u58f0\u7684\u6df1\u5ea6\u56fe\uff0c\u800c\u73b0\u4ee3\u5355\u76ee\u6df1\u5ea6\u6a21\u578b\u5728\u7eb9\u7406\u5f31\u3001\u8ddd\u79bb\u8fdc\u3001\u51e0\u4f55\u6a21\u7cca\u7684\u533a\u57df\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u8fd9\u4e9b\u533a\u57df\u6b63\u662f\u6563\u7126\u7ebf\u7d22\u6700\u6709\u4fe1\u606f\u91cf\u7684\u5730\u65b9\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u57fa\u4e8e\u9884\u8bad\u7ec3\u56fe\u50cf\u7f16\u8f91\u9aa8\u5e72\u6784\u5efa\u7269\u7406\u5f15\u5bfc\u7684\u53ef\u63a7\u6563\u666f\u751f\u6210\u5668\uff0c\u4ece\u5355\u5f20\u6e05\u6670\u8f93\u5165\u4ea7\u751f\u65e0\u6df1\u5ea6\u4fe1\u606f\u7684\u6563\u666f\u5806\u6808\uff1b\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u8f7b\u91cf\u7ea7\u6563\u7126\u611f\u77e5\u805a\u5408\u6a21\u5757\uff0c\u5c06\u6563\u7126\u7ef4\u5ea6\u7279\u5f81\u878d\u5408\u5230\u73b0\u6709\u5355\u76ee\u6df1\u5ea6\u7f16\u7801\u5668\u4e2d\uff0c\u66b4\u9732\u7a33\u5b9a\u7684\u6df1\u5ea6\u654f\u611f\u53d8\u5316\u800c\u4e0d\u6539\u53d8\u4e0b\u6e38\u89e3\u7801\u5668\u3002", "result": "\u5728\u591a\u4e2a\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cBokehDepth\u76f8\u6bd4\u57fa\u4e8e\u6df1\u5ea6\u56fe\u7684\u6563\u666f\u57fa\u7ebf\u63d0\u9ad8\u4e86\u89c6\u89c9\u4fdd\u771f\u5ea6\uff0c\u5e76\u6301\u7eed\u63d0\u5347\u4e86\u5f3a\u5355\u76ee\u6df1\u5ea6\u57fa\u7840\u6a21\u578b\u7684\u5ea6\u91cf\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u5c06\u6563\u666f\u5408\u6210\u4e0e\u6df1\u5ea6\u9884\u6d4b\u89e3\u8026\uff0c\u5e76\u5c06\u6563\u7126\u4f5c\u4e3a\u8f85\u52a9\u7684\u65e0\u76d1\u7763\u51e0\u4f55\u7ebf\u7d22\uff0cBokehDepth\u6846\u67b6\u540c\u65f6\u6539\u5584\u4e86\u6563\u666f\u6e32\u67d3\u8d28\u91cf\u548c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u6027\u80fd\uff0c\u5145\u5206\u5229\u7528\u4e86\u900f\u955c\u6210\u50cf\u51e0\u4f55\u7684\u5185\u5728\u8054\u7cfb\u3002"}}
{"id": "2512.12493", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12493", "abs": "https://arxiv.org/abs/2512.12493", "authors": ["Vaarunay Kaushal", "Rajib Mall"], "title": "AI-Driven Early Warning Systems for Student Success: Discovering Static Feature Dominance in Temporal Prediction Models", "comment": "5 pages, 3 figures, KDD 2026", "summary": "Early identification of at-risk students is critical for effective intervention in online learning environments. This study extends temporal prediction analysis to Week 20 (50% of course duration), comparing Decision Tree and Long Short- Term Memory (LSTM) models across six temporal snapshots. Our analysis reveals that different performance metrics matter at different intervention stages: high recall is critical for early intervention (Weeks 2-4), while balanced precision-recall is important for mid-course resource allocation (Weeks 8-16), and high precision becomes paramount in later stages (Week 20). We demonstrate that static demographic features dominate predictions (68% importance), enabling assessment-free early prediction. The LSTM model achieves 97% recall at Week 2, making it ideal for early intervention, while Decision Tree provides stable balanced performance (78% accuracy) during mid-course. By Week 20, both models converge to similar recall (68%), but LSTM achieves higher precision (90% vs 86%). Our findings also suggest that model selection should depend on intervention timing, and that early signals (Weeks 2-4) are sufficient for reliable initial prediction using primarily demographic and pre-enrollment information.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6bd4\u8f83\u4e86\u51b3\u7b56\u6811\u548cLSTM\u6a21\u578b\u5728\u5728\u7ebf\u5b66\u4e60\u73af\u5883\u4e2d\u9884\u6d4b\u98ce\u9669\u5b66\u751f\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u4e0d\u540c\u5e72\u9884\u9636\u6bb5\u9700\u8981\u4e0d\u540c\u7684\u6027\u80fd\u6307\u6807\uff1a\u65e9\u671f\u9700\u8981\u9ad8\u53ec\u56de\u7387\uff0c\u4e2d\u671f\u9700\u8981\u5e73\u8861\u7684\u7cbe\u786e\u7387-\u53ec\u56de\u7387\uff0c\u540e\u671f\u9700\u8981\u9ad8\u7cbe\u786e\u7387\u3002", "motivation": "\u5728\u7ebf\u5b66\u4e60\u73af\u5883\u4e2d\u65e9\u671f\u8bc6\u522b\u98ce\u9669\u5b66\u751f\u5bf9\u4e8e\u6709\u6548\u5e72\u9884\u81f3\u5173\u91cd\u8981\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4e0d\u540c\u65f6\u95f4\u70b9\u7684\u9884\u6d4b\u6a21\u578b\u8868\u73b0\uff0c\u4ee5\u786e\u5b9a\u6700\u4f73\u5e72\u9884\u65f6\u673a\u548c\u6a21\u578b\u9009\u62e9\u7b56\u7565\u3002", "method": "\u7814\u7a76\u5c06\u65f6\u95f4\u9884\u6d4b\u5206\u6790\u6269\u5c55\u5230\u7b2c20\u5468\uff08\u8bfe\u7a0b\u65f6\u957f\u768450%\uff09\uff0c\u5728\u516d\u4e2a\u65f6\u95f4\u5feb\u7167\u4e0a\u6bd4\u8f83\u51b3\u7b56\u6811\u548cLSTM\u6a21\u578b\u3002\u5206\u6790\u4e0d\u540c\u5e72\u9884\u9636\u6bb5\u6240\u9700\u7684\u6027\u80fd\u6307\u6807\uff0c\u5e76\u8bc4\u4f30\u9759\u6001\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\u7684\u91cd\u8981\u6027\u3002", "result": "\u9759\u6001\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\u5360\u9884\u6d4b\u91cd\u8981\u6027\u768468%\uff0c\u652f\u6301\u65e0\u8bc4\u4f30\u7684\u65e9\u671f\u9884\u6d4b\u3002LSTM\u6a21\u578b\u5728\u7b2c2\u5468\u8fbe\u523097%\u7684\u53ec\u56de\u7387\uff0c\u9002\u5408\u65e9\u671f\u5e72\u9884\uff1b\u51b3\u7b56\u6811\u5728\u4e2d\u671f\u63d0\u4f9b\u7a33\u5b9a\u7684\u5e73\u8861\u6027\u80fd\uff0878%\u51c6\u786e\u7387\uff09\u3002\u5230\u7b2c20\u5468\uff0c\u4e24\u79cd\u6a21\u578b\u7684\u53ec\u56de\u7387\u76f8\u4f3c\uff0868%\uff09\uff0c\u4f46LSTM\u7684\u7cbe\u786e\u7387\u66f4\u9ad8\uff0890% vs 86%\uff09\u3002", "conclusion": "\u6a21\u578b\u9009\u62e9\u5e94\u53d6\u51b3\u4e8e\u5e72\u9884\u65f6\u673a\uff0c\u65e9\u671f\u4fe1\u53f7\uff08\u7b2c2-4\u5468\uff09\u8db3\u4ee5\u4f7f\u7528\u4e3b\u8981\u7684\u4eba\u53e3\u7edf\u8ba1\u548c\u5165\u5b66\u524d\u4fe1\u606f\u8fdb\u884c\u53ef\u9760\u7684\u521d\u59cb\u9884\u6d4b\u3002\u4e0d\u540c\u5e72\u9884\u9636\u6bb5\u9700\u8981\u4e0d\u540c\u7684\u6027\u80fd\u6307\u6807\u4f18\u5148\u7ea7\u3002"}}
{"id": "2512.12430", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12430", "abs": "https://arxiv.org/abs/2512.12430", "authors": ["Ke Zhang", "Yiqun Mei", "Jiacong Xu", "Vishal M. Patel"], "title": "Endless World: Real-Time 3D-Aware Long Video Generation", "comment": "10 pages,7 figures", "summary": "Producing long, coherent video sequences with stable 3D structure remains a major challenge, particularly in streaming scenarios. Motivated by this, we introduce Endless World, a real-time framework for infinite, 3D-consistent video generation.To support infinite video generation, we introduce a conditional autoregressive training strategy that aligns newly generated content with existing video frames. This design preserves long-range dependencies while remaining computationally efficient, enabling real-time inference on a single GPU without additional training overhead.Moreover, our Endless World integrates global 3D-aware attention to provide continuous geometric guidance across time. Our 3D injection mechanism enforces physical plausibility and geometric consistency throughout extended sequences, addressing key challenges in long-horizon and dynamic scene synthesis.Extensive experiments demonstrate that Endless World produces long, stable, and visually coherent videos, achieving competitive or superior performance to existing methods in both visual fidelity and spatial consistency. Our project has been available on https://bwgzk-keke.github.io/EndlessWorld/.", "AI": {"tldr": "Endless World\u662f\u4e00\u4e2a\u5b9e\u65f6\u65e0\u96503D\u4e00\u81f4\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6761\u4ef6\u81ea\u56de\u5f52\u8bad\u7ec3\u548c\u5168\u5c403D\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u5355GPU\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u63a8\u7406\uff0c\u751f\u6210\u957f\u65f6\u7a33\u5b9a\u3001\u89c6\u89c9\u8fde\u8d2f\u7684\u89c6\u9891\u5e8f\u5217\u3002", "motivation": "\u5f53\u524d\u5728\u6d41\u5f0f\u573a\u666f\u4e2d\u751f\u6210\u957f\u65f6\u3001\u8fde\u8d2f\u4e14\u5177\u6709\u7a33\u5b9a3D\u7ed3\u6784\u7684\u89c6\u9891\u5e8f\u5217\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u4fdd\u63013D\u4e00\u81f4\u6027\u548c\u7269\u7406\u5408\u7406\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "method": "1. \u6761\u4ef6\u81ea\u56de\u5f52\u8bad\u7ec3\u7b56\u7565\uff1a\u5c06\u65b0\u751f\u6210\u5185\u5bb9\u4e0e\u73b0\u6709\u89c6\u9891\u5e27\u5bf9\u9f50\uff0c\u4fdd\u6301\u957f\u7a0b\u4f9d\u8d56\u5173\u7cfb\u7684\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\uff1b2. \u5168\u5c403D\u611f\u77e5\u6ce8\u610f\u529b\uff1a\u63d0\u4f9b\u8de8\u65f6\u95f4\u7684\u8fde\u7eed\u51e0\u4f55\u6307\u5bfc\uff1b3. 3D\u6ce8\u5165\u673a\u5236\uff1a\u5728\u6574\u4e2a\u6269\u5c55\u5e8f\u5217\u4e2d\u5f3a\u5236\u6267\u884c\u7269\u7406\u5408\u7406\u6027\u548c\u51e0\u4f55\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660eEndless World\u80fd\u591f\u751f\u6210\u957f\u65f6\u7a33\u5b9a\u3001\u89c6\u89c9\u8fde\u8d2f\u7684\u89c6\u9891\uff0c\u5728\u89c6\u89c9\u4fdd\u771f\u5ea6\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u8fbe\u5230\u6216\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u53ef\u5728\u5355GPU\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u63a8\u7406\u3002", "conclusion": "Endless World\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\u548c3D\u4e00\u81f4\u6027\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u957f\u65f6\u89c6\u9891\u751f\u6210\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u65e0\u9650\u30013D\u4e00\u81f4\u7684\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5b9e\u65f6\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12497", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12497", "abs": "https://arxiv.org/abs/2512.12497", "authors": ["Ioannis Anagnostides", "Zachary W. Sollie", "Arman Kilic", "Tuomas Sandholm"], "title": "Policy Optimization for Dynamic Heart Transplant Allocation", "comment": "An extended abstract of this paper was presented at the scientific sessions of AHA 2025", "summary": "Heart transplantation is a viable path for patients suffering from advanced heart failure, but this lifesaving option is severely limited due to donor shortage. Although the current allocation policy was recently revised in 2018, a major concern is that it does not adequately take into account pretransplant and post-transplant mortality. In this paper, we take an important step toward addressing these deficiencies.\n  To begin with, we use historical data from UNOS to develop a new simulator that enables us to evaluate and compare the performance of different policies. We then leverage our simulator to demonstrate that the status quo policy is considerably inferior to the myopic policy that matches incoming donors to the patient who maximizes the number of years gained by the transplant. Moreover, we develop improved policies that account for the dynamic nature of the allocation process through the use of potentials -- a measure of a patient's utility in future allocations that we introduce. We also show that batching together even a handful of donors -- which is a viable option for a certain type of donors -- further enhances performance. Our simulator also allows us to evaluate the effect of critical, and often unexplored, factors in the allocation, such as geographic proximity and the tendency to reject offers by the transplant centers.", "AI": {"tldr": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u4e2a\u65b0\u7684\u5fc3\u810f\u79fb\u690d\u5206\u914d\u653f\u7b56\u6a21\u62df\u5668\uff0c\u53d1\u73b0\u73b0\u884c\u653f\u7b56\u8fdc\u4e0d\u5982\u6700\u5927\u5316\u79fb\u690d\u83b7\u76ca\u5e74\u6570\u7684\u8fd1\u89c6\u653f\u7b56\uff0c\u5e76\u63d0\u51fa\u4f7f\u7528\"\u6f5c\u529b\u503c\"\u7684\u52a8\u6001\u4f18\u5316\u653f\u7b56\u548c\u6279\u91cf\u5904\u7406\u7b56\u7565\u6765\u6539\u8fdb\u5206\u914d\u6548\u7387\u3002", "motivation": "\u5fc3\u810f\u79fb\u690d\u662f\u665a\u671f\u5fc3\u8870\u60a3\u8005\u7684\u53ef\u884c\u6cbb\u7597\u8def\u5f84\uff0c\u4f46\u4f9b\u4f53\u4e25\u91cd\u77ed\u7f3a\u3002\u5c3d\u7ba12018\u5e74\u4fee\u8ba2\u4e86\u5206\u914d\u653f\u7b56\uff0c\u4f46\u73b0\u6709\u653f\u7b56\u672a\u80fd\u5145\u5206\u8003\u8651\u79fb\u690d\u524d\u548c\u79fb\u690d\u540e\u7684\u6b7b\u4ea1\u7387\uff0c\u9700\u8981\u6539\u8fdb\u5206\u914d\u6548\u7387\u3002", "method": "\u4f7f\u7528UNOS\u5386\u53f2\u6570\u636e\u5f00\u53d1\u65b0\u7684\u6a21\u62df\u5668\u6765\u8bc4\u4f30\u4e0d\u540c\u5206\u914d\u653f\u7b56\uff1b\u6bd4\u8f83\u73b0\u72b6\u653f\u7b56\u4e0e\u8fd1\u89c6\u653f\u7b56\uff08\u6700\u5927\u5316\u79fb\u690d\u83b7\u76ca\u5e74\u6570\uff09\uff1b\u5f15\u5165\"\u6f5c\u529b\u503c\"\u6982\u5ff5\u6765\u8861\u91cf\u60a3\u8005\u5728\u540e\u7eed\u5206\u914d\u4e2d\u7684\u6548\u7528\uff0c\u5f00\u53d1\u52a8\u6001\u4f18\u5316\u653f\u7b56\uff1b\u63a2\u7d22\u6279\u91cf\u5904\u7406\u4f9b\u4f53\u7684\u7b56\u7565\u3002", "result": "\u73b0\u72b6\u653f\u7b56\u660e\u663e\u52a3\u4e8e\u8fd1\u89c6\u653f\u7b56\uff1b\u57fa\u4e8e\u6f5c\u529b\u503c\u7684\u52a8\u6001\u4f18\u5316\u653f\u7b56\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u5206\u914d\u6548\u7387\uff1b\u6279\u91cf\u5904\u7406\u5c11\u91cf\u4f9b\u4f53\uff08\u9002\u7528\u4e8e\u67d0\u4e9b\u7c7b\u578b\u4f9b\u4f53\uff09\u80fd\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6027\u80fd\uff1b\u6a21\u62df\u5668\u8fd8\u80fd\u8bc4\u4f30\u5730\u7406\u90bb\u8fd1\u6027\u548c\u79fb\u690d\u4e2d\u5fc3\u62d2\u7edd\u503e\u5411\u7b49\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u901a\u8fc7\u5f00\u53d1\u65b0\u7684\u6a21\u62df\u5668\u548c\u5f15\u5165\u6f5c\u529b\u503c\u6982\u5ff5\uff0c\u672c\u6587\u4e3a\u6539\u8fdb\u5fc3\u810f\u79fb\u690d\u5206\u914d\u653f\u7b56\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u548c\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u5339\u914d\u4f9b\u4f53\u4e0e\u60a3\u8005\uff0c\u63d0\u9ad8\u6574\u4f53\u79fb\u690d\u6548\u679c\u3002"}}
{"id": "2512.12459", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2512.12459", "abs": "https://arxiv.org/abs/2512.12459", "authors": ["Jiachen Tao", "Benjamin Planche", "Van Nguyen Nguyen", "Junyi Wu", "Yuchun Liu", "Haoxuan Wang", "Zhongpai Gao", "Gengyu Zhang", "Meng Zheng", "Feiran Wang", "Anwesa Choudhuri", "Zhenghao Zhao", "Weitai Kang", "Terrence Chen", "Yan Yan", "Ziyan Wu"], "title": "From Particles to Fields: Reframing Photon Mapping with Continuous Gaussian Photon Fields", "comment": null, "summary": "Accurately modeling light transport is essential for realistic image synthesis. Photon mapping provides physically grounded estimates of complex global illumination effects such as caustics and specular-diffuse interactions, yet its per-view radiance estimation remains computationally inefficient when rendering multiple views of the same scene. The inefficiency arises from independent photon tracing and stochastic kernel estimation at each viewpoint, leading to inevitable redundant computation. To accelerate multi-view rendering, we reformulate photon mapping as a continuous and reusable radiance function. Specifically, we introduce the Gaussian Photon Field (GPF), a learnable representation that encodes photon distributions as anisotropic 3D Gaussian primitives parameterized by position, rotation, scale, and spectrum. GPF is initialized from physically traced photons in the first SPPM iteration and optimized using multi-view supervision of final radiance, distilling photon-based light transport into a continuous field. Once trained, the field enables differentiable radiance evaluation along camera rays without repeated photon tracing or iterative refinement. Extensive experiments on scenes with complex light transport, such as caustics and specular-diffuse interactions, demonstrate that GPF attains photon-level accuracy while reducing computation by orders of magnitude, unifying the physical rigor of photon-based rendering with the efficiency of neural scene representations.", "AI": {"tldr": "\u63d0\u51faGaussian Photon Field (GPF)\uff0c\u5c06\u5149\u5b50\u6620\u5c04\u91cd\u6784\u4e3a\u53ef\u5b66\u4e60\u7684\u8fde\u7eed\u8f90\u5c04\u51fd\u6570\uff0c\u901a\u8fc73D\u9ad8\u65af\u539f\u8bed\u7f16\u7801\u5149\u5b50\u5206\u5e03\uff0c\u5b9e\u73b0\u591a\u89c6\u89d2\u6e32\u67d3\u7684\u663e\u8457\u52a0\u901f\u3002", "motivation": "\u5149\u5b50\u6620\u5c04\u867d\u7136\u80fd\u51c6\u786e\u6a21\u62df\u590d\u6742\u5168\u5c40\u5149\u7167\u6548\u679c\uff08\u5982\u7126\u6563\u548c\u955c\u9762-\u6f2b\u53cd\u5c04\u4ea4\u4e92\uff09\uff0c\u4f46\u5728\u6e32\u67d3\u540c\u4e00\u573a\u666f\u7684\u591a\u4e2a\u89c6\u89d2\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u56e0\u4e3a\u6bcf\u4e2a\u89c6\u89d2\u90fd\u9700\u8981\u72ec\u7acb\u7684\u5149\u5b50\u8ffd\u8e2a\u548c\u968f\u673a\u6838\u4f30\u8ba1\uff0c\u5bfc\u81f4\u5927\u91cf\u5197\u4f59\u8ba1\u7b97\u3002", "method": "\u5f15\u5165Gaussian Photon Field (GPF)\uff0c\u4e00\u79cd\u53ef\u5b66\u4e60\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u5c06\u5149\u5b50\u5206\u5e03\u7f16\u7801\u4e3a\u5404\u5411\u5f02\u6027\u76843D\u9ad8\u65af\u539f\u8bed\uff08\u53c2\u6570\u5305\u62ec\u4f4d\u7f6e\u3001\u65cb\u8f6c\u3001\u5c3a\u5ea6\u548c\u5149\u8c31\uff09\u3002GPF\u4ece\u7b2c\u4e00\u6b21SPPM\u8fed\u4ee3\u7684\u7269\u7406\u8ffd\u8e2a\u5149\u5b50\u521d\u59cb\u5316\uff0c\u5e76\u4f7f\u7528\u6700\u7ec8\u8f90\u5c04\u7684\u591a\u89c6\u89d2\u76d1\u7763\u8fdb\u884c\u4f18\u5316\uff0c\u5c06\u57fa\u4e8e\u5149\u5b50\u7684\u5149\u4f20\u8f93\u63d0\u70bc\u4e3a\u8fde\u7eed\u573a\u3002\u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u8be5\u573a\u652f\u6301\u6cbf\u76f8\u673a\u5149\u7ebf\u7684\u53ef\u5fae\u8f90\u5c04\u8bc4\u4f30\uff0c\u65e0\u9700\u91cd\u590d\u5149\u5b50\u8ffd\u8e2a\u6216\u8fed\u4ee3\u4f18\u5316\u3002", "result": "\u5728\u5177\u6709\u590d\u6742\u5149\u4f20\u8f93\uff08\u5982\u7126\u6563\u548c\u955c\u9762-\u6f2b\u53cd\u5c04\u4ea4\u4e92\uff09\u7684\u573a\u666f\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660eGPF\u5728\u4fdd\u6301\u5149\u5b50\u7ea7\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u5c06\u8ba1\u7b97\u91cf\u51cf\u5c11\u4e86\u6570\u91cf\u7ea7\u3002", "conclusion": "GPF\u5c06\u57fa\u4e8e\u5149\u5b50\u6e32\u67d3\u7684\u7269\u7406\u4e25\u8c28\u6027\u4e0e\u795e\u7ecf\u573a\u666f\u8868\u793a\u7684\u6548\u7387\u7edf\u4e00\u8d77\u6765\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u591a\u89c6\u89d2\u6e32\u67d3\u3002"}}
{"id": "2512.12487", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12487", "abs": "https://arxiv.org/abs/2512.12487", "authors": ["Hoang Anh Just", "Yifei Fan", "Handong Zhao", "Jiuxiang Gu", "Ruiyi Zhang", "Simon Jenni", "Kushal Kafle", "Ruoxi Jia", "Jing Shi"], "title": "More Than the Final Answer: Improving Visual Extraction and Logical Consistency in Vision-Language Models", "comment": null, "summary": "Reinforcement learning from verifiable rewards (RLVR) has recently been extended from text-only LLMs to vision-language models (VLMs) to elicit long-chain multimodal reasoning. However, RLVR-trained VLMs still exhibit two persistent failure modes: inaccurate visual extraction (missing or hallucinating details) and logically inconsistent chains-of-thought, largely because verifiable signals supervise only the final answer. We propose PeRL-VL (Perception and Reasoning Learning for Vision-Language Models), a decoupled framework that separately improves visual perception and textual reasoning on top of RLVR. For perception, PeRL-VL introduces a VLM-based description reward that scores the model's self-generated image descriptions for faithfulness and sufficiency. For reasoning, PeRL-VL adds a text-only Reasoning SFT stage on logic-rich chain-of-thought data, enhancing coherence and logical consistency independently of vision. Across diverse multimodal benchmarks, PeRL-VL improves average Pass@1 accuracy from 63.3% (base Qwen2.5-VL-7B) to 68.8%, outperforming standard RLVR, text-only reasoning SFT, and naive multimodal distillation from GPT-4o.", "AI": {"tldr": "PeRL-VL\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u522b\u6539\u8fdb\u89c6\u89c9\u611f\u77e5\u548c\u6587\u672c\u63a8\u7406\u6765\u89e3\u51b3RLVR\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u89c6\u89c9\u63d0\u53d6\u4e0d\u51c6\u786e\u548c\u903b\u8f91\u4e0d\u4e00\u81f4\u95ee\u9898\u3002", "motivation": "\u867d\u7136RLVR\u5df2\u6269\u5c55\u5230\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ee5\u6fc0\u53d1\u591a\u6a21\u6001\u63a8\u7406\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4ecd\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u89c6\u89c9\u63d0\u53d6\u4e0d\u51c6\u786e\uff08\u9057\u6f0f\u6216\u5e7b\u89c9\u7ec6\u8282\uff09\u548c\u903b\u8f91\u4e0d\u4e00\u81f4\u7684\u601d\u7ef4\u94fe\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u53ef\u9a8c\u8bc1\u4fe1\u53f7\u4ec5\u76d1\u7763\u6700\u7ec8\u7b54\u6848\u3002", "method": "PeRL-VL\u91c7\u7528\u89e3\u8026\u6846\u67b6\uff1a1\uff09\u611f\u77e5\u65b9\u9762\u5f15\u5165\u57fa\u4e8eVLM\u7684\u63cf\u8ff0\u5956\u52b1\uff0c\u5bf9\u6a21\u578b\u81ea\u751f\u6210\u7684\u56fe\u50cf\u63cf\u8ff0\u8fdb\u884c\u5fe0\u5b9e\u6027\u548c\u5145\u5206\u6027\u8bc4\u5206\uff1b2\uff09\u63a8\u7406\u65b9\u9762\u589e\u52a0\u7eaf\u6587\u672c\u63a8\u7406SFT\u9636\u6bb5\uff0c\u5728\u903b\u8f91\u4e30\u5bcc\u7684\u601d\u7ef4\u94fe\u6570\u636e\u4e0a\u8bad\u7ec3\uff0c\u72ec\u7acb\u4e8e\u89c6\u89c9\u589e\u5f3a\u903b\u8f91\u4e00\u81f4\u6027\u3002", "result": "\u5728\u591a\u6837\u5316\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPeRL-VL\u5c06\u5e73\u5747Pass@1\u51c6\u786e\u7387\u4ece63.3%\uff08\u57fa\u7840Qwen2.5-VL-7B\uff09\u63d0\u5347\u523068.8%\uff0c\u4f18\u4e8e\u6807\u51c6RLVR\u3001\u7eaf\u6587\u672c\u63a8\u7406SFT\u548c\u4eceGPT-4o\u7684\u6734\u7d20\u591a\u6a21\u6001\u84b8\u998f\u3002", "conclusion": "PeRL-VL\u901a\u8fc7\u89e3\u8026\u89c6\u89c9\u611f\u77e5\u548c\u6587\u672c\u63a8\u7406\u7684\u6539\u8fdb\uff0c\u6709\u6548\u89e3\u51b3\u4e86RLVR\u8bad\u7ec3VLMs\u4e2d\u7684\u6838\u5fc3\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2512.12492", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12492", "abs": "https://arxiv.org/abs/2512.12492", "authors": ["Shengkai Xu", "Hsiang Lun Kao", "Tianxiang Xu", "Honghui Zhang", "Junqiao Wang", "Runmeng Ding", "Guanyu Liu", "Tianyu Shi", "Zhenyu Yu", "Guofeng Pan", "Ziqian Bi", "Yuqi Ouyang"], "title": "Adaptive Detector-Verifier Framework for Zero-Shot Polyp Detection in Open-World Settings", "comment": null, "summary": "Polyp detectors trained on clean datasets often underperform in real-world endoscopy, where illumination changes, motion blur, and occlusions degrade image quality. Existing approaches struggle with the domain gap between controlled laboratory conditions and clinical practice, where adverse imaging conditions are prevalent. In this work, we propose AdaptiveDetector, a novel two-stage detector-verifier framework comprising a YOLOv11 detector with a vision-language model (VLM) verifier. The detector adaptively adjusts per-frame confidence thresholds under VLM guidance, while the verifier is fine-tuned with Group Relative Policy Optimization (GRPO) using an asymmetric, cost-sensitive reward function specifically designed to discourage missed detections -- a critical clinical requirement. To enable realistic assessment under challenging conditions, we construct a comprehensive synthetic testbed by systematically degrading clean datasets with adverse conditions commonly encountered in clinical practice, providing a rigorous benchmark for zero-shot evaluation. Extensive zero-shot evaluation on synthetically degraded CVC-ClinicDB and Kvasir-SEG images demonstrates that our approach improves recall by 14 to 22 percentage points over YOLO alone, while precision remains within 0.7 points below to 1.7 points above the baseline. This combination of adaptive thresholding and cost-sensitive reinforcement learning achieves clinically aligned, open-world polyp detection with substantially fewer false negatives, thereby reducing the risk of missed precancerous polyps and improving patient outcomes.", "AI": {"tldr": "\u63d0\u51faAdaptiveDetector\u6846\u67b6\uff0c\u7ed3\u5408YOLOv11\u68c0\u6d4b\u5668\u548cVLM\u9a8c\u8bc1\u5668\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u9608\u503c\u8c03\u6574\u548c\u6210\u672c\u654f\u611f\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u6076\u52a3\u5185\u7aa5\u955c\u6761\u4ef6\u4e0b\u663e\u8457\u63d0\u5347\u606f\u8089\u68c0\u6d4b\u53ec\u56de\u7387\uff0c\u51cf\u5c11\u6f0f\u68c0\u98ce\u9669\u3002", "motivation": "\u73b0\u6709\u606f\u8089\u68c0\u6d4b\u5668\u5728\u53d7\u63a7\u5b9e\u9a8c\u5ba4\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u4f46\u5728\u771f\u5b9e\u5185\u7aa5\u955c\u73af\u5883\u4e2d\u6027\u80fd\u4e0b\u964d\uff0c\u56e0\u4e3a\u5b58\u5728\u5149\u7167\u53d8\u5316\u3001\u8fd0\u52a8\u6a21\u7cca\u548c\u906e\u6321\u7b49\u6076\u52a3\u6210\u50cf\u6761\u4ef6\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5f25\u5408\u5b9e\u9a8c\u5ba4\u6761\u4ef6\u4e0e\u4e34\u5e8a\u5b9e\u8df5\u4e4b\u95f4\u7684\u9886\u57df\u5dee\u8ddd\u3002", "method": "\u63d0\u51fa\u4e24\u9636\u6bb5\u68c0\u6d4b\u5668-\u9a8c\u8bc1\u5668\u6846\u67b6\uff1a1\uff09YOLOv11\u68c0\u6d4b\u5668\u5728VLM\u6307\u5bfc\u4e0b\u81ea\u9002\u5e94\u8c03\u6574\u6bcf\u5e27\u7f6e\u4fe1\u5ea6\u9608\u503c\uff1b2\uff09VLM\u9a8c\u8bc1\u5668\u4f7f\u7528Group Relative Policy Optimization\uff08GRPO\uff09\u8fdb\u884c\u5fae\u8c03\uff0c\u91c7\u7528\u975e\u5bf9\u79f0\u6210\u672c\u654f\u611f\u5956\u52b1\u51fd\u6570\uff0c\u4e13\u95e8\u8bbe\u8ba1\u6765\u51cf\u5c11\u6f0f\u68c0\u3002\u6784\u5efa\u5408\u6210\u6d4b\u8bd5\u5e73\u53f0\uff0c\u7cfb\u7edf\u6027\u5730\u5728\u5e72\u51c0\u6570\u636e\u96c6\u4e0a\u6dfb\u52a0\u4e34\u5e8a\u5e38\u89c1\u6076\u52a3\u6761\u4ef6\u3002", "result": "\u5728\u5408\u6210\u9000\u5316\u7684CVC-ClinicDB\u548cKvasir-SEG\u56fe\u50cf\u4e0a\u8fdb\u884c\u96f6\u6837\u672c\u8bc4\u4f30\uff0c\u53ec\u56de\u7387\u6bd4\u5355\u72ec\u4f7f\u7528YOLO\u63d0\u9ad814-22\u4e2a\u767e\u5206\u70b9\uff0c\u7cbe\u5ea6\u4fdd\u6301\u5728\u57fa\u7ebf\u4ee5\u4e0b0.7\u70b9\u5230\u4ee5\u4e0a1.7\u70b9\u8303\u56f4\u5185\u3002\u5b9e\u73b0\u4e86\u4e34\u5e8a\u5bf9\u9f50\u7684\u5f00\u653e\u4e16\u754c\u606f\u8089\u68c0\u6d4b\uff0c\u663e\u8457\u51cf\u5c11\u5047\u9634\u6027\u3002", "conclusion": "\u81ea\u9002\u5e94\u9608\u503c\u8c03\u6574\u4e0e\u6210\u672c\u654f\u611f\u5f3a\u5316\u5b66\u4e60\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u4e34\u5e8a\u5bf9\u9f50\u7684\u606f\u8089\u68c0\u6d4b\uff0c\u5927\u5e45\u51cf\u5c11\u6f0f\u68c0\u98ce\u9669\uff0c\u964d\u4f4e\u9519\u8fc7\u764c\u524d\u606f\u8089\u7684\u53ef\u80fd\u6027\uff0c\u4ece\u800c\u6539\u5584\u60a3\u8005\u9884\u540e\u3002"}}
{"id": "2512.12545", "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2512.12545", "abs": "https://arxiv.org/abs/2512.12545", "authors": ["Bin Mu", "Yuxuan Chen", "Shijin Yuan", "Bo Qin", "Hao Guo"], "title": "Skillful Subseasonal-to-Seasonal Forecasting of Extreme Events with a Multi-Sphere Coupled Probabilistic Model", "comment": null, "summary": "Accurate subseasonal-to-seasonal (S2S) prediction of extreme events is critical for resource planning and disaster mitigation under accelerating climate change. However, such predictions remain challenging due to complex multi-sphere interactions and intrinsic atmospheric uncertainty. Here we present TianXing-S2S, a multi-sphere coupled probabilistic model for global S2S daily ensemble forecast. TianXing-S2S first encodes diverse multi-sphere predictors into a compact latent space, then employs a diffusion model to generate daily ensemble forecasts. A novel coupling module based on optimal transport (OT) is incorporated in the denoiser to optimize the interactions between atmospheric and multi-sphere boundary conditions. Across key atmospheric variables, TianXing-S2S outperforms both the European Centre for Medium-Range Weather Forecasts (ECMWF) S2S system and FuXi-S2S in 45-day daily-mean ensemble forecasts at 1.5 resolution. Our model achieves skillful subseasonal prediction of extreme events including heat waves and anomalous precipitation, identifying soil moisture as a critical precursor signal. Furthermore, we demonstrate that TianXing-S2S can generate stable rollout forecasts up to 180 days, establishing a robust framework for S2S research in a warming world.", "AI": {"tldr": "TianXing-S2S\u662f\u4e00\u4e2a\u591a\u5708\u5c42\u8026\u5408\u7684\u6982\u7387\u6a21\u578b\uff0c\u7528\u4e8e\u5168\u7403\u6b21\u5b63\u8282\u5230\u5b63\u8282(S2S)\u7684\u65e5\u96c6\u5408\u9884\u62a5\uff0c\u901a\u8fc7\u6269\u6563\u6a21\u578b\u548c\u6700\u4f18\u4f20\u8f93\u8026\u5408\u6a21\u5757\u572845\u5929\u9884\u62a5\u4e2d\u8d85\u8d8a\u4e86ECMWF\u548cFuXi-S2S\u7cfb\u7edf\u3002", "motivation": "\u5728\u6c14\u5019\u53d8\u5316\u52a0\u901f\u7684\u80cc\u666f\u4e0b\uff0c\u51c6\u786e\u7684\u6b21\u5b63\u8282\u5230\u5b63\u8282(S2S)\u6781\u7aef\u4e8b\u4ef6\u9884\u6d4b\u5bf9\u4e8e\u8d44\u6e90\u89c4\u5212\u548c\u707e\u5bb3\u51cf\u7f13\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u590d\u6742\u7684\u591a\u5708\u5c42\u76f8\u4e92\u4f5c\u7528\u548c\u56fa\u6709\u7684\u5927\u6c14\u4e0d\u786e\u5b9a\u6027\uff0c\u6b64\u7c7b\u9884\u6d4b\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51faTianXing-S2S\u6a21\u578b\uff1a\u9996\u5148\u5c06\u591a\u6837\u5316\u7684\u591a\u5708\u5c42\u9884\u6d4b\u56e0\u5b50\u7f16\u7801\u5230\u7d27\u51d1\u7684\u6f5c\u5728\u7a7a\u95f4\uff0c\u7136\u540e\u4f7f\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u65e5\u96c6\u5408\u9884\u62a5\u3002\u5728\u53bb\u566a\u5668\u4e2d\u52a0\u5165\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93(OT)\u7684\u65b0\u578b\u8026\u5408\u6a21\u5757\uff0c\u4ee5\u4f18\u5316\u5927\u6c14\u4e0e\u591a\u5708\u5c42\u8fb9\u754c\u6761\u4ef6\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u5728\u5173\u952e\u5927\u6c14\u53d8\u91cf\u4e0a\uff0cTianXing-S2S\u57281.5\u00b0\u5206\u8fa8\u7387\u768445\u5929\u65e5\u5e73\u5747\u96c6\u5408\u9884\u62a5\u4e2d\u8d85\u8d8a\u4e86\u6b27\u6d32\u4e2d\u671f\u5929\u6c14\u9884\u62a5\u4e2d\u5fc3(ECMWF) S2S\u7cfb\u7edf\u548cFuXi-S2S\u3002\u6a21\u578b\u80fd\u591f\u719f\u7ec3\u9884\u6d4b\u6781\u7aef\u4e8b\u4ef6\uff08\u5982\u70ed\u6d6a\u548c\u5f02\u5e38\u964d\u6c34\uff09\uff0c\u5e76\u8bc6\u522b\u571f\u58e4\u6e7f\u5ea6\u4f5c\u4e3a\u5173\u952e\u7684\u524d\u5146\u4fe1\u53f7\u3002\u6b64\u5916\uff0c\u6a21\u578b\u80fd\u591f\u751f\u6210\u957f\u8fbe180\u5929\u7684\u7a33\u5b9a\u6eda\u52a8\u9884\u62a5\u3002", "conclusion": "TianXing-S2S\u4e3a\u53d8\u6696\u4e16\u754c\u4e2d\u7684S2S\u7814\u7a76\u5efa\u7acb\u4e86\u4e00\u4e2a\u7a33\u5065\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5708\u5c42\u8026\u5408\u548c\u6269\u6563\u6a21\u578b\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6b21\u5b63\u8282\u5230\u5b63\u8282\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6781\u7aef\u4e8b\u4ef6\u9884\u6d4b\u80fd\u529b\u3002"}}
{"id": "2512.12508", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12508", "abs": "https://arxiv.org/abs/2512.12508", "authors": ["Jinfan Zhou", "Lixin Luo", "Sungmin Eum", "Heesung Kwon", "Jeong Joon Park"], "title": "Generative Spatiotemporal Data Augmentation", "comment": null, "summary": "We explore spatiotemporal data augmentation using video foundation models to diversify both camera viewpoints and scene dynamics. Unlike existing approaches based on simple geometric transforms or appearance perturbations, our method leverages off-the-shelf video diffusion models to generate realistic 3D spatial and temporal variations from a given image dataset. Incorporating these synthesized video clips as supplemental training data yields consistent performance gains in low-data settings, such as UAV-captured imagery where annotations are scarce. Beyond empirical improvements, we provide practical guidelines for (i) choosing an appropriate spatiotemporal generative setup, (ii) transferring annotations to synthetic frames, and (iii) addressing disocclusion - regions newly revealed and unlabeled in generated views. Experiments on COCO subsets and UAV-captured datasets show that, when applied judiciously, spatiotemporal augmentation broadens the data distribution along axes underrepresented by traditional and prior generative methods, offering an effective lever for improving model performance in data-scarce regimes.", "AI": {"tldr": "\u5229\u7528\u89c6\u9891\u57fa\u7840\u6a21\u578b\u8fdb\u884c\u65f6\u7a7a\u6570\u636e\u589e\u5f3a\uff0c\u901a\u8fc7\u89c6\u9891\u6269\u6563\u6a21\u578b\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u771f\u5b9e\u76843D\u7a7a\u95f4\u548c\u65f6\u95f4\u53d8\u5316\uff0c\u5728\u4f4e\u6570\u636e\u573a\u666f\uff08\u5982\u65e0\u4eba\u673a\u56fe\u50cf\uff09\u4e2d\u63d0\u5347\u6a21\u578b\u6027\u80fd", "motivation": "\u73b0\u6709\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u7b80\u5355\u7684\u51e0\u4f55\u53d8\u6362\u6216\u5916\u89c2\u6270\u52a8\uff0c\u65e0\u6cd5\u6709\u6548\u6a21\u62df\u771f\u5b9e\u76843D\u7a7a\u95f4\u89c6\u89d2\u53d8\u5316\u548c\u573a\u666f\u52a8\u6001\u53d8\u5316\u3002\u7279\u522b\u662f\u5728\u65e0\u4eba\u673a\u56fe\u50cf\u7b49\u6807\u6ce8\u7a00\u7f3a\u7684\u4f4e\u6570\u636e\u573a\u666f\u4e2d\uff0c\u9700\u8981\u66f4\u4e30\u5bcc\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd", "method": "\u4f7f\u7528\u73b0\u6210\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u4ece\u7ed9\u5b9a\u7684\u56fe\u50cf\u6570\u636e\u96c6\u751f\u6210\u5177\u6709\u771f\u5b9e3D\u7a7a\u95f4\u548c\u65f6\u95f4\u53d8\u5316\u7684\u89c6\u9891\u7247\u6bb5\u3002\u63d0\u4f9b\u5b9e\u7528\u6307\u5357\uff1a1)\u9009\u62e9\u5408\u9002\u7684\u65f6\u7a7a\u751f\u6210\u8bbe\u7f6e\uff1b2)\u5c06\u6807\u6ce8\u8f6c\u79fb\u5230\u5408\u6210\u5e27\uff1b3)\u5904\u7406\u65b0\u66b4\u9732\u533a\u57df\uff08disocclusion\uff09\u95ee\u9898", "result": "\u5728COCO\u5b50\u96c6\u548c\u65e0\u4eba\u673a\u6355\u83b7\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u65f6\u7a7a\u6570\u636e\u589e\u5f3a\u80fd\u591f\u6cbf\u7740\u4f20\u7edf\u548c\u5148\u524d\u751f\u6210\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u4ee3\u8868\u7684\u7ef4\u5ea6\u6269\u5c55\u6570\u636e\u5206\u5e03\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u6709\u6548\u63d0\u5347\u6a21\u578b\u6027\u80fd", "conclusion": "\u65f6\u7a7a\u6570\u636e\u589e\u5f3a\u901a\u8fc7\u89c6\u9891\u57fa\u7840\u6a21\u578b\u751f\u6210\u771f\u5b9e\u76843D\u7a7a\u95f4\u548c\u65f6\u95f4\u53d8\u5316\uff0c\u4e3a\u4f4e\u6570\u636e\u573a\u666f\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6027\u80fd\u63d0\u5347\u624b\u6bb5\uff0c\u7279\u522b\u662f\u5728\u65e0\u4eba\u673a\u56fe\u50cf\u7b49\u6807\u6ce8\u7a00\u7f3a\u7684\u5e94\u7528\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c"}}
{"id": "2512.12567", "categories": ["cs.LG", "math.CO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.12567", "abs": "https://arxiv.org/abs/2512.12567", "authors": ["Zachary Chase", "Steve Hanneke", "Shay Moran", "Jonathan Shafer"], "title": "Optimal Mistake Bounds for Transductive Online Learning", "comment": null, "summary": "We resolve a 30-year-old open problem concerning the power of unlabeled data in online learning by tightly quantifying the gap between transductive and standard online learning. In the standard setting, the optimal mistake bound is characterized by the Littlestone dimension $d$ of the concept class $H$ (Littlestone 1987). We prove that in the transductive setting, the mistake bound is at least $\u03a9(\\sqrt{d})$. This constitutes an exponential improvement over previous lower bounds of $\u03a9(\\log\\log d)$, $\u03a9(\\sqrt{\\log d})$, and $\u03a9(\\log d)$, due respectively to Ben-David, Kushilevitz, and Mansour (1995, 1997) and Hanneke, Moran, and Shafer (2023). We also show that this lower bound is tight: for every $d$, there exists a class of Littlestone dimension $d$ with transductive mistake bound $O(\\sqrt{d})$. Our upper bound also improves upon the best known upper bound of $(2/3)d$ from Ben-David, Kushilevitz, and Mansour (1997). These results establish a quadratic gap between transductive and standard online learning, thereby highlighting the benefit of advance access to the unlabeled instance sequence. This contrasts with the PAC setting, where transductive and standard learning exhibit similar sample complexities.", "AI": {"tldr": "\u8be5\u8bba\u6587\u89e3\u51b3\u4e86\u5728\u7ebf\u5b66\u4e60\u4e2d\u65e0\u6807\u7b7e\u6570\u636e\u6548\u529b\u768430\u5e74\u5f00\u653e\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u8f6c\u5bfc\u5f0f\u5728\u7ebf\u5b66\u4e60\u4e0e\u6807\u51c6\u5728\u7ebf\u5b66\u4e60\u4e4b\u95f4\u5b58\u5728\u5e73\u65b9\u6839\u5dee\u8ddd\uff0c\u5373\u8f6c\u5bfc\u5f0f\u5b66\u4e60\u7684\u6700\u4f18\u9519\u8bef\u754c\u9650\u4e3a\u03a9(\u221ad)\uff0c\u800c\u6807\u51c6\u5728\u7ebf\u5b66\u4e60\u4e3ad\u3002", "motivation": "\u89e3\u51b3\u5728\u7ebf\u5b66\u4e60\u4e2d\u5173\u4e8e\u65e0\u6807\u7b7e\u6570\u636e\u6548\u529b\u768430\u5e74\u5f00\u653e\u95ee\u9898\uff0c\u91cf\u5316\u8f6c\u5bfc\u5f0f\u5b66\u4e60\u4e0e\u6807\u51c6\u5728\u7ebf\u5b66\u4e60\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u3002\u5728\u6807\u51c6\u5728\u7ebf\u5b66\u4e60\u4e2d\uff0c\u6700\u4f18\u9519\u8bef\u754c\u9650\u7531\u6982\u5ff5\u7c7b\u7684Littlestone\u7ef4\u5ea6d\u51b3\u5b9a\uff0c\u4f46\u8f6c\u5bfc\u5f0f\u5b66\u4e60\uff08\u63d0\u524d\u8bbf\u95ee\u672a\u6807\u8bb0\u5b9e\u4f8b\u5e8f\u5217\uff09\u7684\u754c\u9650\u4e00\u76f4\u672a\u660e\u786e\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660e\u8f6c\u5bfc\u5f0f\u5728\u7ebf\u5b66\u4e60\u7684\u4e0b\u754c\u4e3a\u03a9(\u221ad)\uff0c\u5e76\u6784\u9020\u5177\u4f53\u6982\u5ff5\u7c7b\u8bc1\u660e\u4e0a\u754c\u4e3aO(\u221ad)\uff0c\u4ece\u800c\u5efa\u7acb\u7d27\u786e\u754c\u9650\u3002\u5bf9\u6bd4\u5148\u524d\u7814\u7a76\u7684\u4e0b\u754c\u03a9(log log d)\u3001\u03a9(\u221alog d)\u548c\u03a9(log d)\uff0c\u4ee5\u53ca\u4e0a\u754c(2/3)d\u3002", "result": "\u8bc1\u660e\u4e86\u8f6c\u5bfc\u5f0f\u5728\u7ebf\u5b66\u4e60\u7684\u6700\u4f18\u9519\u8bef\u754c\u9650\u4e3a\u0398(\u221ad)\uff0c\u76f8\u6bd4\u6807\u51c6\u5728\u7ebf\u5b66\u4e60\u7684d\u5b9e\u73b0\u4e86\u6307\u6570\u7ea7\u6539\u8fdb\u3002\u5efa\u7acb\u4e86\u8f6c\u5bfc\u5f0f\u4e0e\u6807\u51c6\u5728\u7ebf\u5b66\u4e60\u4e4b\u95f4\u7684\u5e73\u65b9\u6839\u5dee\u8ddd\uff0c\u8868\u660e\u63d0\u524d\u8bbf\u95ee\u672a\u6807\u8bb0\u5b9e\u4f8b\u5e8f\u5217\u80fd\u5e26\u6765\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u8f6c\u5bfc\u5f0f\u5728\u7ebf\u5b66\u4e60\u76f8\u6bd4\u6807\u51c6\u5728\u7ebf\u5b66\u4e60\u5b58\u5728\u5e73\u65b9\u6839\u4f18\u52bf\uff0c\u8fd9\u4e0ePAC\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u4e24\u8005\u6837\u672c\u590d\u6742\u5ea6\u76f8\u4f3c\u7684\u60c5\u51b5\u5f62\u6210\u9c9c\u660e\u5bf9\u6bd4\uff0c\u51f8\u663e\u4e86\u5728\u7ebf\u5b66\u4e60\u4e2d\u63d0\u524d\u8bbf\u95ee\u672a\u6807\u8bb0\u6570\u636e\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2512.12534", "categories": ["cs.CV", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12534", "abs": "https://arxiv.org/abs/2512.12534", "authors": ["Qi Sun", "Can Wang", "Jiaxiang Shang", "Wensen Feng", "Jing Liao"], "title": "Animus3D: Text-driven 3D Animation via Motion Score Distillation", "comment": "SIGGRAPH Asia 2025", "summary": "We present Animus3D, a text-driven 3D animation framework that generates motion field given a static 3D asset and text prompt. Previous methods mostly leverage the vanilla Score Distillation Sampling (SDS) objective to distill motion from pretrained text-to-video diffusion, leading to animations with minimal movement or noticeable jitter. To address this, our approach introduces a novel SDS alternative, Motion Score Distillation (MSD). Specifically, we introduce a LoRA-enhanced video diffusion model that defines a static source distribution rather than pure noise as in SDS, while another inversion-based noise estimation technique ensures appearance preservation when guiding motion. To further improve motion fidelity, we incorporate explicit temporal and spatial regularization terms that mitigate geometric distortions across time and space. Additionally, we propose a motion refinement module to upscale the temporal resolution and enhance fine-grained details, overcoming the fixed-resolution constraints of the underlying video model. Extensive experiments demonstrate that Animus3D successfully animates static 3D assets from diverse text prompts, generating significantly more substantial and detailed motion than state-of-the-art baselines while maintaining high visual integrity. Code will be released at https://qiisun.github.io/animus3d_page.", "AI": {"tldr": "Animus3D\u662f\u4e00\u4e2a\u6587\u672c\u9a71\u52a8\u76843D\u52a8\u753b\u6846\u67b6\uff0c\u901a\u8fc7Motion Score Distillation\u6280\u672f\u4ece\u9759\u60013D\u8d44\u4ea7\u751f\u6210\u8fd0\u52a8\u573a\uff0c\u76f8\u6bd4\u4f20\u7edfSDS\u65b9\u6cd5\u80fd\u4ea7\u751f\u66f4\u663e\u8457\u3001\u66f4\u6d41\u7545\u7684\u8fd0\u52a8\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f7f\u7528vanilla Score Distillation Sampling (SDS)\u4ece\u9884\u8bad\u7ec3\u6587\u672c\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u63d0\u53d6\u8fd0\u52a8\uff0c\u5bfc\u81f4\u52a8\u753b\u8fd0\u52a8\u5e45\u5ea6\u5c0f\u6216\u51fa\u73b0\u660e\u663e\u6296\u52a8\uff0c\u9700\u8981\u6539\u8fdb\u8fd0\u52a8\u751f\u6210\u8d28\u91cf\u548c\u89c6\u89c9\u5b8c\u6574\u6027\u3002", "method": "\u63d0\u51faMotion Score Distillation (MSD)\u66ff\u4ee3SDS\uff0c\u4f7f\u7528LoRA\u589e\u5f3a\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\u5b9a\u4e49\u9759\u6001\u6e90\u5206\u5e03\uff0c\u7ed3\u5408\u53cd\u8f6c\u566a\u58f0\u4f30\u8ba1\u6280\u672f\u4fdd\u6301\u5916\u89c2\u4e00\u81f4\u6027\uff1b\u52a0\u5165\u65f6\u95f4\u548c\u7a7a\u95f4\u6b63\u5219\u5316\u9879\u51cf\u5c11\u51e0\u4f55\u5931\u771f\uff1b\u63d0\u51fa\u8fd0\u52a8\u7ec6\u5316\u6a21\u5757\u63d0\u5347\u65f6\u95f4\u5206\u8fa8\u7387\u548c\u7ec6\u8282\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAnimus3D\u80fd\u6210\u529f\u4ece\u591a\u6837\u6587\u672c\u63d0\u793a\u4e2d\u52a8\u753b\u5316\u9759\u60013D\u8d44\u4ea7\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u65b9\u6cd5\u751f\u6210\u66f4\u663e\u8457\u3001\u66f4\u8be6\u7ec6\u7684\u8fd0\u52a8\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u89c6\u89c9\u5b8c\u6574\u6027\u3002", "conclusion": "Animus3D\u901a\u8fc7\u521b\u65b0\u7684MSD\u65b9\u6cd5\u3001\u6b63\u5219\u5316\u6280\u672f\u548c\u8fd0\u52a8\u7ec6\u5316\u6a21\u5757\uff0c\u6709\u6548\u89e3\u51b3\u4e863D\u52a8\u753b\u751f\u6210\u4e2d\u7684\u8fd0\u52a8\u5e45\u5ea6\u4e0d\u8db3\u548c\u6296\u52a8\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u6587\u672c\u9a71\u52a83D\u52a8\u753b\u751f\u6210\u3002"}}
{"id": "2512.12572", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.12572", "abs": "https://arxiv.org/abs/2512.12572", "authors": ["Ittai Rubinstein", "Samuel B. Hopkins"], "title": "On the Accuracy of Newton Step and Influence Function Data Attributions", "comment": null, "summary": "Data attribution aims to explain model predictions by estimating how they would change if certain training points were removed, and is used in a wide range of applications, from interpretability and credit assignment to unlearning and privacy.\n  Even in the relatively simple case of linear regressions, existing mathematical analyses of leading data attribution methods such as Influence Functions (IF) and single Newton Step (NS) remain limited in two key ways. First, they rely on global strong convexity assumptions which are often not satisfied in practice. Second, the resulting bounds scale very poorly with the number of parameters ($d$) and the number of samples removed ($k$). As a result, these analyses are not tight enough to answer fundamental questions such as \"what is the asymptotic scaling of the errors of each method?\" or \"which of these methods is more accurate for a given dataset?\"\n  In this paper, we introduce a new analysis of the NS and IF data attribution methods for convex learning problems. To the best of our knowledge, this is the first analysis of these questions that does not assume global strong convexity and also the first explanation of [KATL19] and [RH25a]'s observation that NS data attribution is often more accurate than IF. We prove that for sufficiently well-behaved logistic regression, our bounds are asymptotically tight up to poly-logarithmic factors, yielding scaling laws for the errors in the average-case sample removals.\n  \\[ \\mathbb{E}_{T \\subseteq [n],\\, |T| = k} \\bigl[ \\|\\hat\u03b8_T - \\hat\u03b8_T^{\\mathrm{NS}}\\|_2 \\bigr] = \\widetilde\u0398\\!\\left(\\frac{k d}{n^2}\\right), \\qquad \\mathbb{E}_{T \\subseteq [n],\\, |T| = k} \\bigl[ \\|\\hat\u03b8_T^{\\mathrm{NS}} - \\hat\u03b8_T^{\\mathrm{IF}}\\|_2 \\bigr] = \\widetilde\u0398\\!\\left( \\frac{(k + d)\\sqrt{k d}}{n^2} \\right). \\]", "AI": {"tldr": "\u672c\u6587\u5bf9\u6570\u636e\u5f52\u56e0\u65b9\u6cd5\uff08NS\u548cIF\uff09\u8fdb\u884c\u4e86\u65b0\u7684\u7406\u8bba\u5206\u6790\uff0c\u9996\u6b21\u5728\u4e0d\u5047\u8bbe\u5168\u5c40\u5f3a\u51f8\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u4e3a\u51f8\u5b66\u4e60\u95ee\u9898\u63d0\u4f9b\u4e86\u6e10\u8fd1\u7d27\u81f4\u7684\u8bef\u5dee\u754c\u9650\uff0c\u5e76\u89e3\u91ca\u4e86NS\u65b9\u6cd5\u901a\u5e38\u6bd4IF\u66f4\u51c6\u786e\u7684\u73b0\u8c61\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u5f52\u56e0\u65b9\u6cd5\uff08\u5982\u5f71\u54cd\u51fd\u6570IF\u548c\u5355\u725b\u987f\u6b65NS\uff09\u7684\u7406\u8bba\u5206\u6790\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u5c40\u9650\uff1a1\uff09\u4f9d\u8d56\u5168\u5c40\u5f3a\u51f8\u6027\u5047\u8bbe\uff0c\u8fd9\u5728\u5b9e\u8df5\u4e2d\u5f80\u5f80\u4e0d\u6210\u7acb\uff1b2\uff09\u8bef\u5dee\u754c\u9650\u5728\u53c2\u6570\u6570\u91cfd\u548c\u79fb\u9664\u6837\u672c\u6570k\u4e0a\u7f29\u653e\u5f88\u5dee\u3002\u8fd9\u5bfc\u81f4\u65e0\u6cd5\u56de\u7b54\"\u5404\u65b9\u6cd5\u7684\u8bef\u5dee\u6e10\u8fd1\u7f29\u653e\u89c4\u5f8b\u662f\u4ec0\u4e48\"\u548c\"\u54ea\u79cd\u65b9\u6cd5\u5bf9\u7ed9\u5b9a\u6570\u636e\u96c6\u66f4\u51c6\u786e\"\u7b49\u57fa\u672c\u95ee\u9898\u3002", "method": "\u9488\u5bf9\u51f8\u5b66\u4e60\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u5bf9NS\u548cIF\u6570\u636e\u5f52\u56e0\u65b9\u6cd5\u7684\u65b0\u5206\u6790\u6846\u67b6\u3002\u8be5\u5206\u6790\u4e0d\u4f9d\u8d56\u5168\u5c40\u5f3a\u51f8\u6027\u5047\u8bbe\uff0c\u901a\u8fc7\u6570\u5b66\u63a8\u5bfc\u8bc1\u660e\u4e86\u5bf9\u4e8e\u884c\u4e3a\u826f\u597d\u7684\u903b\u8f91\u56de\u5f52\uff0c\u6240\u5f97\u754c\u9650\u5728\u591a\u9879\u5f0f\u5bf9\u6570\u56e0\u5b50\u5185\u662f\u6e10\u8fd1\u7d27\u81f4\u7684\u3002", "result": "\u63a8\u5bfc\u51fa\u4e86NS\u65b9\u6cd5\u548cIF\u65b9\u6cd5\u7684\u5e73\u5747\u8bef\u5dee\u7f29\u653e\u89c4\u5f8b\uff1aNS\u65b9\u6cd5\u7684\u8bef\u5dee\u4e3a\u00d5(kd/n\u00b2)\uff0cNS\u4e0eIF\u4e4b\u95f4\u7684\u5dee\u5f02\u4e3a\u00d5((k+d)\u221a(kd)/n\u00b2)\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660eNS\u65b9\u6cd5\u901a\u5e38\u6bd4IF\u66f4\u51c6\u786e\uff0c\u89e3\u91ca\u4e86\u5148\u524d\u5b9e\u9a8c\u89c2\u5bdf\u5230\u7684\u73b0\u8c61\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u5728\u4e0d\u5047\u8bbe\u5168\u5c40\u5f3a\u51f8\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u4e3a\u6570\u636e\u5f52\u56e0\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6e10\u8fd1\u7d27\u81f4\u7684\u7406\u8bba\u5206\u6790\uff0c\u63ed\u793a\u4e86NS\u65b9\u6cd5\u76f8\u5bf9\u4e8eIF\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u5e76\u7ed9\u51fa\u4e86\u660e\u786e\u7684\u8bef\u5dee\u7f29\u653e\u89c4\u5f8b\uff0c\u4e3a\u6570\u636e\u5f52\u56e0\u7684\u7406\u8bba\u7406\u89e3\u548c\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2512.12539", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12539", "abs": "https://arxiv.org/abs/2512.12539", "authors": ["Huan Huang", "Michele Esposito", "Chen Zhao"], "title": "Anatomy Guided Coronary Artery Segmentation from CCTA Using Spatial Frequency Joint Modeling", "comment": "6 figures", "summary": "Accurate coronary artery segmentation from coronary computed tomography angiography is essential for quantitative coronary analysis and clinical decision support. Nevertheless, reliable segmentation remains challenging because of small vessel calibers, complex branching, blurred boundaries, and myocardial interference. We propose a coronary artery segmentation framework that integrates myocardial anatomical priors, structure aware feature encoding, and three dimensional wavelet inverse wavelet transformations. Myocardial priors and residual attention based feature enhancement are incorporated during encoding to strengthen coronary structure representation. Wavelet inverse wavelet based downsampling and upsampling enable joint spatial frequency modeling and preserve multi scale structural consistency, while a multi scale feature fusion module integrates semantic and geometric information in the decoding stage. The model is trained and evaluated on the public ImageCAS dataset using a 3D overlapping patch based strategy with a 7:1:2 split for training, validation, and testing. Experimental results demonstrate that the proposed method achieves a Dice coefficient of 0.8082, Sensitivity of 0.7946, Precision of 0.8471, and an HD95 of 9.77 mm, outperforming several mainstream segmentation models. Ablation studies further confirm the complementary contributions of individual components. The proposed method enables more stable and consistent coronary artery segmentation under complex geometric conditions, providing reliable segmentation results for subsequent coronary structure analysis tasks.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5fc3\u808c\u89e3\u5256\u5148\u9a8c\u3001\u7ed3\u6784\u611f\u77e5\u7279\u5f81\u7f16\u7801\u548c\u4e09\u7ef4\u5c0f\u6ce2\u53d8\u6362\u7684\u51a0\u72b6\u52a8\u8109\u5206\u5272\u6846\u67b6\uff0c\u5728ImageCAS\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u4e3b\u6d41\u5206\u5272\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u51a0\u72b6\u52a8\u8109CT\u8840\u7ba1\u6210\u50cf\u7684\u51c6\u786e\u5206\u5272\u5bf9\u4e8e\u5b9a\u91cf\u5206\u6790\u548c\u4e34\u5e8a\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u8840\u7ba1\u7ec6\u5c0f\u3001\u5206\u652f\u590d\u6742\u3001\u8fb9\u754c\u6a21\u7cca\u4ee5\u53ca\u5fc3\u808c\u5e72\u6270\u7b49\u56e0\u7d20\uff0c\u53ef\u9760\u7684\u5206\u5272\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u51a0\u72b6\u52a8\u8109\u5206\u5272\u6846\u67b6\uff0c\u6574\u5408\u5fc3\u808c\u89e3\u5256\u5148\u9a8c\u3001\u7ed3\u6784\u611f\u77e5\u7279\u5f81\u7f16\u7801\u548c\u4e09\u7ef4\u5c0f\u6ce2-\u9006\u5c0f\u6ce2\u53d8\u6362\u3002\u7f16\u7801\u9636\u6bb5\u7ed3\u5408\u5fc3\u808c\u5148\u9a8c\u548c\u57fa\u4e8e\u6b8b\u5dee\u6ce8\u610f\u529b\u7684\u7279\u5f81\u589e\u5f3a\u6765\u5f3a\u5316\u51a0\u72b6\u52a8\u8109\u7ed3\u6784\u8868\u793a\uff1b\u5c0f\u6ce2-\u9006\u5c0f\u6ce2\u7684\u4e0b\u91c7\u6837\u548c\u4e0a\u91c7\u6837\u5b9e\u73b0\u8054\u5408\u7a7a\u95f4\u9891\u7387\u5efa\u6a21\u5e76\u4fdd\u6301\u591a\u5c3a\u5ea6\u7ed3\u6784\u4e00\u81f4\u6027\uff1b\u89e3\u7801\u9636\u6bb5\u901a\u8fc7\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u6a21\u5757\u6574\u5408\u8bed\u4e49\u548c\u51e0\u4f55\u4fe1\u606f\u3002", "result": "\u5728\u516c\u5f00ImageCAS\u6570\u636e\u96c6\u4e0a\u4f7f\u75283D\u91cd\u53e0\u8865\u4e01\u7b56\u7565\uff087:1:2\u5212\u5206\u8bad\u7ec3/\u9a8c\u8bc1/\u6d4b\u8bd5\uff09\u8fdb\u884c\u8bc4\u4f30\uff0c\u83b7\u5f97Dice\u7cfb\u65700.8082\u3001\u654f\u611f\u60270.7946\u3001\u7cbe\u786e\u5ea60.8471\u3001HD95\u4e3a9.77mm\uff0c\u4f18\u4e8e\u591a\u4e2a\u4e3b\u6d41\u5206\u5272\u6a21\u578b\u3002\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u5404\u7ec4\u6210\u90e8\u5206\u7684\u4e92\u8865\u8d21\u732e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u51e0\u4f55\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u66f4\u7a33\u5b9a\u548c\u4e00\u81f4\u7684\u51a0\u72b6\u52a8\u8109\u5206\u5272\uff0c\u4e3a\u540e\u7eed\u51a0\u72b6\u52a8\u8109\u7ed3\u6784\u5206\u6790\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u5206\u5272\u7ed3\u679c\u3002"}}
{"id": "2512.12560", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12560", "abs": "https://arxiv.org/abs/2512.12560", "authors": ["Xinqi Jin", "Hanxun Yu", "Bohan Yu", "Kebin Liu", "Jian Liu", "Keda Tao", "Yixuan Pei", "Huan Wang", "Fan Dang", "Jiangchuan Liu", "Weiqiang Wang"], "title": "StreamingAssistant: Efficient Visual Token Pruning for Accelerating Online Video Understanding", "comment": null, "summary": "Online video understanding is essential for applications like public surveillance and AI glasses. However, applying Multimodal Large Language Models (MLLMs) to this domain is challenging due to the large number of video frames, resulting in high GPU memory usage and computational latency. To address these challenges, we propose token pruning as a means to reduce context length while retaining critical information. Specifically, we introduce a novel redundancy metric, Maximum Similarity to Spatially Adjacent Video Tokens (MSSAVT), which accounts for both token similarity and spatial position. To mitigate the bidirectional dependency between pruning and redundancy, we further design a masked pruning strategy that ensures only mutually unadjacent tokens are pruned. We also integrate an existing temporal redundancy-based pruning method to eliminate temporal redundancy of the video modality. Experimental results on multiple online and offline video understanding benchmarks demonstrate that our method significantly improves the accuracy (i.e., by 4\\% at most) while incurring a negligible pruning latency (i.e., less than 1ms). Our full implementation will be made publicly available.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u9891token\u526a\u679d\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5927\u7a7a\u95f4\u76f8\u90bb\u89c6\u9891token\u76f8\u4f3c\u5ea6(MSSAVT)\u6307\u6807\u548c\u63a9\u7801\u526a\u679d\u7b56\u7565\uff0c\u5728\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\u4fdd\u6301\u89c6\u9891\u7406\u89e3\u6027\u80fd\u3002", "motivation": "\u5728\u7ebf\u89c6\u9891\u7406\u89e3\u5728\u516c\u5171\u76d1\u63a7\u548cAI\u773c\u955c\u7b49\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u89c6\u9891\u65f6\u9762\u4e34\u5e27\u6570\u8fc7\u591a\u5bfc\u81f4\u7684GPU\u5185\u5b58\u5360\u7528\u9ad8\u548c\u8ba1\u7b97\u5ef6\u8fdf\u5927\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6700\u5927\u7a7a\u95f4\u76f8\u90bb\u89c6\u9891token\u76f8\u4f3c\u5ea6(MSSAVT)\u7684\u5197\u4f59\u5ea6\u91cf\u65b9\u6cd5\uff0c\u7ed3\u5408\u63a9\u7801\u526a\u679d\u7b56\u7565\u907f\u514d\u53cc\u5411\u4f9d\u8d56\u95ee\u9898\uff0c\u5e76\u6574\u5408\u73b0\u6709\u65f6\u95f4\u5197\u4f59\u526a\u679d\u65b9\u6cd5\u6765\u6d88\u9664\u89c6\u9891\u6a21\u6001\u7684\u65f6\u95f4\u5197\u4f59\u3002", "result": "\u5728\u591a\u4e2a\u5728\u7ebf\u548c\u79bb\u7ebf\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6700\u591a\u63d0\u53474%\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u526a\u679d\u5ef6\u8fdf\u53ef\u5ffd\u7565\u4e0d\u8ba1\uff08\u5c0f\u4e8e1\u6beb\u79d2\uff09\u3002", "conclusion": "\u63d0\u51fa\u7684token\u526a\u679d\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u5c11\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u89c6\u9891\u65f6\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u5347\u89c6\u9891\u7406\u89e3\u6027\u80fd\uff0c\u4e3a\u5b9e\u65f6\u89c6\u9891\u7406\u89e3\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12605", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12605", "abs": "https://arxiv.org/abs/2512.12605", "authors": ["Pranav Gupta", "Nithin Surendran"], "title": "Causal inference and model explainability tools for retail", "comment": null, "summary": "Most major retailers today have multiple divisions focused on various aspects, such as marketing, supply chain, online customer experience, store customer experience, employee productivity, and vendor fulfillment. They also regularly collect data corresponding to all these aspects as dashboards and weekly/monthly/quarterly reports. Although several machine learning and statistical techniques have been in place to analyze and predict key metrics, such models typically lack interpretability. Moreover, such techniques also do not allow the validation or discovery of causal links. In this paper, we aim to provide a recipe for applying model interpretability and causal inference for deriving sales insights. In this paper, we review the existing literature on causal inference and interpretability in the context of problems in e-commerce and retail, and apply them to a real-world dataset. We find that an inherently explainable model has a lower variance of SHAP values, and show that including multiple confounders through a double machine learning approach allows us to get the correct sign of causal effect.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4e3a\u96f6\u552e\u5546\u63d0\u4f9b\u5e94\u7528\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u56e0\u679c\u63a8\u65ad\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u771f\u5b9e\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u53ef\u89e3\u91ca\u6a21\u578b\u5177\u6709\u66f4\u7a33\u5b9a\u7684SHAP\u503c\uff0c\u5e76\u5c55\u793a\u4e86\u53cc\u91cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u80fd\u6b63\u786e\u8bc6\u522b\u56e0\u679c\u6548\u5e94\u7b26\u53f7\u3002", "motivation": "\u5f53\u524d\u96f6\u552e\u5546\u867d\u7136\u6536\u96c6\u5927\u91cf\u6570\u636e\u5e76\u5e94\u7528\u673a\u5668\u5b66\u4e60\u6280\u672f\u5206\u6790\u5173\u952e\u6307\u6807\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u4e14\u65e0\u6cd5\u9a8c\u8bc1\u6216\u53d1\u73b0\u56e0\u679c\u5173\u7cfb\uff0c\u8fd9\u9650\u5236\u4e86\u4ece\u6570\u636e\u4e2d\u83b7\u5f97\u6df1\u5165\u4e1a\u52a1\u6d1e\u5bdf\u7684\u80fd\u529b\u3002", "method": "\u8bba\u6587\u56de\u987e\u4e86\u56e0\u679c\u63a8\u65ad\u548c\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u5728\u7535\u5546\u96f6\u552e\u9886\u57df\u7684\u73b0\u6709\u6587\u732e\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u771f\u5b9e\u6570\u636e\u96c6\uff0c\u7279\u522b\u5173\u6ce8\u4e86\u53ef\u89e3\u91ca\u6a21\u578b\u7684SHAP\u503c\u65b9\u5dee\u5206\u6790\uff0c\u4ee5\u53ca\u901a\u8fc7\u53cc\u91cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7eb3\u5165\u591a\u4e2a\u6df7\u6742\u56e0\u7d20\u7684\u6280\u672f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u56fa\u6709\u7684\u53ef\u89e3\u91ca\u6a21\u578b\u5177\u6709\u66f4\u4f4e\u7684SHAP\u503c\u65b9\u5dee\uff1b2\uff09\u901a\u8fc7\u53cc\u91cd\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7eb3\u5165\u591a\u4e2a\u6df7\u6742\u56e0\u7d20\u80fd\u591f\u83b7\u5f97\u6b63\u786e\u7684\u56e0\u679c\u6548\u5e94\u7b26\u53f7\u3002", "conclusion": "\u8bba\u6587\u4e3a\u96f6\u552e\u5546\u63d0\u4f9b\u4e86\u5e94\u7528\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u548c\u56e0\u679c\u63a8\u65ad\u7684\u5b9e\u7528\u6307\u5357\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u80fd\u591f\u5e2e\u52a9\u96f6\u552e\u5546\u4ece\u6570\u636e\u4e2d\u83b7\u5f97\u66f4\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u7684\u4e1a\u52a1\u6d1e\u5bdf\uff0c\u5e76\u9a8c\u8bc1\u56e0\u679c\u5173\u7cfb\u3002"}}
{"id": "2512.12617", "categories": ["cs.LG", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.12617", "abs": "https://arxiv.org/abs/2512.12617", "authors": ["Animesh Mishra"], "title": "Spectral Sentinel: Scalable Byzantine-Robust Decentralized Federated Learning via Sketched Random Matrix Theory on Blockchain", "comment": null, "summary": "Decentralized federated learning (DFL) enables collaborative model training without centralized trust, but it remains vulnerable to Byzantine clients that poison gradients under heterogeneous (Non-IID) data. Existing defenses face a scalability trilemma: distance-based filtering (e.g., Krum) can reject legitimate Non-IID updates, geometric-median methods incur prohibitive $O(n^2 d)$ cost, and many certified defenses are evaluated only on models below 100M parameters. We propose Spectral Sentinel, a Byzantine detection and aggregation framework that leverages a random-matrix-theoretic signature: honest Non-IID gradients produce covariance eigenspectra whose bulk follows the Marchenko-Pastur law, while Byzantine perturbations induce detectable tail anomalies. Our algorithm combines Frequent Directions sketching with data-dependent MP tracking, enabling detection on models up to 1.5B parameters using $O(k^2)$ memory with $k \\ll d$. Under a $(\u03c3,f)$ threat model with coordinate-wise honest variance bounded by $\u03c3^2$ and $f < 1/2$ adversaries, we prove $(\u03b5,\u03b4)$-Byzantine resilience with convergence rate $O(\u03c3f / \\sqrt{T} + f^2 / T)$, and we provide a matching information-theoretic lower bound $\u03a9(\u03c3f / \\sqrt{T})$, establishing minimax optimality. We implement the full system with blockchain integration on Polygon networks and validate it across 144 attack-aggregator configurations, achieving 78.4 percent average accuracy versus 48-63 percent for baseline methods.", "AI": {"tldr": "\u63d0\u51faSpectral Sentinel\u6846\u67b6\uff0c\u5229\u7528\u968f\u673a\u77e9\u9635\u7406\u8bba\u68c0\u6d4b\u62dc\u5360\u5ead\u653b\u51fb\uff0c\u5728\u5f02\u6784\u6570\u636e\u4e0b\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u62dc\u5360\u5ead\u9c81\u68d2\u8054\u90a6\u5b66\u4e60\uff0c\u652f\u6301\u9ad8\u8fbe15\u4ebf\u53c2\u6570\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\u9632\u5fa1\u9762\u4e34\u53ef\u6269\u5c55\u6027\u4e09\u96be\u56f0\u5883\uff1a\u8ddd\u79bb\u8fc7\u6ee4\u53ef\u80fd\u62d2\u7edd\u5408\u6cd5\u7684\u975e\u72ec\u7acb\u540c\u5206\u5e03\u66f4\u65b0\uff0c\u51e0\u4f55\u4e2d\u503c\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u591a\u6570\u8ba4\u8bc1\u9632\u5fa1\u4ec5\u8bc4\u4f30\u4f4e\u4e8e1\u4ebf\u53c2\u6570\u7684\u6a21\u578b\u3002", "method": "\u57fa\u4e8e\u968f\u673a\u77e9\u9635\u7406\u8bba\uff0c\u5229\u7528\u8bda\u5b9e\u975e\u72ec\u7acb\u540c\u5206\u5e03\u68af\u5ea6\u534f\u65b9\u5dee\u77e9\u9635\u7279\u5f81\u8c31\u7b26\u5408Marchenko-Pastur\u5b9a\u5f8b\u7684\u7279\u6027\uff0c\u901a\u8fc7Frequent Directions\u8349\u56fe\u6280\u672f\u548c\u6570\u636e\u4f9d\u8d56\u7684MP\u8ddf\u8e2a\u68c0\u6d4b\u62dc\u5360\u5ead\u6270\u52a8\u5f15\u8d77\u7684\u5c3e\u90e8\u5f02\u5e38\u3002", "result": "\u5728144\u79cd\u653b\u51fb-\u805a\u5408\u5668\u914d\u7f6e\u4e2d\u9a8c\u8bc1\uff0c\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u523078.4%\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u768448-63%\uff1b\u652f\u6301\u9ad8\u8fbe15\u4ebf\u53c2\u6570\u6a21\u578b\uff0c\u5185\u5b58\u4f7f\u7528\u4e3aO(k\u00b2)\u4e14k\u226ad\u3002", "conclusion": "Spectral Sentinel\u6846\u67b6\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u89e3\u51b3\u4e86\u53bb\u4e2d\u5fc3\u5316\u8054\u90a6\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u62dc\u5360\u5ead\u9632\u5fa1\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6700\u5c0f\u5316\u6700\u4f18\u7684\u6536\u655b\u901f\u7387\uff0c\u5e76\u901a\u8fc7\u533a\u5757\u94fe\u96c6\u6210\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u53ef\u884c\u6027\u3002"}}
{"id": "2512.12586", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12586", "abs": "https://arxiv.org/abs/2512.12586", "authors": ["Lixin Chen", "Chaomeng Chen", "Jiale Zhou", "Zhijian Wu", "Xun Lin"], "title": "StegaVAR: Privacy-Preserving Video Action Recognition via Steganographic Domain Analysis", "comment": "13 pages, 10 figures. This is the extended version of the paper accepted at AAAI 2026, including related works and appendix", "summary": "Despite the rapid progress of deep learning in video action recognition (VAR) in recent years, privacy leakage in videos remains a critical concern. Current state-of-the-art privacy-preserving methods often rely on anonymization. These methods suffer from (1) low concealment, where producing visually distorted videos that attract attackers' attention during transmission, and (2) spatiotemporal disruption, where degrading essential spatiotemporal features for accurate VAR. To address these issues, we propose StegaVAR, a novel framework that embeds action videos into ordinary cover videos and directly performs VAR in the steganographic domain for the first time. Throughout both data transmission and action analysis, the spatiotemporal information of hidden secret video remains complete, while the natural appearance of cover videos ensures the concealment of transmission. Considering the difficulty of steganographic domain analysis, we propose Secret Spatio-Temporal Promotion (STeP) and Cross-Band Difference Attention (CroDA) for analysis within the steganographic domain. STeP uses the secret video to guide spatiotemporal feature extraction in the steganographic domain during training. CroDA suppresses cover interference by capturing cross-band semantic differences. Experiments demonstrate that StegaVAR achieves superior VAR and privacy-preserving performance on widely used datasets. Moreover, our framework is effective for multiple steganographic models.", "AI": {"tldr": "StegaVAR\u662f\u4e00\u4e2a\u5c06\u52a8\u4f5c\u89c6\u9891\u5d4c\u5165\u5230\u666e\u901a\u5c01\u9762\u89c6\u9891\u4e2d\uff0c\u5e76\u5728\u9690\u5199\u57df\u76f4\u63a5\u8fdb\u884c\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u7684\u65b0\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u4e2d\u4f4e\u9690\u853d\u6027\u548c\u65f6\u7a7a\u7279\u5f81\u7834\u574f\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1) \u4f4e\u9690\u853d\u6027 - \u4ea7\u751f\u89c6\u89c9\u626d\u66f2\u7684\u89c6\u9891\u5728\u4f20\u8f93\u8fc7\u7a0b\u4e2d\u5bb9\u6613\u5f15\u8d77\u653b\u51fb\u8005\u6ce8\u610f\uff1b2) \u65f6\u7a7a\u7834\u574f - \u7834\u574f\u51c6\u786e\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u6240\u9700\u7684\u65f6\u7a7a\u7279\u5f81\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u62a4\u9690\u79c1\u53c8\u4e0d\u5f71\u54cd\u52a8\u4f5c\u8bc6\u522b\u6027\u80fd\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faStegaVAR\u6846\u67b6\uff0c\u5c06\u79d8\u5bc6\u52a8\u4f5c\u89c6\u9891\u5d4c\u5165\u5230\u666e\u901a\u5c01\u9762\u89c6\u9891\u4e2d\uff0c\u76f4\u63a5\u5728\u9690\u5199\u57df\u8fdb\u884c\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u3002\u63d0\u51fa\u4e24\u79cd\u5173\u952e\u6280\u672f\uff1a1) \u79d8\u5bc6\u65f6\u7a7a\u4fc3\u8fdb(STeP) - \u4f7f\u7528\u79d8\u5bc6\u89c6\u9891\u6307\u5bfc\u9690\u5199\u57df\u4e2d\u7684\u65f6\u7a7a\u7279\u5f81\u63d0\u53d6\uff1b2) \u8de8\u6ce2\u6bb5\u5dee\u5f02\u6ce8\u610f\u529b(CroDA) - \u901a\u8fc7\u6355\u6349\u8de8\u6ce2\u6bb5\u8bed\u4e49\u5dee\u5f02\u6765\u6291\u5236\u5c01\u9762\u5e72\u6270\u3002", "result": "\u5b9e\u9a8c\u8868\u660eStegaVAR\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\u548c\u9690\u79c1\u4fdd\u62a4\u6027\u80fd\u3002\u8be5\u6846\u67b6\u5bf9\u591a\u79cd\u9690\u5199\u6a21\u578b\u90fd\u6709\u6548\u3002", "conclusion": "StegaVAR\u9996\u6b21\u5b9e\u73b0\u4e86\u5728\u9690\u5199\u57df\u76f4\u63a5\u8fdb\u884c\u89c6\u9891\u52a8\u4f5c\u8bc6\u522b\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u79d8\u5bc6\u89c6\u9891\u7684\u5b8c\u6574\u65f6\u7a7a\u4fe1\u606f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2512.12590", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.12590", "abs": "https://arxiv.org/abs/2512.12590", "authors": ["Indiwara Nanayakkara", "Dehan Jayawickrama", "Mervyn Parakrama B. Ekanayake"], "title": "Automatic Wire-Harness Color Sequence Detector", "comment": "6 pages, 20 figures, IEEE ICIIS 2025 Conference - Accepted", "summary": "Wire harness inspection process remains a labor-intensive process prone to errors in the modern Electronics Manufacturing Services (EMS) industry. This paper introduces a semiautomated machine vision system capable of verifying correct wire positioning, correctness of the connector polarity and correctness of color sequences for both linear and circular wire harness configurations. Five industrial standard CMOS cameras are integrated into a modularized mechanical framework in the physical structure of the solution and a HSV and RGB color domain value comparison based color sequence classifier is used in the operation. For each harness batch, a user can train the system using at least five reference samples; the trained file is stored and reused for similar harness types. The Solution is deployed at GPV Lanka Pvt. Ltd. (Fig. 2) and the system achieved 100% detection accuracy and reduced inspection time by 44% compared to manual methods. Additional features include user management, adjustable lighting, session data storage, and secure login. Results of this product usage in the real world situation demonstrate that this approach delivers reliable and efficient inspection capabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7ebf\u675f\u68c0\u6d4b\u7684\u534a\u81ea\u52a8\u5316\u673a\u5668\u89c6\u89c9\u7cfb\u7edf\uff0c\u53ef\u9a8c\u8bc1\u7ebf\u675f\u4f4d\u7f6e\u3001\u8fde\u63a5\u5668\u6781\u6027\u548c\u989c\u8272\u5e8f\u5217\u7684\u6b63\u786e\u6027\uff0c\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u5b9e\u73b0\u4e86100%\u68c0\u6d4b\u7cbe\u5ea6\u548c44%\u7684\u65f6\u95f4\u8282\u7701\u3002", "motivation": "\u73b0\u4ee3\u7535\u5b50\u5236\u9020\u670d\u52a1\u884c\u4e1a\u4e2d\uff0c\u7ebf\u675f\u68c0\u6d4b\u8fc7\u7a0b\u4ecd\u7136\u662f\u52b3\u52a8\u5bc6\u96c6\u578b\u4e14\u5bb9\u6613\u51fa\u9519\u7684\u8fc7\u7a0b\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u6765\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u91c7\u7528\u4e94\u4e2a\u5de5\u4e1a\u7ea7CMOS\u6444\u50cf\u5934\u96c6\u6210\u5230\u6a21\u5757\u5316\u673a\u68b0\u6846\u67b6\u4e2d\uff0c\u4f7f\u7528\u57fa\u4e8eHSV\u548cRGB\u989c\u8272\u57df\u503c\u6bd4\u8f83\u7684\u989c\u8272\u5e8f\u5217\u5206\u7c7b\u5668\uff0c\u7528\u6237\u53ef\u901a\u8fc7\u81f3\u5c11\u4e94\u4e2a\u53c2\u8003\u6837\u672c\u8bad\u7ec3\u7cfb\u7edf\uff0c\u8bad\u7ec3\u6587\u4ef6\u53ef\u5b58\u50a8\u5e76\u91cd\u590d\u7528\u4e8e\u7c7b\u4f3c\u7ebf\u675f\u7c7b\u578b\u3002", "result": "\u7cfb\u7edf\u5728GPV Lanka Pvt. Ltd.\u90e8\u7f72\u540e\u5b9e\u73b0\u4e86100%\u7684\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u76f8\u6bd4\u4eba\u5de5\u65b9\u6cd5\u51cf\u5c11\u4e8644%\u7684\u68c0\u6d4b\u65f6\u95f4\uff0c\u5e76\u5177\u5907\u7528\u6237\u7ba1\u7406\u3001\u53ef\u8c03\u7167\u660e\u3001\u4f1a\u8bdd\u6570\u636e\u5b58\u50a8\u548c\u5b89\u5168\u767b\u5f55\u7b49\u9644\u52a0\u529f\u80fd\u3002", "conclusion": "\u8be5\u534a\u81ea\u52a8\u5316\u673a\u5668\u89c6\u89c9\u7cfb\u7edf\u5728\u5b9e\u9645\u5de5\u4e1a\u5e94\u7528\u4e2d\u8bc1\u660e\u4e86\u5176\u53ef\u9760\u6027\u548c\u9ad8\u6548\u6027\uff0c\u4e3a\u7ebf\u675f\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12595", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12595", "abs": "https://arxiv.org/abs/2512.12595", "authors": ["Karthikeya KV"], "title": "Vision-Enhanced Large Language Models for High-Resolution Image Synthesis and Multimodal Data Interpretation", "comment": null, "summary": "This research introduces a transformative framework for integrating Vision-Enhanced Large Language Models (LLMs) with advanced transformer-based architectures to tackle challenges in high-resolution image synthesis and multimodal data interpretation. The proposed model incorporates a rectified flow mechanism that connects noise and data with linear paths, enabling efficient and high-quality generation. A bidirectional tokenization strategy is employed to seamlessly merge inputs from text, image, and video modalities, fostering a unified understanding across diverse data types. By embedding spatial-temporal features and leveraging a hybrid text-image sequence modeling approach, the framework achieves unparalleled fidelity in synthesized images and coherent multimodal representations. The architecture is optimized with a noise-aware learning algorithm, addressing discrepancies in noisy data distributions and improving generative performance under varying input conditions. Rigorous evaluations on benchmark datasets demonstrate a 25% increase in image resolution clarity and a 20% reduction in computational requirements compared to diffusion-based methods. Furthermore, the model exhibits robust scalability and adaptability, showcasing its potential in applications like autonomous systems, creative content generation, and advanced video analysis. This work underscores the role of vision-centric LLMs in redefining capabilities in computer vision and multimodal artificial intelligence.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5148\u8fdbTransformer\u67b6\u6784\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u6d41\u6d41\u673a\u5236\u548c\u53cc\u5411\u6807\u8bb0\u5316\u7b56\u7565\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5408\u6210\u548c\u591a\u6a21\u6001\u6570\u636e\u7406\u89e3\uff0c\u76f8\u6bd4\u6269\u6563\u65b9\u6cd5\u63d0\u534725%\u56fe\u50cf\u6e05\u6670\u5ea6\u5e76\u51cf\u5c1120%\u8ba1\u7b97\u9700\u6c42\u3002", "motivation": "\u9488\u5bf9\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5408\u6210\u548c\u591a\u6a21\u6001\u6570\u636e\u89e3\u91ca\u4e2d\u7684\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u7edf\u4e00\u5904\u7406\u6587\u672c\u3001\u56fe\u50cf\u548c\u89c6\u9891\u7b49\u591a\u79cd\u6570\u636e\u7c7b\u578b\u7684\u6846\u67b6\uff0c\u540c\u65f6\u63d0\u9ad8\u751f\u6210\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u91c7\u7528\u6574\u6d41\u6d41\u673a\u5236\u8fde\u63a5\u566a\u58f0\u548c\u6570\u636e\uff0c\u4f7f\u7528\u53cc\u5411\u6807\u8bb0\u5316\u7b56\u7565\u878d\u5408\u6587\u672c\u3001\u56fe\u50cf\u548c\u89c6\u9891\u8f93\u5165\uff0c\u5d4c\u5165\u65f6\u7a7a\u7279\u5f81\uff0c\u91c7\u7528\u6df7\u5408\u6587\u672c-\u56fe\u50cf\u5e8f\u5217\u5efa\u6a21\u65b9\u6cd5\uff0c\u5e76\u4f18\u5316\u566a\u58f0\u611f\u77e5\u5b66\u4e60\u7b97\u6cd5\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u6bd4\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\uff0c\u56fe\u50cf\u5206\u8fa8\u7387\u6e05\u6670\u5ea6\u63d0\u534725%\uff0c\u8ba1\u7b97\u9700\u6c42\u51cf\u5c1120%\uff0c\u6a21\u578b\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c55\u793a\u4e86\u89c6\u89c9\u4e2d\u5fc3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u91cd\u65b0\u5b9a\u4e49\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u591a\u6a21\u6001\u4eba\u5de5\u667a\u80fd\u80fd\u529b\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4e3a\u81ea\u4e3b\u7cfb\u7edf\u3001\u521b\u610f\u5185\u5bb9\u751f\u6210\u548c\u9ad8\u7ea7\u89c6\u9891\u5206\u6790\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u529b\u5de5\u5177\u3002"}}
{"id": "2512.12669", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12669", "abs": "https://arxiv.org/abs/2512.12669", "authors": ["Jiawei Shen", "Jia Zhu", "Hanghui Guo", "Weijie Shi", "Guoqing Ma", "Yidan Liang", "Jingjiang Liu", "Hao Chen", "Shimin Di"], "title": "DynaGen: Unifying Temporal Knowledge Graph Reasoning with Dynamic Subgraphs and Generative Regularization", "comment": null, "summary": "Temporal Knowledge Graph Reasoning (TKGR) aims to complete missing factual elements along the timeline. Depending on the temporal position of the query, the task is categorized into interpolation and extrapolation. Existing interpolation methods typically embed temporal information into individual facts to complete missing historical knowledge, while extrapolation techniques often leverage sequence models over graph snapshots to identify recurring patterns for future event prediction. These methods face two critical challenges: limited contextual modeling in interpolation and cognitive generalization bias in extrapolation. To address these, we propose a unified method for TKGR, dubbed DynaGen. For interpolation, DynaGen dynamically constructs entity-centric subgraphs and processes them with a synergistic dual-branch GNN encoder to capture evolving structural context. For extrapolation, it applies a conditional diffusion process, which forces the model to learn underlying evolutionary principles rather than just superficial patterns, enhancing its ability to predict unseen future events. Extensive experiments on six benchmark datasets show DynaGen achieves state-of-the-art performance. On average, compared to the second-best models, DynaGen improves the Mean Reciprocal Rank (MRR) score by 2.61 points for interpolation and 1.45 points for extrapolation.", "AI": {"tldr": "DynaGen\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u6784\u5efa\u5b9e\u4f53\u4e2d\u5fc3\u5b50\u56fe\u548c\u6761\u4ef6\u6269\u6563\u8fc7\u7a0b\uff0c\u5206\u522b\u89e3\u51b3\u63d2\u503c\u548c\u5916\u63a8\u4efb\u52a1\uff0c\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u65b9\u6cd5\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u63d2\u503c\u65b9\u6cd5\u4e2d\u7684\u6709\u9650\u4e0a\u4e0b\u6587\u5efa\u6a21\uff0c\u4ee5\u53ca\u5916\u63a8\u65b9\u6cd5\u4e2d\u7684\u8ba4\u77e5\u6cdb\u5316\u504f\u5dee\u3002\u63d2\u503c\u65b9\u6cd5\u901a\u5e38\u5c06\u65f6\u95f4\u4fe1\u606f\u5d4c\u5165\u5230\u5355\u4e2a\u4e8b\u5b9e\u4e2d\uff0c\u800c\u5916\u63a8\u6280\u672f\u5219\u5229\u7528\u5e8f\u5217\u6a21\u578b\u6765\u8bc6\u522b\u91cd\u590d\u6a21\u5f0f\uff0c\u4f46\u90fd\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "DynaGen\u91c7\u7528\u7edf\u4e00\u65b9\u6cd5\uff1a\u5bf9\u4e8e\u63d2\u503c\u4efb\u52a1\uff0c\u52a8\u6001\u6784\u5efa\u5b9e\u4f53\u4e2d\u5fc3\u5b50\u56fe\uff0c\u5e76\u4f7f\u7528\u534f\u540c\u53cc\u5206\u652fGNN\u7f16\u7801\u5668\u6355\u6349\u6f14\u5316\u7ed3\u6784\u4e0a\u4e0b\u6587\uff1b\u5bf9\u4e8e\u5916\u63a8\u4efb\u52a1\uff0c\u5e94\u7528\u6761\u4ef6\u6269\u6563\u8fc7\u7a0b\uff0c\u8feb\u4f7f\u6a21\u578b\u5b66\u4e60\u5e95\u5c42\u6f14\u5316\u539f\u7406\u800c\u975e\u8868\u9762\u6a21\u5f0f\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDynaGen\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u4e0e\u6b21\u4f18\u6a21\u578b\u76f8\u6bd4\uff0c\u5e73\u5747MRR\u5206\u6570\u5728\u63d2\u503c\u4efb\u52a1\u4e0a\u63d0\u9ad8\u4e862.61\u5206\uff0c\u5728\u5916\u63a8\u4efb\u52a1\u4e0a\u63d0\u9ad8\u4e861.45\u5206\u3002", "conclusion": "DynaGen\u901a\u8fc7\u52a8\u6001\u4e0a\u4e0b\u6587\u5efa\u6a21\u548c\u6761\u4ef6\u6269\u6563\u8fc7\u7a0b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u4e2d\u7684\u63d2\u503c\u548c\u5916\u63a8\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u7edf\u4e00\u7684\u5f3a\u5927\u6846\u67b6\u3002"}}
{"id": "2512.12596", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12596", "abs": "https://arxiv.org/abs/2512.12596", "authors": ["Kei Yoshitake", "Kento Hosono", "Ken Kobayashi", "Kazuhide Nakata"], "title": "Content-Aware Ad Banner Layout Generation with Two-Stage Chain-of-Thought in Vision Language Models", "comment": null, "summary": "In this paper, we propose a method for generating layouts for image-based advertisements by leveraging a Vision-Language Model (VLM). Conventional advertisement layout techniques have predominantly relied on saliency mapping to detect salient regions within a background image, but such approaches often fail to fully account for the image's detailed composition and semantic content. To overcome this limitation, our method harnesses a VLM to recognize the products and other elements depicted in the background and to inform the placement of text and logos. The proposed layout-generation pipeline consists of two steps. In the first step, the VLM analyzes the image to identify object types and their spatial relationships, then produces a text-based \"placement plan\" based on this analysis. In the second step, that plan is rendered into the final layout by generating HTML-format code. We validated the effectiveness of our approach through evaluation experiments, conducting both quantitative and qualitative comparisons against existing methods. The results demonstrate that by explicitly considering the background image's content, our method produces noticeably higher-quality advertisement layouts.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLM)\u7684\u56fe\u50cf\u5e7f\u544a\u5e03\u5c40\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u80cc\u666f\u56fe\u50cf\u5185\u5bb9\u6765\u667a\u80fd\u653e\u7f6e\u6587\u672c\u548clogo\uff0c\u76f8\u6bd4\u4f20\u7edf\u663e\u8457\u6027\u6620\u5c04\u65b9\u6cd5\u80fd\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u5e7f\u544a\u5e03\u5c40\u3002", "motivation": "\u4f20\u7edf\u5e7f\u544a\u5e03\u5c40\u6280\u672f\u4e3b\u8981\u4f9d\u8d56\u663e\u8457\u6027\u6620\u5c04\u6765\u68c0\u6d4b\u80cc\u666f\u56fe\u50cf\u4e2d\u7684\u663e\u8457\u533a\u57df\uff0c\u4f46\u8fd9\u79cd\u65b9\u6cd5\u5f80\u5f80\u65e0\u6cd5\u5145\u5206\u8003\u8651\u56fe\u50cf\u7684\u8be6\u7ec6\u6784\u56fe\u548c\u8bed\u4e49\u5185\u5bb9\uff0c\u5bfc\u81f4\u5e03\u5c40\u8d28\u91cf\u6709\u9650\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6d41\u7a0b\uff1a1) VLM\u5206\u6790\u56fe\u50cf\u8bc6\u522b\u7269\u4f53\u7c7b\u578b\u548c\u7a7a\u95f4\u5173\u7cfb\uff0c\u751f\u6210\u57fa\u4e8e\u6587\u672c\u7684\"\u653e\u7f6e\u8ba1\u5212\"\uff1b2) \u5c06\u8be5\u8ba1\u5212\u6e32\u67d3\u4e3aHTML\u683c\u5f0f\u7684\u6700\u7ec8\u5e03\u5c40\u4ee3\u7801\u3002", "result": "\u901a\u8fc7\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u80fd\u751f\u6210\u660e\u663e\u66f4\u9ad8\u8d28\u91cf\u7684\u5e7f\u544a\u5e03\u5c40\uff0c\u7279\u522b\u662f\u901a\u8fc7\u663e\u5f0f\u8003\u8651\u80cc\u666f\u56fe\u50cf\u5185\u5bb9\u3002", "conclusion": "\u5229\u7528VLM\u7406\u89e3\u56fe\u50cf\u8bed\u4e49\u5185\u5bb9\u7684\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u5e7f\u544a\u5e03\u5c40\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u56fe\u50cf\u5e7f\u544a\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u66f4\u667a\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12598", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12598", "abs": "https://arxiv.org/abs/2512.12598", "authors": ["Cong Xie", "Che Wang", "Yan Zhang", "Zheng Pan", "Han Zou", "Zhenpeng Zhan"], "title": "Geometry-Aware Scene-Consistent Image Generation", "comment": null, "summary": "We study geometry-aware scene-consistent image generation: given a reference scene image and a text condition specifying an entity to be generated in the scene and its spatial relation to the scene, the goal is to synthesize an output image that preserves the same physical environment as the reference scene while correctly generating the entity according to the spatial relation described in the text. Existing methods struggle to balance scene preservation with prompt adherence: they either replicate the scene with high fidelity but poor responsiveness to the prompt, or prioritize prompt compliance at the expense of scene consistency. To resolve this trade-off, we introduce two key contributions: (i) a scene-consistent data construction pipeline that generates diverse, geometrically-grounded training pairs, and (ii) a novel geometry-guided attention loss that leverages cross-view cues to regularize the model's spatial reasoning. Experiments on our scene-consistent benchmark show that our approach achieves better scene alignment and text-image consistency than state-of-the-art baselines, according to both automatic metrics and human preference studies. Our method produces geometrically coherent images with diverse compositions that remain faithful to the textual instructions and the underlying scene structure.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u51e0\u4f55\u611f\u77e5\u7684\u573a\u666f\u4e00\u81f4\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4fdd\u6301\u53c2\u8003\u573a\u666f\u7269\u7406\u73af\u5883\u7684\u540c\u65f6\uff0c\u6839\u636e\u6587\u672c\u63cf\u8ff0\u7684\u7a7a\u95f4\u5173\u7cfb\u751f\u6210\u65b0\u5b9e\u4f53\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u573a\u666f\u4fdd\u6301\u548c\u63d0\u793a\u9075\u5faa\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u51e0\u4f55\u611f\u77e5\u7684\u573a\u666f\u4e00\u81f4\u56fe\u50cf\u751f\u6210\u4e2d\u5b58\u5728\u5e73\u8861\u95ee\u9898\uff1a\u8981\u4e48\u9ad8\u5ea6\u5fe0\u5b9e\u4e8e\u573a\u666f\u4f46\u54cd\u5e94\u63d0\u793a\u80fd\u529b\u5dee\uff0c\u8981\u4e48\u4f18\u5148\u9075\u5faa\u63d0\u793a\u4f46\u727a\u7272\u573a\u666f\u4e00\u81f4\u6027\u3002\u9700\u8981\u89e3\u51b3\u573a\u666f\u4fdd\u6301\u4e0e\u63d0\u793a\u9075\u5faa\u4e4b\u95f4\u7684\u6743\u8861\u3002", "method": "\u63d0\u51fa\u4e24\u4e2a\u5173\u952e\u8d21\u732e\uff1a(1) \u573a\u666f\u4e00\u81f4\u7684\u6570\u636e\u6784\u5efa\u6d41\u7a0b\uff0c\u751f\u6210\u591a\u6837\u5316\u7684\u51e0\u4f55\u57fa\u7840\u8bad\u7ec3\u5bf9\uff1b(2) \u65b0\u9896\u7684\u51e0\u4f55\u5f15\u5bfc\u6ce8\u610f\u529b\u635f\u5931\uff0c\u5229\u7528\u8de8\u89c6\u56fe\u7ebf\u7d22\u6765\u6b63\u5219\u5316\u6a21\u578b\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728\u573a\u666f\u4e00\u81f4\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u81ea\u52a8\u6307\u6807\u548c\u4eba\u7c7b\u504f\u597d\u7814\u7a76\u4e2d\u90fd\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u573a\u666f\u5bf9\u9f50\u548c\u6587\u672c-\u56fe\u50cf\u4e00\u81f4\u6027\uff0c\u751f\u6210\u51e0\u4f55\u8fde\u8d2f\u4e14\u4fdd\u6301\u573a\u666f\u7ed3\u6784\u7684\u591a\u6837\u5316\u56fe\u50cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u573a\u666f\u4e00\u81f4\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u6743\u8861\u95ee\u9898\uff0c\u80fd\u591f\u751f\u6210\u65e2\u5fe0\u5b9e\u4e8e\u6587\u672c\u6307\u4ee4\u53c8\u4fdd\u6301\u5e95\u5c42\u573a\u666f\u7ed3\u6784\u7684\u51e0\u4f55\u8fde\u8d2f\u56fe\u50cf\uff0c\u5728\u573a\u666f\u5bf9\u9f50\u548c\u6587\u672c-\u56fe\u50cf\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2512.12604", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12604", "abs": "https://arxiv.org/abs/2512.12604", "authors": ["Tingyan Wen", "Haoyu Li", "Yihuang Chen", "Xing Zhou", "Lifei Zhu", "Xueqian Wang"], "title": "No Cache Left Idle: Accelerating diffusion model via Extreme-slimming Caching", "comment": "Project page: https://thu-accdiff.github.io/xslim-page/ Code: https://github.com/THU-AccDiff/xslim", "summary": "Diffusion models achieve remarkable generative quality, but computational overhead scales with step count, model depth, and sequence length. Feature caching is effective since adjacent timesteps yield highly similar features. However, an inherent trade-off remains: aggressive timestep reuse offers large speedups but can easily cross the critical line, hurting fidelity, while block- or token-level reuse is safer but yields limited computational savings. We present X-Slim (eXtreme-Slimming Caching), a training-free, cache-based accelerator that, to our knowledge, is the first unified framework to exploit cacheable redundancy across timesteps, structure (blocks), and space (tokens). Rather than simply mixing levels, X-Slim introduces a dual-threshold controller that turns caching into a push-then-polish process: it first pushes reuse at the timestep level up to an early-warning line, then switches to lightweight block- and token-level refresh to polish the remaining redundancy, and triggers full inference once the critical line is crossed to reset accumulated error. At each level, context-aware indicators decide when and where to cache. Across diverse tasks, X-Slim advances the speed-quality frontier. On FLUX.1-dev and HunyuanVideo, it reduces latency by up to 4.97x and 3.52x with minimal perceptual loss. On DiT-XL/2, it reaches 3.13x acceleration and improves FID by 2.42 over prior methods.", "AI": {"tldr": "X-Slim\u662f\u4e00\u79cd\u8bad\u7ec3\u514d\u8d39\u7684\u7f13\u5b58\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u8de8\u65f6\u95f4\u6b65\u3001\u7ed3\u6784\u548c\u7a7a\u95f4\u7684\u4e09\u7ea7\u7f13\u5b58\u5197\u4f59\u5229\u7528\uff0c\u91c7\u7528\u53cc\u9608\u503c\u63a7\u5236\u5668\u5b9e\u73b0\"\u63a8\u9001-\u629b\u5149\"\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u6269\u6563\u6a21\u578b\u63a8\u7406\u901f\u5ea6", "motivation": "\u6269\u6563\u6a21\u578b\u867d\u7136\u751f\u6210\u8d28\u91cf\u51fa\u8272\uff0c\u4f46\u8ba1\u7b97\u5f00\u9500\u968f\u6b65\u6570\u3001\u6a21\u578b\u6df1\u5ea6\u548c\u5e8f\u5217\u957f\u5ea6\u7ebf\u6027\u589e\u957f\u3002\u73b0\u6709\u7279\u5f81\u7f13\u5b58\u65b9\u6cd5\u5b58\u5728\u6743\u8861\uff1a\u6fc0\u8fdb\u7684\u65f6\u95f4\u6b65\u91cd\u7528\u80fd\u5927\u5e45\u52a0\u901f\u4f46\u5bb9\u6613\u635f\u5bb3\u4fdd\u771f\u5ea6\uff0c\u800c\u5757\u7ea7\u6216\u4ee4\u724c\u7ea7\u91cd\u7528\u66f4\u5b89\u5168\u4f46\u8ba1\u7b97\u8282\u7701\u6709\u9650", "method": "\u63d0\u51faX-Slim\u7edf\u4e00\u6846\u67b6\uff0c\u5229\u7528\u65f6\u95f4\u6b65\u3001\u7ed3\u6784\uff08\u5757\uff09\u548c\u7a7a\u95f4\uff08\u4ee4\u724c\uff09\u4e09\u4e2a\u7ef4\u5ea6\u7684\u53ef\u7f13\u5b58\u5197\u4f59\u3002\u91c7\u7528\u53cc\u9608\u503c\u63a7\u5236\u5668\u5b9e\u73b0\u63a8\u9001-\u629b\u5149\u8fc7\u7a0b\uff1a\u9996\u5148\u5c06\u65f6\u95f4\u6b65\u7ea7\u91cd\u7528\u63a8\u81f3\u9884\u8b66\u7ebf\uff0c\u7136\u540e\u5207\u6362\u5230\u8f7b\u91cf\u7ea7\u5757\u7ea7\u548c\u4ee4\u724c\u7ea7\u5237\u65b0\u6765\u629b\u5149\u5269\u4f59\u5197\u4f59\uff0c\u5f53\u8de8\u8fc7\u4e34\u754c\u7ebf\u65f6\u89e6\u53d1\u5b8c\u6574\u63a8\u7406\u4ee5\u91cd\u7f6e\u7d2f\u79ef\u8bef\u5dee\u3002\u6bcf\u4e2a\u7ea7\u522b\u4f7f\u7528\u4e0a\u4e0b\u6587\u611f\u77e5\u6307\u6807\u51b3\u5b9a\u4f55\u65f6\u4f55\u5730\u7f13\u5b58", "result": "\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\uff0cX-Slim\u63a8\u8fdb\u4e86\u901f\u5ea6-\u8d28\u91cf\u524d\u6cbf\u3002\u5728FLUX.1-dev\u548cHunyuanVideo\u4e0a\uff0c\u5206\u522b\u5c06\u5ef6\u8fdf\u964d\u4f4e4.97\u500d\u548c3.52\u500d\uff0c\u611f\u77e5\u635f\u5931\u6700\u5c0f\u3002\u5728DiT-XL/2\u4e0a\uff0c\u8fbe\u52303.13\u500d\u52a0\u901f\uff0cFID\u6bd4\u5148\u524d\u65b9\u6cd5\u6539\u55842.42", "conclusion": "X-Slim\u901a\u8fc7\u7edf\u4e00\u5229\u7528\u8de8\u65f6\u95f4\u6b65\u3001\u7ed3\u6784\u548c\u7a7a\u95f4\u7684\u4e09\u7ea7\u7f13\u5b58\u5197\u4f59\uff0c\u91c7\u7528\u521b\u65b0\u7684\u53cc\u9608\u503c\u63a8\u9001-\u629b\u5149\u673a\u5236\uff0c\u5728\u4fdd\u6301\u751f\u6210\u8d28\u91cf\u7684\u540c\u65f6\u663e\u8457\u52a0\u901f\u6269\u6563\u6a21\u578b\u63a8\u7406\uff0c\u4e3a\u6269\u6563\u6a21\u578b\u52a0\u901f\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bad\u7ec3\u514d\u8d39\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.12690", "categories": ["cs.LG", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12690", "abs": "https://arxiv.org/abs/2512.12690", "authors": ["Yongcan Yu", "Lingxiao He", "Shuo Lu", "Lijun Sheng", "Yinuo Xu", "Yanbo Wang", "Kuangpu Guo", "Jianjie Cheng", "Meng Wang", "Qianlong Xie", "Xingxing Wang", "Dapeng Hu", "Jian Liang"], "title": "Reassessing the Role of Supervised Fine-Tuning: An Empirical Study in VLM Reasoning", "comment": null, "summary": "Recent advances in vision-language models (VLMs) reasoning have been largely attributed to the rise of reinforcement Learning (RL), which has shifted the community's focus away from the supervised fine-tuning (SFT) paradigm. Many studies suggest that introducing the SFT stage not only fails to improve reasoning ability but may also negatively impact model training. In this study, we revisit this RL-centric belief through a systematic and controlled comparison of SFT and RL on VLM Reasoning. Using identical data sources, we find that the relative effectiveness of SFT and RL is conditional and strongly influenced by model capacity, data scale, and data distribution. Contrary to common assumptions, our findings show that SFT plays a crucial role across several scenarios: (1) Effectiveness for weaker models. SFT more reliably elicits reasoning capabilities in smaller or weaker VLMs. (2) Data efficiency. SFT with only 2K achieves comparable or better reasoning performance to RL with 20K. (3) Cross-modal transferability. SFT demonstrates stronger generalization across modalities. Moreover, we identify a pervasive issue of deceptive rewards, where higher rewards fail to correlate with better reasoning accuracy in RL. These results challenge the prevailing \"RL over SFT\" narrative. They highlight that the role of SFT may have been underestimated and support a more balanced post-training pipeline in which SFT and RL function as complementary components.", "AI": {"tldr": "\u672c\u6587\u6311\u6218\u4e86\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8bad\u7ec3\u4e2d\"\u5f3a\u5316\u5b66\u4e60\u4f18\u4e8e\u76d1\u7763\u5fae\u8c03\"\u7684\u4e3b\u6d41\u89c2\u70b9\uff0c\u901a\u8fc7\u7cfb\u7edf\u5bf9\u6bd4\u53d1\u73b0SFT\u5728\u591a\u4e2a\u573a\u666f\u4e0b\u5177\u6709\u5173\u952e\u4f5c\u7528\uff0c\u5305\u62ec\u5bf9\u5f31\u6a21\u578b\u66f4\u6709\u6548\u3001\u6570\u636e\u6548\u7387\u66f4\u9ad8\u3001\u8de8\u6a21\u6001\u6cdb\u5316\u66f4\u5f3a\u7b49\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u9886\u57df\u8fc7\u5ea6\u5f3a\u8c03\u5f3a\u5316\u5b66\u4e60\u7684\u4f5c\u7528\uff0c\u666e\u904d\u8ba4\u4e3a\u76d1\u7763\u5fae\u8c03\u4e0d\u4ec5\u65e0\u6cd5\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u8fd8\u53ef\u80fd\u5bf9\u6a21\u578b\u8bad\u7ec3\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002\u672c\u7814\u7a76\u65e8\u5728\u91cd\u65b0\u5ba1\u89c6\u8fd9\u79cd\u4ee5\u5f3a\u5316\u5b66\u4e60\u4e3a\u4e2d\u5fc3\u7684\u89c2\u70b9\uff0c\u901a\u8fc7\u7cfb\u7edf\u5bf9\u6bd4\u6765\u6f84\u6e05SFT\u548cRL\u5728\u4e0d\u540c\u6761\u4ef6\u4e0b\u7684\u76f8\u5bf9\u6709\u6548\u6027\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u5316\u3001\u53d7\u63a7\u7684\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u5728\u76f8\u540c\u6570\u636e\u6e90\u4e0b\u5bf9\u6bd4\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u4e2d\u7684\u8868\u73b0\u3002\u7814\u7a76\u8003\u5bdf\u4e86\u6a21\u578b\u5bb9\u91cf\u3001\u6570\u636e\u89c4\u6a21\u548c\u6570\u636e\u5206\u5e03\u5bf9\u4e24\u79cd\u65b9\u6cd5\u6548\u679c\u7684\u5f71\u54cd\uff0c\u5e76\u5206\u6790\u4e86\u6b3a\u9a97\u6027\u5956\u52b1\u95ee\u9898\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1) SFT\u5bf9\u8f83\u5f31\u6a21\u578b\u66f4\u6709\u6548\uff0c\u80fd\u66f4\u53ef\u9760\u5730\u6fc0\u53d1\u5176\u63a8\u7406\u80fd\u529b\uff1b2) SFT\u6570\u636e\u6548\u7387\u66f4\u9ad8\uff0c\u4ec5\u75282K\u6570\u636e\u5c31\u80fd\u8fbe\u5230RL\u4f7f\u752820K\u6570\u636e\u7684\u6548\u679c\uff1b3) SFT\u8de8\u6a21\u6001\u6cdb\u5316\u80fd\u529b\u66f4\u5f3a\uff1b4) RL\u5b58\u5728\u6b3a\u9a97\u6027\u5956\u52b1\u95ee\u9898\uff0c\u5373\u66f4\u9ad8\u7684\u5956\u52b1\u5e76\u4e0d\u5bf9\u5e94\u66f4\u597d\u7684\u63a8\u7406\u51c6\u786e\u6027\u3002", "conclusion": "\u76d1\u7763\u5fae\u8c03\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8bad\u7ec3\u4e2d\u7684\u4f5c\u7528\u88ab\u4f4e\u4f30\u4e86\uff0cSFT\u548cRL\u5e94\u88ab\u89c6\u4e3a\u4e92\u8865\u7ec4\u4ef6\u3002\u7814\u7a76\u6311\u6218\u4e86\"RL\u4f18\u4e8eSFT\"\u7684\u4e3b\u6d41\u53d9\u4e8b\uff0c\u652f\u6301\u6784\u5efa\u66f4\u5e73\u8861\u7684\u540e\u8bad\u7ec3\u6d41\u7a0b\uff0c\u6839\u636e\u5177\u4f53\u573a\u666f\u9009\u62e9\u5408\u9002\u7684\u65b9\u6cd5\u7ec4\u5408\u3002"}}
{"id": "2512.12693", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12693", "abs": "https://arxiv.org/abs/2512.12693", "authors": ["Sumantrak Mukherjee", "Serafima Lebedeva", "Valentin Margraf", "Jonas Hanselle", "Kanta Yamaoka", "Viktor Bengs", "Stefan Konigorski", "Eyke H\u00fcllermeier", "Sebastian Josef Vollmer"], "title": "Co-Exploration and Co-Exploitation via Shared Structure in Multi-Task Bandits", "comment": "18 pages, 9 figures, preprint", "summary": "We propose a novel Bayesian framework for efficient exploration in contextual multi-task multi-armed bandit settings, where the context is only observed partially and dependencies between reward distributions are induced by latent context variables. In order to exploit these structural dependencies, our approach integrates observations across all tasks and learns a global joint distribution, while still allowing personalised inference for new tasks. In this regard, we identify two key sources of epistemic uncertainty, namely structural uncertainty in the latent reward dependencies across arms and tasks, and user-specific uncertainty due to incomplete context and limited interaction history. To put our method into practice, we represent the joint distribution over tasks and rewards using a particle-based approximation of a log-density Gaussian process. This representation enables flexible, data-driven discovery of both inter-arm and inter-task dependencies without prior assumptions on the latent variables. Empirically, we demonstrate that our method outperforms baselines such as hierarchical model bandits, especially in settings with model misspecification or complex latent heterogeneity.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4e0a\u4e0b\u6587\u90e8\u5206\u53ef\u89c2\u6d4b\u7684\u591a\u4efb\u52a1\u591a\u81c2\u8001\u864e\u673a\u4e2d\u8fdb\u884c\u9ad8\u6548\u63a2\u7d22\uff0c\u901a\u8fc7\u6f5c\u5728\u4e0a\u4e0b\u6587\u53d8\u91cf\u6355\u6349\u5956\u52b1\u5206\u5e03\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u3002", "motivation": "\u5728\u591a\u4efb\u52a1\u591a\u81c2\u8001\u864e\u673a\u8bbe\u7f6e\u4e2d\uff0c\u4e0a\u4e0b\u6587\u901a\u5e38\u53ea\u80fd\u90e8\u5206\u89c2\u6d4b\uff0c\u4e14\u5956\u52b1\u5206\u5e03\u4e4b\u95f4\u5b58\u5728\u7531\u6f5c\u5728\u4e0a\u4e0b\u6587\u53d8\u91cf\u8bf1\u5bfc\u7684\u7ed3\u6784\u4f9d\u8d56\u5173\u7cfb\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5229\u7528\u8fd9\u4e9b\u8de8\u4efb\u52a1\u7684\u7ed3\u6784\u4f9d\u8d56\uff0c\u7279\u522b\u662f\u5728\u6a21\u578b\u8bbe\u5b9a\u9519\u8bef\u6216\u5b58\u5728\u590d\u6742\u6f5c\u5728\u5f02\u8d28\u6027\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u63d0\u51fa\u8d1d\u53f6\u65af\u6846\u67b6\uff0c\u901a\u8fc7\u7c92\u5b50\u903c\u8fd1\u7684\u5bf9\u6570\u5bc6\u5ea6\u9ad8\u65af\u8fc7\u7a0b\u8868\u793a\u4efb\u52a1\u548c\u5956\u52b1\u7684\u8054\u5408\u5206\u5e03\uff0c\u4ece\u800c\u7075\u6d3b\u5730\u53d1\u73b0\u81c2\u95f4\u548c\u4efb\u52a1\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u65e0\u9700\u5bf9\u6f5c\u5728\u53d8\u91cf\u505a\u5148\u9a8c\u5047\u8bbe\u3002\u8be5\u65b9\u6cd5\u6574\u5408\u6240\u6709\u4efb\u52a1\u7684\u89c2\u6d4b\u6570\u636e\u5b66\u4e60\u5168\u5c40\u8054\u5408\u5206\u5e03\uff0c\u540c\u65f6\u5141\u8bb8\u5bf9\u65b0\u4efb\u52a1\u8fdb\u884c\u4e2a\u6027\u5316\u63a8\u65ad\u3002", "result": "\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6a21\u578b\u8bbe\u5b9a\u9519\u8bef\u6216\u5b58\u5728\u590d\u6742\u6f5c\u5728\u5f02\u8d28\u6027\u7684\u8bbe\u7f6e\u4e2d\uff0c\u4f18\u4e8e\u5206\u5c42\u6a21\u578b\u8001\u864e\u673a\u7b49\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u8d1d\u53f6\u65af\u6846\u67b6\u80fd\u591f\u6709\u6548\u5229\u7528\u8de8\u4efb\u52a1\u7684\u7ed3\u6784\u4f9d\u8d56\u5173\u7cfb\uff0c\u901a\u8fc7\u8bc6\u522b\u7ed3\u6784\u4e0d\u786e\u5b9a\u6027\u548c\u7528\u6237\u7279\u5b9a\u4e0d\u786e\u5b9a\u6027\u4e24\u79cd\u5173\u952e\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\u6765\u6e90\uff0c\u5728\u591a\u4efb\u52a1\u591a\u81c2\u8001\u864e\u673a\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u63a2\u7d22\u3002"}}
{"id": "2512.12622", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12622", "abs": "https://arxiv.org/abs/2512.12622", "authors": ["Zihan Wang", "Seungjun Lee", "Guangzhao Dai", "Gim Hee Lee"], "title": "D3D-VLP: Dynamic 3D Vision-Language-Planning Model for Embodied Grounding and Navigation", "comment": null, "summary": "Embodied agents face a critical dilemma that end-to-end models lack interpretability and explicit 3D reasoning, while modular systems ignore cross-component interdependencies and synergies. To bridge this gap, we propose the Dynamic 3D Vision-Language-Planning Model (D3D-VLP). Our model introduces two key innovations: 1) A Dynamic 3D Chain-of-Thought (3D CoT) that unifies planning, grounding, navigation, and question answering within a single 3D-VLM and CoT pipeline; 2) A Synergistic Learning from Fragmented Supervision (SLFS) strategy, which uses a masked autoregressive loss to learn from massive and partially-annotated hybrid data. This allows different CoT components to mutually reinforce and implicitly supervise each other. To this end, we construct a large-scale dataset with 10M hybrid samples from 5K real scans and 20K synthetic scenes that are compatible with online learning methods such as RL and DAgger. Our D3D-VLP achieves state-of-the-art results on multiple benchmarks, including Vision-and-Language Navigation (R2R-CE, REVERIE-CE, NavRAG-CE), Object-goal Navigation (HM3D-OVON), and Task-oriented Sequential Grounding and Navigation (SG3D). Real-world mobile manipulation experiments further validate the effectiveness.", "AI": {"tldr": "D3D-VLP\u6a21\u578b\u901a\u8fc7\u52a8\u60013D\u601d\u7ef4\u94fe\u548c\u534f\u540c\u5b66\u4e60\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u5177\u8eab\u667a\u80fd\u4e2d\u7aef\u5230\u7aef\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u4e0e\u6a21\u5757\u5316\u7cfb\u7edf\u5ffd\u7565\u8de8\u7ec4\u4ef6\u4f9d\u8d56\u7684\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u5bfc\u822a\u548c\u5b9a\u4f4d\u57fa\u51c6\u4e0a\u53d6\u5f97SOTA\u7ed3\u679c\u3002", "motivation": "\u89e3\u51b3\u5177\u8eab\u667a\u80fd\u4e2d\u7684\u5173\u952e\u56f0\u5883\uff1a\u7aef\u5230\u7aef\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u663e\u5f0f3D\u63a8\u7406\u80fd\u529b\uff0c\u800c\u6a21\u5757\u5316\u7cfb\u7edf\u5219\u5ffd\u7565\u4e86\u4e0d\u540c\u7ec4\u4ef6\u4e4b\u95f4\u7684\u76f8\u4e92\u4f9d\u8d56\u548c\u534f\u540c\u6548\u5e94\u3002", "method": "\u63d0\u51fa\u52a8\u60013D\u89c6\u89c9-\u8bed\u8a00-\u89c4\u5212\u6a21\u578b(D3D-VLP)\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a1) \u52a8\u60013D\u601d\u7ef4\u94fe(3D CoT)\uff0c\u5c06\u89c4\u5212\u3001\u5b9a\u4f4d\u3001\u5bfc\u822a\u548c\u95ee\u7b54\u7edf\u4e00\u5230\u5355\u4e2a3D-VLM\u548cCoT\u6d41\u7a0b\u4e2d\uff1b2) \u788e\u7247\u5316\u76d1\u7763\u534f\u540c\u5b66\u4e60\u7b56\u7565(SLFS)\uff0c\u4f7f\u7528\u63a9\u7801\u81ea\u56de\u5f52\u635f\u5931\u4ece\u5927\u89c4\u6a21\u90e8\u5206\u6807\u6ce8\u7684\u6df7\u5408\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u8ba9\u4e0d\u540cCoT\u7ec4\u4ef6\u76f8\u4e92\u589e\u5f3a\u548c\u9690\u5f0f\u76d1\u7763\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u5148\u8fdb\u7ed3\u679c\uff1a\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a(R2R-CE, REVERIE-CE, NavRAG-CE)\u3001\u76ee\u6807\u5bfc\u822a(HM3D-OVON)\u548c\u4efb\u52a1\u5bfc\u5411\u7684\u987a\u5e8f\u5b9a\u4f4d\u4e0e\u5bfc\u822a(SG3D)\u3002\u771f\u5b9e\u4e16\u754c\u79fb\u52a8\u64cd\u4f5c\u5b9e\u9a8c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "conclusion": "D3D-VLP\u6210\u529f\u5f25\u5408\u4e86\u7aef\u5230\u7aef\u6a21\u578b\u4e0e\u6a21\u5757\u5316\u7cfb\u7edf\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u901a\u8fc7\u7edf\u4e00\u76843D\u601d\u7ef4\u94fe\u6846\u67b6\u548c\u534f\u540c\u5b66\u4e60\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u53ef\u89e3\u91ca\u76843D\u63a8\u7406\u548c\u8de8\u7ec4\u4ef6\u534f\u540c\uff0c\u4e3a\u5177\u8eab\u667a\u80fd\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12623", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.12623", "abs": "https://arxiv.org/abs/2512.12623", "authors": ["Chengzhi Liu", "Yuzhe Yang", "Yue Fan", "Qingyue Wei", "Sheng Liu", "Xin Eric Wang"], "title": "Reasoning Within the Mind: Dynamic Multimodal Interleaving in Latent Space", "comment": null, "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have significantly enhanced cross-modal understanding and reasoning by incorporating Chain-of-Thought (CoT) reasoning in the semantic space. Building upon this, recent studies extend the CoT mechanism to the visual modality, enabling models to integrate visual information during reasoning through external tools or explicit image generation. However, these methods remain dependent on explicit step-by-step reasoning, unstable perception-reasoning interaction and notable computational overhead. Inspired by human cognition, we posit that thinking unfolds not linearly but through the dynamic interleaving of reasoning and perception within the mind. Motivated by this perspective, we propose DMLR, a test-time Dynamic Multimodal Latent Reasoning framework that employs confidence-guided latent policy gradient optimization to refine latent think tokens for in-depth reasoning. Furthermore, a Dynamic Visual Injection Strategy is introduced, which retrieves the most relevant visual features at each latent think token and updates the set of best visual patches. The updated patches are then injected into latent think token to achieve dynamic visual-textual interleaving. Experiments across seven multimodal reasoning benchmarks and various model architectures demonstrate that DMLR significantly improves reasoning and perception performance while maintaining high inference efficiency.", "AI": {"tldr": "DMLR\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u591a\u6a21\u6001\u6f5c\u5728\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u7684\u6f5c\u5728\u7b56\u7565\u68af\u5ea6\u4f18\u5316\u6765\u7cbe\u70bc\u6f5c\u5728\u601d\u8003\u6807\u8bb0\uff0c\u5b9e\u73b0\u89c6\u89c9-\u6587\u672c\u7684\u52a8\u6001\u4ea4\u9519\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u63a8\u7406\u6027\u80fd\u5e76\u4fdd\u6301\u9ad8\u6548\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u867d\u7136\u901a\u8fc7\u601d\u7ef4\u94fe\u673a\u5236\u589e\u5f3a\u4e86\u8de8\u6a21\u6001\u7406\u89e3\uff0c\u4f46\u4ecd\u4f9d\u8d56\u663e\u5f0f\u7684\u9010\u6b65\u63a8\u7406\u3001\u611f\u77e5-\u63a8\u7406\u4ea4\u4e92\u4e0d\u7a33\u5b9a\u4e14\u8ba1\u7b97\u5f00\u9500\u5927\u3002\u53d7\u4eba\u7c7b\u8ba4\u77e5\u542f\u53d1\uff0c\u4f5c\u8005\u8ba4\u4e3a\u601d\u8003\u4e0d\u662f\u7ebf\u6027\u7684\uff0c\u800c\u662f\u63a8\u7406\u548c\u611f\u77e5\u5728\u601d\u7ef4\u4e2d\u52a8\u6001\u4ea4\u9519\u7684\u8fc7\u7a0b\u3002", "method": "\u63d0\u51faDMLR\u6846\u67b6\uff1a1\uff09\u4f7f\u7528\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u7684\u6f5c\u5728\u7b56\u7565\u68af\u5ea6\u4f18\u5316\u6765\u7cbe\u70bc\u6f5c\u5728\u601d\u8003\u6807\u8bb0\u8fdb\u884c\u6df1\u5ea6\u63a8\u7406\uff1b2\uff09\u5f15\u5165\u52a8\u6001\u89c6\u89c9\u6ce8\u5165\u7b56\u7565\uff0c\u5728\u6bcf\u4e2a\u6f5c\u5728\u601d\u8003\u6807\u8bb0\u5904\u68c0\u7d22\u6700\u76f8\u5173\u7684\u89c6\u89c9\u7279\u5f81\u5e76\u66f4\u65b0\u6700\u4f73\u89c6\u89c9\u8865\u4e01\u96c6\uff1b3\uff09\u5c06\u66f4\u65b0\u7684\u8865\u4e01\u6ce8\u5165\u6f5c\u5728\u601d\u8003\u6807\u8bb0\uff0c\u5b9e\u73b0\u52a8\u6001\u89c6\u89c9-\u6587\u672c\u4ea4\u9519\u3002", "result": "\u5728\u4e03\u4e2a\u591a\u6a21\u6001\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u548c\u591a\u79cd\u6a21\u578b\u67b6\u6784\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDMLR\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u548c\u611f\u77e5\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f83\u9ad8\u7684\u63a8\u7406\u6548\u7387\u3002", "conclusion": "DMLR\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u7684\u52a8\u6001\u4ea4\u9519\u7279\u6027\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u6a21\u6001\u63a8\u7406\u6846\u67b6\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u663e\u5f0f\u9010\u6b65\u63a8\u7406\u3001\u4ea4\u4e92\u4e0d\u7a33\u5b9a\u548c\u8ba1\u7b97\u5f00\u9500\u5927\u7684\u95ee\u9898\u3002"}}
{"id": "2512.12657", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12657", "abs": "https://arxiv.org/abs/2512.12657", "authors": ["Hongyang Li", "Junyi Tao", "Qijie Wei", "Ningzhi Yang", "Meng Wang", "Weihong Yu", "Xirong Li"], "title": "Cross-modal Fundus Image Registration under Large FoV Disparity", "comment": "Accepted as a regular paper at MMM 2026", "summary": "Previous work on cross-modal fundus image registration (CMFIR) assumes small cross-modal Field-of-View (FoV) disparity. By contrast, this paper is targeted at a more challenging scenario with large FoV disparity, to which directly applying current methods fails. We propose Crop and Alignment for cross-modal fundus image Registration(CARe), a very simple yet effective method. Specifically, given an OCTA with smaller FoV as a source image and a wide-field color fundus photograph (wfCFP) as a target image, our Crop operation exploits the physiological structure of the retina to crop from the target image a sub-image with its FoV roughly aligned with that of the source. This operation allows us to re-purpose the previous small-FoV-disparity oriented methods for subsequent image registration. Moreover, we improve spatial transformation by a double-fitting based Alignment module that utilizes the classical RANSAC algorithm and polynomial-based coordinate fitting in a sequential manner. Extensive experiments on a newly developed test set of 60 OCTA-wfCFP pairs verify the viability of CARe for CMFIR.", "AI": {"tldr": "CARe\u65b9\u6cd5\u901a\u8fc7\u88c1\u526a\u548c\u5bf9\u9f50\u64cd\u4f5c\u89e3\u51b3\u5927\u89c6\u573a\u5dee\u5f02\u7684\u8de8\u6a21\u6001\u773c\u5e95\u56fe\u50cf\u914d\u51c6\u95ee\u9898\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u5c0f\u89c6\u573a\u5dee\u5f02\u5047\u8bbe\u4e0b\u7684\u5931\u8d25\u8868\u73b0\uff0cCARe\u5728\u65b0\u6d4b\u8bd5\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u8de8\u6a21\u6001\u773c\u5e95\u56fe\u50cf\u914d\u51c6\u65b9\u6cd5\u5047\u8bbe\u89c6\u573a\u5dee\u5f02\u8f83\u5c0f\uff0c\u4f46\u5728\u5b9e\u9645\u4e34\u5e8a\u4e2d\u5e38\u9047\u5230\u5927\u89c6\u573a\u5dee\u5f02\u7684\u60c5\u51b5\uff0c\u73b0\u6709\u65b9\u6cd5\u76f4\u63a5\u5e94\u7528\u4f1a\u5931\u8d25\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u66f4\u5177\u6311\u6218\u6027\u7684\u573a\u666f\u3002", "method": "\u63d0\u51faCARe\u65b9\u6cd5\uff1a1) \u88c1\u526a\u64cd\u4f5c\uff1a\u5229\u7528\u89c6\u7f51\u819c\u751f\u7406\u7ed3\u6784\u4ece\u5927\u89c6\u573a\u76ee\u6807\u56fe\u50cf\u4e2d\u88c1\u526a\u51fa\u4e0e\u6e90\u56fe\u50cf\u89c6\u573a\u5927\u81f4\u5bf9\u9f50\u7684\u5b50\u56fe\u50cf\uff1b2) \u5bf9\u9f50\u6a21\u5757\uff1a\u91c7\u7528\u53cc\u62df\u5408\u65b9\u6cd5\uff0c\u5148\u4f7f\u7528RANSAC\u7b97\u6cd5\uff0c\u518d\u8fdb\u884c\u591a\u9879\u5f0f\u5750\u6807\u62df\u5408\uff0c\u6539\u8fdb\u7a7a\u95f4\u53d8\u6362\u3002", "result": "\u5728\u5305\u542b60\u5bf9OCTA-wfCFP\u56fe\u50cf\u7684\u65b0\u6d4b\u8bd5\u96c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86CARe\u65b9\u6cd5\u5728\u5927\u89c6\u573a\u5dee\u5f02\u8de8\u6a21\u6001\u773c\u5e95\u56fe\u50cf\u914d\u51c6\u4e2d\u7684\u53ef\u884c\u6027\u3002", "conclusion": "CARe\u662f\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u88c1\u526a\u548c\u5bf9\u9f50\u64cd\u4f5c\u89e3\u51b3\u4e86\u5927\u89c6\u573a\u5dee\u5f02\u7684\u8de8\u6a21\u6001\u773c\u5e95\u56fe\u50cf\u914d\u51c6\u95ee\u9898\uff0c\u4e3a\u8fd9\u4e00\u66f4\u5177\u6311\u6218\u6027\u7684\u4e34\u5e8a\u573a\u666f\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12744", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12744", "abs": "https://arxiv.org/abs/2512.12744", "authors": ["Haotian Xu", "Tian Gao", "Tsui-Wei Weng", "Tengfei Ma"], "title": "Resting Neurons, Active Insights: Improving Input Sparsification for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) achieve state-of-the-art performance across a wide range of applications, but their massive scale poses significant challenges for both efficiency and interpretability. Structural pruning, which reduces model size by removing redundant computational units such as neurons, has been widely explored as a solution, and this study devotes to input sparsification, an increasingly popular technique that improves efficiency by selectively activating only a subset of entry values for each input. However, existing approaches focus primarily on computational savings, often overlooking the representational consequences of sparsification and leaving a noticeable performance gap compared to full models. In this work, we first reinterpret input sparsification as a form of dynamic structural pruning. Motivated by the spontaneous baseline firing rates observed in biological neurons, we introduce a small set of trainable spontaneous neurons that act as compensatory units to stabilize activations in sparsified LLMs. Experiments demonstrate that these auxiliary neurons substantially reduce the sparsification-induced performance gap while generalizing effectively across tasks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6dfb\u52a0\u53ef\u8bad\u7ec3\u81ea\u53d1\u795e\u7ecf\u5143\u6765\u6539\u5584\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8f93\u5165\u7a00\u758f\u5316\u6027\u80fd\u7684\u65b9\u6cd5\uff0c\u5c06\u8f93\u5165\u7a00\u758f\u5316\u91cd\u65b0\u89e3\u91ca\u4e3a\u52a8\u6001\u7ed3\u6784\u526a\u679d\uff0c\u5e76\u5f15\u5165\u751f\u7269\u795e\u7ecf\u5143\u542f\u53d1\u7684\u81ea\u53d1\u795e\u7ecf\u5143\u6765\u7a33\u5b9a\u6fc0\u6d3b\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u7a00\u758f\u5316\u5e26\u6765\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u867d\u7136\u6027\u80fd\u4f18\u5f02\uff0c\u4f46\u89c4\u6a21\u5e9e\u5927\u5e26\u6765\u4e86\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u6311\u6218\u3002\u8f93\u5165\u7a00\u758f\u5316\u901a\u8fc7\u9009\u62e9\u6027\u6fc0\u6d3b\u8f93\u5165\u503c\u5b50\u96c6\u6765\u63d0\u9ad8\u6548\u7387\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u8ba1\u7b97\u8282\u7701\uff0c\u5ffd\u89c6\u4e86\u7a00\u758f\u5316\u7684\u8868\u793a\u540e\u679c\uff0c\u5bfc\u81f4\u4e0e\u5b8c\u6574\u6a21\u578b\u76f8\u6bd4\u5b58\u5728\u660e\u663e\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "method": "\u9996\u5148\u5c06\u8f93\u5165\u7a00\u758f\u5316\u91cd\u65b0\u89e3\u91ca\u4e3a\u52a8\u6001\u7ed3\u6784\u526a\u679d\u3002\u53d7\u751f\u7269\u795e\u7ecf\u5143\u81ea\u53d1\u57fa\u7ebf\u653e\u7535\u7387\u7684\u542f\u53d1\uff0c\u5f15\u5165\u4e00\u5c0f\u7ec4\u53ef\u8bad\u7ec3\u7684\u81ea\u53d1\u795e\u7ecf\u5143\u4f5c\u4e3a\u8865\u507f\u5355\u5143\uff0c\u4ee5\u7a33\u5b9a\u7a00\u758f\u5316LLM\u4e2d\u7684\u6fc0\u6d3b\u3002\u8fd9\u4e9b\u8f85\u52a9\u795e\u7ecf\u5143\u5728\u7a00\u758f\u5316\u8fc7\u7a0b\u4e2d\u63d0\u4f9b\u8865\u507f\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u4e9b\u8f85\u52a9\u795e\u7ecf\u5143\u663e\u8457\u51cf\u5c11\u4e86\u7a00\u758f\u5316\u5f15\u8d77\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u540c\u65f6\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u90fd\u80fd\u6709\u6548\u6cdb\u5316\uff0c\u63d0\u9ad8\u4e86\u7a00\u758f\u5316\u6a21\u578b\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u53d7\u751f\u7269\u542f\u53d1\u7684\u81ea\u53d1\u795e\u7ecf\u5143\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u6539\u5584\u8f93\u5165\u7a00\u758f\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u51cf\u5c11\u6027\u80fd\u635f\u5931\uff0c\u4e3a\u9ad8\u6548LLM\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2512.12658", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12658", "abs": "https://arxiv.org/abs/2512.12658", "authors": ["Qixin Xu", "Haozhe Wang", "Che Liu", "Fangzhen Lin", "Wenhu Chen"], "title": "CogDoc: Towards Unified thinking in Documents", "comment": null, "summary": "Current document reasoning paradigms are constrained by a fundamental trade-off between scalability (processing long-context documents) and fidelity (capturing fine-grained, multimodal details). To bridge this gap, we propose CogDoc, a unified coarse-to-fine thinking framework that mimics human cognitive processes: a low-resolution \"Fast Reading\" phase for scalable information localization,followed by a high-resolution \"Focused Thinking\" phase for deep reasoning. We conduct a rigorous investigation into post-training strategies for the unified thinking framework, demonstrating that a Direct Reinforcement Learning (RL) approach outperforms RL with Supervised Fine-Tuning (SFT) initialization. Specifically, we find that direct RL avoids the \"policy conflict\" observed in SFT. Empirically, our 7B model achieves state-of-the-art performance within its parameter class, notably surpassing significantly larger proprietary models (e.g., GPT-4o) on challenging, visually rich document benchmarks.", "AI": {"tldr": "CogDoc\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u7c97\u5230\u7ec6\u7684\u8ba4\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\"\u5feb\u901f\u9605\u8bfb\"\u548c\"\u4e13\u6ce8\u601d\u8003\"\u4e24\u9636\u6bb5\u5904\u7406\u6587\u6863\uff0c\u89e3\u51b3\u4e86\u957f\u6587\u6863\u5904\u7406\u4e0e\u7ec6\u7c92\u5ea6\u7ec6\u8282\u6355\u6349\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u6587\u6863\u63a8\u7406\u8303\u5f0f\u5b58\u5728\u4e00\u4e2a\u57fa\u672c\u6743\u8861\uff1a\u53ef\u6269\u5c55\u6027\uff08\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u6587\u6863\uff09\u4e0e\u4fdd\u771f\u5ea6\uff08\u6355\u6349\u7ec6\u7c92\u5ea6\u591a\u6a21\u6001\u7ec6\u8282\uff09\u4e4b\u95f4\u7684\u77db\u76fe\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u5904\u7406\u957f\u6587\u6863\u5e76\u4fdd\u7559\u7cbe\u7ec6\u7ec6\u8282\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faCogDoc\u7edf\u4e00\u6846\u67b6\uff0c\u6a21\u4eff\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\uff1a1\uff09\u4f4e\u5206\u8fa8\u7387\"\u5feb\u901f\u9605\u8bfb\"\u9636\u6bb5\u8fdb\u884c\u53ef\u6269\u5c55\u4fe1\u606f\u5b9a\u4f4d\uff1b2\uff09\u9ad8\u5206\u8fa8\u7387\"\u4e13\u6ce8\u601d\u8003\"\u9636\u6bb5\u8fdb\u884c\u6df1\u5ea6\u63a8\u7406\u3002\u7814\u7a76\u4e86\u540e\u8bad\u7ec3\u7b56\u7565\uff0c\u53d1\u73b0\u76f4\u63a5\u5f3a\u5316\u5b66\u4e60\u4f18\u4e8eSFT\u521d\u59cb\u5316\u7684RL\u65b9\u6cd5\u3002", "result": "7B\u53c2\u6570\u6a21\u578b\u5728\u5176\u53c2\u6570\u7c7b\u522b\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u89c6\u89c9\u4e30\u5bcc\u6587\u6863\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u8d85\u8d8a\u4e86\u66f4\u5927\u7684\u4e13\u6709\u6a21\u578b\uff08\u5982GPT-4o\uff09\u3002\u76f4\u63a5RL\u907f\u514d\u4e86SFT\u4e2d\u89c2\u5bdf\u5230\u7684\"\u7b56\u7565\u51b2\u7a81\"\u3002", "conclusion": "CogDoc\u6846\u67b6\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u6587\u6863\u63a8\u7406\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u4e0e\u4fdd\u771f\u5ea6\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u6587\u6863\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12762", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12762", "abs": "https://arxiv.org/abs/2512.12762", "authors": ["Incheol Baek", "Hyungbin Kim", "Minseo Kim", "Yon Dohn Chung"], "title": "Federated Learning with Feedback Alignment", "comment": null, "summary": "Federated Learning (FL) enables collaborative training across multiple clients while preserving data privacy, yet it struggles with data heterogeneity, where clients' data are not distributed independently and identically (non-IID). This causes local drift, hindering global model convergence. To address this, we introduce Federated Learning with Feedback Alignment (FLFA), a novel framework that integrates feedback alignment into FL. FLFA uses the global model's weights as a shared feedback matrix during local training's backward pass, aligning local updates with the global model efficiently. This approach mitigates local drift with minimal additional computational cost and no extra communication overhead.\n  Our theoretical analysis supports FLFA's design by showing how it alleviates local drift and demonstrates robust convergence for both local and global models. Empirical evaluations, including accuracy comparisons and measurements of local drift, further illustrate that FLFA can enhance other FL methods demonstrating its effectiveness.", "AI": {"tldr": "FLFA\u5c06\u53cd\u9988\u5bf9\u9f50\u673a\u5236\u5f15\u5165\u8054\u90a6\u5b66\u4e60\uff0c\u901a\u8fc7\u4f7f\u7528\u5168\u5c40\u6a21\u578b\u6743\u91cd\u4f5c\u4e3a\u5171\u4eab\u53cd\u9988\u77e9\u9635\u6765\u5bf9\u9f50\u672c\u5730\u66f4\u65b0\uff0c\u6709\u6548\u7f13\u89e3\u6570\u636e\u5f02\u6784\u6027\u5bfc\u81f4\u7684\u672c\u5730\u6f02\u79fb\u95ee\u9898\uff0c\u4e14\u8ba1\u7b97\u548c\u901a\u4fe1\u5f00\u9500\u6781\u5c0f\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u5728\u6570\u636e\u5f02\u6784\uff08\u975eIID\uff09\u573a\u666f\u4e0b\u5b58\u5728\u672c\u5730\u6f02\u79fb\u95ee\u9898\uff0c\u8fd9\u4f1a\u963b\u788d\u5168\u5c40\u6a21\u578b\u7684\u6536\u655b\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u8fd9\u4e00\u6311\u6218\u65f6\u5f80\u5f80\u5e26\u6765\u8f83\u5927\u7684\u8ba1\u7b97\u6216\u901a\u4fe1\u5f00\u9500\u3002", "method": "\u63d0\u51faFLFA\u6846\u67b6\uff0c\u5c06\u53cd\u9988\u5bf9\u9f50\u673a\u5236\u96c6\u6210\u5230\u8054\u90a6\u5b66\u4e60\u4e2d\u3002\u5728\u672c\u5730\u8bad\u7ec3\u7684\u53cd\u5411\u4f20\u64ad\u8fc7\u7a0b\u4e2d\uff0c\u4f7f\u7528\u5168\u5c40\u6a21\u578b\u7684\u6743\u91cd\u4f5c\u4e3a\u5171\u4eab\u53cd\u9988\u77e9\u9635\uff0c\u4ece\u800c\u6709\u6548\u5bf9\u9f50\u672c\u5730\u66f4\u65b0\u4e0e\u5168\u5c40\u6a21\u578b\u65b9\u5411\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660eFLFA\u80fd\u6709\u6548\u7f13\u89e3\u672c\u5730\u6f02\u79fb\uff0c\u5e76\u4fdd\u8bc1\u672c\u5730\u548c\u5168\u5c40\u6a21\u578b\u7684\u9c81\u68d2\u6536\u655b\u3002\u5b9e\u8bc1\u8bc4\u4f30\u663e\u793aFLFA\u80fd\u63d0\u5347\u5176\u4ed6FL\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5728\u51c6\u786e\u7387\u6bd4\u8f83\u548c\u672c\u5730\u6f02\u79fb\u6d4b\u91cf\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "FLFA\u901a\u8fc7\u521b\u65b0\u7684\u53cd\u9988\u5bf9\u9f50\u673a\u5236\uff0c\u4ee5\u6781\u5c0f\u7684\u989d\u5916\u8ba1\u7b97\u6210\u672c\u548c\u96f6\u989d\u5916\u901a\u4fe1\u5f00\u9500\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u5f02\u6784\u6027\u548c\u672c\u5730\u6f02\u79fb\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2512.12779", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12779", "abs": "https://arxiv.org/abs/2512.12779", "authors": ["Mohammad Abu-Shaira", "Weishi Shi"], "title": "OLR-WAA: Adaptive and Drift-Resilient Online Regression with Dynamic Weighted Averaging", "comment": null, "summary": "Real-world datasets frequently exhibit evolving data distributions, reflecting temporal variations and underlying shifts. Overlooking this phenomenon, known as concept drift, can substantially degrade the predictive performance of the model. Furthermore, the presence of hyperparameters in online models exacerbates this issue, as these parameters are typically fixed and lack the flexibility to dynamically adjust to evolving data. This paper introduces \"OLR-WAA: An Adaptive and Drift-Resilient Online Regression with Dynamic Weighted Average\", a hyperparameter-free model designed to tackle the challenges of non-stationary data streams and enable effective, continuous adaptation. The objective is to strike a balance between model stability and adaptability. OLR-WAA incrementally updates its base model by integrating incoming data streams, utilizing an exponentially weighted moving average. It further introduces a unique optimization mechanism that dynamically detects concept drift, quantifies its magnitude, and adjusts the model based on real-time data characteristics. Rigorous evaluations show that it matches batch regression performance in static settings and consistently outperforms or rivals state-of-the-art online models, confirming its effectiveness. Concept drift datasets reveal a performance gap that OLR-WAA effectively bridges, setting it apart from other online models. In addition, the model effectively handles confidence-based scenarios through a conservative update strategy that prioritizes stable, high-confidence data points. Notably, OLR-WAA converges rapidly, consistently yielding higher R2 values compared to other online models.", "AI": {"tldr": "OLR-WAA\u662f\u4e00\u79cd\u65e0\u8d85\u53c2\u6570\u7684\u81ea\u9002\u5e94\u5728\u7ebf\u56de\u5f52\u6a21\u578b\uff0c\u901a\u8fc7\u52a8\u6001\u52a0\u6743\u5e73\u5747\u673a\u5236\u5904\u7406\u6570\u636e\u6d41\u4e2d\u7684\u6982\u5ff5\u6f02\u79fb\uff0c\u5728\u7a33\u5b9a\u6027\u548c\u9002\u5e94\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u7ecf\u5e38\u5448\u73b0\u4e0d\u65ad\u6f14\u5316\u7684\u6570\u636e\u5206\u5e03\uff08\u6982\u5ff5\u6f02\u79fb\uff09\uff0c\u5ffd\u7565\u8fd9\u4e00\u73b0\u8c61\u4f1a\u663e\u8457\u964d\u4f4e\u6a21\u578b\u9884\u6d4b\u6027\u80fd\u3002\u5728\u7ebf\u6a21\u578b\u4e2d\u7684\u8d85\u53c2\u6570\u901a\u5e38\u662f\u56fa\u5b9a\u7684\uff0c\u7f3a\u4e4f\u52a8\u6001\u9002\u5e94\u53d8\u5316\u6570\u636e\u7684\u80fd\u529b\uff0c\u8fd9\u52a0\u5267\u4e86\u95ee\u9898\u3002", "method": "\u63d0\u51faOLR-WAA\u6a21\u578b\uff1a1\uff09\u901a\u8fc7\u6307\u6570\u52a0\u6743\u79fb\u52a8\u5e73\u5747\u589e\u91cf\u66f4\u65b0\u57fa\u7840\u6a21\u578b\uff1b2\uff09\u5f15\u5165\u72ec\u7279\u7684\u4f18\u5316\u673a\u5236\uff0c\u52a8\u6001\u68c0\u6d4b\u6982\u5ff5\u6f02\u79fb\u3001\u91cf\u5316\u5176\u5e45\u5ea6\uff0c\u5e76\u6839\u636e\u5b9e\u65f6\u6570\u636e\u7279\u5f81\u8c03\u6574\u6a21\u578b\uff1b3\uff09\u91c7\u7528\u4fdd\u5b88\u66f4\u65b0\u7b56\u7565\u5904\u7406\u7f6e\u4fe1\u5ea6\u573a\u666f\uff0c\u4f18\u5148\u8003\u8651\u7a33\u5b9a\u3001\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u6570\u636e\u70b9\u3002", "result": "\u5728\u9759\u6001\u8bbe\u7f6e\u4e0b\u4e0e\u6279\u91cf\u56de\u5f52\u6027\u80fd\u76f8\u5f53\uff1b\u5728\u6982\u5ff5\u6f02\u79fb\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u5728\u7ebf\u6a21\u578b\uff0c\u6709\u6548\u5f25\u8865\u6027\u80fd\u5dee\u8ddd\uff1b\u5feb\u901f\u6536\u655b\uff0c\u76f8\u6bd4\u5176\u4ed6\u5728\u7ebf\u6a21\u578b\u6301\u7eed\u4ea7\u751f\u66f4\u9ad8\u7684R2\u503c\u3002", "conclusion": "OLR-WAA\u662f\u4e00\u79cd\u6709\u6548\u7684\u65e0\u8d85\u53c2\u6570\u5728\u7ebf\u56de\u5f52\u6a21\u578b\uff0c\u80fd\u591f\u5904\u7406\u975e\u5e73\u7a33\u6570\u636e\u6d41\uff0c\u5728\u6982\u5ff5\u6f02\u79fb\u73af\u5883\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u5728\u6a21\u578b\u7a33\u5b9a\u6027\u548c\u9002\u5e94\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2512.12664", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12664", "abs": "https://arxiv.org/abs/2512.12664", "authors": ["Sreehari Rajan", "Kunal Bhosikar", "Charu Sharma"], "title": "InteracTalker: Prompt-Based Human-Object Interaction with Co-Speech Gesture Generation", "comment": null, "summary": "Generating realistic human motions that naturally respond to both spoken language and physical objects is crucial for interactive digital experiences. Current methods, however, address speech-driven gestures or object interactions independently, limiting real-world applicability due to a lack of integrated, comprehensive datasets. To overcome this, we introduce InteracTalker, a novel framework that seamlessly integrates prompt-based object-aware interactions with co-speech gesture generation. We achieve this by employing a multi-stage training process to learn a unified motion, speech, and prompt embedding space. To support this, we curate a rich human-object interaction dataset, formed by augmenting an existing text-to-motion dataset with detailed object interaction annotations. Our framework utilizes a Generalized Motion Adaptation Module that enables independent training, adapting to the corresponding motion condition, which is then dynamically combined during inference. To address the imbalance between heterogeneous conditioning signals, we propose an adaptive fusion strategy, which dynamically reweights the conditioning signals during diffusion sampling. InteracTalker successfully unifies these previously separate tasks, outperforming prior methods in both co-speech gesture generation and object-interaction synthesis, outperforming gesture-focused diffusion methods, yielding highly realistic, object-aware full-body motions with enhanced realism, flexibility, and control.", "AI": {"tldr": "InteracTalker\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u80fd\u591f\u540c\u65f6\u751f\u6210\u8bed\u97f3\u9a71\u52a8\u7684\u624b\u52bf\u548c\u7269\u4f53\u4ea4\u4e92\u52a8\u4f5c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u53ea\u80fd\u72ec\u7acb\u5904\u7406\u8fd9\u4e24\u4e2a\u4efb\u52a1\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5206\u522b\u5904\u7406\u8bed\u97f3\u9a71\u52a8\u624b\u52bf\u548c\u7269\u4f53\u4ea4\u4e92\uff0c\u7f3a\u4e4f\u96c6\u6210\u6570\u636e\u96c6\u548c\u7edf\u4e00\u6846\u67b6\uff0c\u9650\u5236\u4e86\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u7684\u9002\u7528\u6027\u3002", "method": "\u91c7\u7528\u591a\u9636\u6bb5\u8bad\u7ec3\u5b66\u4e60\u7edf\u4e00\u7684\u8fd0\u52a8\u3001\u8bed\u97f3\u548c\u63d0\u793a\u5d4c\u5165\u7a7a\u95f4\uff1b\u6784\u5efa\u4e30\u5bcc\u7684\u4eba-\u7269\u4ea4\u4e92\u6570\u636e\u96c6\uff1b\u4f7f\u7528\u5e7f\u4e49\u8fd0\u52a8\u9002\u5e94\u6a21\u5757\u8fdb\u884c\u72ec\u7acb\u8bad\u7ec3\uff1b\u63d0\u51fa\u81ea\u9002\u5e94\u878d\u5408\u7b56\u7565\u5904\u7406\u5f02\u8d28\u6761\u4ef6\u4fe1\u53f7\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "result": "InteracTalker\u5728\u8bed\u97f3\u624b\u52bf\u751f\u6210\u548c\u7269\u4f53\u4ea4\u4e92\u5408\u6210\u65b9\u9762\u5747\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u8d85\u8d8a\u4e86\u4e13\u6ce8\u4e8e\u624b\u52bf\u7684\u6269\u6563\u65b9\u6cd5\uff0c\u4ea7\u751f\u9ad8\u5ea6\u771f\u5b9e\u3001\u7269\u4f53\u611f\u77e5\u7684\u5168\u8eab\u8fd0\u52a8\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u7edf\u4e00\u4e86\u5148\u524d\u5206\u79bb\u7684\u4efb\u52a1\uff0c\u63d0\u4f9b\u4e86\u589e\u5f3a\u7684\u771f\u5b9e\u611f\u3001\u7075\u6d3b\u6027\u548c\u63a7\u5236\u80fd\u529b\uff0c\u4e3a\u4ea4\u4e92\u5f0f\u6570\u5b57\u4f53\u9a8c\u63d0\u4f9b\u4e86\u66f4\u81ea\u7136\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12783", "categories": ["cs.LG", "q-fin.ST", "stat.AP"], "pdf": "https://arxiv.org/pdf/2512.12783", "abs": "https://arxiv.org/abs/2512.12783", "authors": ["Atalay Denknalbant", "Emre Sezdi", "Zeki Furkan Kutlu", "Polat Goktas"], "title": "Credit Risk Estimation with Non-Financial Features: Evidence from a Synthetic Istanbul Dataset", "comment": null, "summary": "Financial exclusion constrains entrepreneurship, increases income volatility, and widens wealth gaps. Underbanked consumers in Istanbul often have no bureau file because their earnings and payments flow through informal channels. To study how such borrowers can be evaluated we create a synthetic dataset of one hundred thousand Istanbul residents that reproduces first quarter 2025 T\u00dc\u0130K census marginals and telecom usage patterns. Retrieval augmented generation feeds these public statistics into the OpenAI o3 model, which synthesises realistic yet private records. Each profile contains seven socio demographic variables and nine alternative attributes that describe phone specifications, online shopping rhythm, subscription spend, car ownership, monthly rent, and a credit card flag. To test the impact of the alternative financial data CatBoost, LightGBM, and XGBoost are each trained in two versions. Demo models use only the socio demographic variables; Full models include both socio demographic and alternative attributes. Across five fold stratified validation the alternative block raises area under the curve by about one point three percentage and lifts balanced \\(F_{1}\\) from roughly 0.84 to 0.95, a fourteen percent gain. We contribute an open Istanbul 2025 Q1 synthetic dataset, a fully reproducible modeling pipeline, and empirical evidence that a concise set of behavioural attributes can approach bureau level discrimination power while serving borrowers who lack formal credit records. These findings give lenders and regulators a transparent blueprint for extending fair and safe credit access to the underbanked.", "AI": {"tldr": "\u8be5\u7814\u7a76\u521b\u5efa\u4e86\u4f0a\u65af\u5766\u5e03\u5c14\u5c45\u6c11\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u66ff\u4ee3\u91d1\u878d\u6570\u636e\uff08\u5982\u624b\u673a\u4f7f\u7528\u3001\u6d88\u8d39\u4e60\u60ef\u7b49\uff09\u6765\u8bc4\u4f30\u7f3a\u4e4f\u6b63\u5f0f\u4fe1\u7528\u8bb0\u5f55\u7684\u501f\u6b3e\u4eba\uff0c\u8bc1\u660e\u8fd9\u4e9b\u884c\u4e3a\u5c5e\u6027\u80fd\u663e\u8457\u63d0\u5347\u4fe1\u7528\u8bc4\u4f30\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u91d1\u878d\u6392\u65a5\u9650\u5236\u4e86\u521b\u4e1a\u673a\u4f1a\u3001\u589e\u52a0\u4e86\u6536\u5165\u6ce2\u52a8\u5e76\u6269\u5927\u4e86\u8d22\u5bcc\u5dee\u8ddd\u3002\u4f0a\u65af\u5766\u5e03\u5c14\u7684\u94f6\u884c\u670d\u52a1\u4e0d\u8db3\u4eba\u7fa4\u7531\u4e8e\u6536\u5165\u548c\u652f\u4ed8\u901a\u8fc7\u975e\u6b63\u89c4\u6e20\u9053\u6d41\u52a8\uff0c\u5f80\u5f80\u6ca1\u6709\u4fe1\u7528\u5c40\u6863\u6848\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u8bc4\u4f30\u8fd9\u7c7b\u501f\u6b3e\u4eba\u3002", "method": "1. \u521b\u5efa\u5305\u542b10\u4e07\u4f0a\u65af\u5766\u5e03\u5c14\u5c45\u6c11\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u91cd\u73b02025\u5e74\u7b2c\u4e00\u5b63\u5ea6\u571f\u8033\u5176\u7edf\u8ba1\u5c40\u4eba\u53e3\u666e\u67e5\u8fb9\u9645\u548c\u7535\u4fe1\u4f7f\u7528\u6a21\u5f0f\n2. \u4f7f\u7528\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6280\u672f\u5c06\u516c\u5171\u7edf\u8ba1\u6570\u636e\u8f93\u5165OpenAI o3\u6a21\u578b\uff0c\u751f\u6210\u771f\u5b9e\u4f46\u79c1\u5bc6\u7684\u8bb0\u5f55\n3. \u6bcf\u4e2a\u6863\u6848\u5305\u542b7\u4e2a\u793e\u4f1a\u4eba\u53e3\u53d8\u91cf\u548c9\u4e2a\u66ff\u4ee3\u5c5e\u6027\uff08\u624b\u673a\u89c4\u683c\u3001\u5728\u7ebf\u8d2d\u7269\u8282\u594f\u3001\u8ba2\u9605\u652f\u51fa\u3001\u6c7d\u8f66\u6240\u6709\u6743\u3001\u6708\u79df\u91d1\u3001\u4fe1\u7528\u5361\u6807\u5fd7\uff09\n4. \u4f7f\u7528CatBoost\u3001LightGBM\u548cXGBoost\u8bad\u7ec3\u4e24\u4e2a\u7248\u672c\u6a21\u578b\uff1a\u6f14\u793a\u6a21\u578b\u4ec5\u4f7f\u7528\u793e\u4f1a\u4eba\u53e3\u53d8\u91cf\uff0c\u5b8c\u6574\u6a21\u578b\u5305\u542b\u6240\u6709\u5c5e\u6027\n5. \u91c7\u7528\u4e94\u6298\u5206\u5c42\u9a8c\u8bc1\u8bc4\u4f30\u6a21\u578b\u6027\u80fd", "result": "1. \u66ff\u4ee3\u6570\u636e\u5757\u5c06AUC\u63d0\u9ad8\u4e86\u7ea61.3\u4e2a\u767e\u5206\u70b9\n2. \u5e73\u8861F1\u5206\u6570\u4ece\u7ea60.84\u63d0\u5347\u52300.95\uff0c\u589e\u76ca\u8fbe14%\n3. \u7b80\u6d01\u7684\u884c\u4e3a\u5c5e\u6027\u96c6\u80fd\u591f\u63a5\u8fd1\u4fe1\u7528\u5c40\u7684\u533a\u5206\u80fd\u529b\uff0c\u540c\u65f6\u670d\u52a1\u4e8e\u7f3a\u4e4f\u6b63\u5f0f\u4fe1\u7528\u8bb0\u5f55\u7684\u501f\u6b3e\u4eba", "conclusion": "\u7814\u7a76\u8d21\u732e\u5305\u62ec\uff1a\u5f00\u653e\u7684\u4f0a\u65af\u5766\u5e03\u5c142025\u5e74\u7b2c\u4e00\u5b63\u5ea6\u5408\u6210\u6570\u636e\u96c6\u3001\u5b8c\u5168\u53ef\u590d\u73b0\u7684\u5efa\u6a21\u6d41\u7a0b\u3001\u4ee5\u53ca\u7ecf\u9a8c\u8bc1\u636e\u8868\u660e\u7b80\u6d01\u7684\u884c\u4e3a\u5c5e\u6027\u96c6\u80fd\u591f\u63a5\u8fd1\u4fe1\u7528\u5c40\u7684\u533a\u5206\u80fd\u529b\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u8d37\u6b3e\u673a\u6784\u548c\u76d1\u7ba1\u673a\u6784\u63d0\u4f9b\u4e86\u900f\u660e\u84dd\u56fe\uff0c\u4ee5\u6269\u5c55\u5bf9\u94f6\u884c\u670d\u52a1\u4e0d\u8db3\u4eba\u7fa4\u7684\u516c\u5e73\u548c\u5b89\u5168\u4fe1\u8d37\u8bbf\u95ee\u3002"}}
{"id": "2512.12667", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12667", "abs": "https://arxiv.org/abs/2512.12667", "authors": ["Haiyang Zheng", "Nan Pu", "Wenjing Li", "Teng Long", "Nicu Sebe", "Zhun Zhong"], "title": "Open-World Deepfake Attribution via Confidence-Aware Asymmetric Learning", "comment": "Accepted by AAAI2026", "summary": "The proliferation of synthetic facial imagery has intensified the need for robust Open-World DeepFake Attribution (OW-DFA), which aims to attribute both known and unknown forgeries using labeled data for known types and unlabeled data containing a mixture of known and novel types. However, existing OW-DFA methods face two critical limitations: 1) A confidence skew that leads to unreliable pseudo-labels for novel forgeries, resulting in biased training. 2) An unrealistic assumption that the number of unknown forgery types is known *a priori*. To address these challenges, we propose a Confidence-Aware Asymmetric Learning (CAL) framework, which adaptively balances model confidence across known and novel forgery types. CAL mainly consists of two components: Confidence-Aware Consistency Regularization (CCR) and Asymmetric Confidence Reinforcement (ACR). CCR mitigates pseudo-label bias by dynamically scaling sample losses based on normalized confidence, gradually shifting the training focus from high- to low-confidence samples. ACR complements this by separately calibrating confidence for known and novel classes through selective learning on high-confidence samples, guided by their confidence gap. Together, CCR and ACR form a mutually reinforcing loop that significantly improves the model's OW-DFA performance. Moreover, we introduce a Dynamic Prototype Pruning (DPP) strategy that automatically estimates the number of novel forgery types in a coarse-to-fine manner, removing the need for unrealistic prior assumptions and enhancing the scalability of our methods to real-world OW-DFA scenarios. Extensive experiments on the standard OW-DFA benchmark and a newly extended benchmark incorporating advanced manipulations demonstrate that CAL consistently outperforms previous methods, achieving new state-of-the-art performance on both known and novel forgery attribution.", "AI": {"tldr": "\u63d0\u51faCAL\u6846\u67b6\u89e3\u51b3\u5f00\u653e\u4e16\u754c\u6df1\u5ea6\u4f2a\u9020\u6eaf\u6e90\u95ee\u9898\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u611f\u77e5\u7684\u975e\u5bf9\u79f0\u5b66\u4e60\u5e73\u8861\u5df2\u77e5\u548c\u672a\u77e5\u4f2a\u9020\u7c7b\u578b\u7684\u8bc6\u522b\uff0c\u5e76\u5f15\u5165\u52a8\u6001\u539f\u578b\u526a\u679d\u81ea\u52a8\u4f30\u8ba1\u672a\u77e5\u4f2a\u9020\u7c7b\u578b\u6570\u91cf\u3002", "motivation": "\u73b0\u6709\u5f00\u653e\u4e16\u754c\u6df1\u5ea6\u4f2a\u9020\u6eaf\u6e90\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a1\uff09\u7f6e\u4fe1\u5ea6\u504f\u659c\u5bfc\u81f4\u5bf9\u672a\u77e5\u4f2a\u9020\u7684\u4f2a\u6807\u7b7e\u4e0d\u53ef\u9760\uff0c\u9020\u6210\u8bad\u7ec3\u504f\u5dee\uff1b2\uff09\u4e0d\u73b0\u5b9e\u5730\u5047\u8bbe\u672a\u77e5\u4f2a\u9020\u7c7b\u578b\u7684\u6570\u91cf\u5df2\u77e5\u3002", "method": "\u63d0\u51fa\u7f6e\u4fe1\u5ea6\u611f\u77e5\u975e\u5bf9\u79f0\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u7f6e\u4fe1\u5ea6\u611f\u77e5\u4e00\u81f4\u6027\u6b63\u5219\u5316\u548c\u975e\u5bf9\u79f0\u7f6e\u4fe1\u5ea6\u589e\u5f3a\u4e24\u4e2a\u7ec4\u4ef6\uff0c\u4ee5\u53ca\u52a8\u6001\u539f\u578b\u526a\u679d\u7b56\u7565\u81ea\u52a8\u4f30\u8ba1\u672a\u77e5\u4f2a\u9020\u7c7b\u578b\u6570\u91cf\u3002", "result": "\u5728\u6807\u51c6\u5f00\u653e\u4e16\u754c\u6df1\u5ea6\u4f2a\u9020\u6eaf\u6e90\u57fa\u51c6\u548c\u65b0\u6269\u5c55\u7684\u5305\u542b\u9ad8\u7ea7\u64cd\u4f5c\u7684\u57fa\u51c6\u4e0a\uff0cCAL\u65b9\u6cd5\u59cb\u7ec8\u4f18\u4e8e\u5148\u524d\u65b9\u6cd5\uff0c\u5728\u5df2\u77e5\u548c\u672a\u77e5\u4f2a\u9020\u6eaf\u6e90\u4e0a\u90fd\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "CAL\u6846\u67b6\u901a\u8fc7\u5e73\u8861\u7f6e\u4fe1\u5ea6\u548c\u81ea\u52a8\u4f30\u8ba1\u672a\u77e5\u7c7b\u578b\u6570\u91cf\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f00\u653e\u4e16\u754c\u6df1\u5ea6\u4f2a\u9020\u6eaf\u6e90\u7684\u6027\u80fd\uff0c\u589e\u5f3a\u4e86\u65b9\u6cd5\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2512.12785", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12785", "abs": "https://arxiv.org/abs/2512.12785", "authors": ["Mohammad Abu Shaira", "Yunhe Feng", "Heng Fan", "Weishi Shi"], "title": "OLC-WA: Drift Aware Tuning-Free Online Classification with Weighted Average", "comment": null, "summary": "Real-world data sets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. This paper introduces Online Classification with Weighted Average (OLC-WA), an adaptive, hyperparameter-free online classification model equipped with an automated optimization mechanism. OLC-WA operates by blending incoming data streams with an existing base model. This blending is facilitated by an exponentially weighted moving average. Furthermore, an integrated optimization mechanism dynamically detects concept drift, quantifies its magnitude, and adjusts the model based on the observed data stream characteristics. This approach empowers the model to effectively adapt to evolving data distributions within streaming environments. Rigorous empirical evaluation across diverse benchmark datasets shows that OLC-WA achieves performance comparable to batch models in stationary environments, maintaining accuracy within 1-3%, and surpasses leading online baselines by 10-25% under drift, demonstrating its effectiveness in adapting to dynamic data streams.", "AI": {"tldr": "OLC-WA\u662f\u4e00\u79cd\u65e0\u9700\u8d85\u53c2\u6570\u7684\u81ea\u9002\u5e94\u5728\u7ebf\u5206\u7c7b\u6a21\u578b\uff0c\u901a\u8fc7\u6307\u6570\u52a0\u6743\u79fb\u52a8\u5e73\u5747\u878d\u5408\u6570\u636e\u6d41\u4e0e\u57fa\u7840\u6a21\u578b\uff0c\u81ea\u52a8\u68c0\u6d4b\u6982\u5ff5\u6f02\u79fb\u5e76\u52a8\u6001\u8c03\u6574\uff0c\u5728\u52a8\u6001\u6570\u636e\u6d41\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u5e38\u5448\u73b0\u968f\u65f6\u95f4\u6f14\u5316\u7684\u6570\u636e\u5206\u5e03\uff08\u6982\u5ff5\u6f02\u79fb\uff09\uff0c\u5ffd\u89c6\u6b64\u73b0\u8c61\u4f1a\u663e\u8457\u964d\u4f4e\u6a21\u578b\u9884\u6d4b\u7cbe\u5ea6\u3002\u5728\u7ebf\u6a21\u578b\u4e2d\u7684\u8d85\u53c2\u6570\u901a\u5e38\u662f\u56fa\u5b9a\u7684\uff0c\u65e0\u6cd5\u968f\u6570\u636e\u5206\u5e03\u53d8\u5316\u52a8\u6001\u8c03\u6574\uff0c\u8fd9\u52a0\u5267\u4e86\u95ee\u9898\u3002", "method": "\u63d0\u51faOLC-WA\uff08\u5728\u7ebf\u5206\u7c7b\u52a0\u6743\u5e73\u5747\uff09\u6a21\u578b\uff0c\u91c7\u7528\u6307\u6570\u52a0\u6743\u79fb\u52a8\u5e73\u5747\u5c06\u65b0\u6570\u636e\u6d41\u4e0e\u73b0\u6709\u57fa\u7840\u6a21\u578b\u878d\u5408\u3002\u96c6\u6210\u4f18\u5316\u673a\u5236\u80fd\u52a8\u6001\u68c0\u6d4b\u6982\u5ff5\u6f02\u79fb\u3001\u91cf\u5316\u5176\u5e45\u5ea6\uff0c\u5e76\u6839\u636e\u89c2\u6d4b\u5230\u7684\u6570\u636e\u6d41\u7279\u5f81\u8c03\u6574\u6a21\u578b\u3002", "result": "\u5728\u591a\u79cd\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u4e25\u683c\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff1a\u5728\u9759\u6001\u73af\u5883\u4e2d\uff0cOLC-WA\u6027\u80fd\u4e0e\u6279\u5904\u7406\u6a21\u578b\u76f8\u5f53\uff0c\u51c6\u786e\u7387\u4fdd\u6301\u57281-3%\u8303\u56f4\u5185\uff1b\u5728\u6f02\u79fb\u60c5\u51b5\u4e0b\uff0cOLC-WA\u8d85\u8d8a\u9886\u5148\u7684\u5728\u7ebf\u57fa\u7ebf\u65b9\u6cd510-25%\u3002", "conclusion": "OLC-WA\u662f\u4e00\u79cd\u6709\u6548\u7684\u81ea\u9002\u5e94\u5728\u7ebf\u5206\u7c7b\u6a21\u578b\uff0c\u65e0\u9700\u8d85\u53c2\u6570\u4e14\u5177\u5907\u81ea\u52a8\u4f18\u5316\u673a\u5236\uff0c\u80fd\u591f\u5728\u6d41\u5f0f\u73af\u5883\u4e2d\u6709\u6548\u9002\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u6570\u636e\u5206\u5e03\uff0c\u5728\u6982\u5ff5\u6f02\u79fb\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2512.12787", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12787", "abs": "https://arxiv.org/abs/2512.12787", "authors": ["Mohammad Abu-Shaira", "Weishi Shi"], "title": "Unveiling Statistical Significance of Online Regression over Multiple Datasets", "comment": null, "summary": "Despite extensive focus on techniques for evaluating the performance of two learning algorithms on a single dataset, the critical challenge of developing statistical tests to compare multiple algorithms across various datasets has been largely overlooked in most machine learning research. Additionally, in the realm of Online Learning, ensuring statistical significance is essential to validate continuous learning processes, particularly for achieving rapid convergence and effectively managing concept drifts in a timely manner. Robust statistical methods are needed to assess the significance of performance differences as data evolves over time. This article examines the state-of-the-art online regression models and empirically evaluates several suitable tests. To compare multiple online regression models across various datasets, we employed the Friedman test along with corresponding post-hoc tests. For thorough evaluations, utilizing both real and synthetic datasets with 5-fold cross-validation and seed averaging ensures comprehensive assessment across various data subsets. Our tests generally confirmed the performance of competitive baselines as consistent with their individual reports. However, some statistical test results also indicate that there is still room for improvement in certain aspects of state-of-the-art methods.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u673a\u5668\u5b66\u4e60\u4e2d\u591a\u7b97\u6cd5\u8de8\u6570\u636e\u96c6\u6bd4\u8f83\u7684\u7edf\u8ba1\u68c0\u9a8c\u65b9\u6cd5\uff0c\u7279\u522b\u5173\u6ce8\u5728\u7ebf\u5b66\u4e60\u573a\u666f\uff0c\u4f7f\u7528Friedman\u68c0\u9a8c\u548c\u4e8b\u540e\u68c0\u9a8c\u8bc4\u4f30\u591a\u4e2a\u5728\u7ebf\u56de\u5f52\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u5dee\u5f02\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u4e2a\u6570\u636e\u96c6\u4e0a\u4e24\u79cd\u7b97\u6cd5\u7684\u6027\u80fd\u8bc4\u4f30\uff0c\u800c\u591a\u7b97\u6cd5\u8de8\u6570\u636e\u96c6\u6bd4\u8f83\u7684\u7edf\u8ba1\u68c0\u9a8c\u65b9\u6cd5\u5728\u673a\u5668\u5b66\u4e60\u7814\u7a76\u4e2d\u88ab\u5ffd\u89c6\u3002\u5728\u7ebf\u5b66\u4e60\u9886\u57df\u9700\u8981\u7edf\u8ba1\u663e\u8457\u6027\u9a8c\u8bc1\u6765\u786e\u4fdd\u8fde\u7eed\u5b66\u4e60\u8fc7\u7a0b\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5feb\u901f\u6536\u655b\u548c\u53ca\u65f6\u5904\u7406\u6982\u5ff5\u6f02\u79fb\u3002", "method": "\u91c7\u7528Friedman\u68c0\u9a8c\u548c\u76f8\u5e94\u7684\u4e8b\u540e\u68c0\u9a8c\u6765\u6bd4\u8f83\u591a\u4e2a\u5728\u7ebf\u56de\u5f52\u6a21\u578b\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u3002\u4f7f\u7528\u771f\u5b9e\u548c\u5408\u6210\u6570\u636e\u96c6\uff0c\u901a\u8fc75\u6298\u4ea4\u53c9\u9a8c\u8bc1\u548c\u79cd\u5b50\u5e73\u5747\u8fdb\u884c\u5168\u9762\u7684\u8bc4\u4f30\u3002", "result": "\u6d4b\u8bd5\u7ed3\u679c\u603b\u4f53\u4e0a\u8bc1\u5b9e\u4e86\u7ade\u4e89\u57fa\u7ebf\u7684\u6027\u80fd\u4e0e\u5176\u4e2a\u4f53\u62a5\u544a\u4e00\u81f4\u3002\u7136\u800c\uff0c\u4e00\u4e9b\u7edf\u8ba1\u68c0\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6700\u5148\u8fdb\u65b9\u6cd5\u5728\u67d0\u4e9b\u65b9\u9762\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002", "conclusion": "\u9700\u8981\u7a33\u5065\u7684\u7edf\u8ba1\u65b9\u6cd5\u6765\u8bc4\u4f30\u968f\u65f6\u95f4\u53d8\u5316\u7684\u6570\u636e\u6027\u80fd\u5dee\u5f02\uff0c\u591a\u7b97\u6cd5\u8de8\u6570\u636e\u96c6\u6bd4\u8f83\u7684\u7edf\u8ba1\u68c0\u9a8c\u5bf9\u4e8e\u5728\u7ebf\u5b66\u4e60\u9886\u57df\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u73b0\u6709\u65b9\u6cd5\u867d\u7136\u6709\u6548\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002"}}
{"id": "2512.12675", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12675", "abs": "https://arxiv.org/abs/2512.12675", "authors": ["Yuran Wang", "Bohan Zeng", "Chengzhuo Tong", "Wenxuan Liu", "Yang Shi", "Xiaochen Ma", "Hao Liang", "Yuanxing Zhang", "Wentao Zhang"], "title": "Scone: Bridging Composition and Distinction in Subject-Driven Image Generation via Unified Understanding-Generation Modeling", "comment": "Code: https://github.com/Ryann-Ran/Scone", "summary": "Subject-driven image generation has advanced from single- to multi-subject composition, while neglecting distinction, the ability to identify and generate the correct subject when inputs contain multiple candidates. This limitation restricts effectiveness in complex, realistic visual settings. We propose Scone, a unified understanding-generation method that integrates composition and distinction. Scone enables the understanding expert to act as a semantic bridge, conveying semantic information and guiding the generation expert to preserve subject identity while minimizing interference. A two-stage training scheme first learns composition, then enhances distinction through semantic alignment and attention-based masking. We also introduce SconeEval, a benchmark for evaluating both composition and distinction across diverse scenarios. Experiments demonstrate that Scone outperforms existing open-source models in composition and distinction tasks on two benchmarks. Our model, benchmark, and training data are available at: https://github.com/Ryann-Ran/Scone.", "AI": {"tldr": "Scone\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u89e3-\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6848\uff08\u5148\u5b66\u4e60\u7ec4\u5408\uff0c\u518d\u589e\u5f3a\u533a\u5206\uff09\u6765\u540c\u65f6\u5904\u7406\u591a\u4e3b\u4f53\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u7ec4\u5408\u548c\u533a\u5206\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86SconeEval\u8bc4\u4f30\u57fa\u51c6\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u4e3b\u9898\u7684\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u5df2\u4ece\u5355\u4e3b\u4f53\u53d1\u5c55\u5230\u591a\u4e3b\u4f53\u7ec4\u5408\uff0c\u4f46\u5ffd\u89c6\u4e86\u533a\u5206\u80fd\u529b\u2014\u2014\u5373\u5f53\u8f93\u5165\u5305\u542b\u591a\u4e2a\u5019\u9009\u4e3b\u4f53\u65f6\uff0c\u80fd\u591f\u8bc6\u522b\u5e76\u751f\u6210\u6b63\u786e\u4e3b\u4f53\u7684\u80fd\u529b\u3002\u8fd9\u4e00\u9650\u5236\u5f71\u54cd\u4e86\u5728\u590d\u6742\u771f\u5b9e\u89c6\u89c9\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51faScone\u65b9\u6cd5\uff0c\u5c06\u7406\u89e3\u4e13\u5bb6\u4f5c\u4e3a\u8bed\u4e49\u6865\u6881\uff0c\u4f20\u9012\u8bed\u4e49\u4fe1\u606f\u5e76\u6307\u5bfc\u751f\u6210\u4e13\u5bb6\u5728\u6700\u5c0f\u5316\u5e72\u6270\u7684\u540c\u65f6\u4fdd\u6301\u4e3b\u4f53\u8eab\u4efd\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6848\uff1a\u7b2c\u4e00\u9636\u6bb5\u5b66\u4e60\u7ec4\u5408\uff0c\u7b2c\u4e8c\u9636\u6bb5\u901a\u8fc7\u8bed\u4e49\u5bf9\u9f50\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u63a9\u7801\u589e\u5f3a\u533a\u5206\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cScone\u5728\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u7ec4\u5408\u548c\u533a\u5206\u4efb\u52a1\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709\u7684\u5f00\u6e90\u6a21\u578b\u3002\u4f5c\u8005\u8fd8\u53d1\u5e03\u4e86SconeEval\u8bc4\u4f30\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u7ec4\u5408\u548c\u533a\u5206\u80fd\u529b\u3002", "conclusion": "Scone\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u89e3-\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u7ec4\u5408\u548c\u533a\u5206\u80fd\u529b\uff0c\u5728\u591a\u4e3b\u4f53\u56fe\u50cf\u751f\u6210\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u4e3a\u590d\u6742\u771f\u5b9e\u89c6\u89c9\u573a\u666f\u4e2d\u7684\u4e3b\u9898\u9a71\u52a8\u56fe\u50cf\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12792", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12792", "abs": "https://arxiv.org/abs/2512.12792", "authors": ["Shivansh Sahni", "Wenzhi Zhang"], "title": "Liquid Reasoning Transformers: A Sudoku-Based Prototype for Chess-Scale Algorithmic Tasks", "comment": "11 pages, 0 figures", "summary": "The Liquid Reasoning Transformer (LRT) is a transformer architecture designed for inference with adaptive depths using iterative changes, discard-based correction, and a learned stopping mechanism. Instead of relying on a single feedforward pass, the model updates a recurrent reasoning token across multiple internal steps, allowing it to correct early errors and allocate computation based on input difficulty. We evaluate the LRT on Sudoku as a controlled testbed for structured reasoning and show that it achieves strong performance, reaching 98.68% digit accuracy and 36.30% full-puzzle accuracy without using symbolic rules or search. Analyzing internal patterns shows that the discard and stop gates play different, important roles in stabilizing inferences and adjusting computational depth. We discuss how these mechanisms extend naturally to chess-scale reasoning tasks and outline extensions for multi-token reasoning and larger domains.", "AI": {"tldr": "LRT\u662f\u4e00\u79cd\u5177\u6709\u81ea\u9002\u5e94\u63a8\u7406\u6df1\u5ea6\u7684Transformer\u67b6\u6784\uff0c\u901a\u8fc7\u8fed\u4ee3\u66f4\u65b0\u3001\u4e22\u5f03\u6821\u6b63\u548c\u5b66\u4e60\u505c\u6b62\u673a\u5236\uff0c\u5728\u6570\u72ec\u4efb\u52a1\u4e0a\u8fbe\u523098.68%\u6570\u5b57\u51c6\u786e\u7387\u548c36.30%\u5b8c\u6574\u8c1c\u9898\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edfTransformer\u5728\u5355\u6b21\u524d\u5411\u4f20\u64ad\u4e2d\u5b8c\u6210\u63a8\u7406\uff0c\u65e0\u6cd5\u6839\u636e\u8f93\u5165\u96be\u5ea6\u81ea\u9002\u5e94\u8c03\u6574\u8ba1\u7b97\u6df1\u5ea6\uff0c\u4e5f\u96be\u4ee5\u7ea0\u6b63\u65e9\u671f\u9519\u8bef\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u8c03\u6574\u63a8\u7406\u6b65\u9aa4\u3001\u652f\u6301\u9519\u8bef\u6821\u6b63\u7684\u67b6\u6784\u3002", "method": "\u63d0\u51faLiquid Reasoning Transformer\uff0c\u4f7f\u7528\u5faa\u73af\u63a8\u7406token\u8fdb\u884c\u591a\u6b65\u5185\u90e8\u66f4\u65b0\uff0c\u5305\u542b\u4e22\u5f03\u95e8\u6821\u6b63\u65e9\u671f\u9519\u8bef\u3001\u5b66\u4e60\u505c\u6b62\u673a\u5236\u81ea\u9002\u5e94\u786e\u5b9a\u8ba1\u7b97\u6df1\u5ea6\uff0c\u65e0\u9700\u7b26\u53f7\u89c4\u5219\u6216\u641c\u7d22\u3002", "result": "\u5728\u6570\u72ec\u63a8\u7406\u4efb\u52a1\u4e0a\uff0cLRT\u8fbe\u523098.68%\u6570\u5b57\u51c6\u786e\u7387\u548c36.30%\u5b8c\u6574\u8c1c\u9898\u51c6\u786e\u7387\u3002\u5206\u6790\u663e\u793a\u4e22\u5f03\u95e8\u548c\u505c\u6b62\u95e8\u5728\u7a33\u5b9a\u63a8\u7406\u548c\u8c03\u6574\u8ba1\u7b97\u6df1\u5ea6\u4e2d\u53d1\u6325\u4e0d\u540c\u91cd\u8981\u4f5c\u7528\u3002", "conclusion": "LRT\u901a\u8fc7\u81ea\u9002\u5e94\u6df1\u5ea6\u63a8\u7406\u673a\u5236\u5728\u7ed3\u6784\u5316\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5176\u673a\u5236\u53ef\u6269\u5c55\u5230\u56fd\u9645\u8c61\u68cb\u7b49\u66f4\u5927\u89c4\u6a21\u63a8\u7406\u4efb\u52a1\uff0c\u672a\u6765\u53ef\u6269\u5c55\u591atoken\u63a8\u7406\u548c\u66f4\u5927\u9886\u57df\u5e94\u7528\u3002"}}
{"id": "2512.12678", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12678", "abs": "https://arxiv.org/abs/2512.12678", "authors": ["Fatimah Zohra", "Chen Zhao", "Hani Itani", "Bernard Ghanem"], "title": "$\u03b2$-CLIP: Text-Conditioned Contrastive Learning for Multi-Granular Vision-Language Alignment", "comment": null, "summary": "CLIP achieves strong zero-shot image-text retrieval by aligning global vision and text representations, yet it falls behind on fine-grained tasks even when fine-tuned on long, detailed captions. In this work, we propose $\u03b2$-CLIP, a multi-granular text-conditioned contrastive learning framework designed to achieve hierarchical alignment between multiple textual granularities-from full captions to sentences and phrases-and their corresponding visual regions. For each level of granularity, $\u03b2$-CLIP utilizes cross-attention to dynamically pool image patches, producing contextualized visual embeddings. To address the semantic overlap inherent in this hierarchy, we introduce the $\u03b2$-Contextualized Contrastive Alignment Loss ($\u03b2$-CAL). This objective parameterizes the trade-off between strict query-specific matching and relaxed intra-image contextualization, supporting both soft Cross-Entropy and hard Binary Cross-Entropy formulations. Through extensive experiments, we demonstrate that $\u03b2$-CLIP significantly improves dense alignment: achieving 91.8% T2I 92.3% I2T at R@1 on Urban1K and 30.9% on FG-OVD (Hard), setting state-of-the-art among methods trained without hard negatives. $\u03b2$-CLIP establishes a robust, adaptive baseline for dense vision-language correspondence. The code and models are released at https://github.com/fzohra/B-CLIP.", "AI": {"tldr": "\u03b2-CLIP\u901a\u8fc7\u591a\u7c92\u5ea6\u6587\u672c\u6761\u4ef6\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u5b9e\u73b0\u4ece\u5b8c\u6574\u63cf\u8ff0\u5230\u53e5\u5b50\u3001\u77ed\u8bed\u7684\u5c42\u6b21\u5316\u89c6\u89c9-\u6587\u672c\u5bf9\u9f50\uff0c\u663e\u8457\u63d0\u5347\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u6027\u80fd", "motivation": "CLIP\u867d\u7136\u5728\u96f6\u6837\u672c\u56fe\u50cf-\u6587\u672c\u68c0\u7d22\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7ec6\u7c92\u5ea6\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5373\u4f7f\u4f7f\u7528\u8be6\u7ec6\u63cf\u8ff0\u8fdb\u884c\u5fae\u8c03\u4e5f\u96be\u4ee5\u8fbe\u5230\u7406\u60f3\u6548\u679c\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5b9e\u73b0\u591a\u5c42\u6b21\u8bed\u4e49\u5bf9\u9f50\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8bed\u8a00\u5bf9\u5e94\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u03b2-CLIP\u591a\u7c92\u5ea6\u6587\u672c\u6761\u4ef6\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff1a1) \u4f7f\u7528\u8de8\u6ce8\u610f\u529b\u52a8\u6001\u6c60\u5316\u56fe\u50cf\u5757\uff0c\u4e3a\u6bcf\u4e2a\u7c92\u5ea6\u7ea7\u522b\u751f\u6210\u4e0a\u4e0b\u6587\u89c6\u89c9\u5d4c\u5165\uff1b2) \u5f15\u5165\u03b2-\u4e0a\u4e0b\u6587\u5bf9\u6bd4\u5bf9\u9f50\u635f\u5931(\u03b2-CAL)\uff0c\u53c2\u6570\u5316\u4e25\u683c\u67e5\u8be2\u5339\u914d\u4e0e\u5bbd\u677e\u56fe\u50cf\u5185\u4e0a\u4e0b\u6587\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u652f\u6301\u8f6f\u4ea4\u53c9\u71b5\u548c\u786c\u4e8c\u5143\u4ea4\u53c9\u71b5\u4e24\u79cd\u5f62\u5f0f\u3002", "result": "\u5728Urban1K\u6570\u636e\u96c6\u4e0a\u8fbe\u523091.8% T2I\u548c92.3% I2T\u7684R@1\u51c6\u786e\u7387\uff0c\u5728FG-OVD(Hard)\u4e0a\u8fbe\u523030.9%\uff0c\u5728\u6ca1\u6709\u4f7f\u7528\u786c\u8d1f\u6837\u672c\u8bad\u7ec3\u7684\u65b9\u6cd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "\u03b2-CLIP\u5efa\u7acb\u4e86\u9c81\u68d2\u3001\u81ea\u9002\u5e94\u7684\u5bc6\u96c6\u89c6\u89c9\u8bed\u8a00\u5bf9\u5e94\u57fa\u51c6\uff0c\u901a\u8fc7\u5c42\u6b21\u5316\u591a\u7c92\u5ea6\u5bf9\u9f50\u663e\u8457\u63d0\u5347\u4e86\u7ec6\u7c92\u5ea6\u89c6\u89c9\u8bed\u8a00\u4efb\u52a1\u6027\u80fd\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2512.12795", "categories": ["cs.LG", "stat.ME"], "pdf": "https://arxiv.org/pdf/2512.12795", "abs": "https://arxiv.org/abs/2512.12795", "authors": ["Mengying Yan", "Ziye Tian", "Siqi Li", "Nan Liu", "Benjamin A. Goldstein", "Molei Liu", "Chuan Hong"], "title": "TRACER: Transfer Learning based Real-time Adaptation for Clinical Evolving Risk", "comment": null, "summary": "Clinical decision support tools built on electronic health records often experience performance drift due to temporal population shifts, particularly when changes in the clinical environment initially affect only a subset of patients, resulting in a transition to mixed populations. Such case-mix changes commonly arise following system-level operational updates or the emergence of new diseases, such as COVID-19. We propose TRACER (Transfer Learning-based Real-time Adaptation for Clinical Evolving Risk), a framework that identifies encounter-level transition membership and adapts predictive models using transfer learning without full retraining. In simulation studies, TRACER outperformed static models trained on historical or contemporary data. In a real-world application predicting hospital admission following emergency department visits across the COVID-19 transition, TRACER improved both discrimination and calibration. TRACER provides a scalable approach for maintaining robust predictive performance under evolving and heterogeneous clinical conditions.", "AI": {"tldr": "TRACER\u6846\u67b6\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u5b9e\u65f6\u9002\u5e94\u4e34\u5e8a\u73af\u5883\u53d8\u5316\uff0c\u89e3\u51b3\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u9884\u6d4b\u6a21\u578b\u56e0\u65f6\u95f4\u6027\u4eba\u7fa4\u53d8\u5316\u5bfc\u81f4\u7684\u6027\u80fd\u6f02\u79fb\u95ee\u9898", "motivation": "\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u5de5\u5177\u57fa\u4e8e\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6784\u5efa\uff0c\u5e38\u56e0\u65f6\u95f4\u6027\u4eba\u7fa4\u53d8\u5316\u800c\u51fa\u73b0\u6027\u80fd\u6f02\u79fb\uff0c\u7279\u522b\u662f\u5728\u4e34\u5e8a\u73af\u5883\u53d8\u5316\u521d\u671f\u4ec5\u5f71\u54cd\u90e8\u5206\u60a3\u8005\uff0c\u5f62\u6210\u6df7\u5408\u4eba\u7fa4\u65f6\u3002\u8fd9\u79cd\u75c5\u4f8b\u7ec4\u5408\u53d8\u5316\u901a\u5e38\u53d1\u751f\u5728\u7cfb\u7edf\u7ea7\u64cd\u4f5c\u66f4\u65b0\u6216\u65b0\u75be\u75c5\uff08\u5982COVID-19\uff09\u51fa\u73b0\u540e", "method": "\u63d0\u51faTRACER\u6846\u67b6\uff0c\u901a\u8fc7\u8bc6\u522b\u5c31\u8bca\u7ea7\u522b\u7684\u8fc7\u6e21\u6210\u5458\u8eab\u4efd\uff0c\u4f7f\u7528\u8fc1\u79fb\u5b66\u4e60\u8c03\u6574\u9884\u6d4b\u6a21\u578b\u800c\u65e0\u9700\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3", "result": "\u5728\u6a21\u62df\u7814\u7a76\u4e2d\uff0cTRACER\u4f18\u4e8e\u57fa\u4e8e\u5386\u53f2\u6216\u5f53\u4ee3\u6570\u636e\u8bad\u7ec3\u7684\u9759\u6001\u6a21\u578b\u3002\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9884\u6d4bCOVID-19\u8fc7\u6e21\u671f\u95f4\u6025\u8bca\u5c31\u8bca\u540e\u7684\u4f4f\u9662\u60c5\u51b5\uff0cTRACER\u63d0\u9ad8\u4e86\u533a\u5206\u5ea6\u548c\u6821\u51c6\u5ea6", "conclusion": "TRACER\u4e3a\u5728\u4e0d\u65ad\u6f14\u53d8\u548c\u5f02\u8d28\u7684\u4e34\u5e8a\u6761\u4ef6\u4e0b\u4fdd\u6301\u7a33\u5065\u7684\u9884\u6d4b\u6027\u80fd\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u65b9\u6cd5"}}
{"id": "2512.12805", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12805", "abs": "https://arxiv.org/abs/2512.12805", "authors": ["Anastasiia Alokhina", "Pan Li"], "title": "From Small to Large: Generalization Bounds for Transformers on Variable-Size Inputs", "comment": null, "summary": "Transformers exhibit a notable property of \\emph{size generalization}, demonstrating an ability to extrapolate from smaller token sets to significantly longer ones. This behavior has been documented across diverse applications, including point clouds, graphs, and natural language. Despite its empirical success, this capability still lacks some rigorous theoretical characterizations. In this paper, we develop a theoretical framework to analyze this phenomenon for geometric data, which we represent as discrete samples from a continuous source (e.g., point clouds from manifolds, graphs from graphons). Our core contribution is a bound on the error between the Transformer's output for a discrete sample and its continuous-domain equivalent. We prove that for Transformers with stable positional encodings, this bound is determined by the sampling density and the intrinsic dimensionality of the data manifold. Experiments on graphs and point clouds of various sizes confirm the tightness of our theoretical bound.", "AI": {"tldr": "\u672c\u6587\u4e3aTransformer\u7684\u5927\u5c0f\u6cdb\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u8bc1\u660e\u5176\u8f93\u51fa\u8bef\u5dee\u53d7\u91c7\u6837\u5bc6\u5ea6\u548c\u6570\u636e\u5185\u5728\u7ef4\u5ea6\u5f71\u54cd", "motivation": "Transformer\u5728\u591a\u79cd\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u4ece\u8f83\u5c0ftoken\u96c6\u5408\u5916\u63a8\u5230\u8f83\u5927\u96c6\u5408\u7684\u5927\u5c0f\u6cdb\u5316\u80fd\u529b\uff0c\u867d\u7136\u7ecf\u9a8c\u4e0a\u6210\u529f\u4f46\u7f3a\u4e4f\u4e25\u683c\u7406\u8bba\u523b\u753b", "method": "\u5efa\u7acb\u7406\u8bba\u6846\u67b6\u5206\u6790\u51e0\u4f55\u6570\u636e\u7684\u5927\u5c0f\u6cdb\u5316\u73b0\u8c61\uff0c\u5c06\u51e0\u4f55\u6570\u636e\u8868\u793a\u4e3a\u8fde\u7eed\u6e90\u7684\u79bb\u6563\u91c7\u6837\uff08\u5982\u6d41\u5f62\u4e0a\u7684\u70b9\u4e91\u3001\u56fe\u8bba\u4e2d\u7684\u56fe\u5b50\u56fe\uff09\uff0c\u6838\u5fc3\u8d21\u732e\u662fTransformer\u79bb\u6563\u6837\u672c\u8f93\u51fa\u4e0e\u8fde\u7eed\u57df\u7b49\u4ef7\u8f93\u51fa\u4e4b\u95f4\u7684\u8bef\u5dee\u754c", "result": "\u8bc1\u660e\u5bf9\u4e8e\u5177\u6709\u7a33\u5b9a\u4f4d\u7f6e\u7f16\u7801\u7684Transformer\uff0c\u8bef\u5dee\u754c\u7531\u91c7\u6837\u5bc6\u5ea6\u548c\u6570\u636e\u6d41\u5f62\u7684\u5185\u5728\u7ef4\u5ea6\u51b3\u5b9a\uff0c\u4e0d\u540c\u5c3a\u5bf8\u7684\u56fe\u548c\u70b9\u4e91\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u7406\u8bba\u754c\u7684\u7d27\u81f4\u6027", "conclusion": "\u4e3aTransformer\u7684\u5927\u5c0f\u6cdb\u5316\u73b0\u8c61\u63d0\u4f9b\u4e86\u7406\u8bba\u89e3\u91ca\uff0c\u5efa\u7acb\u4e86\u79bb\u6563\u91c7\u6837\u4e0e\u8fde\u7eed\u8868\u793a\u4e4b\u95f4\u7684\u8bef\u5dee\u754c\u9650\uff0c\u4e3a\u7406\u89e3Transformer\u5728\u51e0\u4f55\u6570\u636e\u4e0a\u7684\u5916\u63a8\u80fd\u529b\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840"}}
{"id": "2512.12816", "categories": ["cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.12816", "abs": "https://arxiv.org/abs/2512.12816", "authors": ["Hasan Burhan Beytur", "Gustavo de Veciana", "Haris Vikalo", "Kevin S Chan"], "title": "Optimal Resource Allocation for ML Model Training and Deployment under Concept Drift", "comment": null, "summary": "We study how to allocate resources for training and deployment of machine learning (ML) models under concept drift and limited budgets. We consider a setting in which a model provider distributes trained models to multiple clients whose devices support local inference but lack the ability to retrain those models, placing the burden of performance maintenance on the provider. We introduce a model-agnostic framework that captures the interaction between resource allocation, concept drift dynamics, and deployment timing. We show that optimal training policies depend critically on the aging properties of concept durations. Under sudden concept changes, we derive optimal training policies subject to budget constraints when concept durations follow distributions with Decreasing Mean Residual Life (DMRL), and show that intuitive heuristics are provably suboptimal under Increasing Mean Residual Life (IMRL). We further study model deployment under communication constraints, prove that the associated optimization problem is quasi-convex under mild conditions, and propose a randomized scheduling strategy that achieves near-optimal client-side performance. These results offer theoretical and algorithmic foundations for cost-efficient ML model management under concept drift, with implications for continual learning, distributed inference, and adaptive ML systems.", "AI": {"tldr": "\u7814\u7a76\u5728\u6982\u5ff5\u6f02\u79fb\u548c\u6709\u9650\u9884\u7b97\u4e0b\u5982\u4f55\u5206\u914d\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u90e8\u7f72\u8d44\u6e90\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u5206\u6790\u4e86\u8d44\u6e90\u5206\u914d\u3001\u6982\u5ff5\u6f02\u79fb\u52a8\u6001\u548c\u90e8\u7f72\u65f6\u673a\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "motivation": "\u5f53\u6a21\u578b\u63d0\u4f9b\u5546\u5c06\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u5206\u53d1\u7ed9\u591a\u4e2a\u5ba2\u6237\u7aef\u65f6\uff0c\u8fd9\u4e9b\u5ba2\u6237\u7aef\u8bbe\u5907\u652f\u6301\u672c\u5730\u63a8\u7406\u4f46\u7f3a\u4e4f\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u7684\u80fd\u529b\uff0c\u56e0\u6b64\u6027\u80fd\u7ef4\u62a4\u7684\u8d1f\u62c5\u843d\u5728\u63d0\u4f9b\u5546\u8eab\u4e0a\u3002\u9700\u8981\u89e3\u51b3\u5728\u6982\u5ff5\u6f02\u79fb\u548c\u9884\u7b97\u9650\u5236\u4e0b\u7684\u8d44\u6e90\u5206\u914d\u95ee\u9898\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u6846\u67b6\uff0c\u6355\u6349\u8d44\u6e90\u5206\u914d\u3001\u6982\u5ff5\u6f02\u79fb\u52a8\u6001\u548c\u90e8\u7f72\u65f6\u673a\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002\u5206\u6790\u4e86\u6982\u5ff5\u6301\u7eed\u65f6\u95f4\u5206\u5e03\u5bf9\u8bad\u7ec3\u7b56\u7565\u7684\u5f71\u54cd\uff0c\u7279\u522b\u5173\u6ce8\u4e86\u9012\u51cf\u5e73\u5747\u5269\u4f59\u5bff\u547d\uff08DMRL\uff09\u548c\u9012\u589e\u5e73\u5747\u5269\u4f59\u5bff\u547d\uff08IMRL\uff09\u5206\u5e03\u3002\u5728\u901a\u4fe1\u7ea6\u675f\u4e0b\u7814\u7a76\u4e86\u6a21\u578b\u90e8\u7f72\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u76f8\u5173\u4f18\u5316\u95ee\u9898\u5728\u6e29\u548c\u6761\u4ef6\u4e0b\u7684\u62df\u51f8\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u968f\u673a\u8c03\u5ea6\u7b56\u7565\u3002", "result": "\u5728\u7a81\u7136\u6982\u5ff5\u53d8\u5316\u4e0b\uff0c\u5f53\u6982\u5ff5\u6301\u7eed\u65f6\u95f4\u9075\u5faaDMRL\u5206\u5e03\u65f6\uff0c\u63a8\u5bfc\u51fa\u4e86\u9884\u7b97\u7ea6\u675f\u4e0b\u7684\u6700\u4f18\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u8bc1\u660e\u76f4\u89c2\u542f\u53d1\u5f0f\u65b9\u6cd5\u5728IMRL\u4e0b\u662f\u6b21\u4f18\u7684\u3002\u5bf9\u4e8e\u901a\u4fe1\u7ea6\u675f\u4e0b\u7684\u6a21\u578b\u90e8\u7f72\uff0c\u63d0\u51fa\u7684\u968f\u673a\u8c03\u5ea6\u7b56\u7565\u80fd\u591f\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u7684\u5ba2\u6237\u7aef\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6982\u5ff5\u6f02\u79fb\u4e0b\u7684\u6210\u672c\u9ad8\u6548\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7ba1\u7406\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u7b97\u6cd5\u57fa\u7840\uff0c\u5bf9\u6301\u7eed\u5b66\u4e60\u3001\u5206\u5e03\u5f0f\u63a8\u7406\u548c\u81ea\u9002\u5e94\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2512.12751", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12751", "abs": "https://arxiv.org/abs/2512.12751", "authors": ["Zhenya Yang", "Zhe Liu", "Yuxiang Lu", "Liping Hou", "Chenxuan Miao", "Siyi Peng", "Bailan Feng", "Xiang Bai", "Hengshuang Zhao"], "title": "GenieDrive: Towards Physics-Aware Driving World Model with 4D Occupancy Guided Video Generation", "comment": "The project page is available at https://huster-yzy.github.io/geniedrive_project_page/", "summary": "Physics-aware driving world model is essential for drive planning, out-of-distribution data synthesis, and closed-loop evaluation. However, existing methods often rely on a single diffusion model to directly map driving actions to videos, which makes learning difficult and leads to physically inconsistent outputs. To overcome these challenges, we propose GenieDrive, a novel framework designed for physics-aware driving video generation. Our approach starts by generating 4D occupancy, which serves as a physics-informed foundation for subsequent video generation. 4D occupancy contains rich physical information, including high-resolution 3D structures and dynamics. To facilitate effective compression of such high-resolution occupancy, we propose a VAE that encodes occupancy into a latent tri-plane representation, reducing the latent size to only 58% of that used in previous methods. We further introduce Mutual Control Attention (MCA) to accurately model the influence of control on occupancy evolution, and we jointly train the VAE and the subsequent prediction module in an end-to-end manner to maximize forecasting accuracy. Together, these designs yield a 7.2% improvement in forecasting mIoU at an inference speed of 41 FPS, while using only 3.47 M parameters. Additionally, a Normalized Multi-View Attention is introduced in the video generation model to generate multi-view driving videos with guidance from our 4D occupancy, significantly improving video quality with a 20.7% reduction in FVD. Experiments demonstrate that GenieDrive enables highly controllable, multi-view consistent, and physics-aware driving video generation.", "AI": {"tldr": "GenieDrive\u662f\u4e00\u4e2a\u7528\u4e8e\u7269\u7406\u611f\u77e5\u9a7e\u9a76\u89c6\u9891\u751f\u6210\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u62104D\u5360\u636e\u4f5c\u4e3a\u7269\u7406\u57fa\u7840\uff0c\u4f7f\u7528VAE\u538b\u7f29\u4e3a\u4e09\u5e73\u9762\u8868\u793a\uff0c\u5e76\u5f15\u5165\u4e92\u63a7\u6ce8\u610f\u529b\u6765\u5efa\u6a21\u63a7\u5236\u5bf9\u5360\u636e\u6f14\u5316\u7684\u5f71\u54cd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u7cbe\u5ea6\u548c\u89c6\u9891\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5355\u4e00\u6269\u6563\u6a21\u578b\u76f4\u63a5\u5c06\u9a7e\u9a76\u52a8\u4f5c\u6620\u5c04\u5230\u89c6\u9891\uff0c\u8fd9\u4f7f\u5f97\u5b66\u4e60\u56f0\u96be\u4e14\u4ea7\u751f\u7269\u7406\u4e0d\u4e00\u81f4\u7684\u8f93\u51fa\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u751f\u6210\u7269\u7406\u611f\u77e5\u9a7e\u9a76\u89c6\u9891\u7684\u6846\u67b6\uff0c\u4ee5\u652f\u6301\u9a7e\u9a76\u89c4\u5212\u3001\u5206\u5e03\u5916\u6570\u636e\u5408\u6210\u548c\u95ed\u73af\u8bc4\u4f30\u3002", "method": "1) \u9996\u5148\u751f\u6210\u5305\u542b\u4e30\u5bcc\u7269\u7406\u4fe1\u606f\u76844D\u5360\u636e\u4f5c\u4e3a\u89c6\u9891\u751f\u6210\u7684\u7269\u7406\u57fa\u7840\uff1b2) \u63d0\u51faVAE\u5c06\u9ad8\u5206\u8fa8\u7387\u5360\u636e\u7f16\u7801\u4e3a\u6f5c\u5728\u4e09\u5e73\u9762\u8868\u793a\uff0c\u5c06\u6f5c\u5728\u5c3a\u5bf8\u51cf\u5c11\u5230\u5148\u524d\u65b9\u6cd5\u768458%\uff1b3) \u5f15\u5165\u4e92\u63a7\u6ce8\u610f\u529b(MCA)\u51c6\u786e\u5efa\u6a21\u63a7\u5236\u5bf9\u5360\u636e\u6f14\u5316\u7684\u5f71\u54cd\uff1b4) \u7aef\u5230\u7aef\u8054\u5408\u8bad\u7ec3VAE\u548c\u9884\u6d4b\u6a21\u5757\uff1b5) \u5728\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e2d\u5f15\u5165\u5f52\u4e00\u5316\u591a\u89c6\u56fe\u6ce8\u610f\u529b\uff0c\u57fa\u4e8e4D\u5360\u636e\u5f15\u5bfc\u751f\u6210\u591a\u89c6\u56fe\u9a7e\u9a76\u89c6\u9891\u3002", "result": "1) \u9884\u6d4bmIoU\u63d0\u53477.2%\uff0c\u63a8\u7406\u901f\u5ea6\u8fbe41 FPS\uff0c\u4ec5\u4f7f\u75283.47M\u53c2\u6570\uff1b2) \u89c6\u9891\u8d28\u91cf\u663e\u8457\u6539\u5584\uff0cFVD\u51cf\u5c1120.7%\uff1b3) \u5b9e\u73b0\u4e86\u9ad8\u5ea6\u53ef\u63a7\u3001\u591a\u89c6\u56fe\u4e00\u81f4\u4e14\u7269\u7406\u611f\u77e5\u7684\u9a7e\u9a76\u89c6\u9891\u751f\u6210\u3002", "conclusion": "GenieDrive\u901a\u8fc74D\u5360\u636e\u751f\u6210\u3001\u9ad8\u6548\u538b\u7f29\u548c\u4e92\u63a7\u6ce8\u610f\u529b\u7b49\u521b\u65b0\u8bbe\u8ba1\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u7269\u7406\u4e00\u81f4\u6027\u548c\u5b66\u4e60\u96be\u5ea6\u65b9\u9762\u7684\u6311\u6218\uff0c\u4e3a\u7269\u7406\u611f\u77e5\u9a7e\u9a76\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12756", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12756", "abs": "https://arxiv.org/abs/2512.12756", "authors": ["Yue Jiang", "Dingkang Yang", "Minghao Han", "Jinghang Han", "Zizhi Chen", "Yizhou Liu", "Mingcheng Li", "Peng Zhai", "Lihua Zhang"], "title": "FysicsWorld: A Unified Full-Modality Benchmark for Any-to-Any Understanding, Generation, and Reasoning", "comment": "The omni-modal benchmark report from Fysics AI", "summary": "Despite rapid progress in multimodal large language models (MLLMs) and emerging omni-modal architectures, current benchmarks remain limited in scope and integration, suffering from incomplete modality coverage, restricted interaction to text-centric outputs, and weak interdependence and complementarity among modalities. To bridge these gaps, we introduce FysicsWorld, the first unified full-modality benchmark that supports bidirectional input-output across image, video, audio, and text, enabling comprehensive any-to-any evaluation across understanding, generation, and reasoning. FysicsWorld encompasses 16 primary tasks and 3,268 curated samples, aggregated from over 40 high-quality sources and covering a rich set of open-domain categories with diverse question types. We also propose the Cross-Modal Complementarity Screening (CMCS) strategy integrated in a systematic data construction framework that produces omni-modal data for spoken interaction and fusion-dependent cross-modal reasoning. Through a comprehensive evaluation of over 30 state-of-the-art baselines, spanning MLLMs, modality-specific models, unified understanding-generation models, and omni-modal language models, FysicsWorld exposes the performance disparities and limitations across models in understanding, generation, and reasoning. Our benchmark establishes a unified foundation and strong baselines for evaluating and advancing next-generation full-modality architectures.", "AI": {"tldr": "FysicsWorld\u662f\u9996\u4e2a\u7edf\u4e00\u7684\u5168\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u652f\u6301\u56fe\u50cf\u3001\u89c6\u9891\u3001\u97f3\u9891\u548c\u6587\u672c\u4e4b\u95f4\u7684\u53cc\u5411\u8f93\u5165\u8f93\u51fa\uff0c\u5305\u542b16\u4e2a\u4e3b\u8981\u4efb\u52a1\u548c3268\u4e2a\u6837\u672c\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u6a21\u578b\u7684\u7406\u89e3\u3001\u751f\u6210\u548c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u6a21\u6001\u8986\u76d6\u4e0d\u5168\u3001\u4ec5\u9650\u4e8e\u6587\u672c\u8f93\u51fa\u3001\u6a21\u6001\u95f4\u76f8\u4e92\u4f9d\u8d56\u6027\u548c\u4e92\u8865\u6027\u5f31\u7b49\u95ee\u9898\u3002\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u5168\u9762\u7684\u5168\u6a21\u6001\u8bc4\u4f30\u6846\u67b6\u6765\u63a8\u52a8\u4e0b\u4e00\u4ee3\u591a\u6a21\u6001\u67b6\u6784\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u4e86FysicsWorld\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b16\u4e2a\u4e3b\u8981\u4efb\u52a1\u548c3268\u4e2a\u7cbe\u5fc3\u7b5b\u9009\u7684\u6837\u672c\uff0c\u8986\u76d6\u56fe\u50cf\u3001\u89c6\u9891\u3001\u97f3\u9891\u548c\u6587\u672c\u56db\u79cd\u6a21\u6001\u3002\u91c7\u7528\u8de8\u6a21\u6001\u4e92\u8865\u6027\u7b5b\u9009\uff08CMCS\uff09\u7b56\u7565\u548c\u7cfb\u7edf\u5316\u6570\u636e\u6784\u5efa\u6846\u67b6\uff0c\u751f\u6210\u7528\u4e8e\u53e3\u8bed\u4ea4\u4e92\u548c\u878d\u5408\u4f9d\u8d56\u8de8\u6a21\u6001\u63a8\u7406\u7684\u5168\u6a21\u6001\u6570\u636e\u3002", "result": "\u5bf930\u591a\u4e2a\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u6a21\u578b\uff08\u5305\u62ecMLLMs\u3001\u6a21\u6001\u7279\u5b9a\u6a21\u578b\u3001\u7edf\u4e00\u7406\u89e3-\u751f\u6210\u6a21\u578b\u548c\u5168\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff09\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u8fd9\u4e9b\u6a21\u578b\u5728\u7406\u89e3\u3001\u751f\u6210\u548c\u63a8\u7406\u65b9\u9762\u7684\u6027\u80fd\u5dee\u5f02\u548c\u5c40\u9650\u6027\u3002", "conclusion": "FysicsWorld\u4e3a\u8bc4\u4f30\u548c\u63a8\u8fdb\u4e0b\u4e00\u4ee3\u5168\u6a21\u6001\u67b6\u6784\u5efa\u7acb\u4e86\u7edf\u4e00\u7684\u57fa\u7840\u548c\u5f3a\u5927\u7684\u57fa\u7ebf\uff0c\u586b\u8865\u4e86\u5f53\u524d\u57fa\u51c6\u6d4b\u8bd5\u7684\u7a7a\u767d\uff0c\u652f\u6301\u4efb\u610f\u6a21\u6001\u95f4\u7684\u53cc\u5411\u8bc4\u4f30\u3002"}}
{"id": "2512.12768", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12768", "abs": "https://arxiv.org/abs/2512.12768", "authors": ["Tianjiao Yu", "Xinzhuo Li", "Yifan Shen", "Yuanzhe Liu", "Ismini Lourentzou"], "title": "CoRe3D: Collaborative Reasoning as a Foundation for 3D Intelligence", "comment": null, "summary": "Recent advances in large multimodal models suggest that explicit reasoning mechanisms play a critical role in improving model reliability, interpretability, and cross-modal alignment. While such reasoning-centric approaches have been proven effective in language and vision tasks, their extension to 3D remains underdeveloped. CoRe3D introduces a unified 3D understanding and generation reasoning framework that jointly operates over semantic and spatial abstractions, enabling high-level intent inferred from language to directly guide low-level 3D content formation. Central to this design is a spatially grounded reasoning representation that decomposes 3D latent space into localized regions, allowing the model to reason over geometry in a compositional and procedural manner. By tightly coupling semantic chain-of-thought inference with structured spatial reasoning, CoRe3D produces 3D outputs that exhibit strong local consistency and faithful alignment with linguistic descriptions.", "AI": {"tldr": "CoRe3D\u63d0\u51fa\u7edf\u4e003D\u7406\u89e3\u4e0e\u751f\u6210\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u548c\u7a7a\u95f4\u62bd\u8c61\u8054\u5408\u64cd\u4f5c\uff0c\u8ba9\u8bed\u8a00\u63a8\u65ad\u7684\u9ad8\u5c42\u610f\u56fe\u76f4\u63a5\u6307\u5bfc\u4f4e\u7ea73D\u5185\u5bb9\u5f62\u6210", "motivation": "\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u7684\u663e\u5f0f\u63a8\u7406\u673a\u5236\u5df2\u88ab\u8bc1\u660e\u80fd\u63d0\u5347\u53ef\u9760\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u4f46\u57283D\u9886\u57df\u5e94\u7528\u4e0d\u8db3\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u8bed\u8a00\u548c\u89c6\u89c9\u4efb\u52a1\u4e2d\u6709\u6548\uff0c\u4f46\u6269\u5c55\u52303D\u4ecd\u4e0d\u6210\u719f", "method": "\u5f15\u5165\u7a7a\u95f4\u63a5\u5730\u63a8\u7406\u8868\u793a\uff0c\u5c063D\u6f5c\u5728\u7a7a\u95f4\u5206\u89e3\u4e3a\u5c40\u90e8\u5316\u533a\u57df\uff0c\u4f7f\u6a21\u578b\u80fd\u4ee5\u7ec4\u5408\u548c\u7a0b\u5e8f\u5316\u65b9\u5f0f\u63a8\u7406\u51e0\u4f55\u3002\u901a\u8fc7\u7d27\u5bc6\u8026\u5408\u8bed\u4e49\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u4e0e\u7ed3\u6784\u5316\u7a7a\u95f4\u63a8\u7406", "result": "CoRe3D\u751f\u6210\u76843D\u8f93\u51fa\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u5c40\u90e8\u4e00\u81f4\u6027\u548c\u4e0e\u8bed\u8a00\u63cf\u8ff0\u7684\u5fe0\u5b9e\u5bf9\u9f50", "conclusion": "\u8be5\u6846\u67b6\u4e3a3D\u7406\u89e3\u548c\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u63a8\u7406\u673a\u5236\uff0c\u901a\u8fc7\u8054\u5408\u8bed\u4e49\u548c\u7a7a\u95f4\u62bd\u8c61\u5b9e\u73b0\u4e86\u9ad8\u5c42\u610f\u56fe\u5230\u4f4e\u7ea73D\u5185\u5bb9\u7684\u76f4\u63a5\u6307\u5bfc"}}
{"id": "2512.12844", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12844", "abs": "https://arxiv.org/abs/2512.12844", "authors": ["Yunpeng Xu", "Wenge Guo", "Zhi Wei"], "title": "Selective Conformal Risk Control", "comment": null, "summary": "Reliable uncertainty quantification is essential for deploying machine learning systems in high-stakes domains. Conformal prediction provides distribution-free coverage guarantees but often produces overly large prediction sets, limiting its practical utility. To address this issue, we propose \\textit{Selective Conformal Risk Control} (SCRC), a unified framework that integrates conformal prediction with selective classification. The framework formulates uncertainty control as a two-stage problem: the first stage selects confident samples for prediction, and the second stage applies conformal risk control on the selected subset to construct calibrated prediction sets. We develop two algorithms under this framework. The first, SCRC-T, preserves exchangeability by computing thresholds jointly over calibration and test samples, offering exact finite-sample guarantees. The second, SCRC-I, is a calibration-only variant that provides PAC-style probabilistic guarantees while being more computational efficient. Experiments on two public datasets show that both methods achieve the target coverage and risk levels, with nearly identical performance, while SCRC-I exhibits slightly more conservative risk control but superior computational practicality. Our results demonstrate that selective conformal risk control offers an effective and efficient path toward compact, reliable uncertainty quantification.", "AI": {"tldr": "\u63d0\u51fa\u9009\u62e9\u6027\u5171\u5f62\u98ce\u9669\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\u7ed3\u5408\u9009\u62e9\u6027\u5206\u7c7b\u548c\u5171\u5f62\u9884\u6d4b\uff0c\u5728\u4fdd\u8bc1\u8986\u76d6\u4fdd\u8bc1\u7684\u540c\u65f6\u51cf\u5c11\u9884\u6d4b\u96c6\u5927\u5c0f\uff0c\u63d0\u9ad8\u5b9e\u9645\u6548\u7528\u3002", "motivation": "\u5171\u5f62\u9884\u6d4b\u867d\u7136\u63d0\u4f9b\u5206\u5e03\u65e0\u5173\u7684\u8986\u76d6\u4fdd\u8bc1\uff0c\u4f46\u901a\u5e38\u4ea7\u751f\u8fc7\u5927\u7684\u9884\u6d4b\u96c6\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5728\u4fdd\u6301\u53ef\u9760\u6027\u7684\u540c\u65f6\u51cf\u5c11\u9884\u6d4b\u96c6\u5927\u5c0f\u3002", "method": "\u63d0\u51fa\u9009\u62e9\u6027\u5171\u5f62\u98ce\u9669\u63a7\u5236\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u9009\u62e9\u7f6e\u4fe1\u6837\u672c\u8fdb\u884c\u9884\u6d4b\uff0c\u7b2c\u4e8c\u9636\u6bb5\u5728\u9009\u5b9a\u5b50\u96c6\u4e0a\u5e94\u7528\u5171\u5f62\u98ce\u9669\u63a7\u5236\u6784\u5efa\u6821\u51c6\u9884\u6d4b\u96c6\u3002\u5f00\u53d1\u4e86\u4e24\u79cd\u7b97\u6cd5\uff1aSCRC-T\uff08\u4fdd\u6301\u53ef\u4ea4\u6362\u6027\uff0c\u63d0\u4f9b\u7cbe\u786e\u6709\u9650\u6837\u672c\u4fdd\u8bc1\uff09\u548cSCRC-I\uff08\u4ec5\u6821\u51c6\u53d8\u4f53\uff0c\u63d0\u4f9bPAC\u5f0f\u6982\u7387\u4fdd\u8bc1\uff0c\u8ba1\u7b97\u66f4\u9ad8\u6548\uff09\u3002", "result": "\u5728\u4e24\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u4e24\u79cd\u65b9\u6cd5\u90fd\u8fbe\u5230\u4e86\u76ee\u6807\u8986\u76d6\u548c\u98ce\u9669\u6c34\u5e73\uff0c\u6027\u80fd\u51e0\u4e4e\u76f8\u540c\u3002SCRC-I\u8868\u73b0\u51fa\u7a0d\u5fae\u66f4\u4fdd\u5b88\u7684\u98ce\u9669\u63a7\u5236\uff0c\u4f46\u8ba1\u7b97\u5b9e\u7528\u6027\u66f4\u4f18\u3002", "conclusion": "\u9009\u62e9\u6027\u5171\u5f62\u98ce\u9669\u63a7\u5236\u4e3a\u7d27\u51d1\u3001\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u9ad8\u6548\u7684\u9014\u5f84\u3002"}}
{"id": "2512.12799", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12799", "abs": "https://arxiv.org/abs/2512.12799", "authors": ["Zhe Liu", "Runhui Huang", "Rui Yang", "Siming Yan", "Zining Wang", "Lu Hou", "Di Lin", "Xiang Bai", "Hengshuang Zhao"], "title": "DrivePI: Spatial-aware 4D MLLM for Unified Autonomous Driving Understanding, Perception, Prediction and Planning", "comment": null, "summary": "Although multi-modal large language models (MLLMs) have shown strong capabilities across diverse domains, their application in generating fine-grained 3D perception and prediction outputs in autonomous driving remains underexplored. In this paper, we propose DrivePI, a novel spatial-aware 4D MLLM that serves as a unified Vision-Language-Action (VLA) framework that is also compatible with vision-action (VA) models. Our method jointly performs spatial understanding, 3D perception (i.e., 3D occupancy), prediction (i.e., occupancy flow), and planning (i.e., action outputs) in parallel through end-to-end optimization. To obtain both precise geometric information and rich visual appearance, our approach integrates point clouds, multi-view images, and language instructions within a unified MLLM architecture. We further develop a data engine to generate text-occupancy and text-flow QA pairs for 4D spatial understanding. Remarkably, with only a 0.5B Qwen2.5 model as MLLM backbone, DrivePI as a single unified model matches or exceeds both existing VLA models and specialized VA models. Specifically, compared to VLA models, DrivePI outperforms OpenDriveVLA-7B by 2.5% mean accuracy on nuScenes-QA and reduces collision rate by 70% over ORION (from 0.37% to 0.11%) on nuScenes. Against specialized VA models, DrivePI surpasses FB-OCC by 10.3 RayIoU for 3D occupancy on OpenOcc, reduces the mAVE from 0.591 to 0.509 for occupancy flow on OpenOcc, and achieves 32% lower L2 error than VAD (from 0.72m to 0.49m) for planning on nuScenes. Code will be available at https://github.com/happinesslz/DrivePI", "AI": {"tldr": "DrivePI\u662f\u4e00\u4e2a\u7a7a\u95f4\u611f\u77e5\u76844D\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4f5c\u4e3a\u7edf\u4e00\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6846\u67b6\uff0c\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5b9e\u73b03D\u611f\u77e5\u3001\u9884\u6d4b\u548c\u89c4\u5212\u7684\u5e76\u884c\u5904\u7406\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u591a\u4e2a\u9886\u57df\u8868\u73b0\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u751f\u6210\u7ec6\u7c92\u5ea63D\u611f\u77e5\u548c\u9884\u6d4b\u8f93\u51fa\u7684\u5e94\u7528\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u7a7a\u95f4\u7406\u89e3\u30013D\u611f\u77e5\u3001\u9884\u6d4b\u548c\u89c4\u5212\u7684\u7edf\u4e00\u5904\u7406\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51faDrivePI\u6846\u67b6\uff0c\u96c6\u6210\u70b9\u4e91\u3001\u591a\u89c6\u89d2\u56fe\u50cf\u548c\u8bed\u8a00\u6307\u4ee4\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u4f18\u5316\u5e76\u884c\u5904\u7406\u7a7a\u95f4\u7406\u89e3\u30013D\u5360\u7528\u3001\u5360\u7528\u6d41\u548c\u89c4\u5212\u3002\u5f00\u53d1\u6570\u636e\u5f15\u64ce\u751f\u6210\u6587\u672c-\u5360\u7528\u548c\u6587\u672c-\u6d41\u95ee\u7b54\u5bf9\u7528\u4e8e4D\u7a7a\u95f4\u7406\u89e3\u3002", "result": "\u4ec5\u4f7f\u75280.5B\u53c2\u6570\u7684Qwen2.5\u4f5c\u4e3a\u9aa8\u5e72\u6a21\u578b\uff0cDrivePI\u5728\u591a\u9879\u4efb\u52a1\u4e0a\u8d85\u8d8a\u73b0\u6709VLA\u6a21\u578b\u548c\u4e13\u7528VA\u6a21\u578b\uff1a\u5728nuScenes-QA\u4e0a\u6bd4OpenDriveVLA-7B\u63d0\u53472.5%\u5e73\u5747\u51c6\u786e\u7387\uff1b\u78b0\u649e\u7387\u6bd4ORION\u964d\u4f4e70%\uff1b\u57283D\u5360\u7528\u3001\u5360\u7528\u6d41\u548c\u89c4\u5212\u4efb\u52a1\u4e0a\u5747\u663e\u8457\u4f18\u4e8e\u4e13\u7528\u6a21\u578b\u3002", "conclusion": "DrivePI\u8bc1\u660e\u4e86\u7edf\u4e00\u76844D MLLM\u6846\u67b6\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u540c\u65f6\u5904\u7406\u591a\u79cd\u4efb\u52a1\u5e76\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e13\u95e8\u8bbe\u8ba1\u7684\u6a21\u578b\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u548c\u51b3\u7b56\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12870", "categories": ["cs.LG", "cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2512.12870", "abs": "https://arxiv.org/abs/2512.12870", "authors": ["Pouya Ahadi", "Blair Winograd", "Camille Zaug", "Karunesh Arora", "Lijun Wang", "Kamran Paynabar"], "title": "Optimal Labeler Assignment and Sampling for Active Learning in the Presence of Imperfect Labels", "comment": "22 pages, 6 figures. Preprint under review", "summary": "Active Learning (AL) has garnered significant interest across various application domains where labeling training data is costly. AL provides a framework that helps practitioners query informative samples for annotation by oracles (labelers). However, these labels often contain noise due to varying levels of labeler accuracy. Additionally, uncertain samples are more prone to receiving incorrect labels because of their complexity. Learning from imperfectly labeled data leads to an inaccurate classifier. We propose a novel AL framework to construct a robust classification model by minimizing noise levels. Our approach includes an assignment model that optimally assigns query points to labelers, aiming to minimize the maximum possible noise within each cycle. Additionally, we introduce a new sampling method to identify the best query points, reducing the impact of label noise on classifier performance. Our experiments demonstrate that our approach significantly improves classification performance compared to several benchmark methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u5206\u914d\u67e5\u8be2\u70b9\u7ed9\u6807\u6ce8\u8005\u6765\u6700\u5c0f\u5316\u6807\u7b7e\u566a\u58f0\uff0c\u4ece\u800c\u63d0\u9ad8\u5206\u7c7b\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u4e3b\u52a8\u5b66\u4e60\u4e2d\u6807\u6ce8\u8005\u63d0\u4f9b\u7684\u6807\u7b7e\u5f80\u5f80\u5b58\u5728\u566a\u58f0\uff0c\u7279\u522b\u662f\u590d\u6742\u6837\u672c\u66f4\u5bb9\u6613\u88ab\u9519\u8bef\u6807\u6ce8\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u5206\u7c7b\u5668\u6027\u80fd\u4e0b\u964d\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u6700\u5c0f\u5316\u6807\u7b7e\u566a\u58f0\u5bf9\u6a21\u578b\u8bad\u7ec3\u7684\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\u7684\u4e3b\u52a8\u5b66\u4e60\u6846\u67b6\uff1a1\uff09\u5206\u914d\u6a21\u578b\uff0c\u4f18\u5316\u5730\u5c06\u67e5\u8be2\u70b9\u5206\u914d\u7ed9\u6807\u6ce8\u8005\u4ee5\u6700\u5c0f\u5316\u6bcf\u4e2a\u5468\u671f\u5185\u7684\u6700\u5927\u53ef\u80fd\u566a\u58f0\uff1b2\uff09\u65b0\u7684\u91c7\u6837\u65b9\u6cd5\uff0c\u9009\u62e9\u6700\u4f73\u67e5\u8be2\u70b9\u4ee5\u51cf\u5c11\u6807\u7b7e\u566a\u58f0\u5bf9\u5206\u7c7b\u5668\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u591a\u4e2a\u57fa\u51c6\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u4f18\u5316\u6807\u6ce8\u5206\u914d\u548c\u67e5\u8be2\u70b9\u9009\u62e9\uff0c\u53ef\u4ee5\u6709\u6548\u51cf\u5c11\u4e3b\u52a8\u5b66\u4e60\u4e2d\u7684\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0c\u6784\u5efa\u66f4\u9c81\u68d2\u7684\u5206\u7c7b\u6a21\u578b\u3002"}}
{"id": "2512.12822", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12822", "abs": "https://arxiv.org/abs/2512.12822", "authors": ["Yongyuan Liang", "Xiyao Wang", "Yuanchen Ju", "Jianwei Yang", "Furong Huang"], "title": "Lemon: A Unified and Scalable 3D Multimodal Model for Universal Spatial Understanding", "comment": null, "summary": "Scaling large multimodal models (LMMs) to 3D understanding poses unique challenges: point cloud data is sparse and irregular, existing models rely on fragmented architectures with modality-specific encoders, and training pipelines often suffer from instability and poor scalability. We introduce Lemon, a unified transformer architecture that addresses these challenges by jointly processing 3D point cloud patches and language tokens as a single sequence. Unlike prior work that relies on modality-specific encoders and cross-modal alignment modules, this design enables early spatial-linguistic fusion, eliminates redundant encoders, improves parameter efficiency, and supports more effective model scaling. To handle the complexity of 3D data, we develop a structured patchification and tokenization scheme that preserves spatial context, and a three-stage training curriculum that progressively builds capabilities from object-level recognition to scene-level spatial reasoning. Lemon establishes new state-of-the-art performance across comprehensive 3D understanding and reasoning tasks, from object recognition and captioning to spatial reasoning in 3D scenes, while demonstrating robust scaling properties as model size and training data increase. Our work provides a unified foundation for advancing 3D spatial intelligence in real-world applications.", "AI": {"tldr": "Lemon\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u591a\u6a21\u6001Transformer\u67b6\u6784\uff0c\u901a\u8fc7\u5c063D\u70b9\u4e91\u8865\u4e01\u548c\u8bed\u8a00\u6807\u8bb0\u4f5c\u4e3a\u5355\u4e00\u5e8f\u5217\u8054\u5408\u5904\u7406\uff0c\u89e3\u51b3\u4e86\u73b0\u67093D\u7406\u89e3\u6a21\u578b\u7684\u788e\u7247\u5316\u67b6\u6784\u3001\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u53ef\u6269\u5c55\u6027\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\u5728\u6269\u5c55\u52303D\u7406\u89e3\u65f6\u9762\u4e34\u72ec\u7279\u6311\u6218\uff1a\u70b9\u4e91\u6570\u636e\u7a00\u758f\u4e0d\u89c4\u5219\uff0c\u73b0\u6709\u6a21\u578b\u4f9d\u8d56\u788e\u7247\u5316\u67b6\u6784\uff08\u6a21\u6001\u7279\u5b9a\u7f16\u7801\u5668\uff09\uff0c\u8bad\u7ec3\u6d41\u7a0b\u4e0d\u7a33\u5b9a\u4e14\u53ef\u6269\u5c55\u6027\u5dee\u3002", "method": "1. \u7edf\u4e00Transformer\u67b6\u6784\uff0c\u5c063D\u70b9\u4e91\u8865\u4e01\u548c\u8bed\u8a00\u6807\u8bb0\u4f5c\u4e3a\u5355\u4e00\u5e8f\u5217\u8054\u5408\u5904\u7406\uff1b2. \u7ed3\u6784\u5316\u8865\u4e01\u5316\u548c\u6807\u8bb0\u5316\u65b9\u6848\uff0c\u4fdd\u7559\u7a7a\u95f4\u4e0a\u4e0b\u6587\uff1b3. \u4e09\u9636\u6bb5\u8bad\u7ec3\u8bfe\u7a0b\uff0c\u4ece\u5bf9\u8c61\u7ea7\u8bc6\u522b\u9010\u6b65\u6784\u5efa\u5230\u573a\u666f\u7ea7\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002", "result": "Lemon\u5728\u5168\u9762\u76843D\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\u4e0a\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5305\u62ec\u5bf9\u8c61\u8bc6\u522b\u3001\u63cf\u8ff0\u548c3D\u573a\u666f\u4e2d\u7684\u7a7a\u95f4\u63a8\u7406\uff0c\u540c\u65f6\u968f\u7740\u6a21\u578b\u89c4\u6a21\u548c\u8bad\u7ec3\u6570\u636e\u7684\u589e\u52a0\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "Lemon\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u7840\u67b6\u6784\uff0c\u901a\u8fc7\u65e9\u671f\u7a7a\u95f4-\u8bed\u8a00\u878d\u5408\u3001\u6d88\u9664\u5197\u4f59\u7f16\u7801\u5668\u3001\u63d0\u9ad8\u53c2\u6570\u6548\u7387\u548c\u652f\u6301\u66f4\u6709\u6548\u7684\u6a21\u578b\u6269\u5c55\uff0c\u4e3a\u63a8\u8fdb\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u76843D\u7a7a\u95f4\u667a\u80fd\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2512.12881", "categories": ["cs.LG", "q-bio.NC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.12881", "abs": "https://arxiv.org/abs/2512.12881", "authors": ["DongKyu Kim", "Han-Lin Hsieh", "Maryam M. Shanechi"], "title": "Unsupervised learning of multiscale switching dynamical system models from multimodal neural data", "comment": "30 pages, 8 figures", "summary": "Neural population activity often exhibits regime-dependent non-stationarity in the form of switching dynamics. Learning accurate switching dynamical system models can reveal how behavior is encoded in neural activity. Existing switching approaches have primarily focused on learning models from a single neural modality, either continuous Gaussian signals or discrete Poisson signals. However, multiple neural modalities are often recorded simultaneously to measure different spatiotemporal scales of brain activity, and all these modalities can encode behavior. Moreover, regime labels are typically unavailable in training data, posing a significant challenge for learning models of regime-dependent switching dynamics. To address these challenges, we develop a novel unsupervised learning algorithm that learns the parameters of switching multiscale dynamical system models using only multiscale neural observations. We demonstrate our method using both simulations and two distinct experimental datasets with multimodal spike-LFP observations during different motor tasks. We find that our switching multiscale dynamical system models more accurately decode behavior than switching single-scale dynamical models, showing the success of multiscale neural fusion. Further, our models outperform stationary multiscale models, illustrating the importance of tracking regime-dependent non-stationarity in multimodal neural data. The developed unsupervised learning framework enables more accurate modeling of complex multiscale neural dynamics by leveraging information in multimodal recordings while incorporating regime switches. This approach holds promise for improving the performance and robustness of brain-computer interfaces over time and for advancing our understanding of the neural basis of behavior.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u5b66\u4e60\u5177\u6709\u5207\u6362\u52a8\u6001\u7684\u591a\u5c3a\u5ea6\u795e\u7ecf\u7cfb\u7edf\u6a21\u578b\uff0c\u80fd\u591f\u540c\u65f6\u5904\u7406\u591a\u79cd\u795e\u7ecf\u6a21\u6001\uff08\u5982\u5c16\u5cf0\u548c\u5c40\u90e8\u573a\u7535\u4f4d\uff09\uff0c\u65e0\u9700\u6807\u8bb0\u6570\u636e\u5373\u53ef\u51c6\u786e\u89e3\u7801\u884c\u4e3a\u3002", "motivation": "\u795e\u7ecf\u7fa4\u4f53\u6d3b\u52a8\u5e38\u8868\u73b0\u51fa\u4f9d\u8d56\u4e8e\u72b6\u6001\u7684\u5207\u6362\u52a8\u6001\uff0c\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5355\u4e00\u795e\u7ecf\u6a21\u6001\uff08\u8fde\u7eed\u9ad8\u65af\u4fe1\u53f7\u6216\u79bb\u6563\u6cca\u677e\u4fe1\u53f7\uff09\uff0c\u4f46\u5b9e\u9645\u4e2d\u5e38\u540c\u65f6\u8bb0\u5f55\u591a\u79cd\u6a21\u6001\u6765\u6d4b\u91cf\u4e0d\u540c\u65f6\u7a7a\u5c3a\u5ea6\u7684\u8111\u6d3b\u52a8\u3002\u6b64\u5916\uff0c\u8bad\u7ec3\u6570\u636e\u901a\u5e38\u7f3a\u4e4f\u72b6\u6001\u6807\u7b7e\uff0c\u8fd9\u7ed9\u5b66\u4e60\u5207\u6362\u52a8\u6001\u6a21\u578b\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65e0\u76d1\u7763\u5b66\u4e60\u7b97\u6cd5\uff0c\u4ec5\u4f7f\u7528\u591a\u5c3a\u5ea6\u795e\u7ecf\u89c2\u6d4b\u6570\u636e\u6765\u5b66\u4e60\u5207\u6362\u591a\u5c3a\u5ea6\u52a8\u6001\u7cfb\u7edf\u6a21\u578b\u7684\u53c2\u6570\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u540c\u65f6\u5904\u7406\u591a\u79cd\u795e\u7ecf\u6a21\u6001\uff0c\u65e0\u9700\u72b6\u6001\u6807\u7b7e\u5373\u53ef\u5b66\u4e60\u5207\u6362\u52a8\u6001\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u548c\u4e24\u4e2a\u4e0d\u540c\u7684\u5b9e\u9a8c\u6570\u636e\u96c6\uff08\u5305\u542b\u4e0d\u540c\u8fd0\u52a8\u4efb\u52a1\u671f\u95f4\u7684\u591a\u6a21\u6001\u5c16\u5cf0-LFP\u89c2\u6d4b\uff09\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u3002\u53d1\u73b0\u5207\u6362\u591a\u5c3a\u5ea6\u52a8\u6001\u7cfb\u7edf\u6a21\u578b\u6bd4\u5207\u6362\u5355\u5c3a\u5ea6\u52a8\u6001\u6a21\u578b\u66f4\u51c6\u786e\u5730\u89e3\u7801\u884c\u4e3a\uff0c\u663e\u793a\u4e86\u591a\u5c3a\u5ea6\u795e\u7ecf\u878d\u5408\u7684\u6210\u529f\u3002\u6b64\u5916\uff0c\u6a21\u578b\u4f18\u4e8e\u9759\u6001\u591a\u5c3a\u5ea6\u6a21\u578b\uff0c\u8bf4\u660e\u4e86\u5728\u591a\u79cd\u795e\u7ecf\u6570\u636e\u4e2d\u8ddf\u8e2a\u72b6\u6001\u4f9d\u8d56\u975e\u5e73\u7a33\u6027\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8be5\u65e0\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u901a\u8fc7\u5229\u7528\u591a\u6a21\u6001\u8bb0\u5f55\u4e2d\u7684\u4fe1\u606f\u5e76\u7eb3\u5165\u72b6\u6001\u5207\u6362\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u5efa\u6a21\u590d\u6742\u7684\u591a\u5c3a\u5ea6\u795e\u7ecf\u52a8\u6001\u3002\u8fd9\u79cd\u65b9\u6cd5\u6709\u671b\u63d0\u9ad8\u8111\u673a\u63a5\u53e3\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u589e\u8fdb\u5bf9\u884c\u4e3a\u795e\u7ecf\u57fa\u7840\u7684\u7406\u89e3\u3002"}}
{"id": "2512.12875", "categories": ["cs.CV", "cs.MM", "cs.SD"], "pdf": "https://arxiv.org/pdf/2512.12875", "abs": "https://arxiv.org/abs/2512.12875", "authors": ["Weihan Xu", "Kan Jen Cheng", "Koichi Saito", "Muhammad Jehanzeb Mirza", "Tingle Li", "Yisi Liu", "Alexander H. Liu", "Liming Wang", "Masato Ishii", "Takashi Shibuya", "Yuki Mitsufuji", "Gopala Anumanchipalli", "Paul Pu Liang"], "title": "Schrodinger Audio-Visual Editor: Object-Level Audiovisual Removal", "comment": null, "summary": "Joint editing of audio and visual content is crucial for precise and controllable content creation. This new task poses challenges due to the limitations of paired audio-visual data before and after targeted edits, and the heterogeneity across modalities. To address the data and modeling challenges in joint audio-visual editing, we introduce SAVEBench, a paired audiovisual dataset with text and mask conditions to enable object-grounded source-to-target learning. With SAVEBench, we train the Schrodinger Audio-Visual Editor (SAVE), an end-to-end flow-matching model that edits audio and video in parallel while keeping them aligned throughout processing. SAVE incorporates a Schrodinger Bridge that learns a direct transport from source to target audiovisual mixtures. Our evaluation demonstrates that the proposed SAVE model is able to remove the target objects in audio and visual content while preserving the remaining content, with stronger temporal synchronization and audiovisual semantic correspondence compared with pairwise combinations of an audio editor and a video editor.", "AI": {"tldr": "SAVE\u6a21\u578b\u901a\u8fc7Schrodinger Bridge\u5b9e\u73b0\u97f3\u9891\u548c\u89c6\u9891\u7684\u8054\u5408\u7f16\u8f91\uff0c\u5728SAVEBench\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u80fd\u591f\u540c\u6b65\u79fb\u9664\u76ee\u6807\u5bf9\u8c61\u5e76\u4fdd\u6301\u5185\u5bb9\u4e00\u81f4\u6027\u3002", "motivation": "\u97f3\u9891\u548c\u89c6\u89c9\u5185\u5bb9\u7684\u8054\u5408\u7f16\u8f91\u5bf9\u4e8e\u7cbe\u786e\u53ef\u63a7\u7684\u5185\u5bb9\u521b\u4f5c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u914d\u5bf9\u6570\u636e\u4e0d\u8db3\u548c\u6a21\u6001\u5f02\u8d28\u6027\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faSAVEBench\u914d\u5bf9\u89c6\u542c\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1SAVE\u6a21\u578b\uff1a\u57fa\u4e8e\u6d41\u5339\u914d\u7684\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u4f7f\u7528Schrodinger Bridge\u76f4\u63a5\u4ece\u6e90\u5230\u76ee\u6807\u89c6\u542c\u6df7\u5408\u8fdb\u884c\u4f20\u8f93\u3002", "result": "SAVE\u6a21\u578b\u80fd\u591f\u6709\u6548\u79fb\u9664\u97f3\u9891\u548c\u89c6\u9891\u4e2d\u7684\u76ee\u6807\u5bf9\u8c61\uff0c\u540c\u65f6\u4fdd\u6301\u5269\u4f59\u5185\u5bb9\uff0c\u5728\u65f6\u95f4\u540c\u6b65\u548c\u89c6\u542c\u8bed\u4e49\u5bf9\u5e94\u65b9\u9762\u4f18\u4e8e\u97f3\u9891\u548c\u89c6\u9891\u7f16\u8f91\u5668\u7684\u7ec4\u5408\u65b9\u6cd5\u3002", "conclusion": "SAVE\u6a21\u578b\u901a\u8fc7\u8054\u5408\u7f16\u8f91\u6846\u67b6\u89e3\u51b3\u4e86\u89c6\u542c\u5185\u5bb9\u540c\u6b65\u7f16\u8f91\u7684\u6311\u6218\uff0c\u5728\u4fdd\u6301\u5185\u5bb9\u4e00\u81f4\u6027\u548c\u65f6\u95f4\u540c\u6b65\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2512.12895", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12895", "abs": "https://arxiv.org/abs/2512.12895", "authors": ["Charilaos Pipis", "Shivam Garg", "Vasilis Kontonis", "Vaishnavi Shrivastava", "Akshay Krishnamurthy", "Dimitris Papailiopoulos"], "title": "Wait, Wait, Wait... Why Do Reasoning Models Loop?", "comment": null, "summary": "Reasoning models (e.g., DeepSeek-R1) generate long chains of thought to solve harder problems, but they often loop, repeating the same text at low temperatures or with greedy decoding. We study why this happens and what role temperature plays. With open reasoning models, we find that looping is common at low temperature. Larger models tend to loop less, and distilled students loop significantly even when their teachers rarely do. This points to mismatches between the training distribution and the learned model, which we refer to as errors in learning, as a key cause. To understand how such errors cause loops, we introduce a synthetic graph reasoning task and demonstrate two mechanisms. First, risk aversion caused by hardness of learning: when the correct progress-making action is hard to learn but an easy cyclic action is available, the model puts relatively more probability on the cyclic action and gets stuck. Second, even when there is no hardness, Transformers show an inductive bias toward temporally correlated errors, so the same few actions keep being chosen and loops appear. Higher temperature reduces looping by promoting exploration, but it does not fix the errors in learning, so generations remain much longer than necessary at high temperature; in this sense, temperature is a stopgap rather than a holistic solution. We end with a discussion of training-time interventions aimed at directly reducing errors in learning.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u63a8\u7406\u6a21\u578b\u5728\u4f4e\u6e29\u4e0b\u5bb9\u6613\u9677\u5165\u6587\u672c\u5faa\u73af\u91cd\u590d\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u5b66\u4e60\u8bef\u5dee\u548cTransformer\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u9ad8\u6e29\u867d\u80fd\u7f13\u89e3\u4f46\u975e\u6839\u672c\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u63a8\u7406\u6a21\u578b\uff08\u5982DeepSeek-R1\uff09\u5728\u89e3\u51b3\u590d\u6742\u95ee\u9898\u65f6\u751f\u6210\u957f\u601d\u7ef4\u94fe\uff0c\u4f46\u7ecf\u5e38\u5728\u4f4e\u6e29\u6216\u8d2a\u5a6a\u89e3\u7801\u4e0b\u9677\u5165\u6587\u672c\u5faa\u73af\u91cd\u590d\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u8fd9\u79cd\u73b0\u8c61\u7684\u539f\u56e0\u4ee5\u53ca\u6e29\u5ea6\u53c2\u6570\u5728\u5176\u4e2d\u626e\u6f14\u7684\u89d2\u8272\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5f00\u6e90\u63a8\u7406\u6a21\u578b\uff0c\u53d1\u73b0\u4f4e\u6e29\u4e0b\u5faa\u73af\u73b0\u8c61\u666e\u904d\uff1b\u5f15\u5165\u5408\u6210\u56fe\u63a8\u7406\u4efb\u52a1\u6765\u7814\u7a76\u673a\u5236\uff0c\u8bc6\u522b\u51fa\u4e24\u79cd\u5bfc\u81f4\u5faa\u73af\u7684\u539f\u56e0\uff1a\u5b66\u4e60\u96be\u5ea6\u5bfc\u81f4\u7684\u89c4\u907f\u98ce\u9669\u884c\u4e3a\uff0c\u4ee5\u53caTransformer\u5bf9\u65f6\u95f4\u76f8\u5173\u9519\u8bef\u7684\u5f52\u7eb3\u504f\u7f6e\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u4f4e\u6e29\u4e0b\u5faa\u73af\u73b0\u8c61\u5e38\u89c1\uff1b2\uff09\u5927\u6a21\u578b\u5faa\u73af\u8f83\u5c11\uff0c\u84b8\u998f\u5b66\u751f\u6a21\u578b\u5faa\u73af\u4e25\u91cd\uff1b3\uff09\u5b66\u4e60\u8bef\u5dee\u662f\u4e3b\u8981\u539f\u56e0\uff1b4\uff09\u9ad8\u6e29\u901a\u8fc7\u4fc3\u8fdb\u63a2\u7d22\u51cf\u5c11\u5faa\u73af\uff0c\u4f46\u65e0\u6cd5\u4fee\u590d\u5b66\u4e60\u8bef\u5dee\uff0c\u5bfc\u81f4\u751f\u6210\u6587\u672c\u4ecd\u8fc7\u957f\u3002", "conclusion": "\u6e29\u5ea6\u53c2\u6570\u53ea\u662f\u6743\u5b9c\u4e4b\u8ba1\u800c\u975e\u6839\u672c\u89e3\u51b3\u65b9\u6848\uff0c\u9700\u8981\u8bad\u7ec3\u65f6\u5e72\u9884\u6765\u76f4\u63a5\u51cf\u5c11\u5b66\u4e60\u8bef\u5dee\u3002\u8bba\u6587\u6700\u540e\u8ba8\u8bba\u4e86\u65e8\u5728\u76f4\u63a5\u51cf\u5c11\u5b66\u4e60\u8bef\u5dee\u7684\u8bad\u7ec3\u65f6\u5e72\u9884\u65b9\u6cd5\u3002"}}
{"id": "2512.12884", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.12884", "abs": "https://arxiv.org/abs/2512.12884", "authors": ["Xiangzhong Liu", "Jiajie Zhang", "Hao Shen"], "title": "Cross-Level Sensor Fusion with Object Lists via Transformer for 3D Object Detection", "comment": "6 pages, 3 figures, accepted at IV2025", "summary": "In automotive sensor fusion systems, smart sensors and Vehicle-to-Everything (V2X) modules are commonly utilized. Sensor data from these systems are typically available only as processed object lists rather than raw sensor data from traditional sensors. Instead of processing other raw data separately and then fusing them at the object level, we propose an end-to-end cross-level fusion concept with Transformer, which integrates highly abstract object list information with raw camera images for 3D object detection. Object lists are fed into a Transformer as denoising queries and propagated together with learnable queries through the latter feature aggregation process. Additionally, a deformable Gaussian mask, derived from the positional and size dimensional priors from the object lists, is explicitly integrated into the Transformer decoder. This directs attention toward the target area of interest and accelerates model training convergence. Furthermore, as there is no public dataset containing object lists as a standalone modality, we propose an approach to generate pseudo object lists from ground-truth bounding boxes by simulating state noise and false positives and negatives. As the first work to conduct cross-level fusion, our approach shows substantial performance improvements over the vision-based baseline on the nuScenes dataset. It demonstrates its generalization capability over diverse noise levels of simulated object lists and real detectors.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u6c7d\u8f66\u4f20\u611f\u5668\u878d\u5408\u7684\u7aef\u5230\u7aef\u8de8\u7ea7\u522bTransformer\u878d\u5408\u65b9\u6cd5\uff0c\u5c06\u62bd\u8c61\u7684\u5bf9\u8c61\u5217\u8868\u4fe1\u606f\u4e0e\u539f\u59cb\u76f8\u673a\u56fe\u50cf\u7ed3\u5408\u8fdb\u884c3D\u76ee\u6807\u68c0\u6d4b\uff0c\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u89c6\u89c9\u57fa\u7ebf\u3002", "motivation": "\u5728\u6c7d\u8f66\u4f20\u611f\u5668\u878d\u5408\u7cfb\u7edf\u4e2d\uff0c\u667a\u80fd\u4f20\u611f\u5668\u548cV2X\u6a21\u5757\u901a\u5e38\u53ea\u63d0\u4f9b\u5904\u7406\u540e\u7684\u5bf9\u8c61\u5217\u8868\u800c\u975e\u539f\u59cb\u6570\u636e\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u5206\u522b\u5904\u7406\u539f\u59cb\u6570\u636e\u540e\u5728\u5bf9\u8c61\u7ea7\u522b\u878d\u5408\uff0c\u7f3a\u4e4f\u5c06\u9ad8\u5ea6\u62bd\u8c61\u7684\u5bf9\u8c61\u5217\u8868\u4fe1\u606f\u4e0e\u539f\u59cb\u4f20\u611f\u5668\u6570\u636e\u76f4\u63a5\u6574\u5408\u7684\u65b9\u6cd5\u3002", "method": "1) \u63d0\u51fa\u7aef\u5230\u7aef\u8de8\u7ea7\u522b\u878d\u5408\u6982\u5ff5\uff0c\u4f7f\u7528Transformer\u5c06\u5bf9\u8c61\u5217\u8868\u4fe1\u606f\u4f5c\u4e3a\u53bb\u566a\u67e5\u8be2\u4e0e\u53ef\u5b66\u4e60\u67e5\u8be2\u4e00\u8d77\u4f20\u64ad\uff1b2) \u5f15\u5165\u53ef\u53d8\u5f62\u9ad8\u65af\u63a9\u7801\uff0c\u57fa\u4e8e\u5bf9\u8c61\u5217\u8868\u7684\u4f4d\u7f6e\u548c\u5c3a\u5bf8\u5148\u9a8c\u6307\u5bfc\u6ce8\u610f\u529b\u673a\u5236\uff1b3) \u63d0\u51fa\u4ece\u771f\u5b9e\u8fb9\u754c\u6846\u751f\u6210\u4f2a\u5bf9\u8c61\u5217\u8868\u7684\u65b9\u6cd5\uff0c\u6a21\u62df\u72b6\u6001\u566a\u58f0\u548c\u8bef\u68c0\u6f0f\u68c0\u3002", "result": "\u5728nuScenes\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u57fa\u4e8e\u89c6\u89c9\u7684\u57fa\u7ebf\u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002\u540c\u65f6\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u566a\u58f0\u6c34\u5e73\u7684\u6a21\u62df\u5bf9\u8c61\u5217\u8868\u548c\u771f\u5b9e\u68c0\u6d4b\u5668\u4e0a\u7684\u826f\u597d\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u8fdb\u884c\u8de8\u7ea7\u522b\u878d\u5408\u7684\u5de5\u4f5c\uff0c\u6210\u529f\u5730\u5c06\u62bd\u8c61\u5bf9\u8c61\u5217\u8868\u4fe1\u606f\u4e0e\u539f\u59cb\u56fe\u50cf\u6570\u636e\u878d\u5408\uff0c\u4e3a\u6c7d\u8f66\u4f20\u611f\u5668\u878d\u5408\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5904\u7406\u667a\u80fd\u4f20\u611f\u5668\u548cV2X\u6a21\u5757\u63d0\u4f9b\u7684\u5904\u7406\u540e\u7684\u5bf9\u8c61\u5217\u8868\u6570\u636e\u3002"}}
{"id": "2512.12896", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12896", "abs": "https://arxiv.org/abs/2512.12896", "authors": ["Parthasarathy Nadarajan", "Michael Botsch"], "title": "Probability Estimation for Predicted-Occupancy Grids in Vehicle Safety Applications Based on Machine Learning", "comment": "2016 IEEE Intelligent Vehicles Symposium", "summary": "This paper presents a method to predict the evolution of a complex traffic scenario with multiple objects. The current state of the scenario is assumed to be known from sensors and the prediction is taking into account various hypotheses about the behavior of traffic participants. This way, the uncertainties regarding the behavior of traffic participants can be modelled in detail. In the first part of this paper a model-based approach is presented to compute Predicted-Occupancy Grids (POG), which are introduced as a grid-based probabilistic representation of the future scenario hypotheses. However, due to the large number of possible trajectories for each traffic participant, the model-based approach comes with a very high computational load. Thus, a machine-learning approach is adopted for the computation of POGs. This work uses a novel grid-based representation of the current state of the traffic scenario and performs the mapping to POGs. This representation consists of augmented cells in an occupancy grid. The adopted machine-learning approach is based on the Random Forest algorithm. Simulations of traffic scenarios are performed to compare the machine-learning with the model-based approach. The results are promising and could enable the real-time computation of POGs for vehicle safety applications. With this detailed modelling of uncertainties, crucial components in vehicle safety systems like criticality estimation and trajectory planning can be improved.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u9884\u6d4b\u5360\u7528\u7f51\u683c\u65b9\u6cd5\uff0c\u7528\u4e8e\u590d\u6742\u4ea4\u901a\u573a\u666f\u7684\u591a\u76ee\u6807\u6f14\u5316\u9884\u6d4b\uff0c\u76f8\u6bd4\u4f20\u7edf\u6a21\u578b\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u8d1f\u8f7d\uff0c\u5b9e\u73b0\u5b9e\u65f6\u5e94\u7528\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6a21\u578b\u7684\u9884\u6d4b\u5360\u7528\u7f51\u683c\u65b9\u6cd5\u867d\u7136\u80fd\u8be6\u7ec6\u5efa\u6a21\u4ea4\u901a\u53c2\u4e0e\u8005\u7684\u884c\u4e3a\u4e0d\u786e\u5b9a\u6027\uff0c\u4f46\u7531\u4e8e\u6bcf\u4e2a\u53c2\u4e0e\u8005\u53ef\u80fd\u7684\u8f68\u8ff9\u6570\u91cf\u5de8\u5927\uff0c\u8ba1\u7b97\u8d1f\u8f7d\u975e\u5e38\u9ad8\uff0c\u96be\u4ee5\u5b9e\u73b0\u5b9e\u65f6\u5e94\u7528\u3002\u56e0\u6b64\u9700\u8981\u5bfb\u627e\u66f4\u9ad8\u6548\u7684\u8ba1\u7b97\u65b9\u6cd5\u3002", "method": "\u9996\u5148\u63d0\u51fa\u57fa\u4e8e\u6a21\u578b\u7684\u9884\u6d4b\u5360\u7528\u7f51\u683c\u8ba1\u7b97\u65b9\u6cd5\uff0c\u7136\u540e\u91c7\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u66ff\u4ee3\u3002\u4f7f\u7528\u65b0\u9896\u7684\u7f51\u683c\u8868\u793a\u65b9\u6cd5\uff08\u589e\u5f3a\u5355\u5143\u5360\u7528\u7f51\u683c\uff09\u8868\u793a\u5f53\u524d\u4ea4\u901a\u573a\u666f\u72b6\u6001\uff0c\u5e76\u57fa\u4e8e\u968f\u673a\u68ee\u6797\u7b97\u6cd5\u5c06\u5f53\u524d\u72b6\u6001\u6620\u5c04\u5230\u9884\u6d4b\u5360\u7528\u7f51\u683c\u3002", "result": "\u901a\u8fc7\u4ea4\u901a\u573a\u666f\u6a21\u62df\u5bf9\u6bd4\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u548c\u6a21\u578b\u65b9\u6cd5\uff0c\u7ed3\u679c\u663e\u793a\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u6027\u80fd\u826f\u597d\uff0c\u80fd\u591f\u5b9e\u73b0\u9884\u6d4b\u5360\u7528\u7f51\u683c\u7684\u5b9e\u65f6\u8ba1\u7b97\uff0c\u4e3a\u8f66\u8f86\u5b89\u5168\u5e94\u7528\u63d0\u4f9b\u53ef\u80fd\u3002", "conclusion": "\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u9884\u6d4b\u5360\u7528\u7f51\u683c\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u5b9e\u73b0\u5b9e\u65f6\u9884\u6d4b\uff0c\u901a\u8fc7\u8be6\u7ec6\u5efa\u6a21\u4e0d\u786e\u5b9a\u6027\uff0c\u53ef\u4ee5\u6539\u8fdb\u8f66\u8f86\u5b89\u5168\u7cfb\u7edf\u4e2d\u7684\u5173\u952e\u7ec4\u4ef6\u5982\u4e34\u754c\u6027\u4f30\u8ba1\u548c\u8f68\u8ff9\u89c4\u5212\u3002"}}
{"id": "2512.12901", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12901", "abs": "https://arxiv.org/abs/2512.12901", "authors": ["Parthasarathy Nadarajan", "Michael Botsch", "Sebastian Sardina"], "title": "Predicted-occupancy grids for vehicle safety applications based on autoencoders and the Random Forest algorithm", "comment": "2017 International Joint Conference on Neural Networks (IJCNN)", "summary": "In this paper, a probabilistic space-time representation of complex traffic scenarios is predicted using machine learning algorithms. Such a representation is significant for all active vehicle safety applications especially when performing dynamic maneuvers in a complex traffic scenario. As a first step, a hierarchical situation classifier is used to distinguish the different types of traffic scenarios. This classifier is responsible for identifying the type of the road infrastructure and the safety-relevant traffic participants of the driving environment. With each class representing similar traffic scenarios, a set of Random Forests (RFs) is individually trained to predict the probabilistic space-time representation, which depicts the future behavior of traffic participants. This representation is termed as a Predicted-Occupancy Grid (POG). The input to the RFs is an Augmented Occupancy Grid (AOG). In order to increase the learning accuracy of the RFs and to perform better predictions, the AOG is reduced to low-dimensional features using a Stacked Denoising Autoencoder (SDA). The excellent performance of the proposed machine learning approach consisting of SDAs and RFs is demonstrated in simulations and in experiments with real vehicles. An application of POGs to estimate the criticality of traffic scenarios and to determine safe trajectories is also presented.", "AI": {"tldr": "\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u590d\u6742\u4ea4\u901a\u573a\u666f\u7684\u6982\u7387\u65f6\u7a7a\u8868\u793a\uff0c\u901a\u8fc7\u5c42\u6b21\u5206\u7c7b\u5668\u8bc6\u522b\u573a\u666f\u7c7b\u578b\uff0c\u7528\u968f\u673a\u68ee\u6797\u9884\u6d4b\u4ea4\u901a\u53c2\u4e0e\u8005\u672a\u6765\u884c\u4e3a\u7684\u6982\u7387\u5360\u7528\u7f51\u683c\uff0c\u5e76\u901a\u8fc7\u5806\u53e0\u964d\u566a\u81ea\u7f16\u7801\u5668\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u4e3a\u4e3b\u52a8\u8f66\u8f86\u5b89\u5168\u5e94\u7528\u63d0\u4f9b\u590d\u6742\u4ea4\u901a\u573a\u666f\u7684\u6982\u7387\u65f6\u7a7a\u8868\u793a\uff0c\u7279\u522b\u662f\u5728\u6267\u884c\u52a8\u6001\u673a\u52a8\u64cd\u4f5c\u65f6\uff0c\u9700\u8981\u51c6\u786e\u9884\u6d4b\u4ea4\u901a\u53c2\u4e0e\u8005\u7684\u672a\u6765\u884c\u4e3a\u4ee5\u786e\u4fdd\u5b89\u5168\u3002", "method": "1. \u4f7f\u7528\u5c42\u6b21\u60c5\u5883\u5206\u7c7b\u5668\u533a\u5206\u4e0d\u540c\u7c7b\u578b\u7684\u4ea4\u901a\u573a\u666f\uff0c\u8bc6\u522b\u9053\u8def\u57fa\u7840\u8bbe\u65bd\u548c\u5b89\u5168\u76f8\u5173\u4ea4\u901a\u53c2\u4e0e\u8005\uff1b2. \u5bf9\u6bcf\u7c7b\u76f8\u4f3c\u573a\u666f\uff0c\u7528\u968f\u673a\u68ee\u6797\u9884\u6d4b\u6982\u7387\u65f6\u7a7a\u8868\u793a\uff08\u9884\u6d4b\u5360\u7528\u7f51\u683cPOG\uff09\uff1b3. \u4f7f\u7528\u5806\u53e0\u964d\u566a\u81ea\u7f16\u7801\u5668\u5c06\u589e\u5f3a\u5360\u7528\u7f51\u683c\u964d\u7ef4\u4e3a\u4f4e\u7ef4\u7279\u5f81\uff0c\u63d0\u9ad8\u968f\u673a\u68ee\u6797\u7684\u5b66\u4e60\u7cbe\u5ea6\u3002", "result": "\u63d0\u51fa\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff08SDA+RF\uff09\u5728\u4eff\u771f\u548c\u771f\u5b9e\u8f66\u8f86\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u4f18\u5f02\u6027\u80fd\uff0c\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u4ea4\u901a\u53c2\u4e0e\u8005\u7684\u672a\u6765\u884c\u4e3a\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u8bc4\u4f30\u4ea4\u901a\u573a\u666f\u4e34\u754c\u6027\u548c\u786e\u5b9a\u5b89\u5168\u8f68\u8ff9\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u4ea4\u901a\u573a\u666f\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6982\u7387\u65f6\u7a7a\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e3b\u52a8\u8f66\u8f86\u5b89\u5168\u7cfb\u7edf\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u52a8\u6001\u673a\u52a8\u64cd\u4f5c\u4e2d\u7684\u5b89\u5168\u6027\u8bc4\u4f30\u548c\u8f68\u8ff9\u89c4\u5212\u65b9\u9762\u3002"}}
{"id": "2512.12903", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12903", "abs": "https://arxiv.org/abs/2512.12903", "authors": ["Ken-ichi Kitayama"], "title": "Next-generation reservoir computing validated by classification task", "comment": null, "summary": "An emerging computing paradigm, so-called next-generation reservoir computing (NG-RC) is investigated. True to its namesake, NG-RC requires no actual reservoirs for input data mixing but rather computing the polynomial terms directly from the time series inputs. However, benchmark tests so far reported have been one-sided, limited to prediction tasks of temporal waveforms such as Lorenz 63 attractor and Mackey-Glass chaotic signal. We will demonstrate for the first time that NG-RC can perform classification task as good as conventional RC. This validates the versatile computational capability of NG-RC in tasks of both prediction and classification.", "AI": {"tldr": "NG-RC\uff08\u4e0b\u4e00\u4ee3\u50a8\u5907\u6c60\u8ba1\u7b97\uff09\u65e0\u9700\u5b9e\u9645\u50a8\u5907\u6c60\uff0c\u76f4\u63a5\u4ece\u65f6\u95f4\u5e8f\u5217\u8f93\u5165\u8ba1\u7b97\u591a\u9879\u5f0f\u9879\u3002\u5148\u524d\u57fa\u51c6\u6d4b\u8bd5\u5c40\u9650\u4e8e\u9884\u6d4b\u4efb\u52a1\uff0c\u672c\u6587\u9996\u6b21\u8bc1\u660eNG-RC\u5728\u5206\u7c7b\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0e\u4f20\u7edfRC\u76f8\u5f53\u3002", "motivation": "\u73b0\u6709NG-RC\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u9884\u6d4b\u4efb\u52a1\uff08\u5982Lorenz 63\u5438\u5f15\u5b50\u548cMackey-Glass\u6df7\u6c8c\u4fe1\u53f7\uff09\uff0c\u7f3a\u4e4f\u5bf9\u5176\u5206\u7c7b\u80fd\u529b\u7684\u9a8c\u8bc1\u3002\u9700\u8981\u8bc1\u660eNG-RC\u5728\u5206\u7c7b\u4efb\u52a1\u4e0a\u4e0e\u4f20\u7edf\u50a8\u5907\u6c60\u8ba1\u7b97\u76f8\u5f53\uff0c\u4ee5\u9a8c\u8bc1\u5176\u591a\u529f\u80fd\u8ba1\u7b97\u80fd\u529b\u3002", "method": "\u91c7\u7528\u4e0b\u4e00\u4ee3\u50a8\u5907\u6c60\u8ba1\u7b97\uff08NG-RC\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u5b9e\u9645\u50a8\u5907\u6c60\u8fdb\u884c\u8f93\u5165\u6570\u636e\u6df7\u5408\uff0c\u800c\u662f\u76f4\u63a5\u4ece\u65f6\u95f4\u5e8f\u5217\u8f93\u5165\u8ba1\u7b97\u591a\u9879\u5f0f\u9879\u3002\u5c06NG-RC\u5e94\u7528\u4e8e\u5206\u7c7b\u4efb\u52a1\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u9996\u6b21\u8bc1\u660eNG-RC\u5728\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u4e0e\u4f20\u7edf\u50a8\u5907\u6c60\u8ba1\u7b97\u76f8\u5f53\uff0c\u9a8c\u8bc1\u4e86NG-RC\u5728\u9884\u6d4b\u548c\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u591a\u529f\u80fd\u8ba1\u7b97\u80fd\u529b\u3002", "conclusion": "NG-RC\u4e0d\u4ec5\u9002\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\uff0c\u5728\u5206\u7c7b\u4efb\u52a1\u4e0a\u4e5f\u8868\u73b0\u51fa\u4e0e\u4f20\u7edfRC\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u4f5c\u4e3a\u591a\u529f\u80fd\u8ba1\u7b97\u8303\u5f0f\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.12907", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12907", "abs": "https://arxiv.org/abs/2512.12907", "authors": ["Parthasarathy Nadarajan", "Michael Botsch", "Sebastian Sardina"], "title": "Machine Learning Architectures for the Estimation of Predicted Occupancy Grids in Road Traffic", "comment": "Journal of Advances in Information Technology", "summary": "This paper introduces a novel machine learning architecture for an efficient estimation of the probabilistic space-time representation of complex traffic scenarios. A detailed representation of the future traffic scenario is of significant importance for autonomous driving and for all active safety systems. In order to predict the future space-time representation of the traffic scenario, first the type of traffic scenario is identified and then the machine learning algorithm maps the current state of the scenario to possible future states. The input to the machine learning algorithms is the current state representation of a traffic scenario, termed as the Augmented Occupancy Grid (AOG). The output is the probabilistic space-time representation which includes uncertainties regarding the behaviour of the traffic participants and is termed as the Predicted Occupancy Grid (POG). The novel architecture consists of two Stacked Denoising Autoencoders (SDAs) and a set of Random Forests. It is then compared with the other two existing architectures that comprise of SDAs and DeconvNet. The architectures are validated with the help of simulations and the comparisons are made both in terms of accuracy and computational time. Also, a brief overview on the applications of POGs in the field of active safety is presented.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u9ad8\u6548\u4f30\u8ba1\u590d\u6742\u4ea4\u901a\u573a\u666f\u6982\u7387\u65f6\u7a7a\u8868\u793a\u7684\u65b0\u578b\u673a\u5668\u5b66\u4e60\u67b6\u6784\uff0c\u901a\u8fc7\u8bc6\u522b\u4ea4\u901a\u573a\u666f\u7c7b\u578b\u5e76\u6620\u5c04\u5f53\u524d\u72b6\u6001\u5230\u672a\u6765\u72b6\u6001\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u548c\u4e3b\u52a8\u5b89\u5168\u7cfb\u7edf\u63d0\u4f9b\u8be6\u7ec6\u9884\u6d4b\u3002", "motivation": "\u8be6\u7ec6\u8868\u793a\u672a\u6765\u4ea4\u901a\u573a\u666f\u5bf9\u81ea\u52a8\u9a7e\u9a76\u548c\u4e3b\u52a8\u5b89\u5168\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u9700\u8981\u80fd\u591f\u9884\u6d4b\u4ea4\u901a\u53c2\u4e0e\u8005\u884c\u4e3a\u4e0d\u786e\u5b9a\u6027\u5e76\u751f\u6210\u6982\u7387\u65f6\u7a7a\u8868\u793a\u7684\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u589e\u5f3a\u5360\u7528\u7f51\u683c\uff08AOG\uff09\u4f5c\u4e3a\u8f93\u5165\uff0c\u901a\u8fc7\u4e24\u4e2a\u5806\u53e0\u53bb\u566a\u81ea\u7f16\u7801\u5668\uff08SDAs\uff09\u548c\u968f\u673a\u68ee\u6797\u96c6\u5408\uff0c\u8f93\u51fa\u9884\u6d4b\u5360\u7528\u7f51\u683c\uff08POG\uff09\u4f5c\u4e3a\u6982\u7387\u65f6\u7a7a\u8868\u793a\uff0c\u5e76\u4e0e\u73b0\u6709SDAs+DeconvNet\u67b6\u6784\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u6240\u63d0\u67b6\u6784\u7684\u6709\u6548\u6027\uff0c\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u65f6\u95f4\u65b9\u9762\u4e0e\u5176\u4ed6\u4e24\u79cd\u73b0\u6709\u67b6\u6784\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u5e76\u5c55\u793a\u4e86POG\u5728\u4e3b\u52a8\u5b89\u5168\u9886\u57df\u7684\u5e94\u7528\u524d\u666f\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b0\u578b\u673a\u5668\u5b66\u4e60\u67b6\u6784\u80fd\u591f\u6709\u6548\u751f\u6210\u4ea4\u901a\u573a\u666f\u7684\u6982\u7387\u65f6\u7a7a\u8868\u793a\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u548c\u4e3b\u52a8\u5b89\u5168\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6280\u672f\u652f\u6301\uff0c\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2512.12906", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12906", "abs": "https://arxiv.org/abs/2512.12906", "authors": ["Zhimao Peng", "Enguang Wang", "Xialei Liu", "Ming-Ming Cheng"], "title": "Predictive Sample Assignment for Semantically Coherent Out-of-Distribution Detection", "comment": "Accepted by TCSVT2024", "summary": "Semantically coherent out-of-distribution detection (SCOOD) is a recently proposed realistic OOD detection setting: given labeled in-distribution (ID) data and mixed in-distribution and out-of-distribution unlabeled data as the training data, SCOOD aims to enable the trained model to accurately identify OOD samples in the testing data. Current SCOOD methods mainly adopt various clustering-based in-distribution sample filtering (IDF) strategies to select clean ID samples from unlabeled data, and take the remaining samples as auxiliary OOD data, which inevitably introduces a large number of noisy samples in training. To address the above issue, we propose a concise SCOOD framework based on predictive sample assignment (PSA). PSA includes a dual-threshold ternary sample assignment strategy based on the predictive energy score that can significantly improve the purity of the selected ID and OOD sample sets by assigning unconfident unlabeled data to an additional discard sample set, and a concept contrastive representation learning loss to further expand the distance between ID and OOD samples in the representation space to assist ID/OOD discrimination. In addition, we also introduce a retraining strategy to help the model fully fit the selected auxiliary ID/OOD samples. Experiments on two standard SCOOD benchmarks demonstrate that our approach outperforms the state-of-the-art methods by a significant margin.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u6d4b\u6837\u672c\u5206\u914d\uff08PSA\uff09\u7684\u8bed\u4e49\u8fde\u8d2f\u5206\u5e03\u5916\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u9608\u503c\u4e09\u5143\u6837\u672c\u5206\u914d\u7b56\u7565\u63d0\u9ad8ID/OOD\u6837\u672c\u96c6\u7684\u7eaf\u5ea6\uff0c\u5e76\u5f15\u5165\u6982\u5ff5\u5bf9\u6bd4\u8868\u793a\u5b66\u4e60\u635f\u5931\u6765\u589e\u5f3aID/OOD\u6837\u672c\u5728\u8868\u793a\u7a7a\u95f4\u4e2d\u7684\u533a\u5206\u5ea6\u3002", "motivation": "\u5f53\u524dSCOOD\u65b9\u6cd5\u4e3b\u8981\u91c7\u7528\u57fa\u4e8e\u805a\u7c7b\u7684ID\u6837\u672c\u8fc7\u6ee4\u7b56\u7565\u4ece\u65e0\u6807\u7b7e\u6570\u636e\u4e2d\u9009\u62e9\u5e72\u51c0ID\u6837\u672c\uff0c\u5e76\u5c06\u5269\u4f59\u6837\u672c\u4f5c\u4e3a\u8f85\u52a9OOD\u6570\u636e\uff0c\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u53ef\u907f\u514d\u5730\u5f15\u5165\u4e86\u5927\u91cf\u566a\u58f0\u6837\u672c\uff0c\u5f71\u54cd\u8bad\u7ec3\u6548\u679c\u3002", "method": "\u63d0\u51faPSA\u6846\u67b6\uff1a1\uff09\u57fa\u4e8e\u9884\u6d4b\u80fd\u91cf\u5206\u6570\u7684\u53cc\u9608\u503c\u4e09\u5143\u6837\u672c\u5206\u914d\u7b56\u7565\uff0c\u5c06\u4e0d\u786e\u5b9a\u7684\u65e0\u6807\u7b7e\u6570\u636e\u5206\u914d\u5230\u4e22\u5f03\u6837\u672c\u96c6\u4e2d\uff0c\u63d0\u9ad8ID/OOD\u6837\u672c\u96c6\u7684\u7eaf\u5ea6\uff1b2\uff09\u6982\u5ff5\u5bf9\u6bd4\u8868\u793a\u5b66\u4e60\u635f\u5931\uff0c\u6269\u5927ID\u548cOOD\u6837\u672c\u5728\u8868\u793a\u7a7a\u95f4\u4e2d\u7684\u8ddd\u79bb\uff1b3\uff09\u91cd\u8bad\u7ec3\u7b56\u7565\u5e2e\u52a9\u6a21\u578b\u5145\u5206\u62df\u5408\u9009\u5b9a\u7684\u8f85\u52a9ID/OOD\u6837\u672c\u3002", "result": "\u5728\u4e24\u4e2a\u6807\u51c6SCOOD\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u5f53\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684PSA\u6846\u67b6\u901a\u8fc7\u6539\u8fdb\u6837\u672c\u5206\u914d\u7b56\u7565\u548c\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86SCOOD\u4efb\u52a1\u4e2d\u566a\u58f0\u6837\u672c\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5e03\u5916\u68c0\u6d4b\u6027\u80fd\u3002"}}
{"id": "2512.12925", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12925", "abs": "https://arxiv.org/abs/2512.12925", "authors": ["Zhimao Peng", "Enguang Wang", "Fei Yang", "Xialei Liu", "Ming-Ming Cheng"], "title": "Sharpness-aware Dynamic Anchor Selection for Generalized Category Discovery", "comment": "Accepted by TMM2025", "summary": "Generalized category discovery (GCD) is an important and challenging task in open-world learning. Specifically, given some labeled data of known classes, GCD aims to cluster unlabeled data that contain both known and unknown classes. Current GCD methods based on parametric classification adopt the DINO-like pseudo-labeling strategy, where the sharpened probability output of one view is used as supervision information for the other view. However, large pre-trained models have a preference for some specific visual patterns, resulting in encoding spurious correlation for unlabeled data and generating noisy pseudo-labels. To address this issue, we propose a novel method, which contains two modules: Loss Sharpness Penalty (LSP) and Dynamic Anchor Selection (DAS). LSP enhances the robustness of model parameters to small perturbations by minimizing the worst-case loss sharpness of the model, which suppressing the encoding of trivial features, thereby reducing overfitting of noise samples and improving the quality of pseudo-labels. Meanwhile, DAS selects representative samples for the unknown classes based on KNN density and class probability during the model training and assigns hard pseudo-labels to them, which not only alleviates the confidence difference between known and unknown classes but also enables the model to quickly learn more accurate feature distribution for the unknown classes, thus further improving the clustering accuracy. Extensive experiments demonstrate that the proposed method can effectively mitigate the noise of pseudo-labels, and achieve state-of-the-art results on multiple GCD benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5e7f\u4e49\u7c7b\u522b\u53d1\u73b0\u65b9\u6cd5\uff0c\u5305\u542b\u635f\u5931\u9510\u5ea6\u60e9\u7f5a\u548c\u52a8\u6001\u951a\u70b9\u9009\u62e9\u4e24\u4e2a\u6a21\u5757\uff0c\u901a\u8fc7\u51cf\u5c11\u4f2a\u6807\u7b7e\u566a\u58f0\u63d0\u5347\u805a\u7c7b\u6027\u80fd", "motivation": "\u73b0\u6709\u57fa\u4e8e\u53c2\u6570\u5316\u5206\u7c7b\u7684GCD\u65b9\u6cd5\u4f7f\u7528DINO\u5f0f\u4f2a\u6807\u7b7e\u7b56\u7565\uff0c\u4f46\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u5bf9\u7279\u5b9a\u89c6\u89c9\u6a21\u5f0f\u6709\u504f\u597d\uff0c\u5bfc\u81f4\u5bf9\u672a\u6807\u8bb0\u6570\u636e\u7f16\u7801\u865a\u5047\u76f8\u5173\u6027\u5e76\u4ea7\u751f\u566a\u58f0\u4f2a\u6807\u7b7e", "method": "\u63d0\u51fa\u5305\u542b\u4e24\u4e2a\u6a21\u5757\u7684\u65b0\u65b9\u6cd5\uff1a1) \u635f\u5931\u9510\u5ea6\u60e9\u7f5a(LSP)\uff1a\u901a\u8fc7\u6700\u5c0f\u5316\u6a21\u578b\u7684\u6700\u574f\u60c5\u51b5\u635f\u5931\u9510\u5ea6\uff0c\u589e\u5f3a\u6a21\u578b\u53c2\u6570\u5bf9\u5c0f\u6270\u52a8\u7684\u9c81\u68d2\u6027\uff0c\u6291\u5236\u7410\u788e\u7279\u5f81\u7f16\u7801\uff1b2) \u52a8\u6001\u951a\u70b9\u9009\u62e9(DAS)\uff1a\u57fa\u4e8eKNN\u5bc6\u5ea6\u548c\u7c7b\u522b\u6982\u7387\u4e3a\u672a\u77e5\u7c7b\u9009\u62e9\u4ee3\u8868\u6027\u6837\u672c\u5e76\u5206\u914d\u786c\u4f2a\u6807\u7b7e\uff0c\u7f13\u89e3\u5df2\u77e5\u7c7b\u548c\u672a\u77e5\u7c7b\u4e4b\u95f4\u7684\u7f6e\u4fe1\u5ea6\u5dee\u5f02", "result": "\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u51cf\u8f7b\u4f2a\u6807\u7b7e\u566a\u58f0\uff0c\u5728\u591a\u4e2aGCD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u7ed3\u679c", "conclusion": "\u63d0\u51fa\u7684LSP\u548cDAS\u6a21\u5757\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4f2a\u6807\u7b7e\u566a\u58f0\u95ee\u9898\uff0c\u63d0\u5347\u5e7f\u4e49\u7c7b\u522b\u53d1\u73b0\u7684\u805a\u7c7b\u51c6\u786e\u6027"}}
{"id": "2512.12930", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2512.12930", "abs": "https://arxiv.org/abs/2512.12930", "authors": ["Yuseon Choi", "Sangjin Kim", "Jungjun Oh", "Byeongcheol Kim", "Hoi-Jun Yoo"], "title": "SeVeDo: A Heterogeneous Transformer Accelerator for Low-Bit Inference via Hierarchical Group Quantization and SVD-Guided Mixed Precision", "comment": null, "summary": "Low-bit quantization is a promising technique for efficient transformer inference by reducing computational and memory overhead. However, aggressive bitwidth reduction remains challenging due to activation outliers, leading to accuracy degradation. Existing methods, such as outlier-handling and group quantization, achieve high accuracy but incur substantial energy consumption. To address this, we propose SeVeDo, an energy-efficient SVD-based heterogeneous accelerator that structurally separates outlier-sensitive components into a high-precision low-rank path, while the remaining computations are executed in a low-bit residual datapath with group quantization. To further enhance efficiency, Hierarchical Group Quantization (HGQ) combines coarse-grained floating-point scaling with fine-grained shifting, effectively reducing dequantization cost. Also, SVD-guided mixed precision (SVD-MP) statically allocates higher bitwidths to precision-sensitive components identified through low-rank decomposition, thereby minimizing floating-point operation cost. Experimental results show that SeVeDo achieves a peak energy efficiency of 13.8TOPS/W, surpassing conventional designs, with 12.7TOPS/W on ViT-Base and 13.4TOPS/W on Llama2-7B benchmarks.", "AI": {"tldr": "SeVeDo\u662f\u4e00\u79cd\u57fa\u4e8eSVD\u7684\u5f02\u6784\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u5c06\u5f02\u5e38\u503c\u654f\u611f\u7ec4\u4ef6\u5206\u79bb\u5230\u9ad8\u7cbe\u5ea6\u4f4e\u79e9\u8def\u5f84\uff0c\u5176\u4f59\u8ba1\u7b97\u5728\u4f4e\u6bd4\u7279\u6b8b\u5dee\u6570\u636e\u8def\u5f84\u4e2d\u6267\u884c\uff0c\u7ed3\u5408\u5206\u5c42\u7ec4\u91cf\u5316\u548cSVD\u5f15\u5bfc\u7684\u6df7\u5408\u7cbe\u5ea6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684Transformer\u63a8\u7406\u3002", "motivation": "\u4f4e\u6bd4\u7279\u91cf\u5316\u662f\u63d0\u9ad8Transformer\u63a8\u7406\u6548\u7387\u7684\u6709\u6548\u6280\u672f\uff0c\u4f46\u6fc0\u8fdb\u7684\u6bd4\u7279\u5bbd\u5ea6\u51cf\u5c11\u4f1a\u56e0\u6fc0\u6d3b\u5f02\u5e38\u503c\u5bfc\u81f4\u7cbe\u5ea6\u4e0b\u964d\u3002\u73b0\u6709\u65b9\u6cd5\u5982\u5f02\u5e38\u503c\u5904\u7406\u548c\u7ec4\u91cf\u5316\u867d\u7136\u80fd\u8fbe\u5230\u9ad8\u7cbe\u5ea6\uff0c\u4f46\u80fd\u8017\u8f83\u9ad8\u3002\u9700\u8981\u8bbe\u8ba1\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u7cbe\u5ea6\u53c8\u8282\u80fd\u7684\u91cf\u5316\u65b9\u6848\u3002", "method": "\u63d0\u51faSeVeDo\u52a0\u901f\u5668\uff1a1) \u57fa\u4e8eSVD\u7684\u7ed3\u6784\u5206\u79bb\uff0c\u5c06\u5f02\u5e38\u503c\u654f\u611f\u7ec4\u4ef6\u5206\u914d\u5230\u9ad8\u7cbe\u5ea6\u4f4e\u79e9\u8def\u5f84\uff1b2) \u5176\u4f59\u8ba1\u7b97\u5728\u4f4e\u6bd4\u7279\u6b8b\u5dee\u6570\u636e\u8def\u5f84\u4e2d\u6267\u884c\uff0c\u91c7\u7528\u7ec4\u91cf\u5316\uff1b3) \u5206\u5c42\u7ec4\u91cf\u5316(HGQ)\u7ed3\u5408\u7c97\u7c92\u5ea6\u6d6e\u70b9\u7f29\u653e\u548c\u7ec6\u7c92\u5ea6\u79fb\u4f4d\uff0c\u964d\u4f4e\u53cd\u91cf\u5316\u6210\u672c\uff1b4) SVD\u5f15\u5bfc\u7684\u6df7\u5408\u7cbe\u5ea6(SVD-MP)\u901a\u8fc7\u4f4e\u79e9\u5206\u89e3\u8bc6\u522b\u7cbe\u5ea6\u654f\u611f\u7ec4\u4ef6\uff0c\u9759\u6001\u5206\u914d\u66f4\u9ad8\u6bd4\u7279\u5bbd\u5ea6\u3002", "result": "SeVeDo\u5b9e\u73b0\u4e86\u5cf0\u503c\u80fd\u654813.8TOPS/W\uff0c\u5728ViT-Base\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523012.7TOPS/W\uff0c\u5728Llama2-7B\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523013.4TOPS/W\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u8bbe\u8ba1\u3002", "conclusion": "SeVeDo\u901a\u8fc7SVD\u7ed3\u6784\u5206\u79bb\u3001\u5206\u5c42\u7ec4\u91cf\u5316\u548c\u6df7\u5408\u7cbe\u5ea6\u5206\u914d\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u4f4e\u6bd4\u7279\u91cf\u5316\u4e2d\u7684\u7cbe\u5ea6-\u80fd\u6548\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u9ad8\u6548\u7684Transformer\u63a8\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u786c\u4ef6\u52a0\u901f\u65b9\u6848\u3002"}}
{"id": "2512.12947", "categories": ["cs.LG", "stat.AP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.12947", "abs": "https://arxiv.org/abs/2512.12947", "authors": ["Nischal Subedi", "Ember Kerstetter", "Winnie Li", "Silo Murphy"], "title": "Understanding When Graph Convolutional Networks Help: A Diagnostic Study on Label Scarcity and Structural Properties", "comment": "10 pages, 8 tables,5 figures", "summary": "Graph Convolutional Networks (GCNs) have become a standard approach for semi-supervised node classification, yet practitioners lack clear guidance on when GCNs provide meaningful improvements over simpler baselines. We present a diagnostic study using the Amazon Computers co-purchase data to understand when and why GCNs help. Through systematic experiments with simulated label scarcity, feature ablation, and per-class analysis, we find that GCN performance depends critically on the interaction between graph homophily and feature quality. GCNs provide the largest gains under extreme label scarcity, where they leverage neighborhood structure to compensate for limited supervision. Surprisingly, GCNs can match their original performance even when node features are replaced with random noise, suggesting that structure alone carries sufficient signal on highly homophilous graphs. However, GCNs hurt performance when homophily is low and features are already strong, as noisy neighbors corrupt good predictions. Our quadrant analysis reveals that GCNs help in three of four conditions and only hurt when low homophily meets strong features. These findings offer practical guidance for practitioners deciding whether to adopt graph-based methods.", "AI": {"tldr": "GCNs\u5728\u6807\u7b7e\u6781\u5ea6\u7a00\u7f3a\u65f6\u8868\u73b0\u6700\u4f73\uff0c\u5229\u7528\u56fe\u7ed3\u6784\u8865\u507f\u76d1\u7763\u4e0d\u8db3\uff1b\u5728\u9ad8\u5ea6\u540c\u8d28\u5316\u56fe\u4e0a\uff0c\u5373\u4f7f\u7279\u5f81\u88ab\u968f\u673a\u566a\u58f0\u66ff\u4ee3\uff0cGCNs\u4ecd\u80fd\u4fdd\u6301\u6027\u80fd\uff1b\u4f46\u5728\u4f4e\u540c\u8d28\u5316\u4e14\u7279\u5f81\u5f3a\u65f6\uff0cGCNs\u4f1a\u635f\u5bb3\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5b9e\u8df5\u8005\u7f3a\u4e4f\u5173\u4e8eGCNs\u4f55\u65f6\u6bd4\u7b80\u5355\u57fa\u7ebf\u63d0\u4f9b\u6709\u610f\u4e49\u6539\u8fdb\u7684\u660e\u786e\u6307\u5bfc\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u8bca\u65ad\u7814\u7a76\u7406\u89e3GCNs\u4f55\u65f6\u4ee5\u53ca\u4e3a\u4f55\u6709\u6548\u3002", "method": "\u4f7f\u7528Amazon Computers\u5171\u8d2d\u6570\u636e\u8fdb\u884c\u8bca\u65ad\u7814\u7a76\uff0c\u901a\u8fc7\u7cfb\u7edf\u5b9e\u9a8c\u5305\u62ec\u6a21\u62df\u6807\u7b7e\u7a00\u7f3a\u3001\u7279\u5f81\u6d88\u878d\u548c\u6309\u7c7b\u522b\u5206\u6790\uff0c\u7814\u7a76\u56fe\u540c\u8d28\u5316\u4e0e\u7279\u5f81\u8d28\u91cf\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "result": "GCNs\u5728\u6781\u7aef\u6807\u7b7e\u7a00\u7f3a\u65f6\u63d0\u4f9b\u6700\u5927\u589e\u76ca\uff1b\u5728\u9ad8\u5ea6\u540c\u8d28\u5316\u56fe\u4e0a\uff0c\u5373\u4f7f\u8282\u70b9\u7279\u5f81\u88ab\u968f\u673a\u566a\u58f0\u66ff\u4ee3\uff0cGCNs\u4ecd\u80fd\u5339\u914d\u539f\u59cb\u6027\u80fd\uff1b\u4f46\u5728\u4f4e\u540c\u8d28\u5316\u4e14\u7279\u5f81\u5f3a\u65f6\uff0cGCNs\u4f1a\u635f\u5bb3\u6027\u80fd\u3002\u8c61\u9650\u5206\u6790\u663e\u793aGCNs\u5728\u56db\u79cd\u6761\u4ef6\u4e2d\u7684\u4e09\u79cd\u6709\u5e2e\u52a9\uff0c\u4ec5\u5728\u4f4e\u540c\u8d28\u5316\u9047\u5230\u5f3a\u7279\u5f81\u65f6\u6709\u5bb3\u3002", "conclusion": "GCNs\u7684\u6027\u80fd\u5173\u952e\u53d6\u51b3\u4e8e\u56fe\u540c\u8d28\u5316\u4e0e\u7279\u5f81\u8d28\u91cf\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u5b9e\u8df5\u8005\u51b3\u5b9a\u662f\u5426\u91c7\u7528\u57fa\u4e8e\u56fe\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2512.12936", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.12936", "abs": "https://arxiv.org/abs/2512.12936", "authors": ["Tiange Zhang", "Xiandong Meng", "Siwei Ma"], "title": "Content Adaptive based Motion Alignment Framework for Learned Video Compression", "comment": "Accepted to Data Compression Conference (DCC) 2026 as a poster paper", "summary": "Recent advances in end-to-end video compression have shown promising results owing to their unified end-to-end learning optimization. However, such generalized frameworks often lack content-specific adaptation, leading to suboptimal compression performance. To address this, this paper proposes a content adaptive based motion alignment framework that improves performance by adapting encoding strategies to diverse content characteristics. Specifically, we first introduce a two-stage flow-guided deformable warping mechanism that refines motion compensation with coarse-to-fine offset prediction and mask modulation, enabling precise feature alignment. Second, we propose a multi-reference quality aware strategy that adjusts distortion weights based on reference quality, and applies it to hierarchical training to reduce error propagation. Third, we integrate a training-free module that downsamples frames by motion magnitude and resolution to obtain smooth motion estimation. Experimental results on standard test datasets demonstrate that our framework CAMA achieves significant improvements over state-of-the-art Neural Video Compression models, achieving a 24.95% BD-rate (PSNR) savings over our baseline model DCVC-TCM, while also outperforming reproduced DCVC-DC and traditional codec HM-16.25.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faCAMA\u6846\u67b6\uff0c\u901a\u8fc7\u5185\u5bb9\u81ea\u9002\u5e94\u8fd0\u52a8\u5bf9\u9f50\u63d0\u5347\u7aef\u5230\u7aef\u89c6\u9891\u538b\u7f29\u6027\u80fd\uff0c\u5305\u62ec\u4e24\u9636\u6bb5\u6d41\u5f15\u5bfc\u53ef\u53d8\u5f62\u626d\u66f2\u3001\u591a\u53c2\u8003\u8d28\u91cf\u611f\u77e5\u7b56\u7565\u548c\u8bad\u7ec3\u514d\u8d39\u6a21\u5757\uff0c\u5728\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u5b9e\u73b024.95% BD-rate\u8282\u7701\u3002", "motivation": "\u5f53\u524d\u7aef\u5230\u7aef\u89c6\u9891\u538b\u7f29\u6846\u67b6\u7f3a\u4e4f\u5185\u5bb9\u7279\u5b9a\u9002\u5e94\u6027\uff0c\u5bfc\u81f4\u538b\u7f29\u6027\u80fd\u4e0d\u7406\u60f3\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6839\u636e\u5185\u5bb9\u7279\u6027\u81ea\u9002\u5e94\u8c03\u6574\u7f16\u7801\u7b56\u7565\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u538b\u7f29\u6548\u7387\u3002", "method": "1. \u4e24\u9636\u6bb5\u6d41\u5f15\u5bfc\u53ef\u53d8\u5f62\u626d\u66f2\u673a\u5236\uff1a\u901a\u8fc7\u7c97\u5230\u7ec6\u7684\u504f\u79fb\u9884\u6d4b\u548c\u63a9\u7801\u8c03\u5236\u6765\u7cbe\u5316\u8fd0\u52a8\u8865\u507f\uff0c\u5b9e\u73b0\u7cbe\u786e\u7279\u5f81\u5bf9\u9f50\n2. \u591a\u53c2\u8003\u8d28\u91cf\u611f\u77e5\u7b56\u7565\uff1a\u57fa\u4e8e\u53c2\u8003\u8d28\u91cf\u8c03\u6574\u5931\u771f\u6743\u91cd\uff0c\u5e76\u5e94\u7528\u4e8e\u5206\u5c42\u8bad\u7ec3\u4ee5\u51cf\u5c11\u8bef\u5dee\u4f20\u64ad\n3. \u8bad\u7ec3\u514d\u8d39\u6a21\u5757\uff1a\u6839\u636e\u8fd0\u52a8\u5e45\u5ea6\u548c\u5206\u8fa8\u7387\u4e0b\u91c7\u6837\u5e27\u4ee5\u83b7\u5f97\u5e73\u6ed1\u8fd0\u52a8\u4f30\u8ba1", "result": "\u5728\u6807\u51c6\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\uff0cCAMA\u6846\u67b6\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u795e\u7ecf\u89c6\u9891\u538b\u7f29\u6a21\u578b\u53d6\u5f97\u663e\u8457\u6539\u8fdb\uff0c\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578bDCVC-TCM\u5b9e\u73b024.95% BD-rate\uff08PSNR\uff09\u8282\u7701\uff0c\u540c\u65f6\u4f18\u4e8e\u590d\u73b0\u7684DCVC-DC\u548c\u4f20\u7edf\u7f16\u89e3\u7801\u5668HM-16.25\u3002", "conclusion": "\u63d0\u51fa\u7684\u5185\u5bb9\u81ea\u9002\u5e94\u8fd0\u52a8\u5bf9\u9f50\u6846\u67b6\u901a\u8fc7\u9002\u5e94\u4e0d\u540c\u5185\u5bb9\u7279\u6027\u6709\u6548\u63d0\u5347\u4e86\u7aef\u5230\u7aef\u89c6\u9891\u538b\u7f29\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5185\u5bb9\u81ea\u9002\u5e94\u7b56\u7565\u5728\u89c6\u9891\u538b\u7f29\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2512.12981", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.12981", "abs": "https://arxiv.org/abs/2512.12981", "authors": ["Jonathan Wensh\u00f8j", "Tong Chen", "Bob Pepin", "Raghavendra Selvan"], "title": "CoDeQ: End-to-End Joint Model Compression with Dead-Zone Quantizer for High-Sparsity and Low-Precision Networks", "comment": "Source code at https://github.com/saintslab/CoDeQ", "summary": "While joint pruning--quantization is theoretically superior to sequential application, current joint methods rely on auxiliary procedures outside the training loop for finding compression parameters. This reliance adds engineering complexity and hyperparameter tuning, while also lacking a direct data-driven gradient signal, which might result in sub-optimal compression. In this paper, we introduce CoDeQ, a simple, fully differentiable method for joint pruning--quantization. Our approach builds on a key observation: the dead-zone of a scalar quantizer is equivalent to magnitude pruning, and can be used to induce sparsity directly within the quantization operator. Concretely, we parameterize the dead-zone width and learn it via backpropagation, alongside the quantization parameters. This design provides explicit control of sparsity, regularized by a single global hyperparameter, while decoupling sparsity selection from bit-width selection. The result is a method for Compression with Dead-zone Quantizer (CoDeQ) that supports both fixed-precision and mixed-precision quantization (controlled by an optional second hyperparameter). It simultaneously determines the sparsity pattern and quantization parameters in a single end-to-end optimization. Consequently, CoDeQ does not require any auxiliary procedures, making the method architecture-agnostic and straightforward to implement. On ImageNet with ResNet-18, CoDeQ reduces bit operations to ~5% while maintaining close to full precision accuracy in both fixed and mixed-precision regimes.", "AI": {"tldr": "CoDeQ\u662f\u4e00\u79cd\u5b8c\u5168\u53ef\u5fae\u7684\u8054\u5408\u526a\u679d-\u91cf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u53c2\u6570\u5316\u91cf\u5316\u5668\u7684\u6b7b\u533a\u5bbd\u5ea6\u6765\u76f4\u63a5\u8bf1\u5bfc\u7a00\u758f\u6027\uff0c\u65e0\u9700\u5916\u90e8\u8f85\u52a9\u8fc7\u7a0b\u5373\u53ef\u5728\u5355\u6b21\u7aef\u5230\u7aef\u4f18\u5316\u4e2d\u540c\u65f6\u786e\u5b9a\u7a00\u758f\u6a21\u5f0f\u548c\u91cf\u5316\u53c2\u6570\u3002", "motivation": "\u73b0\u6709\u7684\u8054\u5408\u526a\u679d-\u91cf\u5316\u65b9\u6cd5\u4f9d\u8d56\u8bad\u7ec3\u5faa\u73af\u5916\u7684\u8f85\u52a9\u7a0b\u5e8f\u6765\u786e\u5b9a\u538b\u7f29\u53c2\u6570\uff0c\u8fd9\u589e\u52a0\u4e86\u5de5\u7a0b\u590d\u6742\u6027\u3001\u9700\u8981\u8d85\u53c2\u6570\u8c03\u4f18\uff0c\u4e14\u7f3a\u4e4f\u76f4\u63a5\u7684\u6570\u636e\u9a71\u52a8\u68af\u5ea6\u4fe1\u53f7\uff0c\u53ef\u80fd\u5bfc\u81f4\u6b21\u4f18\u538b\u7f29\u3002", "method": "\u57fa\u4e8e\u91cf\u5316\u5668\u6b7b\u533a\u7b49\u4ef7\u4e8e\u5e45\u5ea6\u526a\u679d\u7684\u5173\u952e\u89c2\u5bdf\uff0c\u53c2\u6570\u5316\u6b7b\u533a\u5bbd\u5ea6\u5e76\u901a\u8fc7\u53cd\u5411\u4f20\u64ad\u5b66\u4e60\uff0c\u540c\u65f6\u5b66\u4e60\u91cf\u5316\u53c2\u6570\u3002\u8be5\u65b9\u6cd5\u63d0\u4f9b\u7a00\u758f\u6027\u7684\u663e\u5f0f\u63a7\u5236\uff0c\u901a\u8fc7\u5355\u4e2a\u5168\u5c40\u8d85\u53c2\u6570\u6b63\u5219\u5316\uff0c\u5e76\u5c06\u7a00\u758f\u6027\u9009\u62e9\u4e0e\u4f4d\u5bbd\u9009\u62e9\u89e3\u8026\u3002", "result": "\u5728ImageNet\u4e0a\u4f7f\u7528ResNet-18\uff0cCoDeQ\u5c06\u6bd4\u7279\u8fd0\u7b97\u51cf\u5c11\u5230\u7ea65%\uff0c\u540c\u65f6\u5728\u56fa\u5b9a\u7cbe\u5ea6\u548c\u6df7\u5408\u7cbe\u5ea6\u673a\u5236\u4e0b\u4fdd\u6301\u63a5\u8fd1\u5168\u7cbe\u5ea6\u51c6\u786e\u7387\u3002", "conclusion": "CoDeQ\u662f\u4e00\u79cd\u7b80\u5355\u3001\u5b8c\u5168\u53ef\u5fae\u7684\u8054\u5408\u526a\u679d-\u91cf\u5316\u65b9\u6cd5\uff0c\u65e0\u9700\u8f85\u52a9\u7a0b\u5e8f\uff0c\u67b6\u6784\u65e0\u5173\u4e14\u6613\u4e8e\u5b9e\u73b0\uff0c\u5728\u5355\u6b21\u7aef\u5230\u7aef\u4f18\u5316\u4e2d\u540c\u65f6\u786e\u5b9a\u7a00\u758f\u6a21\u5f0f\u548c\u91cf\u5316\u53c2\u6570\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6a21\u578b\u538b\u7f29\u3002"}}
{"id": "2512.12963", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12963", "abs": "https://arxiv.org/abs/2512.12963", "authors": ["Luan Thanh Trinh", "Kenji Doi", "Atsuki Osanai"], "title": "SCAdapter: Content-Style Disentanglement for Diffusion Style Transfer", "comment": "Accepted to WACV 2026", "summary": "Diffusion models have emerged as the leading approach for style transfer, yet they struggle with photo-realistic transfers, often producing painting-like results or missing detailed stylistic elements. Current methods inadequately address unwanted influence from original content styles and style reference content features. We introduce SCAdapter, a novel technique leveraging CLIP image space to effectively separate and integrate content and style features. Our key innovation systematically extracts pure content from content images and style elements from style references, ensuring authentic transfers. This approach is enhanced through three components: Controllable Style Adaptive Instance Normalization (CSAdaIN) for precise multi-style blending, KVS Injection for targeted style integration, and a style transfer consistency objective maintaining process coherence. Comprehensive experiments demonstrate SCAdapter significantly outperforms state-of-the-art methods in both conventional and diffusion-based baselines. By eliminating DDIM inversion and inference-stage optimization, our method achieves at least $2\\times$ faster inference than other diffusion-based approaches, making it both more effective and efficient for practical applications.", "AI": {"tldr": "SCAdapter\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b0\u578b\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\uff0c\u901a\u8fc7CLIP\u56fe\u50cf\u7a7a\u95f4\u6709\u6548\u5206\u79bb\u548c\u6574\u5408\u5185\u5bb9\u4e0e\u98ce\u683c\u7279\u5f81\uff0c\u5b9e\u73b0\u66f4\u903c\u771f\u7684\u98ce\u683c\u8fc1\u79fb\uff0c\u540c\u65f6\u63a8\u7406\u901f\u5ea6\u6bd4\u5176\u4ed6\u6269\u6563\u65b9\u6cd5\u5feb2\u500d\u4ee5\u4e0a\u3002", "motivation": "\u5f53\u524d\u6269\u6563\u6a21\u578b\u5728\u98ce\u683c\u8fc1\u79fb\u4e2d\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a1\uff09\u96be\u4ee5\u5b9e\u73b0\u7167\u7247\u7ea7\u903c\u771f\u7684\u8fc1\u79fb\uff0c\u5e38\u4ea7\u751f\u7c7b\u4f3c\u7ed8\u753b\u7684\u6548\u679c\u6216\u9057\u6f0f\u7ec6\u8282\u98ce\u683c\u5143\u7d20\uff1b2\uff09\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5145\u5206\u5904\u7406\u539f\u59cb\u5185\u5bb9\u98ce\u683c\u548c\u98ce\u683c\u53c2\u8003\u5185\u5bb9\u7279\u5f81\u7684\u4e0d\u5fc5\u8981\u5f71\u54cd\u3002", "method": "SCAdapter\u5229\u7528CLIP\u56fe\u50cf\u7a7a\u95f4\u6709\u6548\u5206\u79bb\u548c\u6574\u5408\u5185\u5bb9\u4e0e\u98ce\u683c\u7279\u5f81\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u53ef\u63a7\u98ce\u683c\u81ea\u9002\u5e94\u5b9e\u4f8b\u5f52\u4e00\u5316\uff08CSAdaIN\uff09\u7528\u4e8e\u7cbe\u786e\u7684\u591a\u98ce\u683c\u6df7\u5408\uff1b2\uff09KVS\u6ce8\u5165\u7528\u4e8e\u76ee\u6807\u98ce\u683c\u96c6\u6210\uff1b3\uff09\u98ce\u683c\u8fc1\u79fb\u4e00\u81f4\u6027\u76ee\u6807\u4fdd\u6301\u8fc7\u7a0b\u8fde\u8d2f\u6027\u3002", "result": "SCAdapter\u5728\u4f20\u7edf\u548c\u57fa\u4e8e\u6269\u6563\u7684\u57fa\u7ebf\u65b9\u6cd5\u4e2d\u90fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d88\u9664DDIM\u53cd\u8f6c\u548c\u63a8\u7406\u9636\u6bb5\u4f18\u5316\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4\u5176\u4ed6\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u81f3\u5c11\u5feb2\u500d\u3002", "conclusion": "SCAdapter\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u6709\u6548\u548c\u9ad8\u6548\u7684\u98ce\u683c\u8fc1\u79fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u7cfb\u7edf\u63d0\u53d6\u7eaf\u5185\u5bb9\u548c\u98ce\u683c\u5143\u7d20\uff0c\u7ed3\u5408\u521b\u65b0\u7684\u7ec4\u4ef6\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u66f4\u903c\u771f\u7684\u98ce\u683c\u8fc1\u79fb\u6548\u679c\uff0c\u540c\u65f6\u5927\u5e45\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2512.13010", "categories": ["cs.LG", "q-bio.TO"], "pdf": "https://arxiv.org/pdf/2512.13010", "abs": "https://arxiv.org/abs/2512.13010", "authors": ["Hassan Iftikhar", "Rizwan Ahmad", "Arunark Kolipaka"], "title": "Deep Learning-Driven Inversion Framework for Shear Modulus Estimation in Magnetic Resonance Elastography (DIME)", "comment": null, "summary": "The Multimodal Direct Inversion (MMDI) algorithm is widely used in Magnetic Resonance Elastography (MRE) to estimate tissue shear stiffness. However, MMDI relies on the Helmholtz equation, which assumes wave propagation in a uniform, homogeneous, and infinite medium. Furthermore, the use of the Laplacian operator makes MMDI highly sensitive to noise, which compromises the accuracy and reliability of stiffness estimates. In this study, we propose the Deep-Learning driven Inversion Framework for Shear Modulus Estimation in MRE (DIME), aimed at enhancing the robustness of inversion. DIME is trained on the displacement fields-stiffness maps pair generated through Finite Element Modelling (FEM) simulations. To capture local wave behavior and improve robustness to global image variations, DIME is trained on small image patches. We first validated DIME using homogeneous and heterogeneous datasets simulated with FEM, where DIME produced stiffness maps with low inter-pixel variability, accurate boundary delineation, and higher correlation with ground truth (GT) compared to MMDI. Next, DIME was evaluated in a realistic anatomy-informed simulated liver dataset with known GT and compared directly to MMDI. DIME reproduced ground-truth stiffness patterns with high fidelity (r = 0.99, R^2 = 0.98), while MMDI showed greater underestimation. After validating DIME on synthetic data, we tested the model in in vivo liver MRE data from eight healthy and seven fibrotic subjects. DIME preserved physiologically consistent stiffness patterns and closely matched MMDI, which showed directional bias. Overall, DIME showed higher correlation with ground truth and visually similar stiffness patterns, whereas MMDI displayed a larger bias that can potentially be attributed to directional filtering. These preliminary results highlight the feasibility of DIME for clinical applications in MRE.", "AI": {"tldr": "DIME\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u5728\u78c1\u5171\u632f\u5f39\u6027\u6210\u50cf\u4e2d\u6bd4\u4f20\u7edfMMDI\u7b97\u6cd5\u66f4\u51c6\u786e\u5730\u4f30\u8ba1\u7ec4\u7ec7\u526a\u5207\u521a\u5ea6\uff0c\u5bf9\u566a\u58f0\u66f4\u9c81\u68d2\uff0c\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u4e2d\u90fd\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u4f20\u7edfMMDI\u7b97\u6cd5\u57fa\u4e8e\u4ea5\u59c6\u970d\u5179\u65b9\u7a0b\uff0c\u5047\u8bbe\u5747\u5300\u3001\u65e0\u9650\u4ecb\u8d28\uff0c\u4e14\u62c9\u666e\u62c9\u65af\u7b97\u5b50\u5bf9\u566a\u58f0\u654f\u611f\uff0c\u5bfc\u81f4\u521a\u5ea6\u4f30\u8ba1\u4e0d\u51c6\u786e\u4e0d\u53ef\u9760\u3002", "method": "\u63d0\u51faDIME\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u4f7f\u7528\u6709\u9650\u5143\u6a21\u62df\u751f\u6210\u7684\u4f4d\u79fb\u573a-\u521a\u5ea6\u56fe\u5bf9\u8fdb\u884c\u8bad\u7ec3\uff0c\u91c7\u7528\u5c0f\u56fe\u50cf\u5757\u6355\u6349\u5c40\u90e8\u6ce2\u884c\u4e3a\uff0c\u63d0\u9ad8\u5bf9\u5168\u5c40\u56fe\u50cf\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u4e2d\uff0cDIME\u6bd4MMDI\u4ea7\u751f\u66f4\u4f4e\u7684\u50cf\u7d20\u95f4\u53d8\u5f02\u6027\u3001\u66f4\u51c6\u786e\u7684\u8fb9\u754c\u63cf\u7ed8\u548c\u66f4\u9ad8\u7684\u5730\u9762\u771f\u503c\u76f8\u5173\u6027\uff1b\u5728\u771f\u5b9e\u809d\u810f\u6570\u636e\u4e2d\uff0cDIME\u4fdd\u6301\u751f\u7406\u4e00\u81f4\u7684\u521a\u5ea6\u6a21\u5f0f\uff0c\u800cMMDI\u663e\u793a\u65b9\u5411\u6027\u504f\u5dee\u3002", "conclusion": "DIME\u5728\u78c1\u5171\u632f\u5f39\u6027\u6210\u50cf\u4e2d\u663e\u793a\u51fa\u6bd4\u4f20\u7edfMMDI\u65b9\u6cd5\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u4e3a\u4e34\u5e8a\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u6027\u3002"}}
{"id": "2512.12977", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12977", "abs": "https://arxiv.org/abs/2512.12977", "authors": ["Shengling Qin", "Hao Yu", "Chenxin Wu", "Zheng Li", "Yizhong Cao", "Zhengyang Zhuge", "Yuxin Zhou", "Wentao Yao", "Yi Zhang", "Zhengheng Wang", "Shuai Bai", "Jianwei Zhang", "Junyang Lin"], "title": "VLCache: Computing 2% Vision Tokens and Reusing 98% for Vision-Language Inference", "comment": null, "summary": "This paper presents VLCache, a cache reuse framework that exploits both Key-Value (KV) cache and encoder cache from prior multimodal inputs to eliminate costly recomputation when the same multimodal inputs recur. Unlike previous heuristic approaches, we formally identify the cumulative reuse error effect and demonstrate how to minimize the non-prefix cache reuse error effectively. We further analyze the varying importance of model layers and propose a dynamic, layer-aware recomputation strategy to balance accuracy and efficiency. Experimental results show that VLCache achieves an accuracy on par with full recomputation, while requiring only 2-5% of the tokens to compute, yielding 1.2x-16x TTFT speedups. The proposed VLCache pipeline has been integrated into SGLang, enabling significantly faster inference in practical deployments.", "AI": {"tldr": "VLCache\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u7f13\u5b58\u91cd\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u590d\u7528KV\u7f13\u5b58\u548c\u7f16\u7801\u5668\u7f13\u5b58\u6765\u907f\u514d\u91cd\u590d\u8ba1\u7b97\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u63a8\u7406\u901f\u5ea6", "motivation": "\u5f53\u76f8\u540c\u7684\u591a\u6a21\u6001\u8f93\u5165\u91cd\u590d\u51fa\u73b0\u65f6\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u7684\u91cd\u65b0\u8ba1\u7b97\uff0c\u8fd9\u5f71\u54cd\u4e86\u63a8\u7406\u6548\u7387\u3002\u4f5c\u8005\u65e8\u5728\u901a\u8fc7\u7f13\u5b58\u91cd\u7528\u673a\u5236\u6765\u6d88\u9664\u8fd9\u79cd\u91cd\u590d\u8ba1\u7b97\u7684\u5f00\u9500", "method": "1) \u6b63\u5f0f\u8bc6\u522b\u7d2f\u79ef\u91cd\u7528\u8bef\u5dee\u6548\u5e94\u5e76\u6700\u5c0f\u5316\u975e\u524d\u7f00\u7f13\u5b58\u91cd\u7528\u8bef\u5dee\uff1b2) \u5206\u6790\u6a21\u578b\u5c42\u7684\u91cd\u8981\u6027\u5dee\u5f02\uff0c\u63d0\u51fa\u52a8\u6001\u3001\u5c42\u611f\u77e5\u7684\u91cd\u65b0\u8ba1\u7b97\u7b56\u7565\u6765\u5e73\u8861\u51c6\u786e\u6027\u548c\u6548\u7387", "result": "VLCache\u5728\u4fdd\u6301\u4e0e\u5b8c\u5168\u91cd\u65b0\u8ba1\u7b97\u76f8\u5f53\u7684\u51c6\u786e\u6027\u7684\u540c\u65f6\uff0c\u4ec5\u9700\u8ba1\u7b972-5%\u7684token\uff0c\u5b9e\u73b0\u4e861.2x-16x\u7684TTFT\u52a0\u901f\u3002\u8be5\u6846\u67b6\u5df2\u96c6\u6210\u5230SGLang\u4e2d\uff0c\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u901f\u5ea6", "conclusion": "VLCache\u901a\u8fc7\u6709\u6548\u7684\u7f13\u5b58\u91cd\u7528\u673a\u5236\uff0c\u5728\u591a\u6a21\u6001\u63a8\u7406\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6548\u7387\u63d0\u5347\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u52a0\u901f\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.13033", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13033", "abs": "https://arxiv.org/abs/2512.13033", "authors": ["Jongwook Kim", "Sangheon Yun", "Sukjin Yoon"], "title": "Scaling Bidirectional Spans and Span Violations in Attention Mechanism", "comment": null, "summary": "The canonical $O(N^2)$ Transformer remains the empirical performance frontier in sequence modeling, and its training can be further optimized by addressing geometric inefficiency. We propose an optimization framework that leverages an asymmetric projection to decompose the backward-pass gradients into parallel spans and orthogonal violations, while keeping the canonical forward-pass $QKV$ structure intact. Through consistent experimental validation across various decomposition and projection setups, we provide strong theoretical evidence: the standard attention gradient is suboptimal. We demonstrated that selectively scaling these components, focusing primarily on $0^{th}$ order bidirectional parallel spans, yields the most effective learning signal. On the limited WikiText-2 dataset, and using a crude configuration, this method achieved a $0.56\\%$ reduction in validation loss, confirming the framework's fundamental validity and suggesting significant potential gains on larger datasets and deeper training regimes", "AI": {"tldr": "\u63d0\u51fa\u4f18\u5316Transformer\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u975e\u5bf9\u79f0\u6295\u5f71\u5206\u89e3\u53cd\u5411\u4f20\u64ad\u68af\u5ea6\uff0c\u4fdd\u6301\u524d\u5411QKV\u7ed3\u6784\u4e0d\u53d8\uff0c\u5728WikiText-2\u4e0a\u9a8c\u8bc1\u635f\u5931\u964d\u4f4e0.56%", "motivation": "\u6807\u51c6Transformer\u7684O(N\u00b2)\u590d\u6742\u5ea6\u4ecd\u662f\u5e8f\u5217\u5efa\u6a21\u7684\u5b9e\u8bc1\u6027\u80fd\u524d\u6cbf\uff0c\u4f46\u5176\u8bad\u7ec3\u5b58\u5728\u51e0\u4f55\u6548\u7387\u95ee\u9898\u3002\u6807\u51c6\u6ce8\u610f\u529b\u68af\u5ea6\u53ef\u80fd\u4e0d\u662f\u6700\u4f18\u7684\uff0c\u9700\u8981\u4f18\u5316\u8bad\u7ec3\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u4f18\u5316\u6846\u67b6\uff0c\u4f7f\u7528\u975e\u5bf9\u79f0\u6295\u5f71\u5c06\u53cd\u5411\u4f20\u64ad\u68af\u5ea6\u5206\u89e3\u4e3a\u5e73\u884c\u8de8\u5ea6\u548c\u6b63\u4ea4\u8fdd\u89c4\u5206\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u6807\u51c6\u524d\u5411QKV\u7ed3\u6784\u4e0d\u53d8\u3002\u9009\u62e9\u6027\u5730\u7f29\u653e\u8fd9\u4e9b\u5206\u91cf\uff0c\u4e3b\u8981\u5173\u6ce80\u9636\u53cc\u5411\u5e73\u884c\u8de8\u5ea6\u3002", "result": "\u5728\u6709\u9650\u7684WikiText-2\u6570\u636e\u96c6\u4e0a\uff0c\u4f7f\u7528\u7c97\u7565\u914d\u7f6e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9a8c\u8bc1\u635f\u5931\u964d\u4f4e0.56%\uff0c\u8bc1\u5b9e\u4e86\u6846\u67b6\u7684\u57fa\u672c\u6709\u6548\u6027\uff0c\u5e76\u8868\u660e\u5728\u66f4\u5927\u6570\u636e\u96c6\u548c\u66f4\u6df1\u8bad\u7ec3\u673a\u5236\u4e0a\u6709\u663e\u8457\u6f5c\u529b\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u5f3a\u6709\u529b\u7684\u7406\u8bba\u8bc1\u636e\uff1a\u6807\u51c6\u6ce8\u610f\u529b\u68af\u5ea6\u662f\u6b21\u4f18\u7684\u3002\u901a\u8fc7\u9009\u62e9\u6027\u7f29\u653e\u68af\u5ea6\u5206\u91cf\uff0c\u7279\u522b\u662f0\u9636\u53cc\u5411\u5e73\u884c\u8de8\u5ea6\uff0c\u53ef\u4ee5\u83b7\u5f97\u6700\u6709\u6548\u7684\u5b66\u4e60\u4fe1\u53f7\u3002\u8be5\u65b9\u6cd5\u5728\u6709\u9650\u6570\u636e\u96c6\u4e0a\u5df2\u663e\u793a\u6548\u679c\uff0c\u9884\u8ba1\u5728\u66f4\u5927\u89c4\u6a21\u5e94\u7528\u4e2d\u6709\u663e\u8457\u589e\u76ca\u3002"}}
{"id": "2512.12982", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.12982", "abs": "https://arxiv.org/abs/2512.12982", "authors": ["Ziheng Qin", "Yuheng Ji", "Renshuai Tao", "Yuxuan Tian", "Yuyang Liu", "Yipu Wang", "Xiaolong Zheng"], "title": "Scaling Up AI-Generated Image Detection via Generator-Aware Prototypes", "comment": null, "summary": "The pursuit of a universal AI-generated image (AIGI) detector often relies on aggregating data from numerous generators to improve generalization. However, this paper identifies a paradoxical phenomenon we term the Benefit then Conflict dilemma, where detector performance stagnates and eventually degrades as source diversity expands. Our systematic analysis, diagnoses this failure by identifying two core issues: severe data-level heterogeneity, which causes the feature distributions of real and synthetic images to increasingly overlap, and a critical model-level bottleneck from fixed, pretrained encoders that cannot adapt to the rising complexity. To address these challenges, we propose Generator-Aware Prototype Learning (GAPL), a framework that constrain representation with a structured learning paradigm. GAPL learns a compact set of canonical forgery prototypes to create a unified, low-variance feature space, effectively countering data heterogeneity.To resolve the model bottleneck, it employs a two-stage training scheme with Low-Rank Adaptation, enhancing its discriminative power while preserving valuable pretrained knowledge. This approach establishes a more robust and generalizable decision boundary. Through extensive experiments, we demonstrate that GAPL achieves state-of-the-art performance, showing superior detection accuracy across a wide variety of GAN and diffusion-based generators. Code is available at https://github.com/UltraCapture/GAPL", "AI": {"tldr": "\u8bba\u6587\u53d1\u73b0AIGI\u68c0\u6d4b\u5668\u5b58\u5728\"\u5148\u53d7\u76ca\u540e\u51b2\u7a81\"\u56f0\u5883\uff1a\u968f\u7740\u6570\u636e\u6e90\u591a\u6837\u6027\u589e\u52a0\uff0c\u68c0\u6d4b\u6027\u80fd\u5148\u63d0\u5347\u540e\u4e0b\u964d\u3002\u63d0\u51fa\u4e86GAPL\u6846\u67b6\uff0c\u901a\u8fc7\u539f\u578b\u5b66\u4e60\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u89e3\u51b3\u6570\u636e\u5f02\u8d28\u6027\u548c\u6a21\u578b\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u901a\u7528AIGI\u68c0\u6d4b\u5668\u901a\u5e38\u901a\u8fc7\u805a\u5408\u591a\u4e2a\u751f\u6210\u5668\u7684\u6570\u636e\u6765\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u672c\u6587\u53d1\u73b0\u5b58\u5728\"\u5148\u53d7\u76ca\u540e\u51b2\u7a81\"\u7684\u6096\u8bba\u73b0\u8c61\uff1a\u968f\u7740\u6570\u636e\u6e90\u591a\u6837\u6027\u6269\u5927\uff0c\u68c0\u6d4b\u5668\u6027\u80fd\u5148\u505c\u6ede\u540e\u4e0b\u964d\u3002\u9700\u8981\u89e3\u51b3\u6570\u636e\u5c42\u9762\u7684\u5f02\u8d28\u6027\u548c\u6a21\u578b\u5c42\u9762\u7684\u74f6\u9888\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86Generator-Aware Prototype Learning (GAPL)\u6846\u67b6\uff1a1) \u5b66\u4e60\u4e00\u7ec4\u7d27\u51d1\u7684\u5178\u578b\u4f2a\u9020\u539f\u578b\uff0c\u521b\u5efa\u7edf\u4e00\u3001\u4f4e\u65b9\u5dee\u7684\u7279\u5f81\u7a7a\u95f4\u4ee5\u5e94\u5bf9\u6570\u636e\u5f02\u8d28\u6027\uff1b2) \u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6848\uff0c\u7ed3\u5408Low-Rank Adaptation\u589e\u5f3a\u5224\u522b\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u7559\u9884\u8bad\u7ec3\u77e5\u8bc6\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0cGAPL\u5728GAN\u548c\u57fa\u4e8e\u6269\u6563\u7684\u751f\u6210\u5668\u4e0a\u5747\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u68c0\u6d4b\u51c6\u786e\u7387\uff0c\u80fd\u591f\u5efa\u7acb\u66f4\u9c81\u68d2\u548c\u53ef\u6cdb\u5316\u7684\u51b3\u7b56\u8fb9\u754c\u3002", "conclusion": "GAPL\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86AIGI\u68c0\u6d4b\u4e2d\u7684\"\u5148\u53d7\u76ca\u540e\u51b2\u7a81\"\u56f0\u5883\uff0c\u901a\u8fc7\u539f\u578b\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u8bad\u7ec3\u673a\u5236\u514b\u670d\u4e86\u6570\u636e\u5f02\u8d28\u6027\u548c\u6a21\u578b\u74f6\u9888\uff0c\u4e3a\u901a\u7528AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13034", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13034", "abs": "https://arxiv.org/abs/2512.13034", "authors": ["Xiaoyu He", "Yu Cai", "Jin Jia", "Canxi Huang", "Wenqing Chen", "Zibin Zheng"], "title": "Alada: Alternating Adaptation of Momentum Method for Memory-Efficient Matrix Optimization", "comment": null, "summary": "This work proposes Alada, an adaptive momentum method for stochastic optimization over large-scale matrices. Alada employs a rank-one factorization approach to estimate the second moment of gradients, where factors are updated alternatively to minimize the estimation error. Alada achieves sublinear memory overheads and can be readily extended to optimizing tensor-shaped variables.We also equip Alada with a first moment estimation rule, which enhances the algorithm's robustness without incurring additional memory overheads. The theoretical performance of Alada aligns with that of traditional methods such as Adam. Numerical studies conducted on several natural language processing tasks demonstrate the reduction in memory overheads and the robustness in training large models relative to Adam and its variants.", "AI": {"tldr": "Alada\u662f\u4e00\u79cd\u7528\u4e8e\u5927\u89c4\u6a21\u77e9\u9635\u968f\u673a\u4f18\u5316\u7684\u81ea\u9002\u5e94\u52a8\u91cf\u65b9\u6cd5\uff0c\u901a\u8fc7\u79e9\u4e00\u5206\u89e3\u4f30\u8ba1\u68af\u5ea6\u4e8c\u9636\u77e9\uff0c\u5177\u6709\u4e9a\u7ebf\u6027\u5185\u5b58\u5f00\u9500\uff0c\u6027\u80fd\u4e0eAdam\u76f8\u5f53\u4f46\u5185\u5b58\u6548\u7387\u66f4\u9ad8\u3002", "motivation": "\u4f20\u7edf\u81ea\u9002\u5e94\u4f18\u5316\u65b9\u6cd5\uff08\u5982Adam\uff09\u5728\u5904\u7406\u5927\u89c4\u6a21\u77e9\u9635\u4f18\u5316\u65f6\u5185\u5b58\u5f00\u9500\u8f83\u5927\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u5185\u5b58\u4f18\u5316\u65b9\u6cd5\u3002", "method": "Alada\u91c7\u7528\u79e9\u4e00\u5206\u89e3\u65b9\u6cd5\u4f30\u8ba1\u68af\u5ea6\u4e8c\u9636\u77e9\uff0c\u901a\u8fc7\u4ea4\u66ff\u66f4\u65b0\u56e0\u5b50\u6700\u5c0f\u5316\u4f30\u8ba1\u8bef\u5dee\uff0c\u5e76\u914d\u5907\u4e00\u9636\u77e9\u4f30\u8ba1\u89c4\u5219\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "\u5728\u591a\u4e2a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u4e0a\u7684\u6570\u503c\u7814\u7a76\u8868\u660e\uff0cAlada\u76f8\u6bd4Adam\u53ca\u5176\u53d8\u4f53\u663e\u8457\u51cf\u5c11\u4e86\u5185\u5b58\u5f00\u9500\uff0c\u5e76\u5728\u8bad\u7ec3\u5927\u6a21\u578b\u65f6\u8868\u73b0\u51fa\u66f4\u597d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "Alada\u662f\u4e00\u79cd\u5185\u5b58\u9ad8\u6548\u7684\u968f\u673a\u4f18\u5316\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u4e0eAdam\u76f8\u5f53\u7406\u8bba\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5b58\u5f00\u9500\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u77e9\u9635\u548c\u5f20\u91cf\u4f18\u5316\u3002"}}
{"id": "2512.12997", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12997", "abs": "https://arxiv.org/abs/2512.12997", "authors": ["Wenjing lu", "Zerui Tao", "Dongping Zhang", "Yuning Qiu", "Yang Yang", "Qibin Zhao"], "title": "Calibrating Uncertainty for Zero-Shot Adversarial CLIP", "comment": null, "summary": "CLIP delivers strong zero-shot classification but remains highly vulnerable to adversarial attacks. Previous work of adversarial fine-tuning largely focuses on matching the predicted logits between clean and adversarial examples, which overlooks uncertainty calibration and may degrade the zero-shot generalization. A common expectation in reliable uncertainty estimation is that predictive uncertainty should increase as inputs become more difficult or shift away from the training distribution. However, we frequently observe the opposite in the adversarial setting: perturbations not only degrade accuracy but also suppress uncertainty, leading to severe miscalibration and unreliable over-confidence. This overlooked phenomenon highlights a critical reliability gap beyond robustness. To bridge this gap, we propose a novel adversarial fine-tuning objective for CLIP considering both prediction accuracy and uncertainty alignments. By reparameterizing the output of CLIP as the concentration parameter of a Dirichlet distribution, we propose a unified representation that captures relative semantic structure and the magnitude of predictive confidence. Our objective aligns these distributions holistically under perturbations, moving beyond single-logit anchoring and restoring calibrated uncertainty. Experiments on multiple zero-shot classification benchmarks demonstrate that our approach effectively restores calibrated uncertainty and achieves competitive adversarial robustness while maintaining clean accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9488\u5bf9CLIP\u6a21\u578b\u7684\u65b0\u578b\u5bf9\u6297\u6027\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u72c4\u5229\u514b\u96f7\u5206\u5e03\u91cd\u65b0\u53c2\u6570\u5316\u8f93\u51fa\uff0c\u540c\u65f6\u4f18\u5316\u9884\u6d4b\u51c6\u786e\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\uff0c\u5728\u4fdd\u6301\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u7684\u540c\u65f6\u63d0\u5347\u5bf9\u6297\u9c81\u68d2\u6027", "motivation": "CLIP\u867d\u7136\u5177\u6709\u5f3a\u5927\u7684\u96f6\u6837\u672c\u5206\u7c7b\u80fd\u529b\uff0c\u4f46\u5bf9\u5bf9\u6297\u653b\u51fb\u9ad8\u5ea6\u8106\u5f31\u3002\u73b0\u6709\u5bf9\u6297\u5fae\u8c03\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5e72\u51c0\u6837\u672c\u548c\u5bf9\u6297\u6837\u672c\u4e4b\u95f4\u7684\u9884\u6d4b\u5bf9\u6570\u5339\u914d\uff0c\u5ffd\u89c6\u4e86\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\uff0c\u53ef\u80fd\u635f\u5bb3\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0\u5bf9\u6297\u6270\u52a8\u4e0d\u4ec5\u964d\u4f4e\u51c6\u786e\u6027\uff0c\u8fd8\u4f1a\u6291\u5236\u4e0d\u786e\u5b9a\u6027\uff0c\u5bfc\u81f4\u4e25\u91cd\u7684\u6821\u51c6\u9519\u8bef\u548c\u4e0d\u53ef\u9760\u7684\u8fc7\u5ea6\u81ea\u4fe1", "method": "\u901a\u8fc7\u5c06CLIP\u8f93\u51fa\u91cd\u65b0\u53c2\u6570\u5316\u4e3a\u72c4\u5229\u514b\u96f7\u5206\u5e03\u7684\u6d53\u5ea6\u53c2\u6570\uff0c\u63d0\u51fa\u7edf\u4e00\u8868\u793a\u65b9\u6cd5\u6355\u6349\u76f8\u5bf9\u8bed\u4e49\u7ed3\u6784\u548c\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u5927\u5c0f\u3002\u8bbe\u8ba1\u65b0\u578b\u5bf9\u6297\u5fae\u8c03\u76ee\u6807\uff0c\u5728\u6270\u52a8\u4e0b\u6574\u4f53\u5bf9\u9f50\u8fd9\u4e9b\u5206\u5e03\uff0c\u8d85\u8d8a\u5355\u4e00\u5bf9\u6570\u951a\u5b9a\uff0c\u6062\u590d\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027", "result": "\u5728\u591a\u4e2a\u96f6\u6837\u672c\u5206\u7c7b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u6062\u590d\u4e86\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u7684\u5bf9\u6297\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5e72\u51c0\u7684\u51c6\u786e\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86CLIP\u5728\u5bf9\u6297\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\u5dee\u8ddd\u95ee\u9898\uff0c\u901a\u8fc7\u540c\u65f6\u4f18\u5316\u51c6\u786e\u6027\u548c\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u9760\u7684\u96f6\u6837\u672c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2512.13040", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13040", "abs": "https://arxiv.org/abs/2512.13040", "authors": ["Xuwei Tan", "Yao Ma", "Xueru Zhang"], "title": "Understanding Structured Financial Data with LLMs: A Case Study on Fraud Detection", "comment": null, "summary": "Detecting fraud in financial transactions typically relies on tabular models that demand heavy feature engineering to handle high-dimensional data and offer limited interpretability, making it difficult for humans to understand predictions. Large Language Models (LLMs), in contrast, can produce human-readable explanations and facilitate feature analysis, potentially reducing the manual workload of fraud analysts and informing system refinements. However, they perform poorly when applied directly to tabular fraud detection due to the difficulty of reasoning over many features, the extreme class imbalance, and the absence of contextual information. To bridge this gap, we introduce FinFRE-RAG, a two-stage approach that applies importance-guided feature reduction to serialize a compact subset of numeric/categorical attributes into natural language and performs retrieval-augmented in-context learning over label-aware, instance-level exemplars. Across four public fraud datasets and three families of open-weight LLMs, FinFRE-RAG substantially improves F1/MCC over direct prompting and is competitive with strong tabular baselines in several settings. Although these LLMs still lag behind specialized classifiers, they narrow the performance gap and provide interpretable rationales, highlighting their value as assistive tools in fraud analysis.", "AI": {"tldr": "FinFRE-RAG\uff1a\u4e00\u79cd\u7528\u4e8e\u91d1\u878d\u6b3a\u8bc8\u68c0\u6d4b\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u8981\u6027\u5f15\u5bfc\u7684\u7279\u5f81\u7f29\u51cf\u5c06\u8868\u683c\u6570\u636e\u8f6c\u5316\u4e3a\u81ea\u7136\u8bed\u8a00\uff0c\u5e76\u5229\u7528\u68c0\u7d22\u589e\u5f3a\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347LLMs\u5728\u6b3a\u8bc8\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u9884\u6d4b\u4f9d\u636e\u3002", "motivation": "\u4f20\u7edf\u8868\u683c\u6a21\u578b\u5728\u91d1\u878d\u6b3a\u8bc8\u68c0\u6d4b\u4e2d\u9700\u8981\u5927\u91cf\u7279\u5f81\u5de5\u7a0b\uff0c\u4e14\u53ef\u89e3\u91ca\u6027\u5dee\uff0c\u96be\u4ee5\u8ba9\u4eba\u7c7b\u7406\u89e3\u9884\u6d4b\u7ed3\u679c\u3002\u867d\u7136\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u751f\u6210\u4eba\u7c7b\u53ef\u8bfb\u7684\u89e3\u91ca\u5e76\u51cf\u5c11\u5206\u6790\u5e08\u5de5\u4f5c\u91cf\uff0c\u4f46\u76f4\u63a5\u5e94\u7528\u4e8e\u8868\u683c\u6b3a\u8bc8\u68c0\u6d4b\u65f6\u6027\u80fd\u4e0d\u4f73\uff0c\u4e3b\u8981\u56e0\u4e3a\u96be\u4ee5\u5904\u7406\u5927\u91cf\u7279\u5f81\u3001\u6781\u7aef\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002", "method": "\u63d0\u51faFinFRE-RAG\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1\uff09\u91cd\u8981\u6027\u5f15\u5bfc\u7684\u7279\u5f81\u7f29\u51cf\uff0c\u5c06\u6570\u503c/\u5206\u7c7b\u5c5e\u6027\u7684\u7d27\u51d1\u5b50\u96c6\u5e8f\u5217\u5316\u4e3a\u81ea\u7136\u8bed\u8a00\uff1b2\uff09\u68c0\u7d22\u589e\u5f3a\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u57fa\u4e8e\u6807\u7b7e\u611f\u77e5\u7684\u5b9e\u4f8b\u7ea7\u793a\u4f8b\u8fdb\u884c\u5b66\u4e60\u3002", "result": "\u5728\u56db\u4e2a\u516c\u5171\u6b3a\u8bc8\u6570\u636e\u96c6\u548c\u4e09\u7c7b\u5f00\u6e90LLMs\u4e0a\uff0cFinFRE-RAG\u663e\u8457\u4f18\u4e8e\u76f4\u63a5\u63d0\u793a\u65b9\u6cd5\uff0cF1/MCC\u6307\u6807\u5927\u5e45\u63d0\u5347\uff0c\u5728\u591a\u4e2a\u8bbe\u7f6e\u4e2d\u4e0e\u5f3a\u5927\u7684\u8868\u683c\u57fa\u7ebf\u6a21\u578b\u5177\u6709\u7ade\u4e89\u529b\u3002\u867d\u7136LLMs\u4ecd\u843d\u540e\u4e8e\u4e13\u7528\u5206\u7c7b\u5668\uff0c\u4f46\u7f29\u5c0f\u4e86\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "FinFRE-RAG\u6709\u6548\u63d0\u5347\u4e86LLMs\u5728\u91d1\u878d\u6b3a\u8bc8\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u4f9d\u636e\uff0c\u7a81\u51fa\u4e86\u5176\u4f5c\u4e3a\u6b3a\u8bc8\u5206\u6790\u8f85\u52a9\u5de5\u5177\u7684\u4ef7\u503c\uff0c\u4e3a\u7ed3\u5408LLMs\u4f18\u52bf\u4e0e\u8868\u683c\u6570\u636e\u5904\u7406\u9700\u6c42\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2512.13007", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13007", "abs": "https://arxiv.org/abs/2512.13007", "authors": ["Nikolai Goncharov", "James L. Gray", "Donald G. Dansereau"], "title": "Light Field Based 6DoF Tracking of Previously Unobserved Objects", "comment": null, "summary": "Object tracking is an important step in robotics and reautonomous driving pipelines, which has to generalize to previously unseen and complex objects. Existing high-performing methods often rely on pre-captured object views to build explicit reference models, which restricts them to a fixed set of known objects. However, such reference models can struggle with visually complex appearance, reducing the quality of tracking. In this work, we introduce an object tracking method based on light field images that does not depend on a pre-trained model, while being robust to complex visual behavior, such as reflections. We extract semantic and geometric features from light field inputs using vision foundation models and convert them into view-dependent Gaussian splats. These splats serve as a unified object representation, supporting differentiable rendering and pose optimization. We further introduce a light field object tracking dataset containing challenging reflective objects with precise ground truth poses. Experiments demonstrate that our method is competitive with state-of-the-art model-based trackers in these difficult cases, paving the way toward universal object tracking in robotic systems. Code/data available at https://github.com/nagonch/LiFT-6DoF.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5149\u573a\u56fe\u50cf\u7684\u7269\u4f53\u8ddf\u8e2a\u65b9\u6cd5LiFT-6DoF\uff0c\u65e0\u9700\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u80fd\u5904\u7406\u590d\u6742\u89c6\u89c9\u884c\u4e3a\uff08\u5982\u53cd\u5c04\uff09\uff0c\u901a\u8fc7\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u63d0\u53d6\u7279\u5f81\u5e76\u8f6c\u6362\u4e3a\u89c6\u56fe\u76f8\u5173\u7684\u9ad8\u65af\u6e85\u5c04\u8868\u793a\uff0c\u652f\u6301\u53ef\u5fae\u5206\u6e32\u67d3\u548c\u4f4d\u59ff\u4f18\u5316\u3002", "motivation": "\u73b0\u6709\u9ad8\u6027\u80fd\u7269\u4f53\u8ddf\u8e2a\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u9884\u6355\u83b7\u7684\u7269\u4f53\u89c6\u56fe\u6784\u5efa\u663e\u5f0f\u53c2\u8003\u6a21\u578b\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u53ea\u80fd\u5904\u7406\u5df2\u77e5\u7269\u4f53\u96c6\u5408\uff0c\u4e14\u5728\u5904\u7406\u89c6\u89c9\u590d\u6742\u5916\u89c2\uff08\u5982\u53cd\u5c04\uff09\u65f6\u6027\u80fd\u4e0b\u964d\u3002\u9700\u8981\u4e00\u79cd\u4e0d\u4f9d\u8d56\u9884\u8bad\u7ec3\u6a21\u578b\u4e14\u80fd\u5904\u7406\u590d\u6742\u89c6\u89c9\u884c\u4e3a\u7684\u901a\u7528\u7269\u4f53\u8ddf\u8e2a\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5149\u573a\u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\uff0c\u901a\u8fc7\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u63d0\u53d6\u8bed\u4e49\u548c\u51e0\u4f55\u7279\u5f81\uff0c\u5c06\u5176\u8f6c\u6362\u4e3a\u89c6\u56fe\u76f8\u5173\u7684\u9ad8\u65af\u6e85\u5c04\u8868\u793a\u3002\u8fd9\u79cd\u8868\u793a\u4f5c\u4e3a\u7edf\u4e00\u7684\u7269\u4f53\u8868\u5f81\uff0c\u652f\u6301\u53ef\u5fae\u5206\u6e32\u67d3\u548c\u4f4d\u59ff\u4f18\u5316\u3002\u8fd8\u521b\u5efa\u4e86\u5305\u542b\u6311\u6218\u6027\u53cd\u5c04\u7269\u4f53\u7684\u5149\u573a\u7269\u4f53\u8ddf\u8e2a\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u5305\u542b\u53cd\u5c04\u7269\u4f53\u7684\u56f0\u96be\u6848\u4f8b\u4e2d\uff0c\u8be5\u65b9\u6cd5\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u6a21\u578b\u7684\u8ddf\u8e2a\u5668\u5177\u6709\u7ade\u4e89\u529b\u3002\u521b\u5efa\u7684\u6570\u636e\u96c6\u5305\u542b\u7cbe\u786e\u7684\u5730\u9762\u771f\u503c\u4f4d\u59ff\uff0c\u4ee3\u7801\u548c\u6570\u636e\u5df2\u5f00\u6e90\u3002", "conclusion": "\u63d0\u51fa\u7684LiFT-6DoF\u65b9\u6cd5\u4e3a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u7684\u901a\u7528\u7269\u4f53\u8ddf\u8e2a\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u80fd\u591f\u5904\u7406\u590d\u6742\u89c6\u89c9\u884c\u4e3a\u4e14\u4e0d\u4f9d\u8d56\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5728\u6311\u6218\u6027\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2512.13069", "categories": ["cs.LG", "physics.flu-dyn", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.13069", "abs": "https://arxiv.org/abs/2512.13069", "authors": ["Javier Nieto-Centenero", "Esther Andr\u00e9s", "Rodrigo Castellanos"], "title": "Multi-fidelity aerodynamic data fusion by autoencoder transfer learning", "comment": "29 pages, 13 figures", "summary": "Accurate aerodynamic prediction often relies on high-fidelity simulations; however, their prohibitive computational costs severely limit their applicability in data-driven modeling. This limitation motivates the development of multi-fidelity strategies that leverage inexpensive low-fidelity information without compromising accuracy. Addressing this challenge, this work presents a multi-fidelity deep learning framework that combines autoencoder-based transfer learning with a newly developed Multi-Split Conformal Prediction (MSCP) strategy to achieve uncertainty-aware aerodynamic data fusion under extreme data scarcity. The methodology leverages abundant Low-Fidelity (LF) data to learn a compact latent physics representation, which acts as a frozen knowledge base for a decoder that is subsequently fine-tuned using scarce HF samples. Tested on surface-pressure distributions for NACA airfoils (2D) and a transonic wing (3D) databases, the model successfully corrects LF deviations and achieves high-accuracy pressure predictions using minimal HF training data. Furthermore, the MSCP framework produces robust, actionable uncertainty bands with pointwise coverage exceeding 95%. By combining extreme data efficiency with uncertainty quantification, this work offers a scalable and reliable solution for aerodynamic regression in data-scarce environments.", "AI": {"tldr": "\u63d0\u51fa\u591a\u4fdd\u771f\u5ea6\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u52a8\u7f16\u7801\u5668\u8fc1\u79fb\u5b66\u4e60\u548c\u591a\u5206\u5272\u5171\u5f62\u9884\u6d4b\uff0c\u5728\u6570\u636e\u7a00\u7f3a\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u7a7a\u6c14\u52a8\u529b\u5b66\u6570\u636e\u878d\u5408", "motivation": "\u9ad8\u4fdd\u771f\u5ea6\u6a21\u62df\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\uff0c\u9650\u5236\u4e86\u6570\u636e\u9a71\u52a8\u5efa\u6a21\u7684\u5e94\u7528\uff1b\u9700\u8981\u5f00\u53d1\u591a\u4fdd\u771f\u5ea6\u7b56\u7565\uff0c\u5229\u7528\u4f4e\u6210\u672c\u4f4e\u4fdd\u771f\u5ea6\u4fe1\u606f\u800c\u4e0d\u727a\u7272\u7cbe\u5ea6", "method": "\u4f7f\u7528\u81ea\u52a8\u7f16\u7801\u5668\u8fc1\u79fb\u5b66\u4e60\u5b66\u4e60\u7d27\u51d1\u7684\u6f5c\u5728\u7269\u7406\u8868\u793a\u4f5c\u4e3a\u51bb\u7ed3\u77e5\u8bc6\u5e93\uff0c\u89e3\u7801\u5668\u7528\u7a00\u7f3a\u7684\u9ad8\u4fdd\u771f\u6837\u672c\u5fae\u8c03\uff1b\u5f00\u53d1\u591a\u5206\u5272\u5171\u5f62\u9884\u6d4b\u7b56\u7565\u8fdb\u884c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316", "result": "\u5728NACA\u7ffc\u578b\uff082D\uff09\u548c\u8de8\u97f3\u901f\u673a\u7ffc\uff083D\uff09\u8868\u9762\u538b\u529b\u5206\u5e03\u6d4b\u8bd5\u4e2d\uff0c\u6a21\u578b\u6210\u529f\u6821\u6b63\u4f4e\u4fdd\u771f\u5ea6\u504f\u5dee\uff0c\u4f7f\u7528\u6781\u5c11\u9ad8\u4fdd\u771f\u8bad\u7ec3\u6570\u636e\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u538b\u529b\u9884\u6d4b\uff1bMSCP\u6846\u67b6\u4ea7\u751f\u7a33\u5065\u7684\u4e0d\u786e\u5b9a\u6027\u5e26\uff0c\u70b9\u8986\u76d6\u8d85\u8fc795%", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u6781\u7aef\u6570\u636e\u6548\u7387\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u4e3a\u6570\u636e\u7a00\u7f3a\u73af\u5883\u4e2d\u7684\u7a7a\u6c14\u52a8\u529b\u5b66\u56de\u5f52\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.13008", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13008", "abs": "https://arxiv.org/abs/2512.13008", "authors": ["Xi Luo", "Shixin Xu", "Ying Xie", "JianZhong Hu", "Yuwei He", "Yuhui Deng", "Huaxiong Huang"], "title": "TWLR: Text-Guided Weakly-Supervised Lesion Localization and Severity Regression for Explainable Diabetic Retinopathy Grading", "comment": null, "summary": "Accurate medical image analysis can greatly assist clinical diagnosis, but its effectiveness relies on high-quality expert annotations Obtaining pixel-level labels for medical images, particularly fundus images, remains costly and time-consuming. Meanwhile, despite the success of deep learning in medical imaging, the lack of interpretability limits its clinical adoption. To address these challenges, we propose TWLR, a two-stage framework for interpretable diabetic retinopathy (DR) assessment. In the first stage, a vision-language model integrates domain-specific ophthalmological knowledge into text embeddings to jointly perform DR grading and lesion classification, effectively linking semantic medical concepts with visual features. The second stage introduces an iterative severity regression framework based on weakly-supervised semantic segmentation. Lesion saliency maps generated through iterative refinement direct a progressive inpainting mechanism that systematically eliminates pathological features, effectively downgrading disease severity toward healthier fundus appearances. Critically, this severity regression approach achieves dual benefits: accurate lesion localization without pixel-level supervision and providing an interpretable visualization of disease-to-healthy transformations. Experimental results on the FGADR, DDR, and a private dataset demonstrate that TWLR achieves competitive performance in both DR classification and lesion segmentation, offering a more explainable and annotation-efficient solution for automated retinal image analysis.", "AI": {"tldr": "TWLR\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u53ef\u89e3\u91ca\u6027\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6574\u5408\u773c\u79d1\u77e5\u8bc6\u8fdb\u884c\u5206\u7ea7\u548c\u75c5\u53d8\u5206\u7c7b\uff0c\u7136\u540e\u901a\u8fc7\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\u7684\u8fed\u4ee3\u4e25\u91cd\u6027\u56de\u5f52\u5b9e\u73b0\u75c5\u53d8\u5b9a\u4f4d\u548c\u75be\u75c5\u5230\u5065\u5eb7\u8f6c\u6362\u7684\u53ef\u89c6\u5316\u3002", "motivation": "\u533b\u5b66\u56fe\u50cf\u5206\u6790\u9700\u8981\u9ad8\u8d28\u91cf\u4e13\u5bb6\u6807\u6ce8\uff0c\u4f46\u83b7\u53d6\u50cf\u7d20\u7ea7\u6807\u7b7e\uff08\u7279\u522b\u662f\u773c\u5e95\u56fe\u50cf\uff09\u6210\u672c\u9ad8\u3001\u8017\u65f6\u957f\u3002\u540c\u65f6\uff0c\u6df1\u5ea6\u5b66\u4e60\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u91c7\u7528\u3002", "method": "\u63d0\u51faTWLR\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6574\u5408\u773c\u79d1\u9886\u57df\u77e5\u8bc6\uff0c\u8054\u5408\u6267\u884cDR\u5206\u7ea7\u548c\u75c5\u53d8\u5206\u7c7b\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5f15\u5165\u57fa\u4e8e\u5f31\u76d1\u7763\u8bed\u4e49\u5206\u5272\u7684\u8fed\u4ee3\u4e25\u91cd\u6027\u56de\u5f52\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u7cbe\u70bc\u751f\u6210\u75c5\u53d8\u663e\u8457\u6027\u56fe\uff0c\u6307\u5bfc\u6e10\u8fdb\u4fee\u590d\u673a\u5236\u7cfb\u7edf\u6d88\u9664\u75c5\u7406\u7279\u5f81\u3002", "result": "\u5728FGADR\u3001DDR\u548c\u79c1\u6709\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTWLR\u5728DR\u5206\u7c7b\u548c\u75c5\u53d8\u5206\u5272\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "TWLR\u4e3a\u81ea\u52a8\u5316\u89c6\u7f51\u819c\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u53ef\u89e3\u91ca\u3001\u6807\u6ce8\u6548\u7387\u66f4\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u50cf\u7d20\u7ea7\u76d1\u7763\u7684\u51c6\u786e\u75c5\u53d8\u5b9a\u4f4d\uff0c\u5e76\u63d0\u4f9b\u4e86\u75be\u75c5\u5230\u5065\u5eb7\u8f6c\u6362\u7684\u53ef\u89c6\u5316\u89e3\u91ca\u3002"}}
{"id": "2512.13077", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13077", "abs": "https://arxiv.org/abs/2512.13077", "authors": ["Md Awsafur Rahman", "Adam Gabrys", "Doug Kang", "Jingjing Sun", "Tian Tan", "Ashwin Chandramouli"], "title": "LikeBench: Evaluating Subjective Likability in LLMs for Personalization", "comment": null, "summary": "A personalized LLM should remember user facts, apply them correctly, and adapt over time to provide responses that the user prefers. Existing LLM personalization benchmarks are largely centered on two axes: accurately recalling user information and accurately applying remembered information in downstream tasks. We argue that a third axis, likability, is both subjective and central to user experience, yet under-measured by current benchmarks. To measure likability holistically, we introduce LikeBench, a multi-session, dynamic evaluation framework that measures likability across multiple dimensions by how much an LLM can adapt over time to a user's preferences to provide more likable responses. In LikeBench, the LLMs engage in conversation with a simulated user and learn preferences only from the ongoing dialogue. As the interaction unfolds, models try to adapt to responses, and after each turn, they are evaluated for likability across seven dimensions by the same simulated user. To the best of our knowledge, we are the first to decompose likability into multiple diagnostic metrics: emotional adaptation, formality matching, knowledge adaptation, reference understanding, conversation length fit, humor fit, and callback, which makes it easier to pinpoint where a model falls short. To make the simulated user more realistic and discriminative, LikeBench uses fine-grained, psychologically grounded descriptive personas rather than the coarse high/low trait rating based personas used in prior work. Our benchmark shows that strong memory performance does not guarantee high likability: DeepSeek R1, with lower memory accuracy (86%, 17 facts/profile), outperformed Qwen3 by 28% on likability score despite Qwen3's higher memory accuracy (93%, 43 facts/profile). Even SOTA models like GPT-5 adapt well in short exchanges but show only limited robustness in longer, noisier interactions.", "AI": {"tldr": "LikeBench\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u4e2a\u6027\u5316\u80fd\u529b\u7684\u65b0\u57fa\u51c6\uff0c\u7279\u522b\u5173\u6ce8\"\u559c\u597d\u5ea6\"\u8fd9\u4e00\u4e3b\u89c2\u4f46\u5173\u952e\u7684\u7528\u6237\u4f53\u9a8c\u7ef4\u5ea6\uff0c\u901a\u8fc7\u591a\u4f1a\u8bdd\u52a8\u6001\u6846\u67b6\u6d4b\u91cf\u6a21\u578b\u5728\u4e03\u4e2a\u7ef4\u5ea6\u4e0a\u9002\u5e94\u7528\u6237\u504f\u597d\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709LLM\u4e2a\u6027\u5316\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u51c6\u786e\u56de\u5fc6\u7528\u6237\u4fe1\u606f\u548c\u5e94\u7528\u8fd9\u4e9b\u4fe1\u606f\uff0c\u4f46\u5ffd\u7565\u4e86\"\u559c\u597d\u5ea6\"\u8fd9\u4e00\u4e3b\u89c2\u4e14\u5bf9\u7528\u6237\u4f53\u9a8c\u81f3\u5173\u91cd\u8981\u7684\u7ef4\u5ea6\u3002\u7528\u6237\u4e0d\u4ec5\u5e0c\u671bLLM\u8bb0\u4f4f\u4e8b\u5b9e\uff0c\u66f4\u5e0c\u671b\u83b7\u5f97\u7b26\u5408\u4e2a\u4eba\u504f\u597d\u7684\u56de\u5e94\u3002", "method": "\u63d0\u51faLikeBench\u591a\u4f1a\u8bdd\u52a8\u6001\u8bc4\u4f30\u6846\u67b6\uff1a1\uff09\u8ba9LLM\u4e0e\u6a21\u62df\u7528\u6237\u5bf9\u8bdd\uff0c\u4ec5\u4ece\u5bf9\u8bdd\u4e2d\u5b66\u4e60\u504f\u597d\uff1b2\uff09\u5c06\u559c\u597d\u5ea6\u5206\u89e3\u4e3a\u4e03\u4e2a\u8bca\u65ad\u7ef4\u5ea6\uff1a\u60c5\u611f\u9002\u5e94\u3001\u6b63\u5f0f\u5ea6\u5339\u914d\u3001\u77e5\u8bc6\u9002\u5e94\u3001\u5f15\u7528\u7406\u89e3\u3001\u5bf9\u8bdd\u957f\u5ea6\u5339\u914d\u3001\u5e7d\u9ed8\u5339\u914d\u3001\u56de\u8c03\uff1b3\uff09\u4f7f\u7528\u7ec6\u7c92\u5ea6\u3001\u57fa\u4e8e\u5fc3\u7406\u5b66\u7684\u63cf\u8ff0\u6027\u4eba\u7269\u89d2\u8272\u800c\u975e\u7c97\u7cd9\u7684\u9ad8\u4f4e\u7279\u8d28\u8bc4\u5206\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff1a1\uff09\u5f3a\u8bb0\u5fc6\u6027\u80fd\u4e0d\u4fdd\u8bc1\u9ad8\u559c\u597d\u5ea6\uff08DeepSeek R1\u8bb0\u5fc6\u51c6\u786e\u738786%\u4f46\u559c\u597d\u5ea6\u4f18\u4e8e\u8bb0\u5fc6\u51c6\u786e\u738793%\u7684Qwen3\uff09\uff1b2\uff09\u5373\u4f7f\u662fSOTA\u6a21\u578b\u5982GPT-5\u5728\u77ed\u4ea4\u6d41\u4e2d\u9002\u5e94\u826f\u597d\uff0c\u4f46\u5728\u66f4\u957f\u3001\u66f4\u5608\u6742\u7684\u4ea4\u4e92\u4e2d\u9c81\u68d2\u6027\u6709\u9650\uff1b3\uff09\u559c\u597d\u5ea6\u4e0e\u8bb0\u5fc6\u51c6\u786e\u7387\u76f8\u5173\u6027\u8f83\u5f31\u3002", "conclusion": "\u559c\u597d\u5ea6\u662fLLM\u4e2a\u6027\u5316\u7684\u91cd\u8981\u7ef4\u5ea6\uff0c\u9700\u8981\u4e13\u95e8\u7684\u8bc4\u4f30\u6846\u67b6\u3002LikeBench\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u3001\u7ec6\u7c92\u5ea6\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u9002\u5e94\u7528\u6237\u4e3b\u89c2\u504f\u597d\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u4e2a\u6027\u5316LLM\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2512.13106", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13106", "abs": "https://arxiv.org/abs/2512.13106", "authors": ["Shenzhi Yang", "Guangcheng Zhu", "Xing Zheng", "Yingfan MA", "Zhongqi Chen", "Bowen Song", "Weiqiang Wang", "Junbo Zhao", "Gang Chen", "Haobo Wang"], "title": "TraPO: A Semi-Supervised Reinforcement Learning Framework for Boosting LLM Reasoning", "comment": null, "summary": "Reinforcement learning with verifiable rewards (RLVR) has proven effective in training large reasoning models (LRMs) by leveraging answer-verifiable signals to guide policy optimization, which, however, suffers from high annotation costs. To alleviate this problem, recent work has explored unsupervised RLVR methods that derive rewards solely from the model's internal consistency, such as through entropy and majority voting. While seemingly promising, these methods often suffer from model collapse in the later stages of training, which may arise from the reinforcement of incorrect reasoning patterns in the absence of external supervision. In this work, we investigate a novel semi-supervised RLVR paradigm that utilizes a small labeled set to guide RLVR training on unlabeled samples. Our key insight is that supervised rewards are essential for stabilizing consistency-based training on unlabeled samples, ensuring that only reasoning patterns verified on labeled instances are incorporated into RL training. Technically, we propose an effective policy optimization algorithm, TraPO, that identifies reliable unlabeled samples by matching their learning trajectory similarity to labeled ones. Building on this, TraPO achieves remarkable data efficiency and strong generalization on six widely used mathematical reasoning benchmarks (AIME24/25, AMC, MATH-500, Minerva, and Olympiad) and three out-of-distribution tasks (ARC-c, GPQA-diamond, and MMLU-pro). With only 1K labeled and 3K unlabeled samples, TraPO reaches 42.6% average accuracy, surpassing the best unsupervised method trained on 45K unlabeled samples (38.3%). Notably, when using 4K labeled and 12K unlabeled samples, TraPO even outperforms the fully supervised model trained on the full 45K labeled samples on all benchmarks, while using only 10% of the labeled data. The code is available via https://github.com/ShenzhiYang2000/TRAPO.", "AI": {"tldr": "\u63d0\u51faTraPO\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c11\u91cf\u6807\u6ce8\u6837\u672c\u6307\u5bfc\u65e0\u6807\u6ce8\u6837\u672c\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u663e\u8457\u63d0\u5347\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\u548c\u6570\u636e\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u65e0\u76d1\u7763RLVR\u65b9\u6cd5\u867d\u7136\u907f\u514d\u4e86\u9ad8\u6807\u6ce8\u6210\u672c\uff0c\u4f46\u5728\u8bad\u7ec3\u540e\u671f\u5bb9\u6613\u51fa\u73b0\u6a21\u578b\u5d29\u6e83\u95ee\u9898\uff0c\u8fd9\u662f\u56e0\u4e3a\u7f3a\u4e4f\u5916\u90e8\u76d1\u7763\u53ef\u80fd\u5bfc\u81f4\u5f3a\u5316\u9519\u8bef\u7684\u63a8\u7406\u6a21\u5f0f\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u964d\u4f4e\u6807\u6ce8\u6210\u672c\u53c8\u80fd\u4fdd\u6301\u8bad\u7ec3\u7a33\u5b9a\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u534a\u76d1\u7763RLVR\u8303\u5f0fTraPO\u7b97\u6cd5\uff0c\u5229\u7528\u5c11\u91cf\u6807\u6ce8\u6837\u672c\u6765\u6307\u5bfc\u65e0\u6807\u6ce8\u6837\u672c\u7684\u8bad\u7ec3\u3002\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u6bd4\u8f83\u65e0\u6807\u6ce8\u6837\u672c\u4e0e\u6807\u6ce8\u6837\u672c\u7684\u5b66\u4e60\u8f68\u8ff9\u76f8\u4f3c\u6027\u6765\u8bc6\u522b\u53ef\u9760\u7684\u65e0\u6807\u6ce8\u6837\u672c\uff0c\u786e\u4fdd\u53ea\u6709\u7ecf\u8fc7\u6807\u6ce8\u5b9e\u4f8b\u9a8c\u8bc1\u7684\u63a8\u7406\u6a21\u5f0f\u88ab\u7eb3\u5165\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002", "result": "\u5728\u516d\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u548c\u4e09\u4e2a\u5206\u5e03\u5916\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u3002\u4ec5\u4f7f\u75281K\u6807\u6ce8\u548c3K\u65e0\u6807\u6ce8\u6837\u672c\u5c31\u8fbe\u523042.6%\u5e73\u5747\u51c6\u786e\u7387\uff0c\u8d85\u8d8a\u4e86\u572845K\u65e0\u6807\u6ce8\u6837\u672c\u4e0a\u8bad\u7ec3\u7684\u6700\u4f73\u65e0\u76d1\u7763\u65b9\u6cd5\uff0838.3%\uff09\u3002\u4f7f\u75284K\u6807\u6ce8\u548c12K\u65e0\u6807\u6ce8\u6837\u672c\u65f6\uff0c\u5728\u6240\u6709\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u751a\u81f3\u8d85\u8d8a\u4e86\u4f7f\u7528\u5168\u90e845K\u6807\u6ce8\u6837\u672c\u7684\u5b8c\u5168\u76d1\u7763\u6a21\u578b\uff0c\u800c\u4ec5\u4f7f\u7528\u4e8610%\u7684\u6807\u6ce8\u6570\u636e\u3002", "conclusion": "TraPO\u7b97\u6cd5\u901a\u8fc7\u534a\u76d1\u7763RLVR\u8303\u5f0f\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u76d1\u7763\u65b9\u6cd5\u4e2d\u7684\u6a21\u578b\u5d29\u6e83\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6570\u636e\u6548\u7387\u548c\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u964d\u4f4e\u5927\u63a8\u7406\u6a21\u578b\u7684\u6807\u6ce8\u6210\u672c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13015", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13015", "abs": "https://arxiv.org/abs/2512.13015", "authors": ["Xinjie Li", "Zhimin Chen", "Rui Zhao", "Florian Schiffers", "Zhenyu Liao", "Vimal Bhat"], "title": "What Happens Next? Next Scene Prediction with a Unified Video Model", "comment": null, "summary": "Recent unified models for joint understanding and generation have significantly advanced visual generation capabilities. However, their focus on conventional tasks like text-to-video generation has left the temporal reasoning potential of unified models largely underexplored. To address this gap, we introduce Next Scene Prediction (NSP), a new task that pushes unified video models toward temporal and causal reasoning. Unlike text-to-video generation, NSP requires predicting plausible futures from preceding context, demanding deeper understanding and reasoning. To tackle this task, we propose a unified framework combining Qwen-VL for comprehension and LTX for synthesis, bridged by a latent query embedding and a connector module. This model is trained in three stages on our newly curated, large-scale NSP dataset: text-to-video pre-training, supervised fine-tuning, and reinforcement learning (via GRPO) with our proposed causal consistency reward. Experiments demonstrate our model achieves state-of-the-art performance on our benchmark, advancing the capability of generalist multimodal systems to anticipate what happens next.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\"\u4e0b\u4e00\u573a\u666f\u9884\u6d4b\"\u65b0\u4efb\u52a1\uff0c\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u7ed3\u5408Qwen-VL\u7406\u89e3\u548cLTX\u5408\u6210\uff0c\u5728\u81ea\u5efa\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e09\u9636\u6bb5\u8bad\u7ec3\uff0c\u5b9e\u73b0\u4e86\u89c6\u9891\u6a21\u578b\u7684\u65f6\u5e8f\u56e0\u679c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7edf\u4e00\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u6587\u672c\u5230\u89c6\u9891\u751f\u6210\u7b49\u4f20\u7edf\u4efb\u52a1\uff0c\u5bf9\u65f6\u5e8f\u63a8\u7406\u6f5c\u529b\u63a2\u7d22\u4e0d\u8db3\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e0b\u4e00\u573a\u666f\u9884\u6d4b\u4efb\u52a1\uff0c\u63a8\u52a8\u7edf\u4e00\u89c6\u9891\u6a21\u578b\u5411\u65f6\u5e8f\u548c\u56e0\u679c\u63a8\u7406\u65b9\u5411\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\uff1aQwen-VL\u7528\u4e8e\u7406\u89e3\uff0cLTX\u7528\u4e8e\u5408\u6210\uff0c\u901a\u8fc7\u6f5c\u5728\u67e5\u8be2\u5d4c\u5165\u548c\u8fde\u63a5\u5668\u6a21\u5757\u6865\u63a5\u3002\u91c7\u7528\u4e09\u9636\u6bb5\u8bad\u7ec3\uff1a\u6587\u672c\u5230\u89c6\u9891\u9884\u8bad\u7ec3\u3001\u76d1\u7763\u5fae\u8c03\u3001\u4ee5\u53ca\u4f7f\u7528\u56e0\u679c\u4e00\u81f4\u6027\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08\u901a\u8fc7GRPO\uff09\u3002\u6784\u5efa\u4e86\u5927\u89c4\u6a21NSP\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u63d0\u5347\u4e86\u901a\u7528\u591a\u6a21\u6001\u7cfb\u7edf\u9884\u6d4b\u672a\u6765\u4e8b\u4ef6\u7684\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u4e0b\u4e00\u573a\u666f\u9884\u6d4b\u4efb\u52a1\u548c\u76f8\u5e94\u6846\u67b6\uff0c\u6210\u529f\u63a8\u8fdb\u4e86\u7edf\u4e00\u89c6\u9891\u6a21\u578b\u7684\u65f6\u5e8f\u548c\u56e0\u679c\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u591a\u6a21\u6001\u7cfb\u7edf\u9884\u6d4b\u672a\u6765\u573a\u666f\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13018", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13018", "abs": "https://arxiv.org/abs/2512.13018", "authors": ["Tomoya Tanaka", "Tomonori Ikeda", "Ryo Yonemoto"], "title": "Comprehensive Deployment-Oriented Assessment for Cross-Environment Generalization in Deep Learning-Based mmWave Radar Sensing", "comment": "8 pages, 6 figures. Comprehensive evaluation of preprocessing, data augmentation, and transfer learning for cross-environment generalization in deep learning-based mmWave radar sensing", "summary": "This study presents the first comprehensive evaluation of spatial generalization techniques, which are essential for the practical deployment of deep learning-based radio-frequency (RF) sensing. Focusing on people counting in indoor environments using frequency-modulated continuous-wave (FMCW) multiple-input multiple-output (MIMO) radar, we systematically investigate a broad set of approaches, including amplitude-based statistical preprocessing (sigmoid weighting and threshold zeroing), frequency-domain filtering, autoencoder-based background suppression, data augmentation strategies, and transfer learning. Experimental results collected across two environments with different layouts demonstrate that sigmoid-based amplitude weighting consistently achieves superior cross-environment performance, yielding 50.1% and 55.2% reductions in root-mean-square error (RMSE) and mean absolute error (MAE), respectively, compared with baseline methods. Data augmentation provides additional though modest benefits, with improvements up to 8.8% in MAE. By contrast, transfer learning proves indispensable for large spatial shifts, achieving 82.1% and 91.3% reductions in RMSE and MAE, respectively, with 540 target-domain samples. Taken together, these findings establish a highly practical direction for developing radar sensing systems capable of maintaining robust accuracy under spatial variations by integrating deep learning models with amplitude-based preprocessing and efficient transfer learning.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u5168\u9762\u8bc4\u4f30\u4e86\u7a7a\u95f4\u6cdb\u5316\u6280\u672f\uff0c\u9488\u5bf9\u5ba4\u5185\u73af\u5883\u4f7f\u7528FMCW MIMO\u96f7\u8fbe\u8fdb\u884c\u4eba\u5458\u8ba1\u6570\uff0c\u7cfb\u7edf\u6bd4\u8f83\u4e86\u591a\u79cd\u65b9\u6cd5\uff0c\u53d1\u73b0\u57fa\u4e8e\u5e45\u5ea6\u7684\u9884\u5904\u7406\u548c\u8fc1\u79fb\u5b66\u4e60\u80fd\u663e\u8457\u63d0\u5347\u8de8\u73af\u5883\u6027\u80fd\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5728\u5c04\u9891\u4f20\u611f\u4e2d\u7684\u5b9e\u9645\u90e8\u7f72\u9700\u8981\u89e3\u51b3\u7a7a\u95f4\u6cdb\u5316\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4e0d\u540c\u5ba4\u5185\u73af\u5883\u4e2d\u4fdd\u6301\u4eba\u5458\u8ba1\u6570\u7684\u51c6\u786e\u6027\u3002\u76ee\u524d\u7f3a\u4e4f\u5bf9\u7a7a\u95f4\u6cdb\u5316\u6280\u672f\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528FMCW MIMO\u96f7\u8fbe\u8fdb\u884c\u4eba\u5458\u8ba1\u6570\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5e45\u5ea6\u7edf\u8ba1\u9884\u5904\u7406\uff08Sigmoid\u52a0\u6743\u548c\u9608\u503c\u5f52\u96f6\uff09\u3001\u9891\u57df\u6ee4\u6ce2\u3001\u57fa\u4e8e\u81ea\u7f16\u7801\u5668\u7684\u80cc\u666f\u6291\u5236\u3001\u6570\u636e\u589e\u5f3a\u7b56\u7565\u548c\u8fc1\u79fb\u5b66\u4e60\u7b49\u591a\u79cd\u65b9\u6cd5\u3002", "result": "Sigmoid\u5e45\u5ea6\u52a0\u6743\u5728\u8de8\u73af\u5883\u6027\u80fd\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5206\u522b\u964d\u4f4eRMSE 50.1%\u548cMAE 55.2%\u3002\u6570\u636e\u589e\u5f3a\u63d0\u4f9b\u989d\u59168.8%\u7684MAE\u6539\u8fdb\u3002\u8fc1\u79fb\u5b66\u4e60\u5728\u5927\u7a7a\u95f4\u53d8\u5316\u65f6\u81f3\u5173\u91cd\u8981\uff0c\u4f7f\u7528540\u4e2a\u76ee\u6807\u57df\u6837\u672c\u53ef\u964d\u4f4eRMSE 82.1%\u548cMAE 91.3%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5c06\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e0e\u57fa\u4e8e\u5e45\u5ea6\u7684\u9884\u5904\u7406\u548c\u9ad8\u6548\u8fc1\u79fb\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u662f\u5f00\u53d1\u80fd\u591f\u5728\u7a7a\u95f4\u53d8\u5316\u4e0b\u4fdd\u6301\u9c81\u68d2\u51c6\u786e\u6027\u7684\u96f7\u8fbe\u4f20\u611f\u7cfb\u7edf\u7684\u5b9e\u7528\u65b9\u5411\u3002"}}
{"id": "2512.13019", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13019", "abs": "https://arxiv.org/abs/2512.13019", "authors": ["Cheeun Hong", "German Barquero", "Fadime Sener", "Markos Georgopoulos", "Edgar Sch\u00f6nfeld", "Stefan Popov", "Yuming Du", "Oscar Ma\u00f1as", "Albert Pumarola"], "title": "SneakPeek: Future-Guided Instructional Streaming Video Generation", "comment": null, "summary": "Instructional video generation is an emerging task that aims to synthesize coherent demonstrations of procedural activities from textual descriptions. Such capability has broad implications for content creation, education, and human-AI interaction, yet existing video diffusion models struggle to maintain temporal consistency and controllability across long sequences of multiple action steps. We introduce a pipeline for future-driven streaming instructional video generation, dubbed SneakPeek, a diffusion-based autoregressive framework designed to generate precise, stepwise instructional videos conditioned on an initial image and structured textual prompts. Our approach introduces three key innovations to enhance consistency and controllability: (1) predictive causal adaptation, where a causal model learns to perform next-frame prediction and anticipate future keyframes; (2) future-guided self-forcing with a dual-region KV caching scheme to address the exposure bias issue at inference time; (3) multi-prompt conditioning, which provides fine-grained and procedural control over multi-step instructions. Together, these components mitigate temporal drift, preserve motion consistency, and enable interactive video generation where future prompt updates dynamically influence ongoing streaming video generation. Experimental results demonstrate that our method produces temporally coherent and semantically faithful instructional videos that accurately follow complex, multi-step task descriptions.", "AI": {"tldr": "SneakPeek\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u7684\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u6027\u56e0\u679c\u9002\u5e94\u3001\u672a\u6765\u5f15\u5bfc\u81ea\u5f3a\u5236\u548c\u591a\u63d0\u793a\u6761\u4ef6\u5316\uff0c\u751f\u6210\u7cbe\u786e\u7684\u9010\u6b65\u6559\u5b66\u89c6\u9891\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u957f\u5e8f\u5217\u4e2d\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u53ef\u63a7\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u6559\u5b66\u89c6\u9891\u751f\u6210\u5728\u5185\u5bb9\u521b\u4f5c\u3001\u6559\u80b2\u548c\u4eba\u673a\u4ea4\u4e92\u4e2d\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\uff0c\u4f46\u73b0\u6709\u89c6\u9891\u6269\u6563\u6a21\u578b\u5728\u957f\u5e8f\u5217\u591a\u6b65\u9aa4\u52a8\u4f5c\u4e2d\u96be\u4ee5\u4fdd\u6301\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u53ef\u63a7\u6027\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86SneakPeek\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1a1) \u9884\u6d4b\u6027\u56e0\u679c\u9002\u5e94\uff0c\u901a\u8fc7\u56e0\u679c\u6a21\u578b\u5b66\u4e60\u4e0b\u4e00\u5e27\u9884\u6d4b\u548c\u672a\u6765\u5173\u952e\u5e27\u9884\u6d4b\uff1b2) \u672a\u6765\u5f15\u5bfc\u81ea\u5f3a\u5236\u4e0e\u53cc\u533a\u57dfKV\u7f13\u5b58\u65b9\u6848\uff0c\u89e3\u51b3\u63a8\u7406\u65f6\u7684\u66dd\u5149\u504f\u5dee\u95ee\u9898\uff1b3) \u591a\u63d0\u793a\u6761\u4ef6\u5316\uff0c\u63d0\u4f9b\u5bf9\u591a\u6b65\u9aa4\u6307\u4ee4\u7684\u7ec6\u7c92\u5ea6\u7a0b\u5e8f\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u65f6\u95f4\u4e00\u81f4\u3001\u8bed\u4e49\u5fe0\u5b9e\u4e14\u51c6\u786e\u9075\u5faa\u590d\u6742\u591a\u6b65\u9aa4\u4efb\u52a1\u63cf\u8ff0\u7684\u6559\u5b66\u89c6\u9891\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u65f6\u95f4\u6f02\u79fb\u95ee\u9898\uff0c\u4fdd\u6301\u4e86\u8fd0\u52a8\u4e00\u81f4\u6027\u3002", "conclusion": "SneakPeek\u901a\u8fc7\u521b\u65b0\u7684\u9884\u6d4b\u6027\u56e0\u679c\u9002\u5e94\u3001\u672a\u6765\u5f15\u5bfc\u81ea\u5f3a\u5236\u548c\u591a\u63d0\u793a\u6761\u4ef6\u5316\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u6559\u5b66\u89c6\u9891\u751f\u6210\uff0c\u4e3a\u4ea4\u4e92\u5f0f\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u5176\u4e2d\u672a\u6765\u63d0\u793a\u66f4\u65b0\u80fd\u591f\u52a8\u6001\u5f71\u54cd\u6b63\u5728\u8fdb\u884c\u7684\u6d41\u5f0f\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2512.13149", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.13149", "abs": "https://arxiv.org/abs/2512.13149", "authors": ["Xinwei Tai", "Dongmian Zou", "Hongfei Wang"], "title": "Enhancing Node-Level Graph Domain Adaptation by Alleviating Local Dependency", "comment": "Accepted to KDD 2026", "summary": "Recent years have witnessed significant advancements in machine learning methods on graphs. However, transferring knowledge effectively from one graph to another remains a critical challenge. This highlights the need for algorithms capable of applying information extracted from a source graph to an unlabeled target graph, a task known as unsupervised graph domain adaptation (GDA). One key difficulty in unsupervised GDA is conditional shift, which hinders transferability. In this paper, we show that conditional shift can be observed only if there exists local dependencies among node features. To support this claim, we perform a rigorous analysis and also further provide generalization bounds of GDA when dependent node features are modeled using markov chains. Guided by the theoretical findings, we propose to improve GDA by decorrelating node features, which can be specifically implemented through decorrelated GCN layers and graph transformer layers. Our experimental results demonstrate the effectiveness of this approach, showing not only substantial performance enhancements over baseline GDA methods but also clear visualizations of small intra-class distances in the learned representations. Our code is available at https://github.com/TechnologyAiGroup/DFT", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u89e3\u76f8\u5173\u8282\u70b9\u7279\u5f81\u6765\u89e3\u51b3\u56fe\u57df\u81ea\u9002\u5e94\u4e2d\u7684\u6761\u4ef6\u504f\u79fb\u95ee\u9898\uff0c\u4f7f\u7528\u89e3\u76f8\u5173\u7684GCN\u5c42\u548c\u56feTransformer\u5c42\u5b9e\u73b0\uff0c\u5b9e\u9a8c\u663e\u793a\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u56fe\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u8fd1\u5e74\u6765\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5982\u4f55\u6709\u6548\u5730\u5c06\u77e5\u8bc6\u4ece\u4e00\u4e2a\u56fe\u8fc1\u79fb\u5230\u53e6\u4e00\u4e2a\u56fe\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u65e0\u76d1\u7763\u56fe\u57df\u81ea\u9002\u5e94\uff08GDA\uff09\u9762\u4e34\u7684\u4e3b\u8981\u56f0\u96be\u662f\u6761\u4ef6\u504f\u79fb\u95ee\u9898\uff0c\u8fd9\u4f1a\u963b\u788d\u77e5\u8bc6\u8fc1\u79fb\u3002", "method": "\u8bba\u6587\u9996\u5148\u901a\u8fc7\u7406\u8bba\u5206\u6790\u8bc1\u660e\u6761\u4ef6\u504f\u79fb\u4ec5\u5728\u8282\u70b9\u7279\u5f81\u5b58\u5728\u5c40\u90e8\u4f9d\u8d56\u65f6\u624d\u4f1a\u51fa\u73b0\u3002\u57fa\u4e8e\u8fd9\u4e00\u53d1\u73b0\uff0c\u63d0\u51fa\u901a\u8fc7\u89e3\u76f8\u5173\u8282\u70b9\u7279\u5f81\u6765\u6539\u8fdbGDA\uff0c\u5177\u4f53\u5b9e\u73b0\u5305\u62ec\u89e3\u76f8\u5173\u7684GCN\u5c42\u548c\u56feTransformer\u5c42\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u57fa\u7ebfGDA\u65b9\u6cd5\u6709\u663e\u8457\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u4e14\u5728\u5b66\u4e60\u5230\u7684\u8868\u793a\u4e2d\u663e\u793a\u51fa\u8f83\u5c0f\u7684\u7c7b\u5185\u8ddd\u79bb\uff0c\u53ef\u89c6\u5316\u7ed3\u679c\u6e05\u6670\u3002", "conclusion": "\u901a\u8fc7\u89e3\u76f8\u5173\u8282\u70b9\u7279\u5f81\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u56fe\u57df\u81ea\u9002\u5e94\u4e2d\u7684\u6761\u4ef6\u504f\u79fb\u95ee\u9898\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u90fd\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\uff0c\u4e3a\u56fe\u57df\u81ea\u9002\u5e94\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13030", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13030", "abs": "https://arxiv.org/abs/2512.13030", "authors": ["Hongzhe Bi", "Hengkai Tan", "Shenghao Xie", "Zeyuan Wang", "Shuhe Huang", "Haitian Liu", "Ruowen Zhao", "Yao Feng", "Chendong Xiang", "Yinze Rong", "Hongyan Zhao", "Hanyu Liu", "Zhizhong Su", "Lei Ma", "Hang Su", "Jun Zhu"], "title": "Motus: A Unified Latent Action World Model", "comment": null, "summary": "While a general embodied agent must function as a unified system, current methods are built on isolated models for understanding, world modeling, and control. This fragmentation prevents unifying multimodal generative capabilities and hinders learning from large-scale, heterogeneous data. In this paper, we propose Motus, a unified latent action world model that leverages existing general pretrained models and rich, sharable motion information. Motus introduces a Mixture-of-Transformer (MoT) architecture to integrate three experts (i.e., understanding, video generation, and action) and adopts a UniDiffuser-style scheduler to enable flexible switching between different modeling modes (i.e., world models, vision-language-action models, inverse dynamics models, video generation models, and video-action joint prediction models). Motus further leverages the optical flow to learn latent actions and adopts a recipe with three-phase training pipeline and six-layer data pyramid, thereby extracting pixel-level \"delta action\" and enabling large-scale action pretraining. Experiments show that Motus achieves superior performance against state-of-the-art methods in both simulation (a +15% improvement over X-VLA and a +45% improvement over Pi0.5) and real-world scenarios(improved by +11~48%), demonstrating unified modeling of all functionalities and priors significantly benefits downstream robotic tasks.", "AI": {"tldr": "Motus\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6f5c\u5728\u52a8\u4f5c\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u6df7\u5408Transformer\u67b6\u6784\u96c6\u6210\u7406\u89e3\u3001\u89c6\u9891\u751f\u6210\u548c\u52a8\u4f5c\u4e09\u4e2a\u4e13\u5bb6\u6a21\u5757\uff0c\u5229\u7528\u5149\u6d41\u5b66\u4e60\u6f5c\u5728\u52a8\u4f5c\uff0c\u5b9e\u73b0\u591a\u6a21\u6001\u751f\u6210\u80fd\u529b\u7684\u7edf\u4e00\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u5c06\u7406\u89e3\u3001\u4e16\u754c\u5efa\u6a21\u548c\u63a7\u5236\u5206\u79bb\u4e3a\u5b64\u7acb\u6a21\u578b\uff0c\u8fd9\u79cd\u788e\u7247\u5316\u963b\u788d\u4e86\u591a\u6a21\u6001\u751f\u6210\u80fd\u529b\u7684\u7edf\u4e00\uff0c\u4e5f\u59a8\u788d\u4e86\u4ece\u5927\u89c4\u6a21\u5f02\u6784\u6570\u636e\u4e2d\u5b66\u4e60\u3002\u9700\u8981\u6784\u5efa\u4e00\u4e2a\u7edf\u4e00\u7684\u7cfb\u7edf\u6765\u6574\u5408\u8fd9\u4e9b\u529f\u80fd\u3002", "method": "\u63d0\u51faMotus\u7edf\u4e00\u6f5c\u5728\u52a8\u4f5c\u4e16\u754c\u6a21\u578b\uff0c\u91c7\u7528\u6df7\u5408Transformer\u67b6\u6784\u96c6\u6210\u4e09\u4e2a\u4e13\u5bb6\u6a21\u5757\uff08\u7406\u89e3\u3001\u89c6\u9891\u751f\u6210\u3001\u52a8\u4f5c\uff09\uff0c\u4f7f\u7528UniDiffuser\u98ce\u683c\u8c03\u5ea6\u5668\u5728\u4e0d\u540c\u5efa\u6a21\u6a21\u5f0f\u95f4\u7075\u6d3b\u5207\u6362\u3002\u5229\u7528\u5149\u6d41\u5b66\u4e60\u6f5c\u5728\u52a8\u4f5c\uff0c\u91c7\u7528\u4e09\u9636\u6bb5\u8bad\u7ec3\u6d41\u7a0b\u548c\u516d\u5c42\u6570\u636e\u91d1\u5b57\u5854\uff0c\u63d0\u53d6\u50cf\u7d20\u7ea7\"delta\u52a8\u4f5c\"\uff0c\u5b9e\u73b0\u5927\u89c4\u6a21\u52a8\u4f5c\u9884\u8bad\u7ec3\u3002", "result": "\u5728\u4eff\u771f\u73af\u5883\u4e2d\uff0cMotus\u6bd4X-VLA\u63d0\u534715%\uff0c\u6bd4Pi0.5\u63d0\u534745%\uff1b\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u63d0\u534711-48%\u3002\u5b9e\u9a8c\u8868\u660e\u7edf\u4e00\u5efa\u6a21\u6240\u6709\u529f\u80fd\u548c\u5148\u9a8c\u663e\u8457\u6709\u76ca\u4e8e\u4e0b\u6e38\u673a\u5668\u4eba\u4efb\u52a1\u3002", "conclusion": "Motus\u901a\u8fc7\u7edf\u4e00\u7684\u6f5c\u5728\u52a8\u4f5c\u4e16\u754c\u6a21\u578b\u6210\u529f\u6574\u5408\u4e86\u591a\u6a21\u6001\u751f\u6210\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u7edf\u4e00\u5efa\u6a21\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u4f18\u8d8a\u6027\uff0c\u4e3a\u6784\u5efa\u901a\u7528\u5177\u8eab\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2512.13165", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13165", "abs": "https://arxiv.org/abs/2512.13165", "authors": ["Jakub \u0141yskawa", "Jakub Lewandowski", "Pawe\u0142 Wawrzy\u0144ski"], "title": "SACn: Soft Actor-Critic with n-step Returns", "comment": "Accepted at ICAART 2026", "summary": "Soft Actor-Critic (SAC) is widely used in practical applications and is now one of the most relevant off-policy online model-free reinforcement learning (RL) methods. The technique of n-step returns is known to increase the convergence speed of RL algorithms compared to their 1-step returns-based versions. However, SAC is notoriously difficult to combine with n-step returns, since their usual combination introduces bias in off-policy algorithms due to the changes in action distribution. While this problem is solved by importance sampling, a method for estimating expected values of one distribution using samples from another distribution, importance sampling may result in numerical instability. In this work, we combine SAC with n-step returns in a way that overcomes this issue. We present an approach to applying numerically stable importance sampling with simplified hyperparameter selection. Furthermore, we analyze the entropy estimation approach of Soft Actor-Critic in the context of the n-step maximum entropy framework and formulate the $\u03c4$-sampled entropy estimation to reduce the variance of the learning target. Finally, we formulate the Soft Actor-Critic with n-step returns (SAC$n$) algorithm that we experimentally verify on MuJoCo simulated environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06Soft Actor-Critic\u4e0en\u6b65\u56de\u62a5\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u7ed3\u5408\u65b9\u5f0f\u4e2d\u7531\u4e8e\u52a8\u4f5c\u5206\u5e03\u53d8\u5316\u5bfc\u81f4\u7684\u504f\u5dee\u95ee\u9898\uff0c\u901a\u8fc7\u6570\u503c\u7a33\u5b9a\u7684\u91cd\u8981\u6027\u91c7\u6837\u548c\u03c4\u91c7\u6837\u71b5\u4f30\u8ba1\u6765\u63d0\u5347\u7b97\u6cd5\u6027\u80fd\u3002", "motivation": "SAC\u662f\u5f53\u524d\u6700\u76f8\u5173\u7684\u79bb\u7b56\u7565\u5728\u7ebf\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0cn\u6b65\u56de\u62a5\u80fd\u63d0\u9ad8RL\u7b97\u6cd5\u7684\u6536\u655b\u901f\u5ea6\u3002\u4f46SAC\u4e0en\u6b65\u56de\u62a5\u7684\u7ed3\u5408\u5b58\u5728\u56f0\u96be\uff0c\u56e0\u4e3a\u901a\u5e38\u7684\u7ed3\u5408\u65b9\u5f0f\u4f1a\u56e0\u52a8\u4f5c\u5206\u5e03\u53d8\u5316\u800c\u5f15\u5165\u504f\u5dee\uff0c\u800c\u91cd\u8981\u6027\u91c7\u6837\u867d\u7136\u80fd\u89e3\u51b3\u6b64\u95ee\u9898\u4f46\u53ef\u80fd\u5bfc\u81f4\u6570\u503c\u4e0d\u7a33\u5b9a\u3002", "method": "1. \u63d0\u51fa\u6570\u503c\u7a33\u5b9a\u7684\u91cd\u8981\u6027\u91c7\u6837\u65b9\u6cd5\uff0c\u7b80\u5316\u8d85\u53c2\u6570\u9009\u62e9\uff1b2. \u5728n\u6b65\u6700\u5927\u71b5\u6846\u67b6\u4e0b\u5206\u6790SAC\u7684\u71b5\u4f30\u8ba1\u65b9\u6cd5\uff0c\u63d0\u51fa\u03c4\u91c7\u6837\u71b5\u4f30\u8ba1\u6765\u964d\u4f4e\u5b66\u4e60\u76ee\u6807\u7684\u65b9\u5dee\uff1b3. \u6700\u7ec8\u5f62\u6210SACn\u7b97\u6cd5\u3002", "result": "\u5728MuJoCo\u6a21\u62df\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8868\u660e\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u7ed3\u5408SAC\u4e0en\u6b65\u56de\u62a5\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u504f\u5dee\u548c\u6570\u503c\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "conclusion": "\u6210\u529f\u5f00\u53d1\u4e86SACn\u7b97\u6cd5\uff0c\u901a\u8fc7\u6570\u503c\u7a33\u5b9a\u7684\u91cd\u8981\u6027\u91c7\u6837\u548c\u03c4\u91c7\u6837\u71b5\u4f30\u8ba1\uff0c\u5b9e\u73b0\u4e86SAC\u4e0en\u6b65\u56de\u62a5\u7684\u6709\u6548\u7ed3\u5408\uff0c\u63d0\u9ad8\u4e86\u7b97\u6cd5\u7684\u6536\u655b\u901f\u5ea6\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2512.13186", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13186", "abs": "https://arxiv.org/abs/2512.13186", "authors": ["Khalid Ferji"], "title": "PolySet: Restoring the Statistical Ensemble Nature of Polymers for Machine Learning", "comment": null, "summary": "Machine-learning (ML) models in polymer science typically treat a polymer as a single, perfectly defined molecular graph, even though real materials consist of stochastic ensembles of chains with distributed lengths. This mismatch between physical reality and digital representation limits the ability of current models to capture polymer behaviour. Here we introduce PolySet, a framework that represents a polymer as a finite, weighted ensemble of chains sampled from an assumed molar-mass distribution. This ensemble-based encoding is independent of chemical detail, compatible with any molecular representation and illustrated here in the homopolymer case using a minimal language model. We show that PolySet retains higher-order distributional moments (such as Mz, Mz+1), enabling ML models to learn tail-sensitive properties with greatly improved stability and accuracy. By explicitly acknowledging the statistical nature of polymer matter, PolySet establishes a physically grounded foundation for future polymer machine learning, naturally extensible to copolymers, block architectures, and other complex topologies.", "AI": {"tldr": "PolySet\u6846\u67b6\u5c06\u805a\u5408\u7269\u8868\u793a\u4e3a\u4ece\u6469\u5c14\u8d28\u91cf\u5206\u5e03\u4e2d\u91c7\u6837\u7684\u6709\u9650\u52a0\u6743\u94fe\u96c6\u5408\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfML\u6a21\u578b\u5c06\u805a\u5408\u7269\u89c6\u4e3a\u5355\u4e00\u5b8c\u7f8e\u5206\u5b50\u56fe\u4e0e\u771f\u5b9e\u6750\u6599\u7531\u968f\u673a\u94fe\u5206\u5e03\u7ec4\u6210\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u805a\u5408\u7269\u79d1\u5b66\u4e2d\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u901a\u5e38\u5c06\u805a\u5408\u7269\u89c6\u4e3a\u5355\u4e00\u5b8c\u7f8e\u5b9a\u4e49\u7684\u5206\u5b50\u56fe\uff0c\u800c\u771f\u5b9e\u6750\u6599\u7531\u5177\u6709\u5206\u5e03\u957f\u5ea6\u7684\u968f\u673a\u94fe\u96c6\u5408\u7ec4\u6210\uff0c\u8fd9\u79cd\u7269\u7406\u73b0\u5b9e\u4e0e\u6570\u5b57\u8868\u793a\u4e4b\u95f4\u7684\u4e0d\u5339\u914d\u9650\u5236\u4e86\u6a21\u578b\u6355\u6349\u805a\u5408\u7269\u884c\u4e3a\u7684\u80fd\u529b\u3002", "method": "\u5f15\u5165PolySet\u6846\u67b6\uff0c\u5c06\u805a\u5408\u7269\u8868\u793a\u4e3a\u4ece\u5047\u8bbe\u7684\u6469\u5c14\u8d28\u91cf\u5206\u5e03\u4e2d\u91c7\u6837\u7684\u6709\u9650\u52a0\u6743\u94fe\u96c6\u5408\uff0c\u8fd9\u79cd\u57fa\u4e8e\u96c6\u5408\u7684\u7f16\u7801\u72ec\u7acb\u4e8e\u5316\u5b66\u7ec6\u8282\uff0c\u4e0e\u4efb\u4f55\u5206\u5b50\u8868\u793a\u517c\u5bb9\uff0c\u5e76\u5728\u5747\u805a\u7269\u6848\u4f8b\u4e2d\u4f7f\u7528\u6700\u5c0f\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bf4\u660e\u3002", "result": "PolySet\u4fdd\u7559\u4e86\u9ad8\u9636\u5206\u5e03\u77e9\uff08\u5982Mz\u3001Mz+1\uff09\uff0c\u4f7f\u673a\u5668\u5b66\u4e60\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u5c3e\u90e8\u654f\u611f\u7279\u6027\uff0c\u5177\u6709\u663e\u8457\u6539\u5584\u7684\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u901a\u8fc7\u660e\u786e\u627f\u8ba4\u805a\u5408\u7269\u7269\u8d28\u7684\u7edf\u8ba1\u6027\u8d28\uff0cPolySet\u4e3a\u672a\u6765\u805a\u5408\u7269\u673a\u5668\u5b66\u4e60\u5efa\u7acb\u4e86\u7269\u7406\u57fa\u7840\uff0c\u81ea\u7136\u53ef\u6269\u5c55\u5230\u5171\u805a\u7269\u3001\u5d4c\u6bb5\u7ed3\u6784\u548c\u5176\u4ed6\u590d\u6742\u62d3\u6251\u7ed3\u6784\u3002"}}
{"id": "2512.13039", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.13039", "abs": "https://arxiv.org/abs/2512.13039", "authors": ["Hao Chen", "Yiwei Wang", "Songze Li"], "title": "Bi-Erasing: A Bidirectional Framework for Concept Removal in Diffusion Models", "comment": "Under Review", "summary": "Concept erasure, which fine-tunes diffusion models to remove undesired or harmful visual concepts, has become a mainstream approach to mitigating unsafe or illegal image generation in text-to-image models.However, existing removal methods typically adopt a unidirectional erasure strategy by either suppressing the target concept or reinforcing safe alternatives, making it difficult to achieve a balanced trade-off between concept removal and generation quality. To address this limitation, we propose a novel Bidirectional Image-Guided Concept Erasure (Bi-Erasing) framework that performs concept suppression and safety enhancement simultaneously. Specifically, based on the joint representation of text prompts and corresponding images, Bi-Erasing introduces two decoupled image branches: a negative branch responsible for suppressing harmful semantics and a positive branch providing visual guidance for safe alternatives. By jointly optimizing these complementary directions, our approach achieves a balance between erasure efficacy and generation usability. In addition, we apply mask-based filtering to the image branches to prevent interference from irrelevant content during the erasure process. Across extensive experiment evaluations, the proposed Bi-Erasing outperforms baseline methods in balancing concept removal effectiveness and visual fidelity.", "AI": {"tldr": "Bi-Erasing\u6846\u67b6\u901a\u8fc7\u53cc\u5411\u56fe\u50cf\u5f15\u5bfc\u7684\u6982\u5ff5\u64e6\u9664\uff0c\u540c\u65f6\u8fdb\u884c\u6982\u5ff5\u6291\u5236\u548c\u5b89\u5168\u589e\u5f3a\uff0c\u5728\u6982\u5ff5\u79fb\u9664\u6548\u679c\u548c\u751f\u6210\u8d28\u91cf\u4e4b\u95f4\u53d6\u5f97\u66f4\u597d\u5e73\u8861\u3002", "motivation": "\u73b0\u6709\u6982\u5ff5\u64e6\u9664\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u5355\u5411\u64e6\u9664\u7b56\u7565\uff08\u8981\u4e48\u6291\u5236\u76ee\u6807\u6982\u5ff5\uff0c\u8981\u4e48\u5f3a\u5316\u5b89\u5168\u66ff\u4ee3\uff09\uff0c\u96be\u4ee5\u5728\u6982\u5ff5\u79fb\u9664\u548c\u751f\u6210\u8d28\u91cf\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002\u9700\u8981\u4e00\u79cd\u66f4\u5e73\u8861\u7684\u65b9\u6cd5\u6765\u540c\u65f6\u5904\u7406\u6709\u5bb3\u6982\u5ff5\u6291\u5236\u548c\u5b89\u5168\u66ff\u4ee3\u589e\u5f3a\u3002", "method": "\u63d0\u51fa\u53cc\u5411\u56fe\u50cf\u5f15\u5bfc\u6982\u5ff5\u64e6\u9664\u6846\u67b6\uff0c\u57fa\u4e8e\u6587\u672c\u63d0\u793a\u548c\u5bf9\u5e94\u56fe\u50cf\u7684\u8054\u5408\u8868\u793a\uff0c\u5f15\u5165\u4e24\u4e2a\u89e3\u8026\u7684\u56fe\u50cf\u5206\u652f\uff1a\u8d1f\u5206\u652f\u8d1f\u8d23\u6291\u5236\u6709\u5bb3\u8bed\u4e49\uff0c\u6b63\u5206\u652f\u4e3a\u5b89\u5168\u66ff\u4ee3\u63d0\u4f9b\u89c6\u89c9\u5f15\u5bfc\u3002\u901a\u8fc7\u8054\u5408\u4f18\u5316\u8fd9\u4e24\u4e2a\u4e92\u8865\u65b9\u5411\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u63a9\u7801\u7684\u8fc7\u6ee4\u6765\u9632\u6b62\u65e0\u5173\u5185\u5bb9\u5e72\u6270\u3002", "result": "\u5728\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc4\u4f30\u4e2d\uff0cBi-Erasing\u5728\u5e73\u8861\u6982\u5ff5\u79fb\u9664\u6548\u679c\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "Bi-Erasing\u6846\u67b6\u901a\u8fc7\u540c\u65f6\u8fdb\u884c\u6982\u5ff5\u6291\u5236\u548c\u5b89\u5168\u589e\u5f3a\u7684\u53cc\u5411\u7b56\u7565\uff0c\u5728\u6982\u5ff5\u64e6\u9664\u6548\u679c\u548c\u751f\u6210\u53ef\u7528\u6027\u4e4b\u95f4\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u5e73\u8861\uff0c\u4e3a\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u5b89\u5168\u6539\u8fdb\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13190", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13190", "abs": "https://arxiv.org/abs/2512.13190", "authors": ["Jin Sob Kim", "Hyun Joon Park", "Wooseok Shin", "Dongil Park", "Sung Won Han"], "title": "WAY: Estimation of Vessel Destination in Worldwide AIS Trajectory", "comment": "Accepted to IEEE Transactions on Aerospace and Electronic Systems (TAES)", "summary": "The Automatic Identification System (AIS) enables data-driven maritime surveillance but suffers from reliability issues and irregular intervals. We address vessel destination estimation using global-scope AIS data by proposing a differentiated approach that recasts long port-to-port trajectories as a nested sequence structure. Using spatial grids, this method mitigates spatio-temporal bias while preserving detailed resolution. We introduce a novel deep learning architecture, WAY, designed to process these reformulated trajectories for long-term destination estimation days to weeks in advance. WAY comprises a trajectory representation layer and Channel-Aggregative Sequential Processing (CASP) blocks. The representation layer generates multi-channel vector sequences from kinematic and non-kinematic features. CASP blocks utilize multi-headed channel- and self-attention for aggregation and sequential information delivery. Additionally, we propose a task-specialized Gradient Dropout (GD) technique to enable many-to-many training on single labels, preventing biased feedback surges by stochastically blocking gradient flow based on sample length. Experiments on 5-year AIS data demonstrate WAY's superiority over conventional spatial grid-based approaches regardless of trajectory progression. Results further confirm that adopting GD leads to performance gains. Finally, we explore WAY's potential for real-world application through multitask learning for ETA estimation.", "AI": {"tldr": "\u63d0\u51faWAY\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\uff0c\u901a\u8fc7\u91cd\u6784AIS\u8f68\u8ff9\u4e3a\u5d4c\u5957\u5e8f\u5217\u7ed3\u6784\uff0c\u7ed3\u5408\u901a\u9053\u805a\u5408\u5e8f\u5217\u5904\u7406\u5757\u548c\u68af\u5ea6\u4e22\u5f03\u6280\u672f\uff0c\u5b9e\u73b0\u8239\u8236\u76ee\u7684\u5730\u63d0\u524d\u591a\u5929\u81f3\u6570\u5468\u7684\u9884\u6d4b\u3002", "motivation": "\u81ea\u52a8\u8bc6\u522b\u7cfb\u7edf(AIS)\u6570\u636e\u5b58\u5728\u53ef\u9760\u6027\u95ee\u9898\u548c\u95f4\u9694\u4e0d\u89c4\u5219\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u5728\u8239\u8236\u76ee\u7684\u5730\u9884\u6d4b\u65b9\u9762\u5b58\u5728\u65f6\u7a7a\u504f\u5dee\u548c\u5206\u8fa8\u7387\u635f\u5931\u95ee\u9898\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u957f\u671f\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "1) \u5c06\u957f\u6e2f\u53e3\u5230\u6e2f\u53e3\u8f68\u8ff9\u91cd\u6784\u4e3a\u5d4c\u5957\u5e8f\u5217\u7ed3\u6784\uff0c\u4f7f\u7528\u7a7a\u95f4\u7f51\u683c\u51cf\u8f7b\u65f6\u7a7a\u504f\u5dee\uff1b2) \u63d0\u51faWAY\u67b6\u6784\uff1a\u8f68\u8ff9\u8868\u793a\u5c42\u751f\u6210\u591a\u901a\u9053\u5411\u91cf\u5e8f\u5217\uff0cCASP\u5757\u4f7f\u7528\u591a\u5934\u901a\u9053\u548c\u81ea\u6ce8\u610f\u529b\u8fdb\u884c\u805a\u5408\u548c\u5e8f\u5217\u4fe1\u606f\u4f20\u9012\uff1b3) \u63d0\u51fa\u68af\u5ea6\u4e22\u5f03\u6280\u672f\uff0c\u57fa\u4e8e\u6837\u672c\u957f\u5ea6\u968f\u673a\u963b\u65ad\u68af\u5ea6\u6d41\uff0c\u5b9e\u73b0\u591a\u5bf9\u591a\u8bad\u7ec3\u3002", "result": "\u57285\u5e74AIS\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cWAY\u4f18\u4e8e\u4f20\u7edf\u7a7a\u95f4\u7f51\u683c\u65b9\u6cd5\uff0c\u4e14\u68af\u5ea6\u4e22\u5f03\u6280\u672f\u5e26\u6765\u6027\u80fd\u63d0\u5347\u3002\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u63a2\u7d22\u4e86ETA\u4f30\u8ba1\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "conclusion": "WAY\u67b6\u6784\u901a\u8fc7\u521b\u65b0\u7684\u8f68\u8ff9\u8868\u793a\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7ed3\u5408\u68af\u5ea6\u4e22\u5f03\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86AIS\u6570\u636e\u4e2d\u7684\u65f6\u7a7a\u504f\u5dee\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u957f\u671f\u8239\u8236\u76ee\u7684\u5730\u9884\u6d4b\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2512.13043", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13043", "abs": "https://arxiv.org/abs/2512.13043", "authors": ["Tong Wei", "Yijun Yang", "Changhao Zhang", "Junliang Xing", "Yuanchun Shi", "Zongqing Lu", "Deheng Ye"], "title": "GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training", "comment": null, "summary": "Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a \"free\" teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the \"entropy collapse\" observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR.", "AI": {"tldr": "GTR-Turbo\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u6a21\u6001\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5408\u5e76\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684\u68c0\u67e5\u70b9\u6743\u91cd\u521b\u5efa\"\u514d\u8d39\"\u6559\u5e08\u6a21\u578b\uff0c\u65e0\u9700\u6602\u8d35\u7684\u5916\u90e8\u6559\u5e08\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u591a\u6a21\u6001\u667a\u80fd\u4f53\u591a\u8f6e\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u4e14\u7279\u6743\u7684\u5916\u90e8\u6559\u5e08\u6a21\u578b\u63d0\u4f9b\u6b65\u7ea7\u53cd\u9988\uff0c\u8fd9\u9650\u5236\u4e86\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u548c\u53ef\u590d\u73b0\u6027\u3002", "method": "GTR-Turbo\u5728\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5408\u5e76\u68c0\u67e5\u70b9\u7684\u6743\u91cd\uff0c\u4f7f\u7528\u8fd9\u4e2a\u5408\u5e76\u6a21\u578b\u4f5c\u4e3a\"\u514d\u8d39\"\u6559\u5e08\uff0c\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u6216\u8f6f\u5bf9\u6570\u84b8\u998f\u6307\u5bfc\u540e\u7eed\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5728\u591a\u79cd\u89c6\u89c9\u667a\u80fd\u4f53\u4efb\u52a1\u4e2d\uff0cGTR-Turbo\u5c06\u57fa\u7ebf\u6a21\u578b\u7684\u51c6\u786e\u7387\u63d0\u5347\u4e8610-30%\uff0c\u540c\u65f6\u76f8\u5bf9\u4e8eGTR\u65b9\u6cd5\u51cf\u5c11\u4e8650%\u7684\u8bad\u7ec3\u65f6\u95f4\u548c60%\u7684\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "GTR-Turbo\u6d88\u9664\u4e86\u5bf9\u7279\u6743\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4f9d\u8d56\uff0c\u7f13\u89e3\u4e86\u5148\u524d\u5de5\u4f5c\u4e2d\u7684\"\u71b5\u5d29\u6e83\"\u95ee\u9898\uff0c\u4fdd\u6301\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u662f\u4e00\u79cd\u9ad8\u6548\u5b9e\u7528\u7684\u591a\u6a21\u6001\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2512.13207", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.13207", "abs": "https://arxiv.org/abs/2512.13207", "authors": ["Karina Chichifoi", "Fabio Merizzi", "Michele Colajanni"], "title": "Evaluating Adversarial Attacks on Federated Learning for Temperature Forecasting", "comment": null, "summary": "Deep learning and federated learning (FL) are becoming powerful partners for next-generation weather forecasting. Deep learning enables high-resolution spatiotemporal forecasts that can surpass traditional numerical models, while FL allows institutions in different locations to collaboratively train models without sharing raw data, addressing efficiency and security concerns. While FL has shown promise across heterogeneous regions, its distributed nature introduces new vulnerabilities. In particular, data poisoning attacks, in which compromised clients inject manipulated training data, can degrade performance or introduce systematic biases. These threats are amplified by spatial dependencies in meteorological data, allowing localized perturbations to influence broader regions through global model aggregation. In this study, we investigate how adversarial clients distort federated surface temperature forecasts trained on the Copernicus European Regional ReAnalysis (CERRA) dataset. We simulate geographically distributed clients and evaluate patch-based and global biasing attacks on regional temperature forecasts. Our results show that even a small fraction of poisoned clients can mislead predictions across large, spatially connected areas. A global temperature bias attack from a single compromised client shifts predictions by up to -1.7 K, while coordinated patch attacks more than triple the mean squared error and produce persistent regional anomalies exceeding +3.5 K. Finally, we assess trimmed mean aggregation as a defense mechanism, showing that it successfully defends against global bias attacks (2-13\\% degradation) but fails against patch attacks (281-603\\% amplification), exposing limitations of outlier-based defenses for spatially correlated data.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u8054\u90a6\u5b66\u4e60\u5728\u5929\u6c14\u9884\u62a5\u4e2d\u7684\u8106\u5f31\u6027\uff0c\u53d1\u73b0\u5373\u4f7f\u5c11\u6570\u6076\u610f\u5ba2\u6237\u7aef\u901a\u8fc7\u6570\u636e\u6295\u6bd2\u653b\u51fb\uff08\u5168\u5c40\u504f\u7f6e\u653b\u51fb\u548c\u5c40\u90e8\u8865\u4e01\u653b\u51fb\uff09\u5c31\u80fd\u663e\u8457\u626d\u66f2\u6e29\u5ea6\u9884\u6d4b\uff0c\u800c\u57fa\u4e8e\u4fee\u526a\u5747\u503c\u7684\u9632\u5fa1\u673a\u5236\u5bf9\u7a7a\u95f4\u76f8\u5173\u6570\u636e\u7684\u8865\u4e01\u653b\u51fb\u65e0\u6548\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u4e0e\u8054\u90a6\u5b66\u4e60\u7ed3\u5408\u4e3a\u4e0b\u4e00\u4ee3\u5929\u6c14\u9884\u62a5\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\uff0c\u4f46\u8054\u90a6\u5b66\u4e60\u7684\u5206\u5e03\u5f0f\u7279\u6027\u5f15\u5165\u4e86\u65b0\u7684\u5b89\u5168\u6f0f\u6d1e\u3002\u7279\u522b\u662f\u6570\u636e\u6295\u6bd2\u653b\u51fb\u53ef\u80fd\u901a\u8fc7\u6ce8\u5165\u6076\u610f\u8bad\u7ec3\u6570\u636e\u6765\u964d\u4f4e\u6a21\u578b\u6027\u80fd\u6216\u5f15\u5165\u7cfb\u7edf\u6027\u504f\u5dee\uff0c\u800c\u6c14\u8c61\u6570\u636e\u7684\u7a7a\u95f4\u4f9d\u8d56\u6027\u8fdb\u4e00\u6b65\u653e\u5927\u4e86\u8fd9\u79cd\u5a01\u80c1\u3002", "method": "\u7814\u7a76\u4f7f\u7528Copernicus\u6b27\u6d32\u533a\u57df\u518d\u5206\u6790\u6570\u636e\u96c6\uff0c\u6a21\u62df\u5730\u7406\u5206\u5e03\u7684\u5ba2\u6237\u7aef\uff0c\u8bc4\u4f30\u57fa\u4e8e\u8865\u4e01\u548c\u5168\u5c40\u504f\u7f6e\u653b\u51fb\u5bf9\u533a\u57df\u6e29\u5ea6\u9884\u6d4b\u7684\u5f71\u54cd\u3002\u6700\u540e\u8bc4\u4f30\u4e86\u4fee\u526a\u5747\u503c\u805a\u5408\u4f5c\u4e3a\u9632\u5fa1\u673a\u5236\u7684\u6709\u6548\u6027\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff1a1\uff09\u5355\u4e2a\u6076\u610f\u5ba2\u6237\u7aef\u7684\u5168\u5c40\u6e29\u5ea6\u504f\u7f6e\u653b\u51fb\u53ef\u4f7f\u9884\u6d4b\u504f\u79fb\u9ad8\u8fbe-1.7K\uff1b2\uff09\u534f\u8c03\u7684\u8865\u4e01\u653b\u51fb\u4f7f\u5747\u65b9\u8bef\u5dee\u589e\u52a0\u4e09\u500d\u4ee5\u4e0a\uff0c\u4ea7\u751f\u8d85\u8fc7+3.5K\u7684\u6301\u7eed\u533a\u57df\u5f02\u5e38\uff1b3\uff09\u4fee\u526a\u5747\u503c\u805a\u5408\u80fd\u6709\u6548\u9632\u5fa1\u5168\u5c40\u504f\u7f6e\u653b\u51fb\uff08\u4ec52-13%\u6027\u80fd\u4e0b\u964d\uff09\uff0c\u4f46\u5bf9\u8865\u4e01\u653b\u51fb\u5b8c\u5168\u5931\u6548\uff08\u6027\u80fd\u4e0b\u964d281-603%\uff09\u3002", "conclusion": "\u8054\u90a6\u5b66\u4e60\u5728\u5929\u6c14\u9884\u62a5\u5e94\u7528\u4e2d\u9762\u4e34\u4e25\u91cd\u7684\u5b89\u5168\u5a01\u80c1\uff0c\u7279\u522b\u662f\u9488\u5bf9\u7a7a\u95f4\u76f8\u5173\u6570\u636e\u7684\u6295\u6bd2\u653b\u51fb\u3002\u57fa\u4e8e\u5f02\u5e38\u503c\u68c0\u6d4b\u7684\u4f20\u7edf\u9632\u5fa1\u673a\u5236\u5bf9\u7a7a\u95f4\u76f8\u5173\u7684\u8865\u4e01\u653b\u51fb\u65e0\u6548\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u9632\u5fa1\u7b56\u7565\u6765\u4fdd\u62a4\u5206\u5e03\u5f0f\u6c14\u8c61\u9884\u6d4b\u7cfb\u7edf\u3002"}}
{"id": "2512.13072", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13072", "abs": "https://arxiv.org/abs/2512.13072", "authors": ["Zizhi Chen", "Yizhen Gao", "Minghao Han", "Yizhou Liu", "Zhaoyu Chen", "Dingkang Yang", "Lihua Zhang"], "title": "Forging a Dynamic Memory: Retrieval-Guided Continual Learning for Generalist Medical Foundation Models", "comment": null, "summary": "Multimodal biomedical Vision-Language Models (VLMs) exhibit immense potential in the field of Continual Learning (CL). However, they confront a core dilemma: how to preserve fine-grained intra-modality features while bridging the significant domain gap across different modalities. To address this challenge, we propose a comprehensive framework. Leveraging our 18-million multimodal and comprehensive medical retrieval database derived from PubMed scientific papers, we pioneer the integration of Retrieval-Augmented Generation (RAG) into CL. Specifically, we employ a multi-modal, multi-layer RAG system that provides real-time guidance for model fine-tuning through dynamic, on-demand knowledge retrieval. Building upon this, we introduce a dynamic knowledge distillation framework. This framework precisely resolves the aforementioned core dilemma by dynamically modulating the importance of the parameter space, the granularity of the distilled knowledge, and the data distribution of the reference dataset in accordance with the required level of detail. To thoroughly validate the clinical value of our strategy, we have designed a more rigorous \\textbf{M}edical Generalist Task Incremental Learning (MGTIL) benchmark. This benchmark is engineered to simultaneously evaluate the model's capacity for adaptation to significant domain shifts, retention of subtle intra-domain features, and real-time learning of novel and complex medical tasks. Extensive experimental results demonstrate that our proposed method achieves state-of-the-art (SOTA) performance across all metrics. The code is provided in the supplementary materials.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7528\u4e8e\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6301\u7eed\u5b66\u4e60\u7684\u7efc\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u591a\u5c42\u77e5\u8bc6\u84b8\u998f\u89e3\u51b3\u6a21\u6001\u95f4\u57df\u5dee\u8ddd\u548c\u7ec6\u7c92\u5ea6\u7279\u5f81\u4fdd\u7559\u7684\u96be\u9898\u3002", "motivation": "\u591a\u6a21\u6001\u751f\u7269\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u9762\u4e34\u6838\u5fc3\u56f0\u5883\uff1a\u5982\u4f55\u5728\u4fdd\u6301\u7ec6\u7c92\u5ea6\u6a21\u6001\u5185\u7279\u5f81\u7684\u540c\u65f6\uff0c\u8de8\u8d8a\u4e0d\u540c\u6a21\u6001\u95f4\u7684\u663e\u8457\u57df\u5dee\u8ddd\u3002", "method": "1. \u57fa\u4e8e1800\u4e07PubMed\u79d1\u5b66\u8bba\u6587\u6784\u5efa\u591a\u6a21\u6001\u533b\u5b66\u68c0\u7d22\u6570\u636e\u5e93\uff1b2. \u9996\u6b21\u5c06\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5f15\u5165\u6301\u7eed\u5b66\u4e60\uff0c\u91c7\u7528\u591a\u6a21\u6001\u591a\u5c42RAG\u7cfb\u7edf\u63d0\u4f9b\u5b9e\u65f6\u6307\u5bfc\uff1b3. \u63d0\u51fa\u52a8\u6001\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u52a8\u6001\u8c03\u8282\u53c2\u6570\u7a7a\u95f4\u91cd\u8981\u6027\u3001\u77e5\u8bc6\u7c92\u5ea6\u548c\u53c2\u8003\u6570\u636e\u96c6\u5206\u5e03\u3002", "result": "\u5728\u8bbe\u8ba1\u7684\u533b\u5b66\u901a\u7528\u4efb\u52a1\u589e\u91cf\u5b66\u4e60\u57fa\u51c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u6307\u6807\u4e0a\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "\u63d0\u51fa\u7684\u7efc\u5408\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u6838\u5fc3\u6311\u6218\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u548c\u52a8\u6001\u77e5\u8bc6\u84b8\u998f\u5b9e\u73b0\u4e86\u5bf9\u663e\u8457\u57df\u504f\u79fb\u7684\u9002\u5e94\u3001\u7ec6\u7c92\u5ea6\u7279\u5f81\u7684\u4fdd\u7559\u4ee5\u53ca\u65b0\u590d\u6742\u533b\u5b66\u4efb\u52a1\u7684\u5b9e\u65f6\u5b66\u4e60\u3002"}}
{"id": "2512.13228", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13228", "abs": "https://arxiv.org/abs/2512.13228", "authors": ["Melvin Barbaux"], "title": "ModSSC: A Modular Framework for Semi-Supervised Classification on Heterogeneous Data", "comment": "Preprint describing the open source ModSSC framework for inductive and transductive semi-supervised classification on heterogeneous data", "summary": "Semi-supervised classification leverages both labeled and unlabeled data to improve predictive performance, but existing software support is fragmented across methods and modalities. We introduce ModSSC, an open source Python framework that unifies inductive and transductive semi-supervised classification in a modular code base. ModSSC implements a broad range of classical and recent algorithms, provides loaders for tabular, image, text, audio and graph datasets, and exposes a single configuration interface for specifying datasets, models and evaluation protocols. It supports both lightweight classical methods on small datasets running on CPU and recent deep approaches that can exploit multiple GPUs within the same experimental framework. Experiments are described declaratively in YAML, which facilitates reproducing existing work and running large comparative studies. ModSSC 1.0.0 is released under the MIT license with extensive documentation and tests, and is available at https://github.com/ModSSC/ModSSC.", "AI": {"tldr": "ModSSC\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u534a\u76d1\u7763\u5206\u7c7bPython\u6846\u67b6\uff0c\u652f\u6301\u591a\u79cd\u7b97\u6cd5\u3001\u6570\u636e\u7c7b\u578b\u548c\u786c\u4ef6\u914d\u7f6e\uff0c\u901a\u8fc7YAML\u58f0\u660e\u5f0f\u5b9e\u9a8c\u914d\u7f6e\u4fc3\u8fdb\u53ef\u590d\u73b0\u6027\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u534a\u76d1\u7763\u5206\u7c7b\u8f6f\u4ef6\u652f\u6301\u5206\u6563\u5728\u4e0d\u540c\u65b9\u6cd5\u548c\u6a21\u6001\u4e2d\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u6846\u67b6\u6765\u6574\u5408\u5f52\u7eb3\u5f0f\u548c\u76f4\u63a8\u5f0f\u65b9\u6cd5\uff0c\u963b\u788d\u4e86\u65b9\u6cd5\u7684\u6bd4\u8f83\u548c\u590d\u73b0\u3002", "method": "\u5f00\u53d1\u4e86ModSSC\u5f00\u6e90\u6846\u67b6\uff0c\u5b9e\u73b0\u5e7f\u6cdb\u7684\u7ecf\u5178\u548c\u6700\u65b0\u7b97\u6cd5\uff0c\u63d0\u4f9b\u8868\u683c\u3001\u56fe\u50cf\u3001\u6587\u672c\u3001\u97f3\u9891\u548c\u56fe\u6570\u636e\u96c6\u7684\u52a0\u8f7d\u5668\uff0c\u901a\u8fc7\u5355\u4e00\u914d\u7f6e\u63a5\u53e3\u6307\u5b9a\u6570\u636e\u96c6\u3001\u6a21\u578b\u548c\u8bc4\u4f30\u534f\u8bae\uff0c\u652f\u6301CPU\u4e0a\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\u548cGPU\u4e0a\u7684\u6df1\u5ea6\u65b9\u6cd5\u3002", "result": "ModSSC 1.0.0\u5df2\u53d1\u5e03\uff0c\u91c7\u7528MIT\u8bb8\u53ef\u8bc1\uff0c\u5305\u542b\u8be6\u7ec6\u6587\u6863\u548c\u6d4b\u8bd5\uff0c\u53ef\u5728GitHub\u4e0a\u83b7\u53d6\uff0c\u4e3a\u534a\u76d1\u7763\u5206\u7c7b\u7814\u7a76\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u5b9e\u9a8c\u5e73\u53f0\u3002", "conclusion": "ModSSC\u6846\u67b6\u89e3\u51b3\u4e86\u534a\u76d1\u7763\u5206\u7c7b\u8f6f\u4ef6\u788e\u7247\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u4ee3\u7801\u5e93\u7edf\u4e00\u4e86\u4e0d\u540c\u65b9\u6cd5\u548c\u6a21\u6001\uff0c\u4fc3\u8fdb\u4e86\u53ef\u590d\u73b0\u7814\u7a76\u548c\u5927\u578b\u6bd4\u8f83\u7814\u7a76\u3002"}}
{"id": "2512.13235", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13235", "abs": "https://arxiv.org/abs/2512.13235", "authors": ["Jianyuan Bo", "Yuan Fang"], "title": "CORE: Contrastive Masked Feature Reconstruction on Graphs", "comment": null, "summary": "In the rapidly evolving field of self-supervised learning on graphs, generative and contrastive methodologies have emerged as two dominant approaches. Our study focuses on masked feature reconstruction (MFR), a generative technique where a model learns to restore the raw features of masked nodes in a self-supervised manner. We observe that both MFR and graph contrastive learning (GCL) aim to maximize agreement between similar elements. Building on this observation, we reveal a novel theoretical insight: under specific conditions, the objectives of MFR and node-level GCL converge, despite their distinct operational mechanisms. This theoretical connection suggests these approaches are complementary rather than fundamentally different, prompting us to explore their integration to enhance self-supervised learning on graphs. Our research presents Contrastive Masked Feature Reconstruction (CORE), a novel graph self-supervised learning framework that integrates contrastive learning into MFR. Specifically, we form positive pairs exclusively between the original and reconstructed features of masked nodes, encouraging the encoder to prioritize contextual information over the node's own features. Additionally, we leverage the masked nodes themselves as negative samples, combining MFR's reconstructive power with GCL's discriminative ability to better capture intrinsic graph structures. Empirically, our proposed framework CORE significantly outperforms MFR across node and graph classification tasks, demonstrating state-of-the-art results. In particular, CORE surpasses GraphMAE and GraphMAE2 by up to 2.80% and 3.72% on node classification tasks, and by up to 3.82% and 3.76% on graph classification tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCORE\u7684\u65b0\u578b\u56fe\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u5bf9\u6bd4\u5b66\u4e60\u878d\u5165\u63a9\u7801\u7279\u5f81\u91cd\u5efa\u4e2d\uff0c\u5728\u8282\u70b9\u548c\u56fe\u5206\u7c7b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u5728\u5feb\u901f\u53d1\u5c55\u7684\u56fe\u81ea\u76d1\u7763\u5b66\u4e60\u9886\u57df\uff0c\u751f\u6210\u5f0f\u548c\u5bf9\u6bd4\u5f0f\u65b9\u6cd5\u662f\u4e24\u79cd\u4e3b\u8981\u65b9\u6cd5\u3002\u7814\u7a76\u53d1\u73b0\u63a9\u7801\u7279\u5f81\u91cd\u5efa\uff08MFR\uff09\u548c\u56fe\u5bf9\u6bd4\u5b66\u4e60\uff08GCL\uff09\u90fd\u65e8\u5728\u6700\u5927\u5316\u76f8\u4f3c\u5143\u7d20\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\uff0c\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u5b83\u4eec\u7684\u4f18\u5316\u76ee\u6807\u4f1a\u6536\u655b\u3002\u8fd9\u8868\u660e\u8fd9\u4e24\u79cd\u65b9\u6cd5\u662f\u4e92\u8865\u7684\u800c\u975e\u6839\u672c\u4e0d\u540c\u7684\uff0c\u56e0\u6b64\u63a2\u7d22\u5b83\u4eec\u7684\u6574\u5408\u53ef\u4ee5\u589e\u5f3a\u56fe\u81ea\u76d1\u7763\u5b66\u4e60\u3002", "method": "\u63d0\u51fa\u4e86\u5bf9\u6bd4\u63a9\u7801\u7279\u5f81\u91cd\u5efa\uff08CORE\uff09\u6846\u67b6\uff0c\u5c06\u5bf9\u6bd4\u5b66\u4e60\u96c6\u6210\u5230MFR\u4e2d\u3002\u5177\u4f53\u6765\u8bf4\uff1a1\uff09\u4ec5\u4f7f\u7528\u63a9\u7801\u8282\u70b9\u7684\u539f\u59cb\u7279\u5f81\u548c\u91cd\u5efa\u7279\u5f81\u5f62\u6210\u6b63\u6837\u672c\u5bf9\uff0c\u9f13\u52b1\u7f16\u7801\u5668\u4f18\u5148\u8003\u8651\u4e0a\u4e0b\u6587\u4fe1\u606f\u800c\u975e\u8282\u70b9\u81ea\u8eab\u7279\u5f81\uff1b2\uff09\u5229\u7528\u63a9\u7801\u8282\u70b9\u672c\u8eab\u4f5c\u4e3a\u8d1f\u6837\u672c\uff0c\u7ed3\u5408MFR\u7684\u91cd\u5efa\u80fd\u529b\u548cGCL\u7684\u5224\u522b\u80fd\u529b\uff0c\u66f4\u597d\u5730\u6355\u6349\u5185\u5728\u56fe\u7ed3\u6784\u3002", "result": "CORE\u5728\u8282\u70b9\u548c\u56fe\u5206\u7c7b\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8eMFR\uff0c\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5728\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e0a\uff0cCORE\u6bd4GraphMAE\u548cGraphMAE2\u5206\u522b\u9ad8\u51fa2.80%\u548c3.72%\uff1b\u5728\u56fe\u5206\u7c7b\u4efb\u52a1\u4e0a\uff0c\u5206\u522b\u9ad8\u51fa3.82%\u548c3.76%\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86MFR\u548cGCL\u4e4b\u95f4\u7684\u7406\u8bba\u8054\u7cfb\uff0c\u8868\u660e\u5b83\u4eec\u662f\u4e92\u8865\u7684\u65b9\u6cd5\u3002\u63d0\u51fa\u7684CORE\u6846\u67b6\u6210\u529f\u6574\u5408\u4e86\u8fd9\u4e24\u79cd\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u5728\u591a\u4e2a\u56fe\u5b66\u4e60\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u4e3a\u56fe\u81ea\u76d1\u7763\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2512.13083", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13083", "abs": "https://arxiv.org/abs/2512.13083", "authors": ["Saumyaranjan Mohanty", "Aravind Reddy", "Konda Reddy Mopuri"], "title": "DiRe: Diversity-promoting Regularization for Dataset Condensation", "comment": "Accepted to WACV 2026", "summary": "In Dataset Condensation, the goal is to synthesize a small dataset that replicates the training utility of a large original dataset. Existing condensation methods synthesize datasets with significant redundancy, so there is a dire need to reduce redundancy and improve the diversity of the synthesized datasets. To tackle this, we propose an intuitive Diversity Regularizer (DiRe) composed of cosine similarity and Euclidean distance, which can be applied off-the-shelf to various state-of-the-art condensation methods. Through extensive experiments, we demonstrate that the addition of our regularizer improves state-of-the-art condensation methods on various benchmark datasets from CIFAR-10 to ImageNet-1K with respect to generalization and diversity metrics.", "AI": {"tldr": "\u63d0\u51fa\u591a\u6837\u6027\u6b63\u5219\u5316\u5668DiRe\uff0c\u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u548c\u6b27\u6c0f\u8ddd\u79bb\u51cf\u5c11\u6570\u636e\u96c6\u538b\u7f29\u4e2d\u7684\u5197\u4f59\uff0c\u63d0\u9ad8\u5408\u6210\u6570\u636e\u96c6\u7684\u591a\u6837\u6027", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u538b\u7f29\u65b9\u6cd5\u5408\u6210\u7684\u6570\u636e\u96c6\u5b58\u5728\u663e\u8457\u5197\u4f59\uff0c\u9700\u8981\u51cf\u5c11\u5197\u4f59\u5e76\u63d0\u9ad8\u5408\u6210\u6570\u636e\u96c6\u7684\u591a\u6837\u6027", "method": "\u63d0\u51fa\u76f4\u89c2\u7684\u591a\u6837\u6027\u6b63\u5219\u5316\u5668DiRe\uff0c\u7ed3\u5408\u4f59\u5f26\u76f8\u4f3c\u5ea6\u548c\u6b27\u6c0f\u8ddd\u79bb\uff0c\u53ef\u5373\u63d2\u5373\u7528\u5730\u5e94\u7528\u4e8e\u5404\u79cd\u6700\u5148\u8fdb\u7684\u538b\u7f29\u65b9\u6cd5", "result": "\u901a\u8fc7\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6dfb\u52a0DiRe\u6b63\u5219\u5316\u5668\u80fd\u6539\u8fdb\u6700\u5148\u8fdb\u538b\u7f29\u65b9\u6cd5\u5728CIFAR-10\u5230ImageNet-1K\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u6cdb\u5316\u6027\u548c\u591a\u6837\u6027\u6307\u6807", "conclusion": "DiRe\u6b63\u5219\u5316\u5668\u80fd\u6709\u6548\u51cf\u5c11\u6570\u636e\u96c6\u538b\u7f29\u4e2d\u7684\u5197\u4f59\uff0c\u63d0\u9ad8\u5408\u6210\u6570\u636e\u96c6\u7684\u591a\u6837\u6027\uff0c\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u73b0\u6709\u538b\u7f29\u65b9\u6cd5"}}
{"id": "2512.13089", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13089", "abs": "https://arxiv.org/abs/2512.13089", "authors": ["Ziqiang Zhu", "Bowei Yang"], "title": "UniVCD: A New Method for Unsupervised Change Detection in the Open-Vocabulary Era", "comment": "10 pages, 6 figures", "summary": "Change detection (CD) identifies scene changes from multi-temporal observations and is widely used in urban development and environmental monitoring. Most existing CD methods rely on supervised learning, making performance strongly dataset-dependent and incurring high annotation costs; they typically focus on a few predefined categories and generalize poorly to diverse scenes. With the rise of vision foundation models such as SAM2 and CLIP, new opportunities have emerged to relax these constraints. We propose Unified Open-Vocabulary Change Detection (UniVCD), an unsupervised, open-vocabulary change detection method built on frozen SAM2 and CLIP. UniVCD detects category-agnostic changes across diverse scenes and imaging geometries without any labeled data or paired change images. A lightweight feature alignment module is introduced to bridge the spatially detailed representations from SAM2 and the semantic priors from CLIP, enabling high-resolution, semantically aware change estimation while keeping the number of trainable parameters small. On top of this, a streamlined post-processing pipeline is further introduced to suppress noise and pseudo-changes, improving the detection accuracy for objects with well-defined boundaries. Experiments on several public BCD (Binary Change Detection) and SCD (Semantic Change Detection) benchmarks show that UniVCD achieves consistently strong performance and matches or surpasses existing open-vocabulary CD methods in key metrics such as F1 and IoU. The results demonstrate that unsupervised change detection with frozen vision foundation models and lightweight multi-modal alignment is a practical and effective paradigm for open-vocabulary CD. Code and pretrained models will be released at https://github.com/Die-Xie/UniVCD.", "AI": {"tldr": "UniVCD\u662f\u4e00\u79cd\u57fa\u4e8e\u51bb\u7ed3SAM2\u548cCLIP\u7684\u65e0\u76d1\u7763\u5f00\u653e\u8bcd\u6c47\u53d8\u5316\u68c0\u6d4b\u65b9\u6cd5\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\u6216\u914d\u5bf9\u53d8\u5316\u56fe\u50cf\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7279\u5f81\u5bf9\u9f50\u6a21\u5757\u5b9e\u73b0\u9ad8\u5206\u8fa8\u7387\u8bed\u4e49\u611f\u77e5\u7684\u53d8\u5316\u4f30\u8ba1\u3002", "motivation": "\u73b0\u6709\u53d8\u5316\u68c0\u6d4b\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u76d1\u7763\u5b66\u4e60\uff0c\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u6570\u636e\u96c6\u4e14\u6807\u6ce8\u6210\u672c\u9ad8\uff0c\u901a\u5e38\u53ea\u80fd\u68c0\u6d4b\u9884\u5b9a\u4e49\u7c7b\u522b\uff0c\u5bf9\u4e0d\u540c\u573a\u666f\u6cdb\u5316\u80fd\u529b\u5dee\u3002\u968f\u7740\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08\u5982SAM2\u548cCLIP\uff09\u7684\u5174\u8d77\uff0c\u4e3a\u653e\u677e\u8fd9\u4e9b\u9650\u5236\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002", "method": "\u63d0\u51faUniVCD\u65b9\u6cd5\uff1a1\uff09\u57fa\u4e8e\u51bb\u7ed3\u7684SAM2\u548cCLIP\u6784\u5efa\u65e0\u76d1\u7763\u5f00\u653e\u8bcd\u6c47\u53d8\u5316\u68c0\u6d4b\u6846\u67b6\uff1b2\uff09\u5f15\u5165\u8f7b\u91cf\u7ea7\u7279\u5f81\u5bf9\u9f50\u6a21\u5757\uff0c\u6865\u63a5SAM2\u7684\u7a7a\u95f4\u7ec6\u8282\u8868\u793a\u548cCLIP\u7684\u8bed\u4e49\u5148\u9a8c\uff1b3\uff09\u91c7\u7528\u7b80\u5316\u7684\u540e\u5904\u7406\u6d41\u7a0b\u6291\u5236\u566a\u58f0\u548c\u4f2a\u53d8\u5316\uff0c\u63d0\u9ad8\u8fb9\u754c\u6e05\u6670\u5bf9\u8c61\u7684\u68c0\u6d4b\u7cbe\u5ea6\u3002", "result": "\u5728\u591a\u4e2a\u516c\u5f00BCD\u548cSCD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUniVCD\u8868\u73b0\u51fa\u7a33\u5b9a\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u5728F1\u548cIoU\u7b49\u5173\u952e\u6307\u6807\u4e0a\u5339\u914d\u6216\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u5f00\u653e\u8bcd\u6c47\u53d8\u5316\u68c0\u6d4b\u65b9\u6cd5\u3002", "conclusion": "\u57fa\u4e8e\u51bb\u7ed3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u548c\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u5bf9\u9f50\u7684\u65e0\u76d1\u7763\u53d8\u5316\u68c0\u6d4b\u662f\u5f00\u653e\u8bcd\u6c47\u53d8\u5316\u68c0\u6d4b\u7684\u5b9e\u7528\u6709\u6548\u8303\u5f0f\u3002\u4ee3\u7801\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u5c06\u5728GitHub\u4e0a\u53d1\u5e03\u3002"}}
{"id": "2512.13255", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13255", "abs": "https://arxiv.org/abs/2512.13255", "authors": ["Yunhong Min", "Juil Koo", "Seungwoo Yoo", "Minhyuk Sung"], "title": "B\u00e9zierFlow: B\u00e9zier Stochastic Interpolant Schedulers for Few-Step Generation", "comment": "Project page: https://bezierflow.github.io", "summary": "We introduce B\u00e9zierFlow, a lightweight training approach for few-step generation with pretrained diffusion and flow models. B\u00e9zierFlow achieves a 2-3x performance improvement for sampling with $\\leq$ 10 NFEs while requiring only 15 minutes of training. Recent lightweight training approaches have shown promise by learning optimal timesteps, but their scope remains restricted to ODE discretizations. To broaden this scope, we propose learning the optimal transformation of the sampling trajectory by parameterizing stochastic interpolant (SI) schedulers. The main challenge lies in designing a parameterization that satisfies critical desiderata, including boundary conditions, differentiability, and monotonicity of the SNR. To effectively meet these requirements, we represent scheduler functions as B\u00e9zier functions, where control points naturally enforce these properties. This reduces the problem to learning an ordered set of points in the time range, while the interpretation of the points changes from ODE timesteps to B\u00e9zier control points. Across a range of pretrained diffusion and flow models, B\u00e9zierFlow consistently outperforms prior timestep-learning methods, demonstrating the effectiveness of expanding the search space from discrete timesteps to B\u00e9zier-based trajectory transformations.", "AI": {"tldr": "B\u00e9zierFlow\uff1a\u4e00\u79cd\u8f7b\u91cf\u7ea7\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u8bad\u7ec3\u6269\u6563\u548c\u6d41\u6a21\u578b\u7684\u5c11\u6b65\u751f\u6210\uff0c\u901a\u8fc7\u8d1d\u585e\u5c14\u51fd\u6570\u53c2\u6570\u5316\u8c03\u5ea6\u5668\uff0c\u5728\u226410\u6b65\u5185\u5b9e\u73b02-3\u500d\u6027\u80fd\u63d0\u5347\uff0c\u4ec5\u970015\u5206\u949f\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u73b0\u6709\u8f7b\u91cf\u7ea7\u8bad\u7ec3\u65b9\u6cd5\u4e3b\u8981\u5b66\u4e60\u6700\u4f18\u65f6\u95f4\u6b65\u957f\uff0c\u4f46\u5176\u8303\u56f4\u4ec5\u9650\u4e8eODE\u79bb\u6563\u5316\u3002\u4e3a\u4e86\u6269\u5927\u641c\u7d22\u7a7a\u95f4\uff0c\u9700\u8981\u5b66\u4e60\u91c7\u6837\u8f68\u8ff9\u7684\u6700\u4f18\u53d8\u6362\uff0c\u901a\u8fc7\u53c2\u6570\u5316\u968f\u673a\u63d2\u503c\u8c03\u5ea6\u5668\u6765\u5b9e\u73b0\u3002", "method": "\u63d0\u51fa\u5c06\u8c03\u5ea6\u5668\u51fd\u6570\u8868\u793a\u4e3a\u8d1d\u585e\u5c14\u51fd\u6570\uff0c\u63a7\u5236\u70b9\u81ea\u7136\u6ee1\u8db3\u8fb9\u754c\u6761\u4ef6\u3001\u53ef\u5fae\u6027\u548c\u4fe1\u566a\u6bd4\u5355\u8c03\u6027\u7b49\u5173\u952e\u8981\u6c42\u3002\u5c06\u95ee\u9898\u7b80\u5316\u4e3a\u5b66\u4e60\u65f6\u95f4\u8303\u56f4\u5185\u7684\u6709\u5e8f\u70b9\u96c6\uff0c\u4eceODE\u65f6\u95f4\u6b65\u89e3\u91ca\u8f6c\u53d8\u4e3a\u8d1d\u585e\u5c14\u63a7\u5236\u70b9\u3002", "result": "\u5728\u591a\u79cd\u9884\u8bad\u7ec3\u6269\u6563\u548c\u6d41\u6a21\u578b\u4e0a\uff0cB\u00e9zierFlow\u59cb\u7ec8\u4f18\u4e8e\u5148\u524d\u7684\u65f6\u95f4\u6b65\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728\u226410\u6b65\u91c7\u6837\u65f6\u5b9e\u73b02-3\u500d\u6027\u80fd\u63d0\u5347\uff0c\u4ec5\u970015\u5206\u949f\u8bad\u7ec3\u65f6\u95f4\uff0c\u8bc1\u660e\u4e86\u4ece\u79bb\u6563\u65f6\u95f4\u6b65\u6269\u5c55\u5230\u57fa\u4e8e\u8d1d\u585e\u5c14\u7684\u8f68\u8ff9\u53d8\u6362\u641c\u7d22\u7a7a\u95f4\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u5c06\u8c03\u5ea6\u5668\u53c2\u6570\u5316\u4e3a\u8d1d\u585e\u5c14\u51fd\u6570\uff0cB\u00e9zierFlow\u6210\u529f\u6269\u5c55\u4e86\u8f7b\u91cf\u7ea7\u8bad\u7ec3\u65b9\u6cd5\u7684\u641c\u7d22\u7a7a\u95f4\uff0c\u4ece\u79bb\u6563\u65f6\u95f4\u6b65\u5b66\u4e60\u8f6c\u5411\u8fde\u7eed\u8f68\u8ff9\u53d8\u6362\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5c11\u6b65\u751f\u6210\u7684\u6027\u80fd\u3002"}}
{"id": "2512.13095", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13095", "abs": "https://arxiv.org/abs/2512.13095", "authors": ["Feng Zhang", "Zezhong Tan", "Xinhong Ma", "Ziqiang Dong", "Xi Leng", "Jianfei Zhao", "Xin Sun", "Yang Yang"], "title": "ADHint: Adaptive Hints with Difficulty Priors for Reinforcement Learning", "comment": null, "summary": "To combine the advantages of Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), recent methods have integrated ''hints'' into post-training, which are prefix segments of complete reasoning trajectories, aiming for powerful knowledge expansion and reasoning generalization. However, existing hint-based RL methods typically ignore difficulty when scheduling hint ratios and estimating relative advantages, leading to unstable learning and excessive imitation of off-policy hints. In this work, we propose ADHint, which treats difficulty as a key factor in both hint-ratio schedule and relative-advantage estimation to achieve a better trade-off between exploration and imitation. Specifically, we propose Adaptive Hint with Sample Difficulty Prior, which evaluates each sample's difficulty under the policy model and accordingly schedules an appropriate hint ratio to guide its rollouts. We also introduce Consistency-based Gradient Modulation and Selective Masking for Hint Preservation to modulate token-level gradients within hints, preventing biased and destructive updates. Additionally, we propose Advantage Estimation with Rollout Difficulty Posterior, which leverages the relative difficulty of rollouts with and without hints to estimate their respective advantages, thereby achieving more balanced updates. Extensive experiments across diverse modalities, model scales, and domains demonstrate that ADHint delivers superior reasoning ability and out-of-distribution generalization, consistently surpassing existing methods in both pass@1 and avg@8. Our code and dataset will be made publicly available upon paper acceptance.", "AI": {"tldr": "ADHint\u662f\u4e00\u79cd\u7ed3\u5408SFT\u548cRL\u4f18\u52bf\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u96be\u5ea6\u8c03\u5ea6\u63d0\u793a\u6bd4\u4f8b\u548c\u4f18\u52bf\u4f30\u8ba1\uff0c\u5728\u63a2\u7d22\u548c\u6a21\u4eff\u4e4b\u95f4\u53d6\u5f97\u66f4\u597d\u5e73\u8861\uff0c\u63d0\u5347\u63a8\u7406\u80fd\u529b\u548c\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u63d0\u793a\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u5ffd\u7565\u96be\u5ea6\u56e0\u7d20\uff0c\u5728\u8c03\u5ea6\u63d0\u793a\u6bd4\u4f8b\u548c\u4f30\u8ba1\u76f8\u5bf9\u4f18\u52bf\u65f6\u5b58\u5728\u95ee\u9898\uff0c\u5bfc\u81f4\u5b66\u4e60\u4e0d\u7a33\u5b9a\u548c\u8fc7\u5ea6\u6a21\u4eff\u79bb\u7b56\u7565\u63d0\u793a\u3002\u9700\u8981\u4e00\u79cd\u66f4\u597d\u7684\u65b9\u6cd5\u6765\u5e73\u8861\u63a2\u7d22\u548c\u6a21\u4eff\u3002", "method": "\u63d0\u51faADHint\u65b9\u6cd5\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6280\u672f\uff1a1) \u81ea\u9002\u5e94\u63d0\u793a\u4e0e\u6837\u672c\u96be\u5ea6\u5148\u9a8c\uff0c\u6839\u636e\u6837\u672c\u96be\u5ea6\u8c03\u5ea6\u9002\u5f53\u63d0\u793a\u6bd4\u4f8b\uff1b2) \u4e00\u81f4\u6027\u68af\u5ea6\u8c03\u5236\u548c\u9009\u62e9\u6027\u63a9\u7801\u63d0\u793a\u4fdd\u7559\uff0c\u8c03\u5236\u63d0\u793a\u5185\u7684\u4ee4\u724c\u7ea7\u68af\u5ea6\uff1b3) \u57fa\u4e8e\u63a8\u51fa\u96be\u5ea6\u540e\u9a8c\u7684\u4f18\u52bf\u4f30\u8ba1\uff0c\u5229\u7528\u6709/\u65e0\u63d0\u793a\u63a8\u51fa\u7684\u76f8\u5bf9\u96be\u5ea6\u4f30\u8ba1\u5404\u81ea\u4f18\u52bf\u3002", "result": "\u5728\u591a\u79cd\u6a21\u6001\u3001\u6a21\u578b\u89c4\u6a21\u548c\u9886\u57df\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0cADHint\u5728\u63a8\u7406\u80fd\u529b\u548c\u5206\u5e03\u5916\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5728pass@1\u548cavg@8\u6307\u6807\u4e0a\u6301\u7eed\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ADHint\u901a\u8fc7\u5c06\u96be\u5ea6\u4f5c\u4e3a\u5173\u952e\u56e0\u7d20\u7eb3\u5165\u63d0\u793a\u6bd4\u4f8b\u8c03\u5ea6\u548c\u76f8\u5bf9\u4f18\u52bf\u4f30\u8ba1\uff0c\u5b9e\u73b0\u4e86\u63a2\u7d22\u548c\u6a21\u4eff\u4e4b\u95f4\u66f4\u597d\u7684\u5e73\u8861\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u80fd\u529b\u548c\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2512.13101", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13101", "abs": "https://arxiv.org/abs/2512.13101", "authors": ["Wenjing Lu", "Yi Hong", "Yang Yang"], "title": "Harmonizing Generalization and Specialization: Uncertainty-Informed Collaborative Learning for Semi-supervised Medical Image Segmentation", "comment": "This work has been submitted to the IEEE TMI for possible publication", "summary": "Vision foundation models have demonstrated strong generalization in medical image segmentation by leveraging large-scale, heterogeneous pretraining. However, they often struggle to generalize to specialized clinical tasks under limited annotations or rare pathological variations, due to a mismatch between general priors and task-specific requirements. To address this, we propose Uncertainty-informed Collaborative Learning (UnCoL), a dual-teacher framework that harmonizes generalization and specialization in semi-supervised medical image segmentation. Specifically, UnCoL distills both visual and semantic representations from a frozen foundation model to transfer general knowledge, while concurrently maintaining a progressively adapting teacher to capture fine-grained and task-specific representations. To balance guidance from both teachers, pseudo-label learning in UnCoL is adaptively regulated by predictive uncertainty, which selectively suppresses unreliable supervision and stabilizes learning in ambiguous regions. Experiments on diverse 2D and 3D segmentation benchmarks show that UnCoL consistently outperforms state-of-the-art semi-supervised methods and foundation model baselines. Moreover, our model delivers near fully supervised performance with markedly reduced annotation requirements.", "AI": {"tldr": "UnCoL\u662f\u4e00\u4e2a\u7528\u4e8e\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u7684\u53cc\u6559\u5e08\u6846\u67b6\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u6307\u5bfc\u7684\u534f\u540c\u5b66\u4e60\uff0c\u5e73\u8861\u901a\u7528\u57fa\u7840\u6a21\u578b\u7684\u4e13\u4e1a\u77e5\u8bc6\u8fc1\u79fb\u548c\u4efb\u52a1\u7279\u5b9a\u8868\u793a\u5b66\u4e60\u3002", "motivation": "\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u5728\u6709\u9650\u6807\u6ce8\u6216\u7f55\u89c1\u75c5\u7406\u53d8\u5316\u4e0b\uff0c\u901a\u7528\u5148\u9a8c\u4e0e\u4efb\u52a1\u7279\u5b9a\u9700\u6c42\u4e0d\u5339\u914d\uff0c\u5bfc\u81f4\u5728\u4e13\u4e1a\u4e34\u5e8a\u4efb\u52a1\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u63d0\u51faUnCoL\uff08Uncertainty-informed Collaborative Learning\uff09\u53cc\u6559\u5e08\u6846\u67b6\uff1a1\uff09\u4ece\u51bb\u7ed3\u7684\u57fa\u7840\u6a21\u578b\u4e2d\u84b8\u998f\u89c6\u89c9\u548c\u8bed\u4e49\u8868\u793a\u4ee5\u8fc1\u79fb\u901a\u7528\u77e5\u8bc6\uff1b2\uff09\u7ef4\u62a4\u4e00\u4e2a\u6e10\u8fdb\u9002\u5e94\u7684\u6559\u5e08\u6a21\u578b\u4ee5\u6355\u6349\u7ec6\u7c92\u5ea6\u7684\u4efb\u52a1\u7279\u5b9a\u8868\u793a\uff1b3\uff09\u901a\u8fc7\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u81ea\u9002\u5e94\u8c03\u8282\u4f2a\u6807\u7b7e\u5b66\u4e60\uff0c\u9009\u62e9\u6027\u5730\u6291\u5236\u4e0d\u53ef\u9760\u76d1\u7763\u5e76\u7a33\u5b9a\u6a21\u7cca\u533a\u57df\u7684\u5b66\u4e60\u3002", "result": "\u5728\u591a\u6837\u5316\u76842D\u548c3D\u5206\u5272\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUnCoL\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u534a\u76d1\u7763\u65b9\u6cd5\u548c\u57fa\u7840\u6a21\u578b\u57fa\u7ebf\uff0c\u5728\u663e\u8457\u51cf\u5c11\u6807\u6ce8\u9700\u6c42\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u63a5\u8fd1\u5168\u76d1\u7763\u7684\u6027\u80fd\u3002", "conclusion": "UnCoL\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u6307\u5bfc\u7684\u534f\u540c\u5b66\u4e60\uff0c\u6709\u6548\u534f\u8c03\u4e86\u6cdb\u5316\u4e0e\u4e13\u4e1a\u5316\uff0c\u4e3a\u534a\u76d1\u7763\u533b\u5b66\u56fe\u50cf\u5206\u5272\u63d0\u4f9b\u4e86\u4e00\u79cd\u5e73\u8861\u901a\u7528\u77e5\u8bc6\u548c\u4efb\u52a1\u7279\u5b9a\u9700\u6c42\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13316", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13316", "abs": "https://arxiv.org/abs/2512.13316", "authors": ["Mayank Gulati", "Benedikt Gro\u00df", "Gerhard Wunder"], "title": "ALIGN-FL: Architecture-independent Learning through Invariant Generative component sharing in Federated Learning", "comment": "Accepted at 2025 International Conference on Cyber-Enabled Distributed Computing and Knowledge Discovery (CyberC)", "summary": "We present ALIGN-FL, a novel approach to distributed learning that addresses the challenge of learning from highly disjoint data distributions through selective sharing of generative components. Instead of exchanging full model parameters, our framework enables privacy-preserving learning by transferring only generative capabilities across clients, while the server performs global training using synthetic samples. Through complementary privacy mechanisms: DP-SGD with adaptive clipping and Lipschitz regularized VAE decoders and a stateful architecture supporting heterogeneous clients, we experimentally validate our approach on MNIST and Fashion-MNIST datasets with cross-domain outliers. Our analysis demonstrates that both privacy mechanisms effectively map sensitive outliers to typical data points while maintaining utility in extreme Non-IID scenarios typical of cross-silo collaborations.\n  Index Terms: Client-invariant Learning, Federated Learning (FL), Privacy-preserving Generative Models, Non-Independent and Identically Distributed (Non-IID), Heterogeneous Architectures", "AI": {"tldr": "ALIGN-FL\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5206\u5e03\u5f0f\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u5171\u4eab\u751f\u6210\u7ec4\u4ef6\u6765\u5904\u7406\u9ad8\u5ea6\u4e0d\u76f8\u5173\u7684\u6570\u636e\u5206\u5e03\uff0c\u4f7f\u7528\u9690\u79c1\u4fdd\u62a4\u673a\u5236\u5728\u5ba2\u6237\u7aef\u95f4\u4f20\u8f93\u751f\u6210\u80fd\u529b\u800c\u975e\u5b8c\u6574\u6a21\u578b\u53c2\u6570\u3002", "motivation": "\u89e3\u51b3\u5728\u9ad8\u5ea6\u4e0d\u76f8\u5173\u7684\u6570\u636e\u5206\u5e03\uff08Non-IID\uff09\u4e0b\u8fdb\u884c\u5206\u5e03\u5f0f\u5b66\u4e60\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u8de8\u57df\u534f\u4f5c\u573a\u666f\u4e2d\uff0c\u9700\u8981\u4fdd\u62a4\u9690\u79c1\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6548\u7528\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u9009\u62e9\u6027\u5171\u4eab\u751f\u6210\u7ec4\u4ef6\u7684\u65b9\u6cd5\uff0c\u5ba2\u6237\u7aef\u53ea\u4f20\u8f93\u751f\u6210\u80fd\u529b\u800c\u975e\u5b8c\u6574\u6a21\u578b\u53c2\u6570\uff1b\u670d\u52a1\u5668\u4f7f\u7528\u5408\u6210\u6837\u672c\u8fdb\u884c\u5168\u5c40\u8bad\u7ec3\uff1b\u7ed3\u5408DP-SGD\u81ea\u9002\u5e94\u88c1\u526a\u548cLipschitz\u6b63\u5219\u5316VAE\u89e3\u7801\u5668\u4e24\u79cd\u9690\u79c1\u673a\u5236\uff1b\u652f\u6301\u5f02\u6784\u5ba2\u6237\u7aef\u7684\u67b6\u6784\u8bbe\u8ba1\u3002", "result": "\u5728MNIST\u548cFashion-MNIST\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4e24\u79cd\u9690\u79c1\u673a\u5236\u90fd\u80fd\u5c06\u654f\u611f\u5f02\u5e38\u503c\u6620\u5c04\u5230\u5178\u578b\u6570\u636e\u70b9\uff0c\u540c\u65f6\u5728\u6781\u7aefNon-IID\u573a\u666f\u4e2d\u4fdd\u6301\u6a21\u578b\u6548\u7528\u3002", "conclusion": "ALIGN-FL\u6846\u67b6\u901a\u8fc7\u9690\u79c1\u4fdd\u62a4\u7684\u751f\u6210\u6a21\u578b\u5171\u4eab\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u8de8\u57df\u534f\u4f5c\u4e2d\u7684Non-IID\u6570\u636e\u5206\u5e03\u6311\u6218\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u5b66\u4e60\u6548\u679c\u3002"}}
{"id": "2512.13104", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13104", "abs": "https://arxiv.org/abs/2512.13104", "authors": ["Yan Zhang", "Baoxin Li", "Han Sun", "Yuhang Gao", "Mingtai Zhang", "Pei Wang"], "title": "FID-Net: A Feature-Enhanced Deep Learning Network for Forest Infestation Detection", "comment": null, "summary": "Forest pests threaten ecosystem stability, requiring efficient monitoring. To overcome the limitations of traditional methods in large-scale, fine-grained detection, this study focuses on accurately identifying infected trees and analyzing infestation patterns. We propose FID-Net, a deep learning model that detects pest-affected trees from UAV visible-light imagery and enables infestation analysis via three spatial metrics. Based on YOLOv8n, FID-Net introduces a lightweight Feature Enhancement Module (FEM) to extract disease-sensitive cues, an Adaptive Multi-scale Feature Fusion Module (AMFM) to align and fuse dual-branch features (RGB and FEM-enhanced), and an Efficient Channel Attention (ECA) mechanism to enhance discriminative information efficiently. From detection results, we construct a pest situation analysis framework using: (1) Kernel Density Estimation to locate infection hotspots; (2) neighborhood evaluation to assess healthy trees' infection risk; (3) DBSCAN clustering to identify high-density healthy clusters as priority protection zones. Experiments on UAV imagery from 32 forest plots in eastern Tianshan, China, show that FID-Net achieves 86.10% precision, 75.44% recall, 82.29% mAP@0.5, and 64.30% mAP@0.5:0.95, outperforming mainstream YOLO models. Analysis confirms infected trees exhibit clear clustering, supporting targeted forest protection. FID-Net enables accurate tree health discrimination and, combined with spatial metrics, provides reliable data for intelligent pest monitoring, early warning, and precise management.", "AI": {"tldr": "FID-Net\uff1a\u57fa\u4e8eYOLOv8n\u6539\u8fdb\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5229\u7528\u65e0\u4eba\u673a\u53ef\u89c1\u5149\u56fe\u50cf\u68c0\u6d4b\u68ee\u6797\u75c5\u866b\u5bb3\u6811\u6728\uff0c\u5e76\u901a\u8fc7\u4e09\u79cd\u7a7a\u95f4\u6307\u6807\u8fdb\u884c\u866b\u5bb3\u5206\u6790\uff0c\u5728\u65b0\u7586\u5929\u5c71\u4e1c\u90e832\u4e2a\u6797\u533a\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u4e3b\u6d41YOLO\u6a21\u578b\u3002", "motivation": "\u68ee\u6797\u75c5\u866b\u5bb3\u5a01\u80c1\u751f\u6001\u7cfb\u7edf\u7a33\u5b9a\uff0c\u9700\u8981\u9ad8\u6548\u76d1\u6d4b\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u3001\u7ec6\u7c92\u5ea6\u68c0\u6d4b\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u672c\u7814\u7a76\u65e8\u5728\u51c6\u786e\u8bc6\u522b\u611f\u67d3\u6811\u6728\u5e76\u5206\u6790\u866b\u5bb3\u6a21\u5f0f\u3002", "method": "\u63d0\u51faFID-Net\u6a21\u578b\uff1a\u57fa\u4e8eYOLOv8n\uff0c\u5f15\u5165\u8f7b\u91cf\u7ea7\u7279\u5f81\u589e\u5f3a\u6a21\u5757(FEM)\u63d0\u53d6\u75c5\u5bb3\u654f\u611f\u7279\u5f81\uff0c\u81ea\u9002\u5e94\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u6a21\u5757(AMFM)\u5bf9\u9f50\u878d\u5408RGB\u548cFEM\u589e\u5f3a\u7684\u53cc\u5206\u652f\u7279\u5f81\uff0c\u4ee5\u53ca\u9ad8\u6548\u901a\u9053\u6ce8\u610f\u529b(ECA)\u673a\u5236\u589e\u5f3a\u5224\u522b\u4fe1\u606f\u3002\u4ece\u68c0\u6d4b\u7ed3\u679c\u6784\u5efa\u866b\u5bb3\u5206\u6790\u6846\u67b6\uff1a\u6838\u5bc6\u5ea6\u4f30\u8ba1\u5b9a\u4f4d\u611f\u67d3\u70ed\u70b9\u3001\u90bb\u57df\u8bc4\u4f30\u5065\u5eb7\u6811\u6728\u611f\u67d3\u98ce\u9669\u3001DBSCAN\u805a\u7c7b\u8bc6\u522b\u9ad8\u5bc6\u5ea6\u5065\u5eb7\u96c6\u7fa4\u4f5c\u4e3a\u4f18\u5148\u4fdd\u62a4\u533a\u57df\u3002", "result": "\u5728\u65b0\u7586\u5929\u5c71\u4e1c\u90e832\u4e2a\u6797\u533a\u7684\u65e0\u4eba\u673a\u56fe\u50cf\u5b9e\u9a8c\u4e2d\uff0cFID-Net\u8fbe\u523086.10%\u7cbe\u786e\u7387\u300175.44%\u53ec\u56de\u7387\u300182.29% mAP@0.5\u548c64.30% mAP@0.5:0.95\uff0c\u4f18\u4e8e\u4e3b\u6d41YOLO\u6a21\u578b\u3002\u5206\u6790\u8bc1\u5b9e\u611f\u67d3\u6811\u6728\u5448\u73b0\u660e\u663e\u805a\u7c7b\u6a21\u5f0f\u3002", "conclusion": "FID-Net\u80fd\u591f\u51c6\u786e\u533a\u5206\u6811\u6728\u5065\u5eb7\u72b6\u51b5\uff0c\u7ed3\u5408\u7a7a\u95f4\u6307\u6807\u4e3a\u667a\u80fd\u866b\u5bb3\u76d1\u6d4b\u3001\u65e9\u671f\u9884\u8b66\u548c\u7cbe\u51c6\u7ba1\u7406\u63d0\u4f9b\u53ef\u9760\u6570\u636e\uff0c\u652f\u6301\u6709\u9488\u5bf9\u6027\u7684\u68ee\u6797\u4fdd\u62a4\u3002"}}
{"id": "2512.13107", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13107", "abs": "https://arxiv.org/abs/2512.13107", "authors": ["Zhijian He", "Feifei Liu", "Yuwei Li", "Zhanpeng Liu", "Jintao Cheng", "Xieyuanli Chen", "Xiaoyu Tang"], "title": "Diffusion-Based Restoration for Multi-Modal 3D Object Detection in Adverse Weather", "comment": null, "summary": "Multi-modal 3D object detection is important for reliable perception in robotics and autonomous driving. However, its effectiveness remains limited under adverse weather conditions due to weather-induced distortions and misalignment between different data modalities. In this work, we propose DiffFusion, a novel framework designed to enhance robustness in challenging weather through diffusion-based restoration and adaptive cross-modal fusion. Our key insight is that diffusion models possess strong capabilities for denoising and generating data that can adapt to various weather conditions. Building on this, DiffFusion introduces Diffusion-IR restoring images degraded by weather effects and Point Cloud Restoration (PCR) compensating for corrupted LiDAR data using image object cues. To tackle misalignments between two modalities, we develop Bidirectional Adaptive Fusion and Alignment Module (BAFAM). It enables dynamic multi-modal fusion and bidirectional bird's-eye view (BEV) alignment to maintain consistent spatial correspondence. Extensive experiments on three public datasets show that DiffFusion achieves state-of-the-art robustness under adverse weather while preserving strong clean-data performance. Zero-shot results on the real-world DENSE dataset further validate its generalization. The implementation of our DiffFusion will be released as open-source.", "AI": {"tldr": "DiffFusion\uff1a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u591a\u6a21\u60013D\u76ee\u6807\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u50cf\u548c\u70b9\u4e91\u6062\u590d\u4ee5\u53ca\u81ea\u9002\u5e94\u878d\u5408\u5bf9\u9f50\uff0c\u63d0\u5347\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027", "motivation": "\u73b0\u6709\u591a\u6a21\u60013D\u76ee\u6807\u68c0\u6d4b\u5728\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u6548\u679c\u53d7\u9650\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u5929\u6c14\u5f15\u8d77\u7684\u56fe\u50cf\u5931\u771f\u548c\u4e0d\u540c\u6570\u636e\u6a21\u6001\u4e4b\u95f4\u7684\u4e0d\u5bf9\u9f50\u95ee\u9898", "method": "1. Diffusion-IR\uff1a\u4f7f\u7528\u6269\u6563\u6a21\u578b\u6062\u590d\u53d7\u5929\u6c14\u5f71\u54cd\u7684\u56fe\u50cf\uff1b2. Point Cloud Restoration (PCR)\uff1a\u5229\u7528\u56fe\u50cf\u76ee\u6807\u7ebf\u7d22\u8865\u507f\u53d7\u635f\u7684LiDAR\u6570\u636e\uff1b3. BAFAM\uff1a\u53cc\u5411\u81ea\u9002\u5e94\u878d\u5408\u5bf9\u9f50\u6a21\u5757\uff0c\u5b9e\u73b0\u52a8\u6001\u591a\u6a21\u6001\u878d\u5408\u548c\u53cc\u5411\u9e1f\u77b0\u56fe\u5bf9\u9f50", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6076\u52a3\u5929\u6c14\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u5927\u7684\u5e72\u51c0\u6570\u636e\u6027\u80fd\uff1b\u5728\u771f\u5b9e\u4e16\u754cDENSE\u6570\u636e\u96c6\u4e0a\u7684\u96f6\u6837\u672c\u7ed3\u679c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5176\u6cdb\u5316\u80fd\u529b", "conclusion": "DiffFusion\u901a\u8fc7\u6269\u6563\u6a21\u578b\u6062\u590d\u548c\u81ea\u9002\u5e94\u8de8\u6a21\u6001\u878d\u5408\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u6a21\u60013D\u76ee\u6807\u68c0\u6d4b\u5728\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2512.13337", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13337", "abs": "https://arxiv.org/abs/2512.13337", "authors": ["Si Qi Goh", "Yongsen Zheng", "Ziyao Liu", "Sami Hormi", "Kwok-Yan Lam"], "title": "FROC: A Unified Framework with Risk-Optimized Control for Machine Unlearning in LLMs", "comment": null, "summary": "Machine unlearning (MU) seeks to eliminate the influence of specific training examples from deployed models. As large language models (LLMs) become widely used, managing risks arising from insufficient forgetting or utility loss is increasingly crucial. Current MU techniques lack effective mechanisms for evaluating and controlling these risks, hindering the selection of strategies that appropriately balance safety and utility, and raising trust concerns surrounding the \"right to be forgotten.\" To address these issues, we propose FROC, a unified framework with Risk-Optimized Control for machine unlearning in LLMs. FROC is built around a conformal-style risk-control formulation that expresses a user-specified risk budget on unlearning behavior. This probability-based constraint enables FROC to compare MU strategies, identify feasible operating regions, and guide hyperparameter selection according to desired trade-offs between forgetting sufficiency and utility preservation. To operationalize this constraint, FROC introduces a smoothly varying continuous risk model that aggregates forgetting deficiency and utility degradation into a single configuration-level score. Building on conformal risk analysis, FROC computes (1) the Conformal Unlearning Risk (CUR), a data-driven estimated value on the probability that forgotten samples continue to influence model predictions, and (2) risk-controlled configuration sets, which identify unlearning hyperparameters that are valid under the specified risk budget. Experiments across multiple LLM MU methods demonstrate that FROC produces stable, interpretable risk landscapes and reveals consistent relationships between unlearning configurations, semantic shift, and utility impact. FROC reframes MU as a controllable, risk-aware process and offers a practical foundation for managing unlearning behavior in large-scale LLM deployments.", "AI": {"tldr": "FROC\u662f\u4e00\u4e2a\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u673a\u5668\u9057\u5fd8\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u901a\u8fc7\u98ce\u9669\u4f18\u5316\u63a7\u5236\u6765\u5e73\u8861\u9057\u5fd8\u5145\u5206\u6027\u548c\u6548\u7528\u4fdd\u7559\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u98ce\u9669\u8bc4\u4f30\u548c\u914d\u7f6e\u9009\u62e9\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u9057\u5fd8\u6280\u672f\u7f3a\u4e4f\u8bc4\u4f30\u548c\u63a7\u5236\u98ce\u9669\u7684\u6709\u6548\u673a\u5236\uff0c\u96be\u4ee5\u5728\u5b89\u5168\u6027\u548c\u6548\u7528\u4e4b\u95f4\u53d6\u5f97\u9002\u5f53\u5e73\u8861\uff0c\u8fd9\u963b\u788d\u4e86\"\u88ab\u9057\u5fd8\u6743\"\u7684\u5b9e\u73b0\u5e76\u5f15\u53d1\u4fe1\u4efb\u95ee\u9898\u3002", "method": "FROC\u91c7\u7528\u7b26\u5408\u6027\u98ce\u683c\u7684\u98ce\u9669\u63a7\u5236\u6846\u67b6\uff0c\u5f15\u5165\u8fde\u7eed\u98ce\u9669\u6a21\u578b\u805a\u5408\u9057\u5fd8\u4e0d\u8db3\u548c\u6548\u7528\u9000\u5316\uff0c\u8ba1\u7b97\u7b26\u5408\u6027\u9057\u5fd8\u98ce\u9669(CUR)\u548c\u98ce\u9669\u63a7\u5236\u914d\u7f6e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660eFROC\u80fd\u4ea7\u751f\u7a33\u5b9a\u3001\u53ef\u89e3\u91ca\u7684\u98ce\u9669\u666f\u89c2\uff0c\u63ed\u793a\u9057\u5fd8\u914d\u7f6e\u3001\u8bed\u4e49\u504f\u79fb\u548c\u6548\u7528\u5f71\u54cd\u4e4b\u95f4\u7684\u4e00\u81f4\u5173\u7cfb\uff0c\u4e3a\u5927\u89c4\u6a21LLM\u90e8\u7f72\u63d0\u4f9b\u5b9e\u7528\u57fa\u7840\u3002", "conclusion": "FROC\u5c06\u673a\u5668\u9057\u5fd8\u91cd\u65b0\u5b9a\u4e49\u4e3a\u53ef\u63a7\u3001\u98ce\u9669\u611f\u77e5\u7684\u8fc7\u7a0b\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u90e8\u7f72\u4e2d\u7684\u9057\u5fd8\u884c\u4e3a\u7ba1\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\u3002"}}
{"id": "2512.13352", "categories": ["cs.LG", "cs.CL", "cs.CR"], "pdf": "https://arxiv.org/pdf/2512.13352", "abs": "https://arxiv.org/abs/2512.13352", "authors": ["Ali Al Sahili", "Ali Chehab", "Razane Tajeddine"], "title": "On the Effectiveness of Membership Inference in Targeted Data Extraction from Large Language Models", "comment": "Accepted to IEEE Conference on Secure and Trustworthy Machine Learning (SaTML) 2026", "summary": "Large Language Models (LLMs) are prone to memorizing training data, which poses serious privacy risks. Two of the most prominent concerns are training data extraction and Membership Inference Attacks (MIAs). Prior research has shown that these threats are interconnected: adversaries can extract training data from an LLM by querying the model to generate a large volume of text and subsequently applying MIAs to verify whether a particular data point was included in the training set. In this study, we integrate multiple MIA techniques into the data extraction pipeline to systematically benchmark their effectiveness. We then compare their performance in this integrated setting against results from conventional MIA benchmarks, allowing us to evaluate their practical utility in real-world extraction scenarios.", "AI": {"tldr": "\u7814\u7a76\u5c06\u591a\u79cd\u6210\u5458\u63a8\u7406\u653b\u51fb\u6280\u672f\u96c6\u6210\u5230LLM\u8bad\u7ec3\u6570\u636e\u63d0\u53d6\u6d41\u7a0b\u4e2d\uff0c\u7cfb\u7edf\u8bc4\u4f30\u5b83\u4eec\u5728\u771f\u5b9e\u6570\u636e\u63d0\u53d6\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e0e\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u5bf9\u6bd4\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5bb9\u6613\u8bb0\u5fc6\u8bad\u7ec3\u6570\u636e\uff0c\u5e26\u6765\u4e25\u91cd\u7684\u9690\u79c1\u98ce\u9669\uff0c\u7279\u522b\u662f\u8bad\u7ec3\u6570\u636e\u63d0\u53d6\u548c\u6210\u5458\u63a8\u7406\u653b\u51fb\u3002\u73b0\u6709\u7814\u7a76\u8868\u660e\u8fd9\u4e24\u79cd\u5a01\u80c1\u76f8\u4e92\u5173\u8054\uff0c\u4f46\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540cMIA\u6280\u672f\u5728\u771f\u5b9e\u6570\u636e\u63d0\u53d6\u573a\u666f\u4e2d\u7684\u5b9e\u9645\u6548\u679c\u3002", "method": "\u5c06\u591a\u79cd\u6210\u5458\u63a8\u7406\u653b\u51fb\u6280\u672f\u96c6\u6210\u5230\u6570\u636e\u63d0\u53d6\u6d41\u7a0b\u4e2d\uff0c\u901a\u8fc7\u67e5\u8be2\u6a21\u578b\u751f\u6210\u5927\u91cf\u6587\u672c\u6765\u63d0\u53d6\u8bad\u7ec3\u6570\u636e\uff0c\u7136\u540e\u5e94\u7528MIA\u9a8c\u8bc1\u7279\u5b9a\u6570\u636e\u70b9\u662f\u5426\u5728\u8bad\u7ec3\u96c6\u4e2d\u3002\u7cfb\u7edf\u6027\u5730\u5bf9\u591a\u79cdMIA\u6280\u672f\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u6bd4\u8f83\u4e86\u96c6\u6210\u8bbe\u7f6e\u4e0bMIA\u6280\u672f\u7684\u6027\u80fd\u4e0e\u4f20\u7edfMIA\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\uff0c\u8bc4\u4f30\u4e86\u5b83\u4eec\u5728\u771f\u5b9e\u4e16\u754c\u63d0\u53d6\u573a\u666f\u4e2d\u7684\u5b9e\u9645\u6548\u7528\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u96c6\u6210\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e3a\u8bc4\u4f30MIA\u6280\u672f\u5728\u771f\u5b9eLLM\u8bad\u7ec3\u6570\u636e\u63d0\u53d6\u653b\u51fb\u4e2d\u7684\u6709\u6548\u6027\u63d0\u4f9b\u4e86\u5b9e\u7528\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u4e0d\u540c\u653b\u51fb\u65b9\u6cd5\u5728\u5b9e\u9645\u9690\u79c1\u5a01\u80c1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2512.13144", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.13144", "abs": "https://arxiv.org/abs/2512.13144", "authors": ["Chun Kit Wong", "Paraskevas Pegios", "Nina Weng", "Emilie Pi Fogtmann Sejer", "Martin Gr\u00f8nneb\u00e6k Tolsgaard", "Anders Nymark Christensen", "Aasa Feragen"], "title": "Weight Space Correlation Analysis: Quantifying Feature Utilization in Deep Learning Models", "comment": "46 pages", "summary": "Deep learning models in medical imaging are susceptible to shortcut learning, relying on confounding metadata (e.g., scanner model) that is often encoded in image embeddings. The crucial question is whether the model actively utilizes this encoded information for its final prediction. We introduce Weight Space Correlation Analysis, an interpretable methodology that quantifies feature utilization by measuring the alignment between the classification heads of a primary clinical task and auxiliary metadata tasks. We first validate our method by successfully detecting artificially induced shortcut learning. We then apply it to probe the feature utilization of an SA-SonoNet model trained for Spontaneous Preterm Birth (sPTB) prediction. Our analysis confirmed that while the embeddings contain substantial metadata, the sPTB classifier's weight vectors were highly correlated with clinically relevant factors (e.g., birth weight) but decoupled from clinically irrelevant acquisition factors (e.g. scanner). Our methodology provides a tool to verify model trustworthiness, demonstrating that, in the absence of induced bias, the clinical model selectively utilizes features related to the genuine clinical signal.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\"\u6743\u91cd\u7a7a\u95f4\u76f8\u5173\u6027\u5206\u6790\"\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u7528\u4e8e\u91cf\u5316\u533b\u5b66\u5f71\u50cf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e2d\u7279\u5f81\u5229\u7528\u60c5\u51b5\uff0c\u7279\u522b\u662f\u68c0\u6d4b\u6a21\u578b\u662f\u5426\u5229\u7528\u4e86\u5d4c\u5165\u4e2d\u7684\u5143\u6570\u636e\u4fe1\u606f\u8fdb\u884c\u9884\u6d4b\u3002", "motivation": "\u533b\u5b66\u5f71\u50cf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u6377\u5f84\u5b66\u4e60\u7684\u5f71\u54cd\uff0c\u4f9d\u8d56\u56fe\u50cf\u5d4c\u5165\u4e2d\u7f16\u7801\u7684\u6df7\u6742\u5143\u6570\u636e\uff08\u5982\u626b\u63cf\u4eea\u578b\u53f7\uff09\u3002\u5173\u952e\u95ee\u9898\u662f\u6a21\u578b\u662f\u5426\u4e3b\u52a8\u5229\u7528\u8fd9\u4e9b\u7f16\u7801\u4fe1\u606f\u8fdb\u884c\u6700\u7ec8\u9884\u6d4b\u3002", "method": "\u5f15\u5165\u6743\u91cd\u7a7a\u95f4\u76f8\u5173\u6027\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d4b\u91cf\u4e3b\u8981\u4e34\u5e8a\u4efb\u52a1\u5206\u7c7b\u5934\u4e0e\u8f85\u52a9\u5143\u6570\u636e\u4efb\u52a1\u5206\u7c7b\u5934\u4e4b\u95f4\u7684\u5bf9\u9f50\u7a0b\u5ea6\u6765\u91cf\u5316\u7279\u5f81\u5229\u7528\u3002\u9996\u5148\u901a\u8fc7\u68c0\u6d4b\u4eba\u5de5\u8bf1\u5bfc\u7684\u6377\u5f84\u5b66\u4e60\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u7136\u540e\u5e94\u7528\u4e8eSA-SonoNet\u6a21\u578b\u7684\u81ea\u53d1\u6027\u65e9\u4ea7\u9884\u6d4b\u5206\u6790\u3002", "result": "\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u80fd\u6210\u529f\u68c0\u6d4b\u4eba\u5de5\u8bf1\u5bfc\u7684\u6377\u5f84\u5b66\u4e60\u3002\u5728sPTB\u9884\u6d4b\u6a21\u578b\u4e2d\uff0c\u5206\u6790\u786e\u8ba4\u867d\u7136\u5d4c\u5165\u5305\u542b\u5927\u91cf\u5143\u6570\u636e\uff0c\u4f46sPTB\u5206\u7c7b\u5668\u7684\u6743\u91cd\u5411\u91cf\u4e0e\u4e34\u5e8a\u76f8\u5173\u56e0\u7d20\uff08\u5982\u51fa\u751f\u4f53\u91cd\uff09\u9ad8\u5ea6\u76f8\u5173\uff0c\u800c\u4e0e\u4e34\u5e8a\u65e0\u5173\u7684\u91c7\u96c6\u56e0\u7d20\uff08\u5982\u626b\u63cf\u4eea\uff09\u89e3\u8026\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9a8c\u8bc1\u6a21\u578b\u53ef\u4fe1\u5ea6\u63d0\u4f9b\u4e86\u5de5\u5177\uff0c\u8868\u660e\u5728\u6ca1\u6709\u8bf1\u5bfc\u504f\u5dee\u7684\u60c5\u51b5\u4e0b\uff0c\u4e34\u5e8a\u6a21\u578b\u4f1a\u9009\u62e9\u6027\u5730\u5229\u7528\u4e0e\u771f\u5b9e\u4e34\u5e8a\u4fe1\u53f7\u76f8\u5173\u7684\u7279\u5f81\u3002"}}
{"id": "2512.13381", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13381", "abs": "https://arxiv.org/abs/2512.13381", "authors": ["Changjun Zhou", "Jintao Zheng", "Leyou Yang", "Pengfei Wang"], "title": "Dual-Phase Federated Deep Unlearning via Weight-Aware Rollback and Reconstruction", "comment": "10 pages, submitted to INFOCOM 2026", "summary": "Federated Unlearning (FUL) focuses on client data and computing power to offer a privacy-preserving solution. However, high computational demands, complex incentive mechanisms, and disparities in client-side computing power often lead to long times and higher costs. To address these challenges, many existing methods rely on server-side knowledge distillation that solely removes the updates of the target client, overlooking the privacy embedded in the contributions of other clients, which can lead to privacy leakage. In this work, we introduce DPUL, a novel server-side unlearning method that deeply unlearns all influential weights to prevent privacy pitfalls. Our approach comprises three components: (i) identifying high-weight parameters by filtering client update magnitudes, and rolling them back to ensure deep removal. (ii) leveraging the variational autoencoder (VAE) to reconstruct and eliminate low-weight parameters. (iii) utilizing a projection-based technique to recover the model. Experimental results on four datasets demonstrate that DPUL surpasses state-of-the-art baselines, providing a 1%-5% improvement in accuracy and up to 12x reduction in time cost.", "AI": {"tldr": "DPUL\u662f\u4e00\u79cd\u65b0\u9896\u7684\u670d\u52a1\u5668\u7aef\u8054\u90a6\u9057\u5fd8\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df1\u5ea6\u9057\u5fd8\u6240\u6709\u5f71\u54cd\u6743\u91cd\u6765\u9632\u6b62\u9690\u79c1\u6cc4\u9732\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u65f6\u95f4\u6210\u672c\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u9057\u5fd8\u65b9\u6cd5\u5b58\u5728\u9ad8\u8ba1\u7b97\u9700\u6c42\u3001\u590d\u6742\u6fc0\u52b1\u673a\u5236\u548c\u5ba2\u6237\u7aef\u8ba1\u7b97\u80fd\u529b\u5dee\u5f02\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u65f6\u95f4\u957f\u3001\u6210\u672c\u9ad8\u3002\u540c\u65f6\uff0c\u73b0\u6709\u670d\u52a1\u5668\u7aef\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u4ec5\u79fb\u9664\u76ee\u6807\u5ba2\u6237\u7aef\u7684\u66f4\u65b0\uff0c\u5ffd\u7565\u4e86\u5176\u4ed6\u5ba2\u6237\u7aef\u8d21\u732e\u4e2d\u5d4c\u5165\u7684\u9690\u79c1\uff0c\u53ef\u80fd\u5bfc\u81f4\u9690\u79c1\u6cc4\u9732\u3002", "method": "DPUL\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a(1)\u901a\u8fc7\u8fc7\u6ee4\u5ba2\u6237\u7aef\u66f4\u65b0\u5e45\u5ea6\u8bc6\u522b\u9ad8\u6743\u91cd\u53c2\u6570\uff0c\u5e76\u5c06\u5176\u56de\u6eda\u4ee5\u786e\u4fdd\u6df1\u5ea6\u79fb\u9664\uff1b(2)\u5229\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668(VAE)\u91cd\u6784\u548c\u6d88\u9664\u4f4e\u6743\u91cd\u53c2\u6570\uff1b(3)\u4f7f\u7528\u57fa\u4e8e\u6295\u5f71\u7684\u6280\u672f\u6062\u590d\u6a21\u578b\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDPUL\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u63d0\u9ad8\u4e861%-5%\uff0c\u65f6\u95f4\u6210\u672c\u6700\u591a\u51cf\u5c11\u4e8612\u500d\u3002", "conclusion": "DPUL\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u670d\u52a1\u5668\u7aef\u8054\u90a6\u9057\u5fd8\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6df1\u5ea6\u9057\u5fd8\u6240\u6709\u5f71\u54cd\u6743\u91cd\uff0c\u9632\u6b62\u9690\u79c1\u6cc4\u9732\uff0c\u540c\u65f6\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u5747\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2512.13157", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13157", "abs": "https://arxiv.org/abs/2512.13157", "authors": ["Peter Kocsis", "Lukas H\u00f6llein", "Matthias Nie\u00dfner"], "title": "Intrinsic Image Fusion for Multi-View 3D Material Reconstruction", "comment": "Project page: https://peter-kocsis.github.io/IntrinsicImageFusion/ Video: https://www.youtube.com/watch?v=-Vs3tR1Xl7k", "summary": "We introduce Intrinsic Image Fusion, a method that reconstructs high-quality physically based materials from multi-view images. Material reconstruction is highly underconstrained and typically relies on analysis-by-synthesis, which requires expensive and noisy path tracing. To better constrain the optimization, we incorporate single-view priors into the reconstruction process. We leverage a diffusion-based material estimator that produces multiple, but often inconsistent, candidate decompositions per view. To reduce the inconsistency, we fit an explicit low-dimensional parametric function to the predictions. We then propose a robust optimization framework using soft per-view prediction selection together with confidence-based soft multi-view inlier set to fuse the most consistent predictions of the most confident views into a consistent parametric material space. Finally, we use inverse path tracing to optimize for the low-dimensional parameters. Our results outperform state-of-the-art methods in material disentanglement on both synthetic and real scenes, producing sharp and clean reconstructions suitable for high-quality relighting.", "AI": {"tldr": "\u63d0\u51faIntrinsic Image Fusion\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5355\u89c6\u56fe\u5148\u9a8c\u548c\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u4f18\u5316\uff0c\u4ece\u591a\u89c6\u89d2\u56fe\u50cf\u91cd\u5efa\u9ad8\u8d28\u91cf\u7269\u7406\u6750\u8d28\uff0c\u5728\u6750\u8d28\u89e3\u8026\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6750\u8d28\u91cd\u5efa\u662f\u4e00\u4e2a\u9ad8\u5ea6\u6b20\u7ea6\u675f\u7684\u95ee\u9898\uff0c\u4f20\u7edf\u57fa\u4e8e\u5206\u6790-\u5408\u6210\u7684\u65b9\u6cd5\u9700\u8981\u6602\u8d35\u4e14\u566a\u58f0\u5927\u7684\u8def\u5f84\u8ffd\u8e2a\u3002\u9700\u8981\u66f4\u597d\u7684\u7ea6\u675f\u6765\u4f18\u5316\u6750\u8d28\u91cd\u5efa\u8fc7\u7a0b\u3002", "method": "1) \u5229\u7528\u57fa\u4e8e\u6269\u6563\u7684\u6750\u8d28\u4f30\u8ba1\u5668\u751f\u6210\u6bcf\u89c6\u56fe\u7684\u5019\u9009\u5206\u89e3\uff1b2) \u62df\u5408\u663e\u5f0f\u4f4e\u7ef4\u53c2\u6570\u51fd\u6570\u51cf\u5c11\u4e0d\u4e00\u81f4\u6027\uff1b3) \u63d0\u51fa\u9c81\u68d2\u4f18\u5316\u6846\u67b6\uff0c\u7ed3\u5408\u8f6f\u6bcf\u89c6\u56fe\u9884\u6d4b\u9009\u62e9\u548c\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u8f6f\u591a\u89c6\u56fe\u5185\u70b9\u96c6\uff1b4) \u4f7f\u7528\u9006\u8def\u5f84\u8ffd\u8e2a\u4f18\u5316\u4f4e\u7ef4\u53c2\u6570\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u573a\u666f\u7684\u6750\u8d28\u89e3\u8026\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u4ea7\u751f\u6e05\u6670\u5e72\u51c0\u7684\u91cd\u5efa\u7ed3\u679c\uff0c\u9002\u7528\u4e8e\u9ad8\u8d28\u91cf\u91cd\u5149\u7167\u3002", "conclusion": "Intrinsic Image Fusion\u65b9\u6cd5\u901a\u8fc7\u878d\u5408\u5355\u89c6\u56fe\u5148\u9a8c\u548c\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u7ea6\u675f\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u7269\u7406\u6750\u8d28\u91cd\u5efa\uff0c\u4e3a\u6750\u8d28\u89e3\u8026\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13164", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13164", "abs": "https://arxiv.org/abs/2512.13164", "authors": ["Xianchao Guan", "Zhiyuan Fan", "Yifeng Wang", "Fuqiang Chen", "Yanjiang Zhou", "Zengyang Che", "Hongxue Meng", "Xin Li", "Yaowei Wang", "Hongpeng Wang", "Min Zhang", "Heng Tao Shen", "Zheng Zhang", "Yongbing Zhang"], "title": "A Semantically Enhanced Generative Foundation Model Improves Pathological Image Synthesis", "comment": "67 pages, 9 figures, 16 tables", "summary": "The development of clinical-grade artificial intelligence in pathology is limited by the scarcity of diverse, high-quality annotated datasets. Generative models offer a potential solution but suffer from semantic instability and morphological hallucinations that compromise diagnostic reliability. To address this challenge, we introduce a Correlation-Regulated Alignment Framework for Tissue Synthesis (CRAFTS), the first generative foundation model for pathology-specific text-to-image synthesis. By leveraging a dual-stage training strategy on approximately 2.8 million image-caption pairs, CRAFTS incorporates a novel alignment mechanism that suppresses semantic drift to ensure biological accuracy. This model generates diverse pathological images spanning 30 cancer types, with quality rigorously validated by objective metrics and pathologist evaluations. Furthermore, CRAFTS-augmented datasets enhance the performance across various clinical tasks, including classification, cross-modal retrieval, self-supervised learning, and visual question answering. In addition, coupling CRAFTS with ControlNet enables precise control over tissue architecture from inputs such as nuclear segmentation masks and fluorescence images. By overcoming the critical barriers of data scarcity and privacy concerns, CRAFTS provides a limitless source of diverse, annotated histology data, effectively unlocking the creation of robust diagnostic tools for rare and complex cancer phenotypes.", "AI": {"tldr": "CRAFTS\u662f\u9996\u4e2a\u9488\u5bf9\u75c5\u7406\u5b66\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u76f8\u5173\u6027\u8c03\u8282\u5bf9\u9f50\u6846\u67b6\u89e3\u51b3\u751f\u6210\u6a21\u578b\u5728\u75c5\u7406\u56fe\u50cf\u5408\u6210\u4e2d\u7684\u8bed\u4e49\u4e0d\u7a33\u5b9a\u548c\u5f62\u6001\u5e7b\u89c9\u95ee\u9898\uff0c\u80fd\u591f\u751f\u621030\u79cd\u764c\u75c7\u7c7b\u578b\u7684\u591a\u6837\u5316\u75c5\u7406\u56fe\u50cf\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u63d0\u5347\u4e34\u5e8a\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u75c5\u7406\u5b66\u4e2d\u4e34\u5e8a\u7ea7\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\u53d7\u5230\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u96c6\u7a00\u7f3a\u7684\u9650\u5236\uff0c\u73b0\u6709\u751f\u6210\u6a21\u578b\u5b58\u5728\u8bed\u4e49\u4e0d\u7a33\u5b9a\u548c\u5f62\u6001\u5e7b\u89c9\u95ee\u9898\uff0c\u5f71\u54cd\u8bca\u65ad\u53ef\u9760\u6027\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u751f\u6210\u751f\u7269\u51c6\u786e\u3001\u591a\u6837\u5316\u75c5\u7406\u56fe\u50cf\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faCRAFTS\uff08\u76f8\u5173\u6027\u8c03\u8282\u5bf9\u9f50\u6846\u67b6\uff09\uff0c\u91c7\u7528\u53cc\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5728\u7ea6280\u4e07\u56fe\u50cf-\u6807\u9898\u5bf9\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5f15\u5165\u65b0\u9896\u7684\u5bf9\u9f50\u673a\u5236\u6291\u5236\u8bed\u4e49\u6f02\u79fb\uff0c\u786e\u4fdd\u751f\u7269\u51c6\u786e\u6027\u3002\u6a21\u578b\u53ef\u4e0eControlNet\u7ed3\u5408\uff0c\u901a\u8fc7\u6838\u5206\u5272\u63a9\u7801\u548c\u8367\u5149\u56fe\u50cf\u7b49\u8f93\u5165\u7cbe\u786e\u63a7\u5236\u7ec4\u7ec7\u7ed3\u6784\u3002", "result": "CRAFTS\u80fd\u591f\u751f\u621030\u79cd\u764c\u75c7\u7c7b\u578b\u7684\u591a\u6837\u5316\u75c5\u7406\u56fe\u50cf\uff0c\u8d28\u91cf\u901a\u8fc7\u5ba2\u89c2\u6307\u6807\u548c\u75c5\u7406\u5b66\u5bb6\u8bc4\u4f30\u4e25\u683c\u9a8c\u8bc1\u3002CRAFTS\u589e\u5f3a\u7684\u6570\u636e\u96c6\u5728\u5206\u7c7b\u3001\u8de8\u6a21\u6001\u68c0\u7d22\u3001\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u89c6\u89c9\u95ee\u7b54\u7b49\u591a\u79cd\u4e34\u5e8a\u4efb\u52a1\u4e2d\u63d0\u5347\u4e86\u6027\u80fd\u3002", "conclusion": "CRAFTS\u901a\u8fc7\u514b\u670d\u6570\u636e\u7a00\u7f3a\u548c\u9690\u79c1\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u65e0\u9650\u591a\u6837\u5316\u7684\u6807\u6ce8\u7ec4\u7ec7\u5b66\u6570\u636e\u6e90\uff0c\u4e3a\u7f55\u89c1\u548c\u590d\u6742\u764c\u75c7\u8868\u578b\u7684\u7a33\u5065\u8bca\u65ad\u5de5\u5177\u5f00\u53d1\u5f00\u8f9f\u4e86\u9053\u8def\uff0c\u662f\u75c5\u7406\u5b66AI\u53d1\u5c55\u7684\u91cd\u8981\u7a81\u7834\u3002"}}
{"id": "2512.13175", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13175", "abs": "https://arxiv.org/abs/2512.13175", "authors": ["Hongxuan Sun", "Tao Wu"], "title": "Seeing the Whole Picture: Distribution-Guided Data-Free Distillation for Semantic Segmentation", "comment": null, "summary": "Semantic segmentation requires a holistic understanding of the physical world, as it assigns semantic labels to spatially continuous and structurally coherent objects rather than to isolated pixels. However, existing data-free knowledge distillation (DFKD) methods-primarily designed for classification-often disregard this continuity, resulting in significant performance degradation when applied directly to segmentation tasks. In this paper, we introduce DFSS, a novel data-free distillation framework tailored for semantic segmentation. Unlike prior approaches that treat pixels independently, DFSS respects the structural and contextual continuity of real-world scenes. Our key insight is to leverage Batch Normalization (BN) statistics from a teacher model to guide Approximate Distribution Sampling (ADS), enabling the selection of data that better reflects the original training distribution-without relying on potentially misleading teacher predictions. Additionally, we propose Weighted Distribution Progressive Distillation (WDPD), which dynamically prioritizes reliable samples that are more closely aligned with the original data distribution early in training and gradually incorporates more challenging cases, mirroring the natural progression of learning in human perception. Extensive experiments on standard benchmarks demonstrate that DFSS consistently outperforms existing data-free distillation methods for semantic segmentation, achieving state-of-the-art results with significantly reduced reliance on auxiliary data.", "AI": {"tldr": "DFSS\u662f\u4e00\u4e2a\u4e13\u95e8\u4e3a\u8bed\u4e49\u5206\u5272\u8bbe\u8ba1\u7684\u6570\u636e\u65e0\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u6559\u5e08\u6a21\u578b\u7684BN\u7edf\u8ba1\u4fe1\u606f\u6307\u5bfc\u8fd1\u4f3c\u5206\u5e03\u91c7\u6837\uff0c\u5e76\u91c7\u7528\u52a0\u6743\u5206\u5e03\u6e10\u8fdb\u84b8\u998f\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u6570\u636e\u65e0\u77e5\u8bc6\u84b8\u998f\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u6570\u636e\u65e0\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u5206\u7c7b\u4efb\u52a1\u8bbe\u8ba1\uff0c\u5ffd\u7565\u4e86\u8bed\u4e49\u5206\u5272\u4e2d\u7269\u4f53\u7a7a\u95f4\u8fde\u7eed\u6027\u548c\u7ed3\u6784\u4e00\u81f4\u6027\u7684\u7279\u70b9\uff0c\u76f4\u63a5\u5e94\u7528\u4e8e\u5206\u5272\u4efb\u52a1\u65f6\u4f1a\u5bfc\u81f4\u663e\u8457\u7684\u6027\u80fd\u4e0b\u964d\u3002\u9700\u8981\u4e13\u95e8\u4e3a\u8bed\u4e49\u5206\u5272\u8bbe\u8ba1\u7684\u6570\u636e\u65e0\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\u3002", "method": "1. \u5229\u7528\u6559\u5e08\u6a21\u578b\u7684Batch Normalization\u7edf\u8ba1\u4fe1\u606f\u6307\u5bfc\u8fd1\u4f3c\u5206\u5e03\u91c7\u6837\uff0c\u9009\u62e9\u66f4\u80fd\u53cd\u6620\u539f\u59cb\u8bad\u7ec3\u5206\u5e03\u7684\u6570\u636e\uff0c\u800c\u4e0d\u4f9d\u8d56\u53ef\u80fd\u8bef\u5bfc\u7684\u6559\u5e08\u9884\u6d4b\uff1b2. \u63d0\u51fa\u52a0\u6743\u5206\u5e03\u6e10\u8fdb\u84b8\u998f\uff0c\u5728\u8bad\u7ec3\u65e9\u671f\u4f18\u5148\u5904\u7406\u4e0e\u539f\u59cb\u6570\u636e\u5206\u5e03\u66f4\u63a5\u8fd1\u7684\u53ef\u9760\u6837\u672c\uff0c\u9010\u6b65\u5f15\u5165\u66f4\u5177\u6311\u6218\u6027\u7684\u6837\u672c\uff0c\u6a21\u62df\u4eba\u7c7b\u611f\u77e5\u7684\u81ea\u7136\u5b66\u4e60\u8fdb\u7a0b\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cDFSS\u5728\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u7684\u6570\u636e\u65e0\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\uff0c\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u5bf9\u8f85\u52a9\u6570\u636e\u7684\u4f9d\u8d56\u3002", "conclusion": "DFSS\u901a\u8fc7\u5c0a\u91cd\u771f\u5b9e\u573a\u666f\u7684\u7ed3\u6784\u548c\u4e0a\u4e0b\u6587\u8fde\u7eed\u6027\uff0c\u4e13\u95e8\u4e3a\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u8bbe\u8ba1\u4e86\u6709\u6548\u7684\u6570\u636e\u65e0\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u7a7a\u95f4\u8fde\u7eed\u7269\u4f53\u65f6\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u8bed\u4e49\u5206\u5272\u7684\u77e5\u8bc6\u84b8\u998f\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13177", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13177", "abs": "https://arxiv.org/abs/2512.13177", "authors": ["Minghui Hou", "Wei-Hsing Huang", "Shaofeng Liang", "Daizong Liu", "Tai-Hao Wen", "Gang Wang", "Runwei Guan", "Weiping Ding"], "title": "MMDrive: Interactive Scene Understanding Beyond Vision with Multi-representational Fusion", "comment": null, "summary": "Vision-language models enable the understanding and reasoning of complex traffic scenarios through multi-source information fusion, establishing it as a core technology for autonomous driving. However, existing vision-language models are constrained by the image understanding paradigm in 2D plane, which restricts their capability to perceive 3D spatial information and perform deep semantic fusion, resulting in suboptimal performance in complex autonomous driving environments. This study proposes MMDrive, an multimodal vision-language model framework that extends traditional image understanding to a generalized 3D scene understanding framework. MMDrive incorporates three complementary modalities, including occupancy maps, LiDAR point clouds, and textual scene descriptions. To this end, it introduces two novel components for adaptive cross-modal fusion and key information extraction. Specifically, the Text-oriented Multimodal Modulator dynamically weights the contributions of each modality based on the semantic cues in the question, guiding context-aware feature integration. The Cross-Modal Abstractor employs learnable abstract tokens to generate compact, cross-modal summaries that highlight key regions and essential semantics. Comprehensive evaluations on the DriveLM and NuScenes-QA benchmarks demonstrate that MMDrive achieves significant performance gains over existing vision-language models for autonomous driving, with a BLEU-4 score of 54.56 and METEOR of 41.78 on DriveLM, and an accuracy score of 62.7% on NuScenes-QA. MMDrive effectively breaks the traditional image-only understanding barrier, enabling robust multimodal reasoning in complex driving environments and providing a new foundation for interpretable autonomous driving scene understanding.", "AI": {"tldr": "MMDrive\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7684\u591a\u6a21\u6001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6846\u67b6\uff0c\u5c06\u4f20\u7edf2D\u56fe\u50cf\u7406\u89e3\u6269\u5c55\u52303D\u573a\u666f\u7406\u89e3\uff0c\u878d\u5408\u5360\u7528\u56fe\u3001LiDAR\u70b9\u4e91\u548c\u6587\u672c\u63cf\u8ff0\u4e09\u79cd\u6a21\u6001\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8de8\u6a21\u6001\u878d\u5408\u548c\u5173\u952e\u4fe1\u606f\u63d0\u53d6\u63d0\u5347\u590d\u6742\u9a7e\u9a76\u73af\u5883\u4e0b\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u53d7\u9650\u4e8e2D\u5e73\u9762\u56fe\u50cf\u7406\u89e3\u8303\u5f0f\uff0c\u96be\u4ee5\u611f\u77e53D\u7a7a\u95f4\u4fe1\u606f\u548c\u8fdb\u884c\u6df1\u5ea6\u8bed\u4e49\u878d\u5408\uff0c\u5bfc\u81f4\u5728\u590d\u6742\u81ea\u52a8\u9a7e\u9a76\u73af\u5883\u4e2d\u6027\u80fd\u4e0d\u4f73\u3002\u9700\u8981\u7a81\u7834\u4f20\u7edf\u56fe\u50cf\u7406\u89e3\u9650\u5236\uff0c\u5b9e\u73b0\u66f4\u5f3a\u5927\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faMMDrive\u6846\u67b6\uff0c\u878d\u5408\u4e09\u79cd\u4e92\u8865\u6a21\u6001\uff1a\u5360\u7528\u56fe\u3001LiDAR\u70b9\u4e91\u548c\u6587\u672c\u573a\u666f\u63cf\u8ff0\u3002\u5f15\u5165\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u9762\u5411\u6587\u672c\u7684\u591a\u6a21\u6001\u8c03\u5236\u5668\uff0c\u6839\u636e\u95ee\u9898\u8bed\u4e49\u52a8\u6001\u52a0\u6743\u5404\u6a21\u6001\u8d21\u732e\uff1b2) \u8de8\u6a21\u6001\u62bd\u8c61\u5668\uff0c\u4f7f\u7528\u53ef\u5b66\u4e60\u7684\u62bd\u8c61\u4ee4\u724c\u751f\u6210\u7d27\u51d1\u7684\u8de8\u6a21\u6001\u6458\u8981\uff0c\u7a81\u51fa\u5173\u952e\u533a\u57df\u548c\u8bed\u4e49\u3002", "result": "\u5728DriveLM\u548cNuScenes-QA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff1aDriveLM\u4e0aBLEU-4\u5f97\u520654.56\uff0cMETEOR\u5f97\u520641.78\uff1bNuScenes-QA\u4e0a\u51c6\u786e\u738762.7%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "MMDrive\u6709\u6548\u7a81\u7834\u4e86\u4f20\u7edf\u4ec5\u56fe\u50cf\u7406\u89e3\u7684\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u590d\u6742\u9a7e\u9a76\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u591a\u6a21\u6001\u63a8\u7406\uff0c\u4e3a\u53ef\u89e3\u91ca\u7684\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u57fa\u7840\u3002"}}
{"id": "2512.13191", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13191", "abs": "https://arxiv.org/abs/2512.13191", "authors": ["Gong Chen", "Chaokun Zhang", "Pengcheng Lv", "Xiaohui Xie"], "title": "CoRA: A Collaborative Robust Architecture with Hybrid Fusion for Efficient Perception", "comment": "Accepted by AAAI2026", "summary": "Collaborative perception has garnered significant attention as a crucial technology to overcome the perceptual limitations of single-agent systems. Many state-of-the-art (SOTA) methods have achieved communication efficiency and high performance via intermediate fusion. However, they share a critical vulnerability: their performance degrades under adverse communication conditions due to the misalignment induced by data transmission, which severely hampers their practical deployment. To bridge this gap, we re-examine different fusion paradigms, and recover that the strengths of intermediate and late fusion are not a trade-off, but a complementary pairing. Based on this key insight, we propose CoRA, a novel collaborative robust architecture with a hybrid approach to decouple performance from robustness with low communication. It is composed of two components: a feature-level fusion branch and an object-level correction branch. Its first branch selects critical features and fuses them efficiently to ensure both performance and scalability. The second branch leverages semantic relevance to correct spatial displacements, guaranteeing resilience against pose errors. Experiments demonstrate the superiority of CoRA. Under extreme scenarios, CoRA improves upon its baseline performance by approximately 19% in AP@0.7 with more than 5x less communication volume, which makes it a promising solution for robust collaborative perception.", "AI": {"tldr": "CoRA\u662f\u4e00\u79cd\u65b0\u9896\u7684\u534f\u4f5c\u611f\u77e5\u67b6\u6784\uff0c\u901a\u8fc7\u6df7\u5408\u878d\u5408\u65b9\u6cd5\u89e3\u8026\u6027\u80fd\u4e0e\u9c81\u68d2\u6027\uff0c\u5728\u4f4e\u901a\u4fe1\u6210\u672c\u4e0b\u5b9e\u73b0\u9ad8\u6027\u80fd\u548c\u6297\u5e72\u6270\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u534f\u4f5c\u611f\u77e5\u65b9\u6cd5\u5728\u6076\u52a3\u901a\u4fe1\u6761\u4ef6\u4e0b\u6027\u80fd\u4e0b\u964d\uff0c\u56e0\u4e3a\u6570\u636e\u4f20\u8f93\u5bfc\u81f4\u7684\u5bf9\u9f50\u95ee\u9898\u963b\u788d\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u9ad8\u6027\u80fd\u53c8\u5177\u5907\u901a\u4fe1\u9c81\u68d2\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faCoRA\u67b6\u6784\uff0c\u5305\u542b\u4e24\u4e2a\u5206\u652f\uff1a\u7279\u5f81\u7ea7\u878d\u5408\u5206\u652f\u9009\u62e9\u5173\u952e\u7279\u5f81\u8fdb\u884c\u9ad8\u6548\u878d\u5408\uff1b\u5bf9\u8c61\u7ea7\u6821\u6b63\u5206\u652f\u5229\u7528\u8bed\u4e49\u76f8\u5173\u6027\u6821\u6b63\u7a7a\u95f4\u4f4d\u79fb\uff0c\u62b5\u6297\u59ff\u6001\u8bef\u5dee\u3002", "result": "\u5728\u6781\u7aef\u573a\u666f\u4e0b\uff0cCoRA\u6bd4\u57fa\u7ebf\u6027\u80fd\u63d0\u5347\u7ea619%\uff08AP@0.7\uff09\uff0c\u540c\u65f6\u901a\u4fe1\u91cf\u51cf\u5c115\u500d\u4ee5\u4e0a\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u4e0e\u9c81\u68d2\u6027\u7684\u5e73\u8861\u3002", "conclusion": "CoRA\u8bc1\u660e\u4e86\u4e2d\u95f4\u878d\u5408\u548c\u540e\u671f\u878d\u5408\u7684\u4f18\u52bf\u662f\u4e92\u8865\u800c\u975e\u6743\u8861\uff0c\u4e3a\u9c81\u68d2\u534f\u4f5c\u611f\u77e5\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u5408\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u901a\u4fe1\u6311\u6218\u3002"}}
{"id": "2512.13497", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13497", "abs": "https://arxiv.org/abs/2512.13497", "authors": ["Haoyu Ren", "Kay Koehle", "Kirill Dorofeev", "Darko Anicic"], "title": "On-Device Continual Learning for Unsupervised Visual Anomaly Detection in Dynamic Manufacturing", "comment": "Accepted by European Conference on EDGE AI Technologies and Applications (EEAI) 2025", "summary": "In modern manufacturing, Visual Anomaly Detection (VAD) is essential for automated inspection and consistent product quality. Yet, increasingly dynamic and flexible production environments introduce key challenges: First, frequent product changes in small-batch and on-demand manufacturing require rapid model updates. Second, legacy edge hardware lacks the resources to train and run large AI models. Finally, both anomalous and normal training data are often scarce, particularly for newly introduced product variations. We investigate on-device continual learning for unsupervised VAD with localization, extending the PatchCore to incorporate online learning for real-world industrial scenarios. The proposed method leverages a lightweight feature extractor and an incremental coreset update mechanism based on k-center selection, enabling rapid, memory-efficient adaptation from limited data while eliminating costly cloud retraining. Evaluations on an industrial use case are conducted using a testbed designed to emulate flexible production with frequent variant changes in a controlled environment. Our method achieves a 12% AUROC improvement over the baseline, an 80% reduction in memory usage, and faster training compared to batch retraining. These results confirm that our method delivers accurate, resource-efficient, and adaptive VAD suitable for dynamic and smart manufacturing.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u52a8\u6001\u5236\u9020\u73af\u5883\u7684\u8bbe\u5907\u7aef\u6301\u7eed\u5b66\u4e60\u89c6\u89c9\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u7279\u5f81\u63d0\u53d6\u5668\u548c\u589e\u91cf\u6838\u5fc3\u96c6\u66f4\u65b0\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u3001\u5185\u5b58\u9ad8\u6548\u7684\u6a21\u578b\u9002\u5e94\u3002", "motivation": "\u73b0\u4ee3\u5236\u9020\u4e1a\u4e2d\u52a8\u6001\u7075\u6d3b\u7684\u751f\u4ea7\u73af\u5883\u5e26\u6765\u4e09\u5927\u6311\u6218\uff1a\u9891\u7e41\u4ea7\u54c1\u53d8\u66f4\u9700\u8981\u5feb\u901f\u6a21\u578b\u66f4\u65b0\uff1b\u8fb9\u7f18\u786c\u4ef6\u8d44\u6e90\u6709\u9650\u65e0\u6cd5\u8bad\u7ec3\u5927\u578bAI\u6a21\u578b\uff1b\u5f02\u5e38\u548c\u6b63\u5e38\u8bad\u7ec3\u6570\u636e\u901a\u5e38\u7a00\u7f3a\uff0c\u7279\u522b\u662f\u65b0\u4ea7\u54c1\u53d8\u4f53\u3002", "method": "\u57fa\u4e8ePatchCore\u6269\u5c55\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u7279\u5f81\u63d0\u53d6\u5668\u548c\u57fa\u4e8ek-center\u9009\u62e9\u7684\u589e\u91cf\u6838\u5fc3\u96c6\u66f4\u65b0\u673a\u5236\uff0c\u5b9e\u73b0\u8bbe\u5907\u7aef\u6301\u7eed\u5b66\u4e60\uff0c\u65e0\u9700\u6602\u8d35\u7684\u4e91\u7aef\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "\u5728\u6a21\u62df\u7075\u6d3b\u751f\u4ea7\u7684\u5de5\u4e1a\u7528\u4f8b\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e8612%\u7684AUROC\u63d0\u5347\uff0c\u5185\u5b58\u4f7f\u7528\u51cf\u5c1180%\uff0c\u8bad\u7ec3\u901f\u5ea6\u6bd4\u6279\u91cf\u91cd\u65b0\u8bad\u7ec3\u66f4\u5feb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u52a8\u6001\u667a\u80fd\u5236\u9020\u63d0\u4f9b\u4e86\u51c6\u786e\u3001\u8d44\u6e90\u9ad8\u6548\u4e14\u81ea\u9002\u5e94\u7684\u89c6\u89c9\u5f02\u5e38\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u5408\u5c0f\u6279\u91cf\u6309\u9700\u5236\u9020\u73af\u5883\u3002"}}
{"id": "2512.13192", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13192", "abs": "https://arxiv.org/abs/2512.13192", "authors": ["Zhuo Chen", "Chengqun Yang", "Zhuo Su", "Zheng Lv", "Jingnan Gao", "Xiaoyuan Zhang", "Xiaokang Yang", "Yichao Yan"], "title": "POLAR: A Portrait OLAT Dataset and Generative Framework for Illumination-Aware Face Modeling", "comment": "19 pages, 19 figures", "summary": "Face relighting aims to synthesize realistic portraits under novel illumination while preserving identity and geometry. However, progress remains constrained by the limited availability of large-scale, physically consistent illumination data. To address this, we introduce POLAR, a large-scale and physically calibrated One-Light-at-a-Time (OLAT) dataset containing over 200 subjects captured under 156 lighting directions, multiple views, and diverse expressions. Building upon POLAR, we develop a flow-based generative model POLARNet that predicts per-light OLAT responses from a single portrait, capturing fine-grained and direction-aware illumination effects while preserving facial identity. Unlike diffusion or background-conditioned methods that rely on statistical or contextual cues, our formulation models illumination as a continuous, physically interpretable transformation between lighting states, enabling scalable and controllable relighting. Together, POLAR and POLARNet form a unified illumination learning framework that links real data, generative synthesis, and physically grounded relighting, establishing a self-sustaining \"chicken-and-egg\" cycle for scalable and reproducible portrait illumination.", "AI": {"tldr": "POLAR\uff1a\u5927\u89c4\u6a21\u7269\u7406\u6821\u51c6\u7684\u5355\u5149\u6e90\u6570\u636e\u96c6\u4e0e\u57fa\u4e8e\u6d41\u7684\u751f\u6210\u6a21\u578b\uff0c\u5b9e\u73b0\u53ef\u63a7\u4eba\u8138\u91cd\u5149\u7167", "motivation": "\u4eba\u8138\u91cd\u5149\u7167\u7814\u7a76\u53d7\u9650\u4e8e\u5927\u89c4\u6a21\u3001\u7269\u7406\u4e00\u81f4\u7684\u5149\u7167\u6570\u636e\u53ef\u7528\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u4fdd\u6301\u8eab\u4efd\u7279\u5f81\u548c\u51e0\u4f55\u7ed3\u6784", "method": "\u63d0\u51faPOLAR\u6570\u636e\u96c6\uff08200+\u53d7\u8bd5\u8005\u00d7156\u5149\u7167\u65b9\u5411\u00d7\u591a\u89c6\u89d2\u00d7\u591a\u8868\u60c5\uff09\uff0c\u5e76\u5f00\u53d1\u57fa\u4e8e\u6d41\u7684\u751f\u6210\u6a21\u578bPOLARNet\uff0c\u4ece\u5355\u5f20\u8096\u50cf\u9884\u6d4b\u6bcf\u5149\u6e90\u54cd\u5e94", "result": "\u5efa\u7acb\u4e86\u7edf\u4e00\u7684\u5149\u7167\u5b66\u4e60\u6846\u67b6\uff0c\u5c06\u771f\u5b9e\u6570\u636e\u3001\u751f\u6210\u5408\u6210\u4e0e\u7269\u7406\u57fa\u7840\u91cd\u5149\u7167\u8054\u7cfb\u8d77\u6765\uff0c\u5f62\u6210\u53ef\u6269\u5c55\u3001\u53ef\u590d\u73b0\u7684\"\u9e21\u4e0e\u86cb\"\u5faa\u73af", "conclusion": "POLAR\u548cPOLARNet\u5171\u540c\u89e3\u51b3\u4e86\u4eba\u8138\u91cd\u5149\u7167\u7684\u6570\u636e\u74f6\u9888\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u7ec6\u7c92\u5ea6\u3001\u65b9\u5411\u611f\u77e5\u7684\u5149\u7167\u6548\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u9762\u90e8\u8eab\u4efd\u7279\u5f81"}}
{"id": "2512.13506", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2512.13506", "abs": "https://arxiv.org/abs/2512.13506", "authors": ["Sofiya Zaichyk"], "title": "Learning under Distributional Drift: Reproducibility as an Intrinsic Statistical Resource", "comment": "37 pages, 4 figures", "summary": "Statistical learning under distributional drift remains insufficiently characterized: when each observation alters the data-generating law, classical generalization bounds can collapse. We introduce a new statistical primitive, the reproducibility budget $C_T$, which quantifies a system's finite capacity for statistical reproducibility - the extent to which its sampling process can remain governed by a consistent underlying distribution in the presence of both exogenous change and endogenous feedback. Formally, $C_T$ is defined as the cumulative Fisher-Rao path length of the coupled learner-environment evolution, measuring the total distributional motion accumulated during learning. From this construct we derive a drift-feedback generalization bound of order $O(T^{-1/2} + C_T/T)$, and we prove a matching minimax lower bound showing that this rate is minimax-optimal. Consequently, the results establish a reproducibility speed limit: no algorithm can achieve smaller worst-case generalization error than that imposed by the average Fisher-Rao drift rate $C_T/T$ of the data-generating process. The framework situates exogenous drift, adaptive data analysis, and performative prediction within a common geometric structure, with $C_T$ emerging as the intrinsic quantity measuring distributional motion across these settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\"\u53ef\u91cd\u590d\u6027\u9884\u7b97\"$C_T$\u4f5c\u4e3a\u8861\u91cf\u5b66\u4e60\u7cfb\u7edf\u5728\u5206\u5e03\u6f02\u79fb\u4e0b\u7edf\u8ba1\u53ef\u91cd\u590d\u6027\u7684\u65b0\u7edf\u8ba1\u539f\u8bed\uff0c\u63a8\u5bfc\u51fa\u6700\u4f18\u7684\u6f02\u79fb-\u53cd\u9988\u6cdb\u5316\u754c\uff0c\u5efa\u7acb\u4e86\u53ef\u91cd\u590d\u6027\u901f\u5ea6\u6781\u9650\u3002", "motivation": "\u5728\u5206\u5e03\u6f02\u79fb\u4e0b\uff0c\u6bcf\u4e2a\u89c2\u6d4b\u90fd\u4f1a\u6539\u53d8\u6570\u636e\u751f\u6210\u89c4\u5f8b\uff0c\u7ecf\u5178\u6cdb\u5316\u754c\u53ef\u80fd\u5931\u6548\u3002\u9700\u8981\u91cf\u5316\u7cfb\u7edf\u5728\u5916\u90e8\u53d8\u5316\u548c\u5185\u90e8\u53cd\u9988\u4e0b\u7684\u7edf\u8ba1\u53ef\u91cd\u590d\u6027\u6709\u9650\u5bb9\u91cf\u3002", "method": "\u5f15\u5165\u53ef\u91cd\u590d\u6027\u9884\u7b97$C_T$\u4f5c\u4e3aFisher-Rao\u8def\u5f84\u957f\u5ea6\u7684\u7d2f\u79ef\u91cf\uff0c\u8861\u91cf\u5b66\u4e60-\u73af\u5883\u8026\u5408\u6f14\u5316\u4e2d\u7684\u603b\u5206\u5e03\u8fd0\u52a8\u3002\u4ece\u8be5\u6784\u9020\u63a8\u5bfc\u51fa$O(T^{-1/2} + C_T/T)$\u9636\u7684\u6f02\u79fb-\u53cd\u9988\u6cdb\u5316\u754c\uff0c\u5e76\u8bc1\u660e\u5339\u914d\u7684\u6781\u5c0f\u6781\u5927\u4e0b\u754c\u3002", "result": "\u5efa\u7acb\u4e86\u53ef\u91cd\u590d\u6027\u901f\u5ea6\u6781\u9650\uff1a\u4efb\u4f55\u7b97\u6cd5\u7684\u6700\u574f\u60c5\u51b5\u6cdb\u5316\u8bef\u5dee\u90fd\u4e0d\u80fd\u4f4e\u4e8e\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u7684\u5e73\u5747Fisher-Rao\u6f02\u79fb\u7387$C_T/T$\u6240\u65bd\u52a0\u7684\u9650\u5236\u3002\u8be5\u6846\u67b6\u5c06\u5916\u90e8\u6f02\u79fb\u3001\u81ea\u9002\u5e94\u6570\u636e\u5206\u6790\u548c\u6267\u884c\u9884\u6d4b\u7edf\u4e00\u5728\u5171\u540c\u7684\u51e0\u4f55\u7ed3\u6784\u4e2d\u3002", "conclusion": "\u53ef\u91cd\u590d\u6027\u9884\u7b97$C_T$\u6210\u4e3a\u8861\u91cf\u5206\u5e03\u8fd0\u52a8\u7684\u5185\u5728\u91cf\uff0c\u4e3a\u5206\u5e03\u6f02\u79fb\u4e0b\u7684\u7edf\u8ba1\u5b66\u4e60\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u6700\u4f18\u6027\u80fd\u754c\u9650\u3002"}}
{"id": "2512.13238", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13238", "abs": "https://arxiv.org/abs/2512.13238", "authors": ["Francesco Ragusa", "Michele Mazzamuto", "Rosario Forte", "Irene D'Ambra", "James Fort", "Jakob Engel", "Antonino Furnari", "Giovanni Maria Farinella"], "title": "Ego-EXTRA: video-language Egocentric Dataset for EXpert-TRAinee assistance", "comment": null, "summary": "We present Ego-EXTRA, a video-language Egocentric Dataset for EXpert-TRAinee assistance. Ego-EXTRA features 50 hours of unscripted egocentric videos of subjects performing procedural activities (the trainees) while guided by real-world experts who provide guidance and answer specific questions using natural language. Following a ``Wizard of OZ'' data collection paradigm, the expert enacts a wearable intelligent assistant, looking at the activities performed by the trainee exclusively from their egocentric point of view, answering questions when asked by the trainee, or proactively interacting with suggestions during the procedures. This unique data collection protocol enables Ego-EXTRA to capture a high-quality dialogue in which expert-level feedback is provided to the trainee. Two-way dialogues between experts and trainees are recorded, transcribed, and used to create a novel benchmark comprising more than 15k high-quality Visual Question Answer sets, which we use to evaluate Multimodal Large Language Models. The results show that Ego-EXTRA is challenging and highlight the limitations of current models when used to provide expert-level assistance to the user. The Ego-EXTRA dataset is publicly available to support the benchmark of egocentric video-language assistants: https://fpv-iplab.github.io/Ego-EXTRA/.", "AI": {"tldr": "Ego-EXTRA\u662f\u4e00\u4e2a\u7528\u4e8e\u4e13\u5bb6-\u5b66\u5458\u8f85\u52a9\u768450\u5c0f\u65f6\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u8bed\u8a00\u6570\u636e\u96c6\uff0c\u5305\u542b\u771f\u5b9e\u4e13\u5bb6\u6307\u5bfc\u5b66\u5458\u6267\u884c\u7a0b\u5e8f\u6027\u6d3b\u52a8\u7684\u5bf9\u8bdd\uff0c\u521b\u5efa\u4e86\u8d85\u8fc715k\u4e2a\u89c6\u89c9\u95ee\u7b54\u5bf9\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u771f\u5b9e\u4e13\u5bb6\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u5bfc\u5b66\u5458\u6267\u884c\u7a0b\u5e8f\u6027\u6d3b\u52a8\u7684\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u5bf9\u8bdd\u6570\u636e\uff0c\u9700\u8981\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u6765\u8bc4\u4f30\u548c\u5f00\u53d1\u80fd\u591f\u63d0\u4f9b\u4e13\u5bb6\u7ea7\u8f85\u52a9\u7684\u591a\u6a21\u6001\u667a\u80fd\u52a9\u624b\u3002", "method": "\u91c7\u7528\"\u7eff\u91ce\u4ed9\u8e2a\"\u6570\u636e\u6536\u96c6\u8303\u5f0f\uff0c\u4e13\u5bb6\u626e\u6f14\u53ef\u7a7f\u6234\u667a\u80fd\u52a9\u624b\u89d2\u8272\uff0c\u4ec5\u901a\u8fc7\u5b66\u5458\u7684\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u89c2\u5bdf\u6d3b\u52a8\uff0c\u56de\u7b54\u5b66\u5458\u95ee\u9898\u6216\u4e3b\u52a8\u63d0\u4f9b\u5efa\u8bae\u3002\u8bb0\u5f55\u4e13\u5bb6\u4e0e\u5b66\u5458\u7684\u53cc\u5411\u5bf9\u8bdd\uff0c\u8f6c\u5f55\u5e76\u521b\u5efa\u89c6\u89c9\u95ee\u7b54\u5bf9\u57fa\u51c6\u3002", "result": "\u521b\u5efa\u4e86\u5305\u542b50\u5c0f\u65f6\u975e\u811a\u672c\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u548c\u8d85\u8fc715k\u4e2a\u9ad8\u8d28\u91cf\u89c6\u89c9\u95ee\u7b54\u5bf9\u7684Ego-EXTRA\u6570\u636e\u96c6\u3002\u8bc4\u4f30\u663e\u793a\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u7a81\u663e\u4e86\u63d0\u4f9b\u4e13\u5bb6\u7ea7\u8f85\u52a9\u7684\u6311\u6218\u6027\u3002", "conclusion": "Ego-EXTRA\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u63d0\u4f9b\u4e13\u5bb6\u7ea7\u8f85\u52a9\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5f00\u53d1\u7b2c\u4e00\u4eba\u79f0\u89c6\u9891\u8bed\u8a00\u52a9\u624b\u63d0\u4f9b\u4e86\u91cd\u8981\u8d44\u6e90\u3002"}}
{"id": "2512.13247", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13247", "abs": "https://arxiv.org/abs/2512.13247", "authors": ["Foivos Paraperas Papantoniou", "Stathis Galanakis", "Rolandos Alexandros Potamias", "Bernhard Kainz", "Stefanos Zafeiriou"], "title": "STARCaster: Spatio-Temporal AutoRegressive Video Diffusion for Identity- and View-Aware Talking Portraits", "comment": "Project page: https://foivospar.github.io/STARCaster/", "summary": "This paper presents STARCaster, an identity-aware spatio-temporal video diffusion model that addresses both speech-driven portrait animation and free-viewpoint talking portrait synthesis, given an identity embedding or reference image, within a unified framework. Existing 2D speech-to-video diffusion models depend heavily on reference guidance, leading to limited motion diversity. At the same time, 3D-aware animation typically relies on inversion through pre-trained tri-plane generators, which often leads to imperfect reconstructions and identity drift. We rethink reference- and geometry-based paradigms in two ways. First, we deviate from strict reference conditioning at pre-training by introducing softer identity constraints. Second, we address 3D awareness implicitly within the 2D video domain by leveraging the inherent multi-view nature of video data. STARCaster adopts a compositional approach progressing from ID-aware motion modeling, to audio-visual synchronization via lip reading-based supervision, and finally to novel view animation through temporal-to-spatial adaptation. To overcome the scarcity of 4D audio-visual data, we propose a decoupled learning approach in which view consistency and temporal coherence are trained independently. A self-forcing training scheme enables the model to learn from longer temporal contexts than those generated at inference, mitigating the overly static animations common in existing autoregressive approaches. Comprehensive evaluations demonstrate that STARCaster generalizes effectively across tasks and identities, consistently surpassing prior approaches in different benchmarks.", "AI": {"tldr": "STARCaster\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u8eab\u4efd\u611f\u77e5\u65f6\u7a7a\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u80fd\u591f\u540c\u65f6\u5904\u7406\u8bed\u97f3\u9a71\u52a8\u8096\u50cf\u52a8\u753b\u548c\u81ea\u7531\u89c6\u89d2\u8bf4\u8bdd\u8096\u50cf\u5408\u6210\uff0c\u901a\u8fc7\u8f6f\u8eab\u4efd\u7ea6\u675f\u548c\u9690\u5f0f3D\u611f\u77e5\u57282D\u89c6\u9891\u57df\u4e2d\u5b9e\u73b0\u9ad8\u8d28\u91cf\u52a8\u753b\u751f\u6210\u3002", "motivation": "\u73b0\u67092D\u8bed\u97f3\u5230\u89c6\u9891\u6269\u6563\u6a21\u578b\u8fc7\u5ea6\u4f9d\u8d56\u53c2\u8003\u6307\u5bfc\uff0c\u5bfc\u81f4\u8fd0\u52a8\u591a\u6837\u6027\u6709\u9650\uff1b\u800c3D\u611f\u77e5\u52a8\u753b\u901a\u5e38\u4f9d\u8d56\u9884\u8bad\u7ec3\u7684\u4e09\u5e73\u9762\u751f\u6210\u5668\u53cd\u6f14\uff0c\u5bfc\u81f4\u91cd\u5efa\u4e0d\u5b8c\u7f8e\u548c\u8eab\u4efd\u6f02\u79fb\u3002\u9700\u8981\u91cd\u65b0\u601d\u8003\u57fa\u4e8e\u53c2\u8003\u548c\u51e0\u4f55\u7684\u8303\u5f0f\u3002", "method": "\u91c7\u7528\u7ec4\u5408\u65b9\u6cd5\uff1a1\uff09\u5f15\u5165\u8f6f\u8eab\u4efd\u7ea6\u675f\u800c\u975e\u4e25\u683c\u53c2\u8003\u6761\u4ef6\uff1b2\uff09\u5229\u7528\u89c6\u9891\u6570\u636e\u7684\u591a\u89c6\u89d2\u7279\u6027\u57282D\u89c6\u9891\u57df\u4e2d\u9690\u5f0f\u5b9e\u73b03D\u611f\u77e5\uff1b3\uff09\u901a\u8fc7\u89e3\u8026\u5b66\u4e60\u5206\u522b\u8bad\u7ec3\u89c6\u89d2\u4e00\u81f4\u6027\u548c\u65f6\u95f4\u8fde\u8d2f\u6027\uff1b4\uff09\u4f7f\u7528\u81ea\u5f3a\u5236\u8bad\u7ec3\u65b9\u6848\u5b66\u4e60\u66f4\u957f\u7684\u65f6\u95f4\u4e0a\u4e0b\u6587\uff1b5\uff09\u4eceID\u611f\u77e5\u8fd0\u52a8\u5efa\u6a21\u5230\u97f3\u9891-\u89c6\u89c9\u540c\u6b65\u518d\u5230\u65f6\u7a7a\u9002\u5e94\u7684\u6e10\u8fdb\u5f0f\u6846\u67b6\u3002", "result": "\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0cSTARCaster\u5728\u8de8\u4efb\u52a1\u548c\u8eab\u4efd\u4e0a\u5177\u6709\u826f\u597d\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u8d85\u8d8a\u5148\u524d\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u81ea\u56de\u5f52\u65b9\u6cd5\u4e2d\u5e38\u89c1\u7684\u8fc7\u5ea6\u9759\u6001\u52a8\u753b\u95ee\u9898\u3002", "conclusion": "STARCaster\u901a\u8fc7\u91cd\u65b0\u601d\u8003\u53c2\u8003\u548c\u51e0\u4f55\u8303\u5f0f\uff0c\u5728\u7edf\u4e00\u6846\u67b6\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u8bed\u97f3\u9a71\u52a8\u8096\u50cf\u52a8\u753b\u548c\u81ea\u7531\u89c6\u89d2\u8bf4\u8bdd\u8096\u50cf\u5408\u6210\uff0c\u4e3a\u97f3\u9891-\u89c6\u89c9\u5185\u5bb9\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13250", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13250", "abs": "https://arxiv.org/abs/2512.13250", "authors": ["Juil Koo", "Daehyeon Choi", "Sangwoo Youn", "Phillip Y. Lee", "Minhyuk Sung"], "title": "Toward Ambulatory Vision: Learning Visually-Grounded Active View Selection", "comment": "Project page: https://active-view-selection.github.io/", "summary": "Vision Language Models (VLMs) excel at visual question answering (VQA) but remain limited to snapshot vision, reasoning from static images. In contrast, embodied agents require ambulatory vision, actively moving to obtain more informative views. We introduce Visually Grounded Active View Selection (VG-AVS), a task that selects the most informative next viewpoint using only the visual information in the current image, without relying on scene memory or external knowledge. To support this task, we construct a synthetic dataset with automatically generated paired query-target views and question-answer prompts. We also propose a framework that fine-tunes pretrained VLMs through supervised fine-tuning (SFT) followed by RL-based policy optimization. Our approach achieves strong question answering performance based on viewpoint selection and generalizes robustly to unseen synthetic and real scenes. Furthermore, incorporating our learned VG-AVS framework into existing scene-exploration-based EQA systems improves downstream question-answering accuracy.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u89c6\u89c9\u57fa\u7840\u4e3b\u52a8\u89c6\u89d2\u9009\u62e9\uff08VG-AVS\uff09\u4efb\u52a1\uff0c\u901a\u8fc7\u5f53\u524d\u89c6\u89c9\u4fe1\u606f\u9009\u62e9\u6700\u6709\u4fe1\u606f\u91cf\u7684\u4e0b\u4e00\u4e2a\u89c6\u89d2\uff0c\u5e76\u6784\u5efa\u4e86\u5408\u6210\u6570\u636e\u96c6\u548c\u8bad\u7ec3\u6846\u67b6\uff0c\u63d0\u5347\u4e86\u89c6\u89c9\u95ee\u7b54\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4ec5\u9650\u4e8e\u9759\u6001\u56fe\u50cf\u7684\u5feb\u7167\u89c6\u89c9\uff0c\u800c\u5177\u8eab\u667a\u80fd\u4f53\u9700\u8981\u4e3b\u52a8\u79fb\u52a8\u83b7\u53d6\u66f4\u6709\u4fe1\u606f\u91cf\u7684\u89c6\u89d2\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u57fa\u4e8e\u5f53\u524d\u89c6\u89c9\u4fe1\u606f\u4e3b\u52a8\u9009\u62e9\u4e0b\u4e00\u4e2a\u6700\u4f73\u89c6\u89d2\u7684\u80fd\u529b\u3002", "method": "1. \u63d0\u51faVG-AVS\u4efb\u52a1\uff1a\u4ec5\u4f7f\u7528\u5f53\u524d\u56fe\u50cf\u4e2d\u7684\u89c6\u89c9\u4fe1\u606f\u9009\u62e9\u6700\u6709\u4fe1\u606f\u91cf\u7684\u4e0b\u4e00\u4e2a\u89c6\u89d2\uff1b2. \u6784\u5efa\u5408\u6210\u6570\u636e\u96c6\uff1a\u81ea\u52a8\u751f\u6210\u914d\u5bf9\u7684\u67e5\u8be2-\u76ee\u6807\u89c6\u89d2\u548c\u95ee\u7b54\u63d0\u793a\uff1b3. \u63d0\u51fa\u8bad\u7ec3\u6846\u67b6\uff1a\u5148\u901a\u8fc7\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u5fae\u8c03\u9884\u8bad\u7ec3VLMs\uff0c\u7136\u540e\u8fdb\u884c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u7b56\u7565\u4f18\u5316\u3002", "result": "1. \u5728\u89c6\u89d2\u9009\u62e9\u57fa\u7840\u4e0a\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u95ee\u7b54\u6027\u80fd\uff1b2. \u5728\u672a\u89c1\u8fc7\u7684\u5408\u6210\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u7684\u6cdb\u5316\u80fd\u529b\uff1b3. \u5c06VG-AVS\u6846\u67b6\u6574\u5408\u5230\u73b0\u6709\u7684\u57fa\u4e8e\u573a\u666f\u63a2\u7d22\u7684EQA\u7cfb\u7edf\u4e2d\uff0c\u63d0\u9ad8\u4e86\u4e0b\u6e38\u95ee\u7b54\u51c6\u786e\u6027\u3002", "conclusion": "VG-AVS\u4efb\u52a1\u548c\u63d0\u51fa\u7684\u6846\u67b6\u6210\u529f\u5730\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4ece\u9759\u6001\u5feb\u7167\u89c6\u89c9\u6269\u5c55\u5230\u4e3b\u52a8\u89c6\u89d2\u9009\u62e9\uff0c\u4e3a\u5177\u8eab\u667a\u80fd\u4f53\u7684\u89c6\u89c9\u611f\u77e5\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u5e76\u80fd\u591f\u63d0\u5347\u73b0\u6709\u573a\u666f\u63a2\u7d22\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2512.13583", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13583", "abs": "https://arxiv.org/abs/2512.13583", "authors": ["Zehan Zhu", "Heng Zhao", "Yan Huang", "Joey Tianyi Zhou", "Shouling Ji", "Jinming Xu"], "title": "DP-CSGP: Differentially Private Stochastic Gradient Push with Compressed Communication", "comment": "13 pages", "summary": "In this paper, we propose a Differentially Private Stochastic Gradient Push with Compressed communication (termed DP-CSGP) for decentralized learning over directed graphs. Different from existing works, the proposed algorithm is designed to maintain high model utility while ensuring both rigorous differential privacy (DP) guarantees and efficient communication. For general non-convex and smooth objective functions, we show that the proposed algorithm achieves a tight utility bound of $\\mathcal{O}\\left( \\sqrt{d\\log \\left( \\frac{1}\u03b4 \\right)}/(\\sqrt{n}J\u03b5) \\right)$ ($J$ and $d$ are the number of local samples and the dimension of decision variables, respectively) with $\\left(\u03b5, \u03b4\\right)$-DP guarantee for each node, matching that of decentralized counterparts with exact communication. Extensive experiments on benchmark tasks show that, under the same privacy budget, DP-CSGP achieves comparable model accuracy with significantly lower communication cost than existing decentralized counterparts with exact communication.", "AI": {"tldr": "\u63d0\u51faDP-CSGP\u7b97\u6cd5\uff0c\u5728\u5b9a\u5411\u56fe\u4e0a\u7684\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u4e2d\u5b9e\u73b0\u5dee\u5206\u9690\u79c1\u4fdd\u62a4\u3001\u538b\u7f29\u901a\u4fe1\u548c\u9ad8\u6a21\u578b\u6548\u7528", "motivation": "\u73b0\u6709\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u540c\u65f6\u4fdd\u8bc1\u5dee\u5206\u9690\u79c1\u3001\u9ad8\u6548\u901a\u4fe1\u548c\u9ad8\u6a21\u578b\u6548\u7528\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u5b9a\u5411\u56fe\u901a\u4fe1\u573a\u666f\u4e0b", "method": "\u63d0\u51fa\u5dee\u5206\u9690\u79c1\u968f\u673a\u68af\u5ea6\u63a8\u9001\u538b\u7f29\u901a\u4fe1\u7b97\u6cd5(DP-CSGP)\uff0c\u7ed3\u5408\u5dee\u5206\u9690\u79c1\u4fdd\u62a4\u3001\u538b\u7f29\u901a\u4fe1\u6280\u672f\u548c\u5b9a\u5411\u56fe\u4e0a\u7684\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60", "result": "\u7b97\u6cd5\u8fbe\u5230\u7d27\u81f4\u7684\u6548\u7528\u8fb9\u754cO(\u221a(d log(1/\u03b4))/(\u221anJ\u03b5))\uff0c\u5728\u76f8\u540c\u9690\u79c1\u9884\u7b97\u4e0b\uff0c\u901a\u4fe1\u6210\u672c\u663e\u8457\u4f4e\u4e8e\u73b0\u6709\u7cbe\u786e\u901a\u4fe1\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u6a21\u578b\u7cbe\u5ea6", "conclusion": "DP-CSGP\u7b97\u6cd5\u6210\u529f\u5e73\u8861\u4e86\u5dee\u5206\u9690\u79c1\u4fdd\u62a4\u3001\u901a\u4fe1\u6548\u7387\u548c\u6a21\u578b\u6548\u7528\uff0c\u4e3a\u5b9a\u5411\u56fe\u4e0a\u7684\u9690\u79c1\u4fdd\u62a4\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.13276", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13276", "abs": "https://arxiv.org/abs/2512.13276", "authors": ["Yan Li", "Lin Liu", "Xiaopeng Zhang", "Wei Xue", "Wenhan Luo", "Yike Guo", "Qi Tian"], "title": "CogniEdit: Dense Gradient Flow Optimization for Fine-Grained Image Editing", "comment": null, "summary": "Instruction-based image editing with diffusion models has achieved impressive results, yet existing methods strug- gle with fine-grained instructions specifying precise attributes such as colors, positions, and quantities. While recent approaches employ Group Relative Policy Optimization (GRPO) for alignment, they optimize only at individual sampling steps, providing sparse feedback that limits trajectory-level control. We propose a unified framework CogniEdit, combining multi-modal reasoning with dense reward optimization that propagates gradients across con- secutive denoising steps, enabling trajectory-level gradient flow through the sampling process. Our method comprises three components: (1) Multi-modal Large Language Models for decomposing complex instructions into actionable directives, (2) Dynamic Token Focus Relocation that adaptively emphasizes fine-grained attributes, and (3) Dense GRPO-based optimization that propagates gradients across consecutive steps for trajectory-level supervision. Extensive experiments on benchmark datasets demonstrate that our CogniEdit achieves state-of-the-art performance in balancing fine-grained instruction following with visual quality and editability preservation", "AI": {"tldr": "CogniEdit\uff1a\u901a\u8fc7\u591a\u6a21\u6001\u63a8\u7406\u548c\u5bc6\u96c6\u5956\u52b1\u4f18\u5316\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u89e3\u51b3\u6269\u6563\u6a21\u578b\u5728\u9075\u5faa\u7ec6\u7c92\u5ea6\u6307\u4ee4\uff08\u5982\u989c\u8272\u3001\u4f4d\u7f6e\u3001\u6570\u91cf\uff09\u65b9\u9762\u7684\u56f0\u96be\uff0c\u5b9e\u73b0\u8f68\u8ff9\u7ea7\u68af\u5ea6\u6d41\u63a7\u5236", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6307\u4ee4\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u5728\u9075\u5faa\u7ec6\u7c92\u5ea6\u6307\u4ee4\uff08\u5982\u989c\u8272\u3001\u4f4d\u7f6e\u3001\u6570\u91cf\uff09\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u800c\u73b0\u6709\u7684GRPO\u65b9\u6cd5\u4ec5\u5728\u5355\u4e2a\u91c7\u6837\u6b65\u9aa4\u8fdb\u884c\u4f18\u5316\uff0c\u53cd\u9988\u7a00\u758f\uff0c\u9650\u5236\u4e86\u8f68\u8ff9\u7ea7\u63a7\u5236\u80fd\u529b", "method": "\u63d0\u51faCogniEdit\u7edf\u4e00\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a1\uff09\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5206\u89e3\u590d\u6742\u6307\u4ee4\u4e3a\u53ef\u6267\u884c\u6307\u4ee4\uff1b2\uff09\u52a8\u6001\u4ee4\u724c\u7126\u70b9\u91cd\u5b9a\u4f4d\u81ea\u9002\u5e94\u5f3a\u8c03\u7ec6\u7c92\u5ea6\u5c5e\u6027\uff1b3\uff09\u57fa\u4e8e\u5bc6\u96c6GRPO\u7684\u4f18\u5316\uff0c\u5728\u8fde\u7eed\u53bb\u566a\u6b65\u9aa4\u95f4\u4f20\u64ad\u68af\u5ea6\uff0c\u5b9e\u73b0\u8f68\u8ff9\u7ea7\u76d1\u7763", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cCogniEdit\u5728\u5e73\u8861\u7ec6\u7c92\u5ea6\u6307\u4ee4\u9075\u5faa\u3001\u89c6\u89c9\u8d28\u91cf\u548c\u53ef\u7f16\u8f91\u6027\u4fdd\u6301\u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd", "conclusion": "CogniEdit\u901a\u8fc7\u7ed3\u5408\u591a\u6a21\u6001\u63a8\u7406\u548c\u5bc6\u96c6\u5956\u52b1\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u6307\u4ee4\u9075\u5faa\u65b9\u9762\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u8f68\u8ff9\u7ea7\u68af\u5ea6\u6d41\u63a7\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u50cf\u7f16\u8f91\u7684\u7cbe\u786e\u6027\u548c\u8d28\u91cf"}}
{"id": "2512.13592", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13592", "abs": "https://arxiv.org/abs/2512.13592", "authors": ["Fu-Yun Wang", "Hao Zhou", "Liangzhe Yuan", "Sanghyun Woo", "Boqing Gong", "Bohyung Han", "Ming-Hsuan Yang", "Han Zhang", "Yukun Zhu", "Ting Liu", "Long Zhao"], "title": "Image Diffusion Preview with Consistency Solver", "comment": null, "summary": "The slow inference process of image diffusion models significantly degrades interactive user experiences. To address this, we introduce Diffusion Preview, a novel paradigm employing rapid, low-step sampling to generate preliminary outputs for user evaluation, deferring full-step refinement until the preview is deemed satisfactory. Existing acceleration methods, including training-free solvers and post-training distillation, struggle to deliver high-quality previews or ensure consistency between previews and final outputs. We propose ConsistencySolver derived from general linear multistep methods, a lightweight, trainable high-order solver optimized via Reinforcement Learning, that enhances preview quality and consistency. Experimental results demonstrate that ConsistencySolver significantly improves generation quality and consistency in low-step scenarios, making it ideal for efficient preview-and-refine workflows. Notably, it achieves FID scores on-par with Multistep DPM-Solver using 47% fewer steps, while outperforming distillation baselines. Furthermore, user studies indicate our approach reduces overall user interaction time by nearly 50% while maintaining generation quality. Code is available at https://github.com/G-U-N/consolver.", "AI": {"tldr": "\u63d0\u51faDiffusion Preview\u8303\u5f0f\uff0c\u901a\u8fc7\u5feb\u901f\u4f4e\u6b65\u6570\u91c7\u6837\u751f\u6210\u9884\u89c8\u4f9b\u7528\u6237\u8bc4\u4f30\uff0c\u6ee1\u610f\u540e\u518d\u8fdb\u884c\u5168\u6b65\u6570\u7cbe\u70bc\u3002\u4e3a\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u9884\u89c8\u8d28\u91cf\u5dee\u548c\u4e00\u81f4\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u7684\u8f7b\u91cf\u7ea7\u9ad8\u9636\u6c42\u89e3\u5668ConsistencySolver\u3002", "motivation": "\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u7f13\u6162\u63a8\u7406\u8fc7\u7a0b\u4e25\u91cd\u5f71\u54cd\u4e86\u4ea4\u4e92\u5f0f\u7528\u6237\u4f53\u9a8c\u3002\u73b0\u6709\u52a0\u901f\u65b9\u6cd5\uff08\u5305\u62ec\u514d\u8bad\u7ec3\u6c42\u89e3\u5668\u548c\u8bad\u7ec3\u540e\u84b8\u998f\uff09\u96be\u4ee5\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u9884\u89c8\uff0c\u4e5f\u65e0\u6cd5\u786e\u4fdd\u9884\u89c8\u4e0e\u6700\u7ec8\u8f93\u51fa\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51faConsistencySolver\uff0c\u4e00\u79cd\u57fa\u4e8e\u901a\u7528\u7ebf\u6027\u591a\u6b65\u65b9\u6cd5\u7684\u8f7b\u91cf\u7ea7\u53ef\u8bad\u7ec3\u9ad8\u9636\u6c42\u89e3\u5668\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u4f18\u5316\uff0c\u65e8\u5728\u63d0\u5347\u9884\u89c8\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cConsistencySolver\u5728\u4f4e\u6b65\u6570\u573a\u666f\u4e0b\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u8d28\u91cf\u548c\u4e00\u81f4\u6027\u3002\u4e0eMultistep DPM-Solver\u76f8\u6bd4\uff0c\u4f7f\u752847%\u66f4\u5c11\u7684\u6b65\u9aa4\u8fbe\u5230\u76f8\u5f53\u7684FID\u5206\u6570\uff0c\u540c\u65f6\u4f18\u4e8e\u84b8\u998f\u57fa\u7ebf\u65b9\u6cd5\u3002\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5c06\u7528\u6237\u4ea4\u4e92\u65f6\u95f4\u51cf\u5c11\u4e86\u8fd150%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u751f\u6210\u8d28\u91cf\u3002", "conclusion": "ConsistencySolver\u901a\u8fc7\u63d0\u5347\u9884\u89c8\u8d28\u91cf\u548c\u4e00\u81f4\u6027\uff0c\u4e3a\u9ad8\u6548\u7684\u9884\u89c8-\u7cbe\u70bc\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u7406\u60f3\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u6539\u5584\u4e86\u6269\u6563\u6a21\u578b\u7684\u4ea4\u4e92\u4f53\u9a8c\u3002"}}
{"id": "2512.13285", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13285", "abs": "https://arxiv.org/abs/2512.13285", "authors": ["Bo Liu", "Qiao Qin", "Qinghui He"], "title": "CausalCLIP: Causally-Informed Feature Disentanglement and Filtering for Generalizable Detection of Generated Images", "comment": "9 pages Accepted to AAAI 2026", "summary": "The rapid advancement of generative models has increased the demand for generated image detectors capable of generalizing across diverse and evolving generation techniques. However, existing methods, including those leveraging pre-trained vision-language models, often produce highly entangled representations, mixing task-relevant forensic cues (causal features) with spurious or irrelevant patterns (non-causal features), thus limiting generalization. To address this issue, we propose CausalCLIP, a framework that explicitly disentangles causal from non-causal features and employs targeted filtering guided by causal inference principles to retain only the most transferable and discriminative forensic cues. By modeling the generation process with a structural causal model and enforcing statistical independence through Gumbel-Softmax-based feature masking and Hilbert-Schmidt Independence Criterion (HSIC) constraints, CausalCLIP isolates stable causal features robust to distribution shifts. When tested on unseen generative models from different series, CausalCLIP demonstrates strong generalization ability, achieving improvements of 6.83% in accuracy and 4.06% in average precision over state-of-the-art methods.", "AI": {"tldr": "CausalCLIP\uff1a\u57fa\u4e8e\u56e0\u679c\u63a8\u65ad\u7684\u56fe\u50cf\u751f\u6210\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u56e0\u679c\u4e0e\u975e\u56e0\u679c\u7279\u5f81\u63d0\u5347\u8de8\u751f\u6210\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b", "motivation": "\u73b0\u6709\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u65b9\u6cd5\uff08\u5305\u62ec\u57fa\u4e8e\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5\uff09\u5f80\u5f80\u4ea7\u751f\u9ad8\u5ea6\u7ea0\u7f20\u7684\u7279\u5f81\u8868\u793a\uff0c\u6df7\u5408\u4e86\u4efb\u52a1\u76f8\u5173\u7684\u53d6\u8bc1\u7ebf\u7d22\uff08\u56e0\u679c\u7279\u5f81\uff09\u4e0e\u865a\u5047\u6216\u65e0\u5173\u6a21\u5f0f\uff08\u975e\u56e0\u679c\u7279\u5f81\uff09\uff0c\u9650\u5236\u4e86\u8de8\u4e0d\u540c\u751f\u6210\u6280\u672f\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faCausalCLIP\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u5efa\u6a21\u751f\u6210\u8fc7\u7a0b\uff0c\u5229\u7528Gumbel-Softmax\u7279\u5f81\u63a9\u7801\u548cHilbert-Schmidt\u72ec\u7acb\u6027\u51c6\u5219\u7ea6\u675f\u5b9e\u73b0\u7edf\u8ba1\u72ec\u7acb\u6027\uff0c\u660e\u786e\u89e3\u8026\u56e0\u679c\u4e0e\u975e\u56e0\u679c\u7279\u5f81\uff0c\u5e76\u57fa\u4e8e\u56e0\u679c\u63a8\u65ad\u539f\u7406\u8fdb\u884c\u9488\u5bf9\u6027\u8fc7\u6ee4\uff0c\u4ec5\u4fdd\u7559\u6700\u5177\u53ef\u8fc1\u79fb\u6027\u548c\u5224\u522b\u6027\u7684\u53d6\u8bc1\u7ebf\u7d22\u3002", "result": "\u5728\u672a\u89c1\u8fc7\u7684\u4e0d\u540c\u7cfb\u5217\u751f\u6210\u6a21\u578b\u4e0a\u6d4b\u8bd5\u65f6\uff0cCausalCLIP\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u5728\u51c6\u786e\u7387\u4e0a\u63d0\u53476.83%\uff0c\u5e73\u5747\u7cbe\u5ea6\u63d0\u53474.06%\u3002", "conclusion": "\u901a\u8fc7\u56e0\u679c\u7279\u5f81\u89e3\u8026\u548c\u9488\u5bf9\u6027\u8fc7\u6ee4\uff0cCausalCLIP\u80fd\u591f\u6709\u6548\u63d0\u5347\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u5668\u5728\u591a\u6837\u5316\u751f\u6210\u6280\u672f\u4e2d\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u4e3a\u5e94\u5bf9\u5feb\u901f\u53d1\u5c55\u7684\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u7684\u68c0\u6d4b\u65b9\u6848\u3002"}}
{"id": "2512.13290", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13290", "abs": "https://arxiv.org/abs/2512.13290", "authors": ["Shu Yu", "Chaochao Lu"], "title": "LINA: Learning INterventions Adaptively for Physical Alignment and Generalization in Diffusion Models", "comment": null, "summary": "Diffusion models (DMs) have achieved remarkable success in image and video generation. However, they still struggle with (1) physical alignment and (2) out-of-distribution (OOD) instruction following. We argue that these issues stem from the models' failure to learn causal directions and to disentangle causal factors for novel recombination. We introduce the Causal Scene Graph (CSG) and the Physical Alignment Probe (PAP) dataset to enable diagnostic interventions. This analysis yields three key insights. First, DMs struggle with multi-hop reasoning for elements not explicitly determined in the prompt. Second, the prompt embedding contains disentangled representations for texture and physics. Third, visual causal structure is disproportionately established during the initial, computationally limited denoising steps. Based on these findings, we introduce LINA (Learning INterventions Adaptively), a novel framework that learns to predict prompt-specific interventions, which employs (1) targeted guidance in the prompt and visual latent spaces, and (2) a reallocated, causality-aware denoising schedule. Our approach enforces both physical alignment and OOD instruction following in image and video DMs, achieving state-of-the-art performance on challenging causal generation tasks and the Winoground dataset. Our project page is at https://opencausalab.github.io/LINA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faLINA\u6846\u67b6\uff0c\u901a\u8fc7\u56e0\u679c\u5e72\u9884\u89e3\u51b3\u6269\u6563\u6a21\u578b\u5728\u7269\u7406\u5bf9\u9f50\u548cOOD\u6307\u4ee4\u8ddf\u968f\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5229\u7528\u56e0\u679c\u573a\u666f\u56fe\u548c\u7269\u7406\u5bf9\u9f50\u63a2\u9488\u6570\u636e\u96c6\u8fdb\u884c\u8bca\u65ad\uff0c\u5b9e\u73b0\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u4e2d\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\u63d0\u5347\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5728\u7269\u7406\u5bf9\u9f50\u548c\u5206\u5e03\u5916\u6307\u4ee4\u8ddf\u968f\u65b9\u9762\u4ecd\u5b58\u5728\u56f0\u96be\u3002\u8fd9\u4e9b\u95ee\u9898\u6e90\u4e8e\u6a21\u578b\u672a\u80fd\u5b66\u4e60\u56e0\u679c\u65b9\u5411\u548c\u56e0\u679c\u56e0\u7d20\u89e3\u8026\u4ee5\u8fdb\u884c\u65b0\u9896\u91cd\u7ec4\u3002", "method": "\u5f15\u5165\u56e0\u679c\u573a\u666f\u56fe\u548c\u7269\u7406\u5bf9\u9f50\u63a2\u9488\u6570\u636e\u96c6\u8fdb\u884c\u8bca\u65ad\u5206\u6790\uff0c\u63d0\u51faLINA\u6846\u67b6\uff1a\u5b66\u4e60\u9884\u6d4b\u7279\u5b9a\u63d0\u793a\u7684\u5e72\u9884\u63aa\u65bd\uff0c\u5305\u62ec\u5728\u63d0\u793a\u548c\u89c6\u89c9\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u5b9a\u5411\u5f15\u5bfc\uff0c\u4ee5\u53ca\u91cd\u65b0\u5206\u914d\u7684\u56e0\u679c\u611f\u77e5\u53bb\u566a\u8c03\u5ea6\u3002", "result": "LINA\u6846\u67b6\u5728\u56fe\u50cf\u548c\u89c6\u9891\u6269\u6563\u6a21\u578b\u4e2d\u540c\u65f6\u589e\u5f3a\u4e86\u7269\u7406\u5bf9\u9f50\u548cOOD\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u56e0\u679c\u751f\u6210\u4efb\u52a1\u548cWinoground\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u56e0\u679c\u5e72\u9884\u548c\u91cd\u65b0\u5206\u914d\u7684\u53bb\u566a\u8c03\u5ea6\uff0cLINA\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u6269\u6563\u6a21\u578b\u5728\u7269\u7406\u5bf9\u9f50\u548cOOD\u6307\u4ee4\u8ddf\u968f\u65b9\u9762\u7684\u6838\u5fc3\u95ee\u9898\uff0c\u4e3a\u56e0\u679c\u611f\u77e5\u7684\u56fe\u50cf\u548c\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2512.13632", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13632", "abs": "https://arxiv.org/abs/2512.13632", "authors": ["Guransh Singh", "Md Shah Fahad"], "title": "StutterFuse: Mitigating Modality Collapse in Stuttering Detection with Jaccard-Weighted Metric Learning and Gated Fusion", "comment": "13 pages, 10 figures", "summary": "Stuttering detection breaks down when disfluencies overlap. Existing parametric models struggle to distinguish complex, simultaneous disfluencies (e.g., a 'block' with a 'prolongation') due to the scarcity of these specific combinations in training data. While Retrieval-Augmented Generation (RAG) has revolutionized NLP by grounding models in external knowledge, this paradigm remains unexplored in pathological speech processing. To bridge this gap, we introduce StutterFuse, the first Retrieval-Augmented Classifier (RAC) for multi-label stuttering detection. By conditioning a Conformer encoder on a non-parametric memory bank of clinical examples, we allow the model to classify by reference rather than memorization. We further identify and solve \"Modality Collapse\", an \"Echo Chamber\" effect where naive retrieval boosts recall but degrades precision. We mitigate this using: (1) SetCon, a Jaccard-Weighted Metric Learning objective that optimizes for multi-label set similarity, and (2) a Gated Mixture-of-Experts fusion strategy that dynamically arbitrates between acoustic evidence and retrieved context. On the SEP-28k dataset, StutterFuse achieves a weighted F1-score of 0.65, outperforming strong baselines and demonstrating remarkable zero-shot cross-lingual generalization.", "AI": {"tldr": "\u63d0\u51faStutterFuse\uff0c\u9996\u4e2a\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u5206\u7c7b\u5668\u7684\u591a\u6807\u7b7e\u53e3\u5403\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u68c0\u7d22\u4e34\u5e8a\u6848\u4f8b\u5e93\u89e3\u51b3\u91cd\u53e0\u6027\u53e3\u5403\u68c0\u6d4b\u96be\u9898\uff0c\u5e76\u89e3\u51b3\u4e86\u6a21\u6001\u5d29\u6e83\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u53c2\u6570\u5316\u6a21\u578b\u96be\u4ee5\u68c0\u6d4b\u91cd\u53e0\u6027\u53e3\u5403\uff08\u5982\"\u963b\u585e\"\u4e0e\"\u5ef6\u957f\"\u540c\u65f6\u53d1\u751f\uff09\uff0c\u56e0\u4e3a\u8bad\u7ec3\u6570\u636e\u4e2d\u8fd9\u4e9b\u7279\u5b9a\u7ec4\u5408\u7a00\u7f3a\u3002\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u5728NLP\u9886\u57df\u53d6\u5f97\u7a81\u7834\uff0c\u4f46\u5728\u75c5\u7406\u8bed\u97f3\u5904\u7406\u4e2d\u5c1a\u672a\u63a2\u7d22\u3002", "method": "\u63d0\u51faStutterFuse\u68c0\u7d22\u589e\u5f3a\u5206\u7c7b\u5668\uff1a1\uff09\u57fa\u4e8e\u4e34\u5e8a\u6848\u4f8b\u7684\u975e\u53c2\u6570\u8bb0\u5fc6\u5e93\u4e3aConformer\u7f16\u7801\u5668\u63d0\u4f9b\u53c2\u8003\uff1b2\uff09\u4f7f\u7528SetCon\uff08Jaccard\u52a0\u6743\u5ea6\u91cf\u5b66\u4e60\uff09\u4f18\u5316\u591a\u6807\u7b7e\u96c6\u76f8\u4f3c\u6027\uff1b3\uff09\u91c7\u7528\u95e8\u63a7\u4e13\u5bb6\u6df7\u5408\u878d\u5408\u7b56\u7565\u52a8\u6001\u5e73\u8861\u58f0\u5b66\u8bc1\u636e\u4e0e\u68c0\u7d22\u4e0a\u4e0b\u6587\u3002", "result": "\u5728SEP-28k\u6570\u636e\u96c6\u4e0a\u83b7\u5f970.65\u52a0\u6743F1\u5206\u6570\uff0c\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u5c55\u73b0\u51fa\u663e\u8457\u7684\u96f6\u6837\u672c\u8de8\u8bed\u8a00\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "StutterFuse\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u8303\u5f0f\u6210\u529f\u89e3\u51b3\u4e86\u91cd\u53e0\u6027\u53e3\u5403\u68c0\u6d4b\u96be\u9898\uff0c\u4e3a\u75c5\u7406\u8bed\u97f3\u5904\u7406\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u540c\u65f6\u63d0\u51fa\u7684\u6a21\u6001\u5d29\u6e83\u89e3\u51b3\u65b9\u6848\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2512.13303", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13303", "abs": "https://arxiv.org/abs/2512.13303", "authors": ["Zhihang Liu", "Xiaoyi Bao", "Pandeng Li", "Junjie Zhou", "Zhaohe Liao", "Yefei He", "Kaixun Jiang", "Chen-Wei Xie", "Yun Zheng", "Hongtao Xie"], "title": "ShowTable: Unlocking Creative Table Visualization with Collaborative Reflection and Refinement", "comment": "project page: https://lntzm.github.io/showtable-page/", "summary": "While existing generation and unified models excel at general image generation, they struggle with tasks requiring deep reasoning, planning, and precise data-to-visual mapping abilities beyond general scenarios. To push beyond the existing limitations, we introduce a new and challenging task: creative table visualization, requiring the model to generate an infographic that faithfully and aesthetically visualizes the data from a given table. To address this challenge, we propose ShowTable, a pipeline that synergizes MLLMs with diffusion models via a progressive self-correcting process. The MLLM acts as the central orchestrator for reasoning the visual plan and judging visual errors to provide refined instructions, the diffusion execute the commands from MLLM, achieving high-fidelity results. To support this task and our pipeline, we introduce three automated data construction pipelines for training different modules. Furthermore, we introduce TableVisBench, a new benchmark with 800 challenging instances across 5 evaluation dimensions, to assess performance on this task. Experiments demonstrate that our pipeline, instantiated with different models, significantly outperforms baselines, highlighting its effective multi-modal reasoning, generation, and error correction capabilities.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u521b\u9020\u6027\u8868\u683c\u53ef\u89c6\u5316\u65b0\u4efb\u52a1\uff0c\u5f00\u53d1ShowTable\u7ba1\u9053\u7ed3\u5408MLLM\u548c\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u6821\u6b63\u8fc7\u7a0b\u751f\u6210\u9ad8\u8d28\u91cf\u4fe1\u606f\u56fe\u8868\uff0c\u5e76\u5efa\u7acbTableVisBench\u57fa\u51c6\u8fdb\u884c\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\u5728\u9700\u8981\u6df1\u5ea6\u63a8\u7406\u3001\u89c4\u5212\u548c\u7cbe\u786e\u6570\u636e\u5230\u89c6\u89c9\u6620\u5c04\u7684\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u5728\u8d85\u8d8a\u4e00\u822c\u573a\u666f\u7684\u521b\u9020\u6027\u8868\u683c\u53ef\u89c6\u5316\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "\u63d0\u51faShowTable\u7ba1\u9053\uff0c\u901a\u8fc7\u6e10\u8fdb\u5f0f\u81ea\u6821\u6b63\u8fc7\u7a0b\u534f\u540c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u6269\u6563\u6a21\u578b\u3002MLLM\u4f5c\u4e3a\u4e2d\u592e\u534f\u8c03\u5668\u8fdb\u884c\u89c6\u89c9\u89c4\u5212\u63a8\u7406\u548c\u89c6\u89c9\u9519\u8bef\u5224\u65ad\uff0c\u6269\u6563\u6a21\u578b\u6267\u884cMLLM\u6307\u4ee4\uff0c\u5b9e\u73b0\u9ad8\u4fdd\u771f\u7ed3\u679c\u3002\u5f00\u53d1\u4e09\u79cd\u81ea\u52a8\u6570\u636e\u6784\u5efa\u7ba1\u9053\u8bad\u7ec3\u4e0d\u540c\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u4e0d\u540c\u6a21\u578b\u5b9e\u4f8b\u5316\u7684ShowTable\u7ba1\u9053\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728TableVisBench\u57fa\u51c6\u76845\u4e2a\u8bc4\u4f30\u7ef4\u5ea6\u4e0a\u5c55\u73b0\u51fa\u6709\u6548\u7684\u591a\u6a21\u6001\u63a8\u7406\u3001\u751f\u6210\u548c\u9519\u8bef\u6821\u6b63\u80fd\u529b\u3002", "conclusion": "\u8bba\u6587\u6210\u529f\u89e3\u51b3\u4e86\u521b\u9020\u6027\u8868\u683c\u53ef\u89c6\u5316\u8fd9\u4e00\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u63d0\u51fa\u7684ShowTable\u7ba1\u9053\u901a\u8fc7\u591a\u6a21\u6001\u534f\u540c\u548c\u81ea\u6821\u6b63\u673a\u5236\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u4fe1\u606f\u56fe\u8868\u751f\u6210\uff0c\u4e3a\u6570\u636e\u53ef\u89c6\u5316\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2512.13641", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13641", "abs": "https://arxiv.org/abs/2512.13641", "authors": ["Gabriel Vitorino de Andrade", "Saulo Roberto dos Santos", "Itallo Patrick Castro Alves da Silva", "Emanuel Adler Medeiros Pereira", "Erick de Andrade Barboza"], "title": "From Code to Field: Evaluating the Robustness of Convolutional Neural Networks for Disease Diagnosis in Mango Leaves", "comment": "This work was presented at the BRACIS 2025 conference in Fortaleza", "summary": "The validation and verification of artificial intelligence (AI) models through robustness assessment are essential to guarantee the reliable performance of intelligent systems facing real-world challenges, such as image corruptions including noise, blurring, and weather variations. Despite the global importance of mango (Mangifera indica L.), there is a lack of studies on the robustness of models for the diagnosis of disease in its leaves. This paper proposes a methodology to evaluate convolutional neural networks (CNNs) under adverse conditions. We adapted the MangoLeafDB dataset, generating MangoLeafDB-C with 19 types of artificial corruptions at five severity levels. We conducted a benchmark comparing five architectures: ResNet-50, ResNet-101, VGG-16, Xception, and LCNN (the latter being a lightweight architecture designed specifically for mango leaf diagnosis). The metrics include the F1 score, the corruption error (CE) and the relative mean corruption error (relative mCE). The results show that LCNN outperformed complex models in corruptions that can be present in real-world scenarios such as Defocus Blur, Motion Blur, while also achieving the lowest mCE. Modern architectures (e.g., ResNet-101) exhibited significant performance degradation in corrupted scenarios, despite their high accuracy under ideal conditions. These findings suggest that lightweight and specialized models may be more suitable for real-world applications in edge devices, where robustness and efficiency are critical. The study highlights the need to incorporate robustness assessments in the development of intelligent systems for agriculture, particularly in regions with technological limitations.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u8292\u679c\u53f6\u75c5\u5bb3\u8bca\u65adCNN\u6a21\u578b\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u521b\u5efa\u5305\u542b19\u79cd\u4eba\u5de5\u635f\u574f\u7684MangoLeafDB-C\u6570\u636e\u96c6\uff0c\u6bd4\u8f83\u4e865\u79cd\u67b6\u6784\u5728\u6076\u52a3\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u8f7b\u91cf\u7ea7\u4e13\u7528\u6a21\u578bLCNN\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u5c3d\u7ba1\u8292\u679c\u5177\u6709\u5168\u7403\u91cd\u8981\u6027\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u8292\u679c\u53f6\u75c5\u5bb3\u8bca\u65ad\u6a21\u578b\u9c81\u68d2\u6027\u7684\u7814\u7a76\u3002\u9700\u8981\u8bc4\u4f30AI\u6a21\u578b\u5728\u73b0\u5b9e\u4e16\u754c\u6311\u6218\uff08\u5982\u56fe\u50cf\u635f\u574f\uff09\u4e0b\u7684\u53ef\u9760\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u519c\u4e1a\u667a\u80fd\u7cfb\u7edf\u5f00\u53d1\u4e2d\u3002", "method": "1. \u6539\u7f16MangoLeafDB\u6570\u636e\u96c6\uff0c\u751f\u6210\u5305\u542b19\u79cd\u4eba\u5de5\u635f\u574f\u7c7b\u578b\u30015\u4e2a\u4e25\u91cd\u7a0b\u5ea6\u7684MangoLeafDB-C\u6570\u636e\u96c6\uff1b2. \u5bf95\u79cdCNN\u67b6\u6784\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff1aResNet-50\u3001ResNet-101\u3001VGG-16\u3001Xception\u548c\u4e13\u95e8\u8bbe\u8ba1\u7684\u8f7b\u91cf\u7ea7\u67b6\u6784LCNN\uff1b3. \u4f7f\u7528F1\u5206\u6570\u3001\u635f\u574f\u9519\u8bef\u7387\uff08CE\uff09\u548c\u76f8\u5bf9\u5e73\u5747\u635f\u574f\u9519\u8bef\u7387\uff08\u76f8\u5bf9mCE\uff09\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\u3002", "result": "1. LCNN\u5728\u53ef\u80fd\u51fa\u73b0\u5728\u73b0\u5b9e\u573a\u666f\u7684\u635f\u574f\u7c7b\u578b\uff08\u5982\u6563\u7126\u6a21\u7cca\u3001\u8fd0\u52a8\u6a21\u7cca\uff09\u4e2d\u8868\u73b0\u4f18\u4e8e\u590d\u6742\u6a21\u578b\uff1b2. LCNN\u83b7\u5f97\u4e86\u6700\u4f4e\u7684\u5e73\u5747\u635f\u574f\u9519\u8bef\u7387\uff08mCE\uff09\uff1b3. \u73b0\u4ee3\u67b6\u6784\uff08\u5982ResNet-101\uff09\u5728\u7406\u60f3\u6761\u4ef6\u4e0b\u51c6\u786e\u7387\u9ad8\uff0c\u4f46\u5728\u635f\u574f\u573a\u666f\u4e2d\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff1b4. \u8f7b\u91cf\u7ea7\u4e13\u7528\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u5e94\u7528\u4e2d\u66f4\u5177\u4f18\u52bf\u3002", "conclusion": "\u8f7b\u91cf\u7ea7\u548c\u4e13\u7528\u6a21\u578b\u53ef\u80fd\u66f4\u9002\u5408\u8fb9\u7f18\u8bbe\u5907\u7684\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\uff0c\u5176\u4e2d\u9c81\u68d2\u6027\u548c\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u519c\u4e1a\u667a\u80fd\u7cfb\u7edf\u5f00\u53d1\u4e2d\uff0c\u7279\u522b\u662f\u5728\u6280\u672f\u53d7\u9650\u5730\u533a\uff0c\u9700\u8981\u7eb3\u5165\u9c81\u68d2\u6027\u8bc4\u4f30\u3002"}}
{"id": "2512.13313", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13313", "abs": "https://arxiv.org/abs/2512.13313", "authors": ["Kling Team", "Jialu Chen", "Yikang Ding", "Zhixue Fang", "Kun Gai", "Yuan Gao", "Kang He", "Jingyun Hua", "Boyuan Jiang", "Mingming Lao", "Xiaohan Li", "Hui Liu", "Jiwen Liu", "Xiaoqiang Liu", "Yuan Liu", "Shun Lu", "Yongsen Mao", "Yingchao Shao", "Huafeng Shi", "Xiaoyu Shi", "Peiqin Sun", "Songlin Tang", "Pengfei Wan", "Chao Wang", "Xuebo Wang", "Haoxian Zhang", "Yuanxing Zhang", "Yan Zhou"], "title": "KlingAvatar 2.0 Technical Report", "comment": "14 pages, 7 figures", "summary": "Avatar video generation models have achieved remarkable progress in recent years. However, prior work exhibits limited efficiency in generating long-duration high-resolution videos, suffering from temporal drifting, quality degradation, and weak prompt following as video length increases. To address these challenges, we propose KlingAvatar 2.0, a spatio-temporal cascade framework that performs upscaling in both spatial resolution and temporal dimension. The framework first generates low-resolution blueprint video keyframes that capture global semantics and motion, and then refines them into high-resolution, temporally coherent sub-clips using a first-last frame strategy, while retaining smooth temporal transitions in long-form videos. To enhance cross-modal instruction fusion and alignment in extended videos, we introduce a Co-Reasoning Director composed of three modality-specific large language model (LLM) experts. These experts reason about modality priorities and infer underlying user intent, converting inputs into detailed storylines through multi-turn dialogue. A Negative Director further refines negative prompts to improve instruction alignment. Building on these components, we extend the framework to support ID-specific multi-character control. Extensive experiments demonstrate that our model effectively addresses the challenges of efficient, multimodally aligned long-form high-resolution video generation, delivering enhanced visual clarity, realistic lip-teeth rendering with accurate lip synchronization, strong identity preservation, and coherent multimodal instruction following.", "AI": {"tldr": "KlingAvatar 2.0\u662f\u4e00\u4e2a\u65f6\u7a7a\u7ea7\u8054\u6846\u67b6\uff0c\u901a\u8fc7\u4f4e\u5206\u8fa8\u7387\u84dd\u56fe\u89c6\u9891\u5173\u952e\u5e27\u751f\u6210\u548c\u9ad8\u5206\u8fa8\u7387\u5b50\u7247\u6bb5\u7ec6\u5316\uff0c\u89e3\u51b3\u4e86\u957f\u65f6\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u751f\u6210\u4e2d\u7684\u6548\u7387\u3001\u65f6\u95f4\u6f02\u79fb\u548c\u8d28\u91cf\u9000\u5316\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u534f\u540c\u63a8\u7406\u5bfc\u6f14\u589e\u5f3a\u591a\u6a21\u6001\u6307\u4ee4\u5bf9\u9f50\u3002", "motivation": "\u73b0\u6709\u5934\u50cf\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u751f\u6210\u957f\u65f6\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u65f6\u5b58\u5728\u6548\u7387\u4f4e\u3001\u65f6\u95f4\u6f02\u79fb\u3001\u8d28\u91cf\u9000\u5316\u548c\u63d0\u793a\u8ddf\u968f\u80fd\u529b\u5f31\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9ad8\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u957f\u89c6\u9891\u5e76\u4fdd\u6301\u591a\u6a21\u6001\u6307\u4ee4\u5bf9\u9f50\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u65f6\u7a7a\u7ea7\u8054\u6846\u67b6\uff1a\u9996\u5148\u751f\u6210\u4f4e\u5206\u8fa8\u7387\u84dd\u56fe\u89c6\u9891\u5173\u952e\u5e27\u6355\u6349\u5168\u5c40\u8bed\u4e49\u548c\u8fd0\u52a8\uff0c\u7136\u540e\u4f7f\u7528\u9996\u5c3e\u5e27\u7b56\u7565\u5c06\u5176\u7ec6\u5316\u4e3a\u9ad8\u5206\u8fa8\u7387\u3001\u65f6\u95f4\u8fde\u8d2f\u7684\u5b50\u7247\u6bb5\u3002\u5f15\u5165\u534f\u540c\u63a8\u7406\u5bfc\u6f14\uff0c\u7531\u4e09\u4e2a\u6a21\u6001\u7279\u5b9a\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4e13\u5bb6\u7ec4\u6210\uff0c\u901a\u8fc7\u591a\u8f6e\u5bf9\u8bdd\u63a8\u7406\u6a21\u6001\u4f18\u5148\u7ea7\u548c\u7528\u6237\u610f\u56fe\uff0c\u5c06\u8f93\u5165\u8f6c\u6362\u4e3a\u8be6\u7ec6\u6545\u4e8b\u60c5\u8282\u3002\u8d1f\u5411\u5bfc\u6f14\u8fdb\u4e00\u6b65\u4f18\u5316\u8d1f\u5411\u63d0\u793a\u4ee5\u6539\u5584\u6307\u4ee4\u5bf9\u9f50\u3002\u6269\u5c55\u6846\u67b6\u652f\u6301ID\u7279\u5b9a\u7684\u591a\u89d2\u8272\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u6548\u3001\u591a\u6a21\u6001\u5bf9\u9f50\u7684\u957f\u65f6\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u751f\u6210\u6311\u6218\uff0c\u63d0\u4f9b\u589e\u5f3a\u7684\u89c6\u89c9\u6e05\u6670\u5ea6\u3001\u903c\u771f\u7684\u5507\u9f7f\u6e32\u67d3\u4e0e\u51c6\u786e\u7684\u53e3\u578b\u540c\u6b65\u3001\u5f3a\u5927\u7684\u8eab\u4efd\u4fdd\u6301\u4ee5\u53ca\u8fde\u8d2f\u7684\u591a\u6a21\u6001\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\u3002", "conclusion": "KlingAvatar 2.0\u901a\u8fc7\u65f6\u7a7a\u7ea7\u8054\u6846\u67b6\u548c\u534f\u540c\u63a8\u7406\u5bfc\u6f14\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u957f\u65f6\u9ad8\u5206\u8fa8\u7387\u5934\u50cf\u89c6\u9891\u751f\u6210\u4e2d\u7684\u5173\u952e\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u4e14\u591a\u6a21\u6001\u5bf9\u9f50\u7684\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2512.13668", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13668", "abs": "https://arxiv.org/abs/2512.13668", "authors": ["Guoqing Liu", "Junren Li", "Zihan Zhao", "Eray Inanc", "Krzysztof Maziarz", "Jose Garrido Torres", "Victor Garcia Satorras", "Shoko Ueda", "Christopher M. Bishop", "Marwin Segler"], "title": "A Scientific Reasoning Model for Organic Synthesis Procedure Generation", "comment": null, "summary": "Solving computer-aided synthesis planning is essential for enabling fully automated, robot-assisted synthesis workflows and improving the efficiency of drug discovery. A key challenge, however, is bridging the gap between computational route design and practical laboratory execution, particularly the accurate prediction of viable experimental procedures for each synthesis step. In this work, we present QFANG, a scientific reasoning language model capable of generating precise, structured experimental procedures directly from reaction equations, with explicit chain-of-thought reasoning. To develop QFANG, we curated a high-quality dataset comprising 905,990 chemical reactions paired with structured action sequences, extracted and processed from patent literature using large language models. We introduce a Chemistry-Guided Reasoning (CGR) framework that produces chain-of-thought data grounded in chemical knowledge at scale. The model subsequently undergoes supervised fine-tuning to elicit complex chemistry reasoning. Finally, we apply Reinforcement Learning from Verifiable Rewards (RLVR) to further enhance procedural accuracy. Experimental results demonstrate that QFANG outperforms advanced general-purpose reasoning models and nearest-neighbor retrieval baselines, measured by traditional NLP similarity metrics and a chemically aware evaluator using an LLM-as-a-judge. Moreover, QFANG generalizes to certain out-of-domain reaction classes and adapts to variations in laboratory conditions and user-specific constraints. We believe that QFANG's ability to generate high-quality synthesis procedures represents an important step toward bridging the gap between computational synthesis planning and fully automated laboratory synthesis.", "AI": {"tldr": "QFANG\u662f\u4e00\u4e2a\u79d1\u5b66\u63a8\u7406\u8bed\u8a00\u6a21\u578b\uff0c\u80fd\u591f\u4ece\u5316\u5b66\u53cd\u5e94\u65b9\u7a0b\u5f0f\u76f4\u63a5\u751f\u6210\u7cbe\u786e\u7684\u7ed3\u6784\u5316\u5b9e\u9a8c\u7a0b\u5e8f\uff0c\u901a\u8fc7\u5316\u5b66\u5f15\u5bfc\u63a8\u7406\u6846\u67b6\u548c\u5f3a\u5316\u5b66\u4e60\u63d0\u9ad8\u7a0b\u5e8f\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u8ba1\u7b97\u673a\u8f85\u52a9\u5408\u6210\u89c4\u5212\u7684\u5173\u952e\u6311\u6218\u662f\u5f25\u5408\u8ba1\u7b97\u8def\u7ebf\u8bbe\u8ba1\u4e0e\u5b9e\u9645\u5b9e\u9a8c\u5ba4\u6267\u884c\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u51c6\u786e\u9884\u6d4b\u6bcf\u4e2a\u5408\u6210\u6b65\u9aa4\u7684\u53ef\u884c\u5b9e\u9a8c\u7a0b\u5e8f\u3002", "method": "1) \u6784\u5efa\u5305\u542b905,990\u4e2a\u5316\u5b66\u53cd\u5e94\u4e0e\u7ed3\u6784\u5316\u52a8\u4f5c\u5e8f\u5217\u7684\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff1b2) \u5f15\u5165\u5316\u5b66\u5f15\u5bfc\u63a8\u7406\u6846\u67b6\u751f\u6210\u57fa\u4e8e\u5316\u5b66\u77e5\u8bc6\u7684\u94fe\u5f0f\u601d\u7ef4\u6570\u636e\uff1b3) \u901a\u8fc7\u76d1\u7763\u5fae\u8c03\u6fc0\u53d1\u590d\u6742\u5316\u5b66\u63a8\u7406\uff1b4) \u5e94\u7528\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\u8fdb\u4e00\u6b65\u63d0\u9ad8\u7a0b\u5e8f\u51c6\u786e\u6027\u3002", "result": "QFANG\u5728\u4f20\u7edfNLP\u76f8\u4f3c\u6027\u6307\u6807\u548c\u5316\u5b66\u611f\u77e5\u8bc4\u4f30\u5668\u4e0a\u5747\u4f18\u4e8e\u5148\u8fdb\u7684\u901a\u7528\u63a8\u7406\u6a21\u578b\u548c\u6700\u8fd1\u90bb\u68c0\u7d22\u57fa\u7ebf\uff0c\u80fd\u591f\u6cdb\u5316\u5230\u67d0\u4e9b\u57df\u5916\u53cd\u5e94\u7c7b\u522b\uff0c\u5e76\u9002\u5e94\u5b9e\u9a8c\u5ba4\u6761\u4ef6\u548c\u7528\u6237\u7279\u5b9a\u7ea6\u675f\u7684\u53d8\u5316\u3002", "conclusion": "QFANG\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u7a0b\u5e8f\u7684\u80fd\u529b\u4ee3\u8868\u4e86\u5411\u5f25\u5408\u8ba1\u7b97\u5408\u6210\u89c4\u5212\u4e0e\u5168\u81ea\u52a8\u5316\u5b9e\u9a8c\u5ba4\u5408\u6210\u4e4b\u95f4\u5dee\u8ddd\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2512.13317", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13317", "abs": "https://arxiv.org/abs/2512.13317", "authors": ["Mikhail Zakharov"], "title": "Face Identity Unlearning for Retrieval via Embedding Dispersion", "comment": "12 pages, 1 figure, 5 tables, 10 equations. Preprint", "summary": "Face recognition systems rely on learning highly discriminative and compact identity clusters to enable accurate retrieval. However, as with other surveillance-oriented technologies, such systems raise serious privacy concerns due to their potential for unauthorized identity tracking. While several works have explored machine unlearning as a means of privacy protection, their applicability to face retrieval - especially for modern embedding-based recognition models - remains largely unexplored. In this work, we study the problem of face identity unlearning for retrieval systems and present its inherent challenges. The goal is to make selected identities unretrievable by dispersing their embeddings on the hypersphere and preventing the formation of compact identity clusters that enable re-identification in the gallery. The primary challenge is to achieve this forgetting effect while preserving the discriminative structure of the embedding space and the retrieval performance of the model for the remaining identities. To address this, we evaluate several existing approximate class unlearning methods (e.g., Random Labeling, Gradient Ascent, Boundary Unlearning, and other recent approaches) in the context of face retrieval and propose a simple yet effective dispersion-based unlearning approach. Extensive experiments on standard benchmarks (VGGFace2, CelebA) demonstrate that our method achieves superior forgetting behavior while preserving retrieval utility.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4eba\u8138\u68c0\u7d22\u7cfb\u7edf\u4e2d\u7684\u8eab\u4efd\u9057\u5fd8\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u6563\u7684\u9057\u5fd8\u65b9\u6cd5\uff0c\u4f7f\u7279\u5b9a\u8eab\u4efd\u65e0\u6cd5\u88ab\u68c0\u7d22\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u4ed6\u8eab\u4efd\u7684\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u4eba\u8138\u8bc6\u522b\u7cfb\u7edf\u867d\u7136\u80fd\u6709\u6548\u8fdb\u884c\u8eab\u4efd\u68c0\u7d22\uff0c\u4f46\u5b58\u5728\u4e25\u91cd\u7684\u9690\u79c1\u98ce\u9669\uff0c\u53ef\u80fd\u88ab\u7528\u4e8e\u672a\u7ecf\u6388\u6743\u7684\u8eab\u4efd\u8ffd\u8e2a\u3002\u73b0\u6709\u673a\u5668\u5b66\u4e60\u9057\u5fd8\u65b9\u6cd5\u5728\u4eba\u8138\u68c0\u7d22\u9886\u57df\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u7279\u522b\u662f\u5728\u73b0\u4ee3\u57fa\u4e8e\u5d4c\u5165\u7684\u8bc6\u522b\u6a21\u578b\u4e2d\u3002", "method": "\u8bc4\u4f30\u4e86\u591a\u79cd\u73b0\u6709\u7684\u8fd1\u4f3c\u7c7b\u522b\u9057\u5fd8\u65b9\u6cd5\uff08\u5982\u968f\u673a\u6807\u8bb0\u3001\u68af\u5ea6\u4e0a\u5347\u3001\u8fb9\u754c\u9057\u5fd8\u7b49\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u57fa\u4e8e\u5206\u6563\u7684\u9057\u5fd8\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5728\u8d85\u7403\u9762\u4e0a\u5206\u6563\u76ee\u6807\u8eab\u4efd\u7684\u5d4c\u5165\uff0c\u9632\u6b62\u5f62\u6210\u7d27\u51d1\u7684\u8eab\u4efd\u805a\u7c7b\uff0c\u4ece\u800c\u5b9e\u73b0\u9057\u5fd8\u6548\u679c\u3002", "result": "\u5728\u6807\u51c6\u57fa\u51c6\u6570\u636e\u96c6\uff08VGGFace2\u3001CelebA\uff09\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u9057\u5fd8\u6548\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u68c0\u7d22\u6548\u7528\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4eba\u8138\u68c0\u7d22\u7cfb\u7edf\u7684\u9690\u79c1\u4fdd\u62a4\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5206\u6563\u5d4c\u5165\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7279\u5b9a\u8eab\u4efd\u7684\u9057\u5fd8\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5d4c\u5165\u7a7a\u95f4\u7684\u5224\u522b\u7ed3\u6784\u548c\u5269\u4f59\u8eab\u4efd\u7684\u68c0\u7d22\u6027\u80fd\u3002"}}
{"id": "2512.13672", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13672", "abs": "https://arxiv.org/abs/2512.13672", "authors": ["Kunhee Kim", "NaHyeon Park", "Kibeom Hong", "Hyunjung Shim"], "title": "Directional Textual Inversion for Personalized Text-to-Image Generation", "comment": "Project page: https://kunheek.github.io/dti", "summary": "Textual Inversion (TI) is an efficient approach to text-to-image personalization but often fails on complex prompts. We trace these failures to embedding norm inflation: learned tokens drift to out-of-distribution magnitudes, degrading prompt conditioning in pre-norm Transformers. Empirically, we show semantics are primarily encoded by direction in CLIP token space, while inflated norms harm contextualization; theoretically, we analyze how large magnitudes attenuate positional information and hinder residual updates in pre-norm blocks. We propose Directional Textual Inversion (DTI), which fixes the embedding magnitude to an in-distribution scale and optimizes only direction on the unit hypersphere via Riemannian SGD. We cast direction learning as MAP with a von Mises-Fisher prior, yielding a constant-direction prior gradient that is simple and efficient to incorporate. Across personalization tasks, DTI improves text fidelity over TI and TI-variants while maintaining subject similarity. Crucially, DTI's hyperspherical parameterization enables smooth, semantically coherent interpolation between learned concepts (slerp), a capability that is absent in standard TI. Our findings suggest that direction-only optimization is a robust and scalable path for prompt-faithful personalization.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u65b9\u5411\u6027\u6587\u672c\u53cd\u8f6c\uff08DTI\uff09\uff0c\u901a\u8fc7\u56fa\u5b9a\u5d4c\u5165\u5411\u91cf\u7684\u6a21\u957f\u5e76\u4ec5\u5728\u5355\u4f4d\u8d85\u7403\u9762\u4e0a\u4f18\u5316\u65b9\u5411\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6587\u672c\u53cd\u8f6c\u4e2d\u5d4c\u5165\u5411\u91cf\u6a21\u957f\u81a8\u80c0\u5bfc\u81f4\u590d\u6742\u63d0\u793a\u5931\u8d25\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u6587\u672c\u53cd\u8f6c\uff08TI\uff09\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u63d0\u793a\u65f6\u7ecf\u5e38\u5931\u8d25\uff0c\u4f5c\u8005\u53d1\u73b0\u8fd9\u662f\u7531\u4e8e\u5d4c\u5165\u5411\u91cf\u7684\u6a21\u957f\u81a8\u80c0\u5bfc\u81f4\u7684\u3002\u8fd9\u79cd\u6a21\u957f\u81a8\u80c0\u4f1a\u4f7f\u5b66\u4e60\u5230\u7684token\u504f\u79bb\u5206\u5e03\u8303\u56f4\uff0c\u4ece\u800c\u5728\u9884\u5f52\u4e00\u5316Transformer\u4e2d\u964d\u4f4e\u63d0\u793a\u6761\u4ef6\u7684\u6548\u679c\u3002", "method": "\u63d0\u51fa\u65b9\u5411\u6027\u6587\u672c\u53cd\u8f6c\uff08DTI\uff09\uff0c\u56fa\u5b9a\u5d4c\u5165\u5411\u91cf\u7684\u6a21\u957f\u4e3a\u5206\u5e03\u5185\u5c3a\u5ea6\uff0c\u4ec5\u901a\u8fc7\u9ece\u66fcSGD\u5728\u5355\u4f4d\u8d85\u7403\u9762\u4e0a\u4f18\u5316\u65b9\u5411\u3002\u5c06\u65b9\u5411\u5b66\u4e60\u5efa\u6a21\u4e3a\u5177\u6709von Mises-Fisher\u5148\u9a8c\u7684\u6700\u5927\u540e\u9a8c\u6982\u7387\u4f30\u8ba1\uff0c\u4ea7\u751f\u7b80\u5355\u9ad8\u6548\u7684\u6052\u5b9a\u65b9\u5411\u5148\u9a8c\u68af\u5ea6\u3002", "result": "\u5728\u4e2a\u6027\u5316\u4efb\u52a1\u4e2d\uff0cDTI\u5728\u4fdd\u6301\u4e3b\u4f53\u76f8\u4f3c\u6027\u7684\u540c\u65f6\uff0c\u6bd4TI\u53ca\u5176\u53d8\u4f53\u663e\u8457\u63d0\u9ad8\u4e86\u6587\u672c\u4fdd\u771f\u5ea6\u3002DTI\u7684\u8d85\u7403\u9762\u53c2\u6570\u5316\u8fd8\u5b9e\u73b0\u4e86\u5b66\u4e60\u6982\u5ff5\u4e4b\u95f4\u7684\u5e73\u6ed1\u3001\u8bed\u4e49\u8fde\u8d2f\u63d2\u503c\uff08\u7403\u9762\u7ebf\u6027\u63d2\u503c\uff09\uff0c\u8fd9\u662f\u6807\u51c6TI\u6240\u4e0d\u5177\u5907\u7684\u80fd\u529b\u3002", "conclusion": "\u4ec5\u4f18\u5316\u65b9\u5411\u662f\u5b9e\u73b0\u63d0\u793a\u5fe0\u5b9e\u4e2a\u6027\u5316\u7684\u9c81\u68d2\u4e14\u53ef\u6269\u5c55\u7684\u8def\u5f84\u3002DTI\u901a\u8fc7\u89e3\u51b3\u5d4c\u5165\u5411\u91cf\u6a21\u957f\u81a8\u80c0\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230\u56fe\u50cf\u4e2a\u6027\u5316\u7684\u6548\u679c\u3002"}}
{"id": "2511.16619", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.16619", "abs": "https://arxiv.org/abs/2511.16619", "authors": ["Satyam Gaba"], "title": "Improving Long-Tailed Object Detection with Balanced Group Softmax and Metric Learning", "comment": "8 pages, 7 figures, International Conference on Semantic Computing", "summary": "Object detection has been widely explored for class-balanced datasets such as COCO. However, real-world scenarios introduce the challenge of long-tailed distributions, where numerous categories contain only a few instances. This inherent class imbalance biases detection models towards the more frequent classes, degrading performance on rare categories. In this paper, we tackle the problem of long-tailed 2D object detection using the LVISv1 dataset, which consists of 1,203 categories and 164,000 images. We employ a two-stage Faster R-CNN architecture and propose enhancements to the Balanced Group Softmax (BAGS) framework to mitigate class imbalance. Our approach achieves a new state-of-the-art performance with a mean Average Precision (mAP) of 24.5%, surpassing the previous benchmark of 24.0%.\n  Additionally, we hypothesize that tail class features may form smaller, denser clusters within the feature space of head classes, making classification challenging for regression-based classifiers. To address this issue, we explore metric learning to produce feature embeddings that are both well-separated across classes and tightly clustered within each class. For inference, we utilize a k-Nearest Neighbors (k-NN) approach to improve classification performance, particularly for rare classes. Our results demonstrate the effectiveness of these methods in advancing long-tailed object detection.", "AI": {"tldr": "\u8be5\u8bba\u6587\u9488\u5bf9\u957f\u5c3e\u5206\u5e03\u4e0b\u76842D\u76ee\u6807\u68c0\u6d4b\u95ee\u9898\uff0c\u5728LVISv1\u6570\u636e\u96c6\u4e0a\u901a\u8fc7\u6539\u8fdb\u7684\u5e73\u8861\u7ec4Softmax\u6846\u67b6\u548c\u5ea6\u91cf\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e8624.5%\u7684mAP\uff0c\u5237\u65b0\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\u9762\u4e34\u957f\u5c3e\u5206\u5e03\u6311\u6218\uff0c\u5373\u5927\u91cf\u7c7b\u522b\u53ea\u6709\u5c11\u6570\u5b9e\u4f8b\uff0c\u5bfc\u81f4\u68c0\u6d4b\u6a21\u578b\u504f\u5411\u9891\u7e41\u7c7b\u522b\uff0c\u5728\u7a00\u6709\u7c7b\u522b\u4e0a\u6027\u80fd\u4e0b\u964d\u3002\u9700\u8981\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u4ee5\u63d0\u5347\u6574\u4f53\u68c0\u6d4b\u6027\u80fd\u3002", "method": "1. \u91c7\u7528\u4e24\u9636\u6bb5Faster R-CNN\u67b6\u6784\uff1b2. \u6539\u8fdb\u5e73\u8861\u7ec4Softmax\uff08BAGS\uff09\u6846\u67b6\u7f13\u89e3\u7c7b\u522b\u4e0d\u5e73\u8861\uff1b3. \u63a2\u7d22\u5ea6\u91cf\u5b66\u4e60\u751f\u6210\u7c7b\u522b\u95f4\u5206\u79bb\u3001\u7c7b\u5185\u7d27\u5bc6\u7684\u7279\u5f81\u5d4c\u5165\uff1b4. \u63a8\u7406\u65f6\u4f7f\u7528k\u8fd1\u90bb\u65b9\u6cd5\u63d0\u5347\u5206\u7c7b\u6027\u80fd\uff0c\u7279\u522b\u662f\u7a00\u6709\u7c7b\u522b\u3002", "result": "\u5728LVISv1\u6570\u636e\u96c6\uff081,203\u4e2a\u7c7b\u522b\uff0c164,000\u5f20\u56fe\u50cf\uff09\u4e0a\u5b9e\u73b0\u4e8624.5%\u7684\u5e73\u5747\u7cbe\u5ea6\u5747\u503c\uff08mAP\uff09\uff0c\u8d85\u8d8a\u4e86\u4e4b\u524d\u768424.0%\u57fa\u51c6\uff0c\u521b\u9020\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u6539\u8fdbBAGS\u6846\u67b6\u548c\u5ea6\u91cf\u5b66\u4e60\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86\u957f\u5c3e\u5206\u5e03\u4e0b\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u7a00\u6709\u7c7b\u522b\u4e0a\uff0c\u4e3a\u957f\u5c3e\u76ee\u6807\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13376", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13376", "abs": "https://arxiv.org/abs/2512.13376", "authors": ["Carla Monteiro", "Valentina Corbetta", "Regina Beets-Tan", "Lu\u00eds F. Teixeira", "Wilson Silva"], "title": "Unlocking Generalization in Polyp Segmentation with DINO Self-Attention \"keys\"", "comment": "29 pages, 10 figures, 8 tables, under review at MIDL 2026", "summary": "Automatic polyp segmentation is crucial for improving the clinical identification of colorectal cancer (CRC). While Deep Learning (DL) techniques have been extensively researched for this problem, current methods frequently struggle with generalization, particularly in data-constrained or challenging settings. Moreover, many existing polyp segmentation methods rely on complex, task-specific architectures. To address these limitations, we present a framework that leverages the intrinsic robustness of DINO self-attention \"key\" features for robust segmentation. Unlike traditional methods that extract tokens from the deepest layers of the Vision Transformer (ViT), our approach leverages the key features of the self-attention module with a simple convolutional decoder to predict polyp masks, resulting in enhanced performance and better generalizability. We validate our approach using a multi-center dataset under two rigorous protocols: Domain Generalization (DG) and Extreme Single Domain Generalization (ESDG). Our results, supported by a comprehensive statistical analysis, demonstrate that this pipeline achieves state-of-the-art (SOTA) performance, significantly enhancing generalization, particularly in data-scarce and challenging scenarios. While avoiding a polyp-specific architecture, we surpass well-established models like nnU-Net and UM-Net. Additionally, we provide a systematic benchmark of the DINO framework's evolution, quantifying the specific impact of architectural advancements on downstream polyp segmentation performance.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528DINO\u81ea\u6ce8\u610f\u529b\"key\"\u7279\u5f81\u8fdb\u884c\u606f\u8089\u5206\u5272\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7b80\u5355\u5377\u79ef\u89e3\u7801\u5668\u5b9e\u73b0\uff0c\u5728\u9886\u57df\u6cdb\u5316\u548c\u6781\u7aef\u5355\u9886\u57df\u6cdb\u5316\u573a\u666f\u4e0b\u53d6\u5f97SOTA\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u606f\u8089\u5206\u5272\u65b9\u6cd5\u5b58\u5728\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u53d7\u9650\u6216\u6311\u6218\u6027\u573a\u666f\u4e0b\uff0c\u4e14\u8bb8\u591a\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u7684\u4efb\u52a1\u7279\u5b9a\u67b6\u6784\u3002", "method": "\u5229\u7528DINO\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u7684\"key\"\u7279\u5f81\uff0c\u7ed3\u5408\u7b80\u5355\u5377\u79ef\u89e3\u7801\u5668\u9884\u6d4b\u606f\u8089\u63a9\u7801\uff0c\u907f\u514d\u4eceViT\u6700\u6df1\u5c42\u63d0\u53d6token\u7684\u4f20\u7edf\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u4e2d\u5fc3\u6570\u636e\u96c6\u4e0a\u901a\u8fc7\u9886\u57df\u6cdb\u5316\u548c\u6781\u7aef\u5355\u9886\u57df\u6cdb\u5316\u534f\u8bae\u9a8c\u8bc1\uff0c\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u7a00\u7f3a\u548c\u6311\u6218\u6027\u573a\u666f\u4e0b\uff0c\u8d85\u8d8a\u4e86nnU-Net\u548cUM-Net\u7b49\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u57fa\u4e8eDINO\u81ea\u6ce8\u610f\u529bkey\u7279\u5f81\u7684\u6846\u67b6\u5728\u907f\u514d\u606f\u8089\u7279\u5b9a\u67b6\u6784\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6cdb\u5316\u6027\u80fd\u548c\u5206\u5272\u6548\u679c\uff0c\u4e3a\u606f\u8089\u5206\u5272\u63d0\u4f9b\u4e86\u66f4\u7a33\u5065\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13397", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.13397", "abs": "https://arxiv.org/abs/2512.13397", "authors": ["Malte Silbernagel", "Albert Alonso", "Jens Petersen", "Bulat Ibragimov", "Marleen de Bruijne", "Madeleine K. Wyburd"], "title": "rNCA: Self-Repairing Segmentation Masks", "comment": null, "summary": "Accurately predicting topologically correct masks remains a difficult task for general segmentation models, which often produce fragmented or disconnected outputs. Fixing these artifacts typically requires hand-crafted refinement rules or architectures specialized to a particular task. Here, we show that Neural Cellular Automata (NCA) can be directly re-purposed as an effective refinement mechanism, using local, iterative updates guided by image context to repair segmentation masks. By training on imperfect masks and ground truths, the automaton learns the structural properties of the target shape while relying solely on local information. When applied to coarse, globally predicted masks, the learned dynamics progressively reconnect broken regions, prune loose fragments and converge towards stable, topologically consistent results. We show how refinement NCA (rNCA) can be easily applied to repair common topological errors produced by different base segmentation models and tasks: for fragmented retinal vessels, it yields 2-3% gains in Dice/clDice and improves Betti errors, reducing $\u03b2_0$ errors by 60% and $\u03b2_1$ by 20%; for myocardium, it repairs 61.5% of broken cases in a zero-shot setting while lowering ASSD and HD by 19% and 16%, respectively. This showcases NCA as effective and broadly applicable refiners.", "AI": {"tldr": "\u795e\u7ecf\u7ec6\u80de\u81ea\u52a8\u673a\uff08NCA\uff09\u88ab\u91cd\u65b0\u7528\u4f5c\u6709\u6548\u7684\u5206\u5272\u63a9\u7801\u7ec6\u5316\u673a\u5236\uff0c\u901a\u8fc7\u5c40\u90e8\u8fed\u4ee3\u66f4\u65b0\u4fee\u590d\u62d3\u6251\u9519\u8bef\uff0c\u63d0\u5347\u5206\u5272\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u901a\u7528\u5206\u5272\u6a21\u578b\u7ecf\u5e38\u4ea7\u751f\u788e\u7247\u5316\u6216\u4e0d\u8fde\u8d2f\u7684\u63a9\u7801\u8f93\u51fa\uff0c\u4fee\u590d\u8fd9\u4e9b\u4f2a\u5f71\u901a\u5e38\u9700\u8981\u624b\u5de5\u8bbe\u8ba1\u7684\u7ec6\u5316\u89c4\u5219\u6216\u9488\u5bf9\u7279\u5b9a\u4efb\u52a1\u7684\u4e13\u95e8\u67b6\u6784\u3002\u9700\u8981\u4e00\u79cd\u901a\u7528\u4e14\u6709\u6548\u7684\u7ec6\u5316\u673a\u5236\u6765\u6539\u5584\u5206\u5272\u63a9\u7801\u7684\u62d3\u6251\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51fa\u7ec6\u5316\u795e\u7ecf\u7ec6\u80de\u81ea\u52a8\u673a\uff08rNCA\uff09\uff0c\u5c06\u5176\u91cd\u65b0\u7528\u4f5c\u7ec6\u5316\u673a\u5236\u3002\u901a\u8fc7\u5728\u4e0d\u5b8c\u7f8e\u63a9\u7801\u548c\u771f\u5b9e\u6807\u6ce8\u4e0a\u8bad\u7ec3\uff0c\u81ea\u52a8\u673a\u5b66\u4e60\u76ee\u6807\u5f62\u72b6\u7684\u7ed3\u6784\u7279\u6027\uff0c\u4ec5\u4f9d\u8d56\u5c40\u90e8\u4fe1\u606f\u3002\u5f53\u5e94\u7528\u4e8e\u7c97\u7cd9\u7684\u5168\u5c40\u9884\u6d4b\u63a9\u7801\u65f6\uff0c\u5b66\u4e60\u5230\u7684\u52a8\u6001\u8fc7\u7a0b\u9010\u6b65\u91cd\u65b0\u8fde\u63a5\u65ad\u88c2\u533a\u57df\u3001\u4fee\u526a\u677e\u6563\u788e\u7247\uff0c\u5e76\u6536\u655b\u5230\u7a33\u5b9a\u3001\u62d3\u6251\u4e00\u81f4\u7684\u7ed3\u679c\u3002", "result": "\u5728\u89c6\u7f51\u819c\u8840\u7ba1\u5206\u5272\u4e2d\uff0cDice/clDice\u6307\u6807\u63d0\u53472-3%\uff0cBetti\u9519\u8bef\u51cf\u5c11\uff08\u03b20\u9519\u8bef\u51cf\u5c1160%\uff0c\u03b21\u51cf\u5c1120%\uff09\uff1b\u5728\u5fc3\u808c\u5206\u5272\u4e2d\uff0c\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u4fee\u590d\u4e8661.5%\u7684\u65ad\u88c2\u6848\u4f8b\uff0cASSD\u548cHD\u5206\u522b\u964d\u4f4e19%\u548c16%\u3002", "conclusion": "\u795e\u7ecf\u7ec6\u80de\u81ea\u52a8\u673a\uff08NCA\uff09\u53ef\u4ee5\u4f5c\u4e3a\u6709\u6548\u4e14\u5e7f\u6cdb\u9002\u7528\u7684\u5206\u5272\u7ec6\u5316\u5668\uff0c\u901a\u8fc7\u5c40\u90e8\u8fed\u4ee3\u66f4\u65b0\u4fee\u590d\u5e38\u89c1\u62d3\u6251\u9519\u8bef\uff0c\u663e\u8457\u6539\u5584\u5206\u5272\u63a9\u7801\u7684\u62d3\u6251\u4e00\u81f4\u6027\u3002"}}
{"id": "2512.13416", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13416", "abs": "https://arxiv.org/abs/2512.13416", "authors": ["Haoxuan Qu", "Qiuchi Xiang", "Yujun Cai", "Yirui Wu", "Majid Mirmehdi", "Hossein Rahmani", "Jun Liu"], "title": "Learning to Generate Cross-Task Unexploitable Examples", "comment": null, "summary": "Unexploitable example generation aims to transform personal images into their unexploitable (unlearnable) versions before they are uploaded online, thereby preventing unauthorized exploitation of online personal images. Recently, this task has garnered significant research attention due to its critical relevance to personal data privacy. Yet, despite recent progress, existing methods for this task can still suffer from limited practical applicability, as they can fail to generate examples that are broadly unexploitable across different real-world computer vision tasks. To deal with this problem, in this work, we propose a novel Meta Cross-Task Unexploitable Example Generation (MCT-UEG) framework. At the core of our framework, to optimize the unexploitable example generator for effectively producing broadly unexploitable examples, we design a flat-minima-oriented meta training and testing scheme. Extensive experiments show the efficacy of our framework.", "AI": {"tldr": "\u63d0\u51faMCT-UEG\u6846\u67b6\uff0c\u901a\u8fc7\u5143\u8de8\u4efb\u52a1\u8bad\u7ec3\u751f\u6210\u5e7f\u6cdb\u4e0d\u53ef\u5229\u7528\u7684\u4e2a\u4eba\u56fe\u50cf\uff0c\u9632\u6b62\u5728\u7ebf\u56fe\u50cf\u88ab\u672a\u7ecf\u6388\u6743\u5229\u7528", "motivation": "\u73b0\u6709\u4e0d\u53ef\u5229\u7528\u793a\u4f8b\u751f\u6210\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u751f\u6210\u8de8\u4e0d\u540c\u771f\u5b9e\u4e16\u754c\u8ba1\u7b97\u673a\u89c6\u89c9\u4efb\u52a1\u7684\u5e7f\u6cdb\u4e0d\u53ef\u5229\u7528\u793a\u4f8b\uff0c\u9650\u5236\u4e86\u65b9\u6cd5\u7684\u5b9e\u9645\u9002\u7528\u6027", "method": "\u63d0\u51faMeta Cross-Task Unexploitable Example Generation (MCT-UEG)\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u8bbe\u8ba1\u9762\u5411\u5e73\u5766\u6700\u5c0f\u503c\u7684\u5143\u8bad\u7ec3\u548c\u6d4b\u8bd5\u65b9\u6848\uff0c\u4f18\u5316\u4e0d\u53ef\u5229\u7528\u793a\u4f8b\u751f\u6210\u5668\u4ee5\u6709\u6548\u4ea7\u751f\u5e7f\u6cdb\u4e0d\u53ef\u5229\u7528\u7684\u793a\u4f8b", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027", "conclusion": "MCT-UEG\u6846\u67b6\u80fd\u591f\u751f\u6210\u8de8\u4efb\u52a1\u7684\u5e7f\u6cdb\u4e0d\u53ef\u5229\u7528\u793a\u4f8b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5c40\u9650\u6027\u95ee\u9898"}}
{"id": "2512.13678", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.13678", "abs": "https://arxiv.org/abs/2512.13678", "authors": ["Ziqi Ma", "Hongqiao Chen", "Yisong Yue", "Georgia Gkioxari"], "title": "Feedforward 3D Editing via Text-Steerable Image-to-3D", "comment": "https://glab-caltech.github.io/steer3d/", "summary": "Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and robotics. However, to use AI-generated 3D assets in real applications, a critical requirement is the capability to edit them easily. We present a feedforward method, Steer3D, to add text steerability to image-to-3D models, which enables editing of generated 3D assets with language. Our approach is inspired by ControlNet, which we adapt to image-to-3D generation to enable text steering directly in a forward pass. We build a scalable data engine for automatic data generation, and develop a two-stage training recipe based on flow-matching training and Direct Preference Optimization (DPO). Compared to competing methods, Steer3D more faithfully follows the language instruction and maintains better consistency with the original 3D asset, while being 2.4x to 28.5x faster. Steer3D demonstrates that it is possible to add a new modality (text) to steer the generation of pretrained image-to-3D generative models with 100k data. Project website: https://glab-caltech.github.io/steer3d/", "AI": {"tldr": "Steer3D\uff1a\u4e00\u79cd\u4e3a\u56fe\u50cf\u52303D\u6a21\u578b\u6dfb\u52a0\u6587\u672c\u53ef\u63a7\u6027\u7684\u524d\u9988\u65b9\u6cd5\uff0c\u652f\u6301\u7528\u8bed\u8a00\u7f16\u8f91\u751f\u6210\u76843D\u8d44\u4ea7", "motivation": "\u867d\u7136\u56fe\u50cf\u52303D\u751f\u6210\u6280\u672f\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u4f46\u8981\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u4f7f\u7528AI\u751f\u6210\u76843D\u8d44\u4ea7\uff0c\u5173\u952e\u9700\u6c42\u662f\u80fd\u591f\u8f7b\u677e\u7f16\u8f91\u8fd9\u4e9b\u8d44\u4ea7\u3002\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u751f\u62103D\u8d44\u4ea7\u7684\u6587\u672c\u5f15\u5bfc\u7f16\u8f91\u80fd\u529b\u3002", "method": "\u53d7ControlNet\u542f\u53d1\uff0c\u5c06\u6587\u672c\u53ef\u63a7\u6027\u9002\u914d\u5230\u56fe\u50cf\u52303D\u751f\u6210\u4e2d\uff1b\u6784\u5efa\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u6570\u636e\u751f\u6210\u5f15\u64ce\uff1b\u91c7\u7528\u57fa\u4e8e\u6d41\u5339\u914d\u8bad\u7ec3\u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u65b9\u6848", "result": "Steer3D\u76f8\u6bd4\u7ade\u4e89\u65b9\u6cd5\u66f4\u5fe0\u5b9e\u5730\u9075\u5faa\u8bed\u8a00\u6307\u4ee4\uff0c\u4e0e\u539f\u59cb3D\u8d44\u4ea7\u4fdd\u6301\u66f4\u597d\u7684\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u901f\u5ea6\u63d0\u53472.4\u500d\u523028.5\u500d", "conclusion": "Steer3D\u8bc1\u660e\u53ef\u4ee5\u901a\u8fc710\u4e07\u6570\u636e\u4e3a\u9884\u8bad\u7ec3\u7684\u56fe\u50cf\u52303D\u751f\u6210\u6a21\u578b\u6dfb\u52a0\u65b0\u7684\u6a21\u6001\uff08\u6587\u672c\uff09\u6765\u5f15\u5bfc\u751f\u6210\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6587\u672c\u53ef\u63a73D\u7f16\u8f91"}}
{"id": "2512.13421", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13421", "abs": "https://arxiv.org/abs/2512.13421", "authors": ["Qingyu Shi", "Size Wu", "Jinbin Bai", "Kaidong Yu", "Yujing Wang", "Yunhai Tong", "Xiangtai Li", "Xuelong Li"], "title": "RecTok: Reconstruction Distillation along Rectified Flow", "comment": null, "summary": "Visual tokenizers play a crucial role in diffusion models. The dimensionality of latent space governs both reconstruction fidelity and the semantic expressiveness of the latent feature. However, a fundamental trade-off is inherent between dimensionality and generation quality, constraining existing methods to low-dimensional latent spaces. Although recent works have leveraged vision foundation models to enrich the semantics of visual tokenizers and accelerate convergence, high-dimensional tokenizers still underperform their low-dimensional counterparts. In this work, we propose RecTok, which overcomes the limitations of high-dimensional visual tokenizers through two key innovations: flow semantic distillation and reconstruction--alignment distillation. Our key insight is to make the forward flow in flow matching semantically rich, which serves as the training space of diffusion transformers, rather than focusing on the latent space as in previous works. Specifically, our method distills the semantic information in VFMs into the forward flow trajectories in flow matching. And we further enhance the semantics by introducing a masked feature reconstruction loss. Our RecTok achieves superior image reconstruction, generation quality, and discriminative performance. It achieves state-of-the-art results on the gFID-50K under both with and without classifier-free guidance settings, while maintaining a semantically rich latent space structure. Furthermore, as the latent dimensionality increases, we observe consistent improvements. Code and model are available at https://shi-qingyu.github.io/rectok.github.io.", "AI": {"tldr": "RecTok\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89c9tokenizer\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d41\u8bed\u4e49\u84b8\u998f\u548c\u91cd\u5efa\u5bf9\u9f50\u84b8\u998f\u89e3\u51b3\u9ad8\u7ef4\u6f5c\u5728\u7a7a\u95f4\u4e2d\u91cd\u5efa\u8d28\u91cf\u4e0e\u8bed\u4e49\u8868\u8fbe\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u4e30\u5bcc\u8bed\u4e49\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u56fe\u50cf\u91cd\u5efa\u548c\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9tokenizer\u5728\u6f5c\u5728\u7a7a\u95f4\u7ef4\u5ea6\u4e0e\u751f\u6210\u8d28\u91cf\u4e4b\u95f4\u5b58\u5728\u6839\u672c\u6027\u6743\u8861\uff0c\u9ad8\u7ef4tokenizer\u6027\u80fd\u901a\u5e38\u4e0d\u5982\u4f4e\u7ef4\u7248\u672c\u3002\u867d\u7136\u5df2\u6709\u5de5\u4f5c\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u589e\u5f3a\u8bed\u4e49\u5e76\u52a0\u901f\u6536\u655b\uff0c\u4f46\u9ad8\u7ef4tokenizer\u4ecd\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faRecTok\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a1) \u6d41\u8bed\u4e49\u84b8\u998f\uff1a\u5c06\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u8bed\u4e49\u4fe1\u606f\u84b8\u998f\u5230\u6d41\u5339\u914d\u7684\u524d\u5411\u6d41\u8f68\u8ff9\u4e2d\uff0c\u4f7f\u5176\u6210\u4e3a\u6269\u6563\u53d8\u6362\u5668\u7684\u8bad\u7ec3\u7a7a\u95f4\uff1b2) \u91cd\u5efa\u5bf9\u9f50\u84b8\u998f\uff1a\u5f15\u5165\u63a9\u7801\u7279\u5f81\u91cd\u5efa\u635f\u5931\u8fdb\u4e00\u6b65\u589e\u5f3a\u8bed\u4e49\u3002", "result": "RecTok\u5728gFID-50K\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u65e0\u8bba\u662f\u5426\u4f7f\u7528\u5206\u7c7b\u5668\u81ea\u7531\u5f15\u5bfc\uff0c\u90fd\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\u968f\u7740\u6f5c\u5728\u7ef4\u5ea6\u589e\u52a0\uff0c\u6027\u80fd\u6301\u7eed\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8bed\u4e49\u4e30\u5bcc\u7684\u6f5c\u5728\u7a7a\u95f4\u7ed3\u6784\u3002", "conclusion": "RecTok\u901a\u8fc7\u521b\u65b0\u7684\u84b8\u998f\u65b9\u6cd5\u6210\u529f\u514b\u670d\u4e86\u9ad8\u7ef4\u89c6\u89c9tokenizer\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u56fe\u50cf\u91cd\u5efa\u3001\u751f\u6210\u8d28\u91cf\u548c\u5224\u522b\u6027\u80fd\uff0c\u4e3a\u89c6\u89c9tokenizer\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2512.13690", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13690", "abs": "https://arxiv.org/abs/2512.13690", "authors": ["Susung Hong", "Chongjian Ge", "Zhifei Zhang", "Jui-Hsien Wang"], "title": "DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders", "comment": "Project page: https://susunghong.github.io/DiffusionBrowser", "summary": "Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generation -- keeping users in the dark for a prolonged period. In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4$\\times$ real-time speed (less than 1 second for a 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process.", "AI": {"tldr": "DiffusionBrowser\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u8f7b\u91cf\u7ea7\u89e3\u7801\u5668\u6846\u67b6\uff0c\u53ef\u5728\u53bb\u566a\u8fc7\u7a0b\u7684\u4efb\u4f55\u65f6\u95f4\u6b65\u6216Transformer\u5757\u5904\u4ea4\u4e92\u5f0f\u751f\u6210\u9884\u89c8\uff0c\u652f\u6301RGB\u548c\u573a\u666f\u5185\u5728\u8868\u793a\uff0c\u901f\u5ea6\u8d85\u8fc7\u5b9e\u65f64\u500d\uff0c\u5e76\u652f\u6301\u901a\u8fc7\u968f\u673a\u6027\u91cd\u6ce8\u5165\u548c\u6a21\u6001\u5f15\u5bfc\u8fdb\u884c\u4ea4\u4e92\u63a7\u5236\u3002", "motivation": "\u89c6\u9891\u6269\u6563\u6a21\u578b\u867d\u7136\u9769\u65b0\u4e86\u89c6\u9891\u751f\u6210\uff0c\u4f46\u5b58\u5728\u4e0d\u7cbe\u786e\u3001\u901f\u5ea6\u6162\u3001\u751f\u6210\u8fc7\u7a0b\u4e0d\u900f\u660e\u7684\u95ee\u9898\uff0c\u7528\u6237\u5728\u957f\u65f6\u95f4\u5185\u65e0\u6cd5\u4e86\u89e3\u751f\u6210\u8fdb\u5ea6\u3002", "method": "\u63d0\u51faDiffusionBrowser\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u8f7b\u91cf\u7ea7\u89e3\u7801\u5668\uff0c\u53ef\u5728\u53bb\u566a\u8fc7\u7a0b\u7684\u4efb\u4f55\u65f6\u95f4\u6b65\u6216Transformer\u5757\u5904\u751f\u6210\u591a\u6a21\u6001\u9884\u89c8\u8868\u793a\uff08\u5305\u62ecRGB\u548c\u573a\u666f\u5185\u5728\u8868\u793a\uff09\u3002", "result": "\u6a21\u578b\u80fd\u4ee5\u8d85\u8fc7\u5b9e\u65f64\u500d\u7684\u901f\u5ea6\u751f\u6210\u9884\u89c8\uff084\u79d2\u89c6\u9891\u4e0d\u52301\u79d2\uff09\uff0c\u9884\u89c8\u4e0e\u6700\u7ec8\u89c6\u9891\u7684\u5916\u89c2\u548c\u8fd0\u52a8\u4fdd\u6301\u4e00\u81f4\u3002\u901a\u8fc7\u8bad\u7ec3\u7684\u89e3\u7801\u5668\uff0c\u53ef\u4ee5\u5728\u4e2d\u95f4\u566a\u58f0\u6b65\u901a\u8fc7\u968f\u673a\u6027\u91cd\u6ce8\u5165\u548c\u6a21\u6001\u5f15\u5bfc\u8fdb\u884c\u4ea4\u4e92\u5f0f\u63a7\u5236\u3002", "conclusion": "DiffusionBrowser\u4e0d\u4ec5\u63d0\u4f9b\u4e86\u4ea4\u4e92\u5f0f\u9884\u89c8\u548c\u63a7\u5236\u80fd\u529b\uff0c\u8fd8\u901a\u8fc7\u5b66\u4e60\u7684\u89e3\u7801\u5668\u7cfb\u7edf\u6027\u5730\u63a2\u7d22\u4e86\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u539f\u672c\u9ed1\u76d2\u7684\u53bb\u566a\u8fc7\u7a0b\u4e2d\u573a\u666f\u3001\u7269\u4f53\u548c\u5176\u4ed6\u7ec6\u8282\u662f\u5982\u4f55\u7ec4\u5408\u548c\u7ec4\u88c5\u7684\u3002"}}
{"id": "2512.13427", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13427", "abs": "https://arxiv.org/abs/2512.13427", "authors": ["Noa Cohen", "Nurit Spingarn-Eliezer", "Inbar Huberman-Spiegelglas", "Tomer Michaeli"], "title": "MineTheGap: Automatic Mining of Biases in Text-to-Image Models", "comment": "Code and examples are available on the project's webpage at https://noa-cohen.github.io/MineTheGap/", "summary": "Text-to-Image (TTI) models generate images based on text prompts, which often leave certain aspects of the desired image ambiguous. When faced with these ambiguities, TTI models have been shown to exhibit biases in their interpretations. These biases can have societal impacts, e.g., when showing only a certain race for a stated occupation. They can also affect user experience when creating redundancy within a set of generated images instead of spanning diverse possibilities. Here, we introduce MineTheGap - a method for automatically mining prompts that cause a TTI model to generate biased outputs. Our method goes beyond merely detecting bias for a given prompt. Rather, it leverages a genetic algorithm to iteratively refine a pool of prompts, seeking for those that expose biases. This optimization process is driven by a novel bias score, which ranks biases according to their severity, as we validate on a dataset with known biases. For a given prompt, this score is obtained by comparing the distribution of generated images to the distribution of LLM-generated texts that constitute variations on the prompt. Code and examples are available on the project's webpage.", "AI": {"tldr": "MineTheGap\u662f\u4e00\u79cd\u81ea\u52a8\u6316\u6398\u5bfc\u81f4\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u751f\u6210\u504f\u89c1\u8f93\u51fa\u7684\u63d0\u793a\u8bcd\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u9057\u4f20\u7b97\u6cd5\u8fed\u4ee3\u4f18\u5316\u63d0\u793a\u8bcd\u6c60\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u504f\u89c1\u8bc4\u5206\u6765\u8bc6\u522b\u548c\u91cf\u5316\u504f\u89c1\u4e25\u91cd\u7a0b\u5ea6\u3002", "motivation": "\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u5904\u7406\u6a21\u7cca\u6587\u672c\u63d0\u793a\u65f6\u8868\u73b0\u51fa\u504f\u89c1\uff0c\u8fd9\u4e9b\u504f\u89c1\u53ef\u80fd\u4ea7\u751f\u793e\u4f1a\u5f71\u54cd\uff08\u5982\u804c\u4e1a\u4e0e\u79cd\u65cf\u7684\u523b\u677f\u5370\u8c61\uff09\u5e76\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\uff08\u751f\u6210\u5197\u4f59\u800c\u975e\u591a\u6837\u5316\u7684\u56fe\u50cf\uff09\u3002\u73b0\u6709\u65b9\u6cd5\u4ec5\u80fd\u68c0\u6d4b\u7ed9\u5b9a\u63d0\u793a\u7684\u504f\u89c1\uff0c\u800c\u65e0\u6cd5\u4e3b\u52a8\u53d1\u73b0\u5bfc\u81f4\u504f\u89c1\u7684\u63d0\u793a\u3002", "method": "\u63d0\u51faMineTheGap\u65b9\u6cd5\uff1a1\uff09\u4f7f\u7528\u9057\u4f20\u7b97\u6cd5\u8fed\u4ee3\u4f18\u5316\u63d0\u793a\u8bcd\u6c60\uff0c\u5bfb\u627e\u80fd\u66b4\u9732\u504f\u89c1\u7684\u63d0\u793a\uff1b2\uff09\u8bbe\u8ba1\u65b0\u9896\u7684\u504f\u89c1\u8bc4\u5206\u673a\u5236\uff0c\u901a\u8fc7\u6bd4\u8f83\u751f\u6210\u56fe\u50cf\u5206\u5e03\u4e0eLLM\u751f\u6210\u7684\u6587\u672c\u53d8\u4f53\u5206\u5e03\u6765\u91cf\u5316\u504f\u89c1\u4e25\u91cd\u7a0b\u5ea6\uff1b3\uff09\u5728\u5df2\u77e5\u504f\u89c1\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u8bc4\u5206\u6709\u6548\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u81ea\u52a8\u6316\u6398\u5bfc\u81f4TTI\u6a21\u578b\u751f\u6210\u504f\u89c1\u8f93\u51fa\u7684\u63d0\u793a\u8bcd\uff0c\u8d85\u8d8a\u4e86\u4ec5\u68c0\u6d4b\u7ed9\u5b9a\u63d0\u793a\u504f\u89c1\u7684\u80fd\u529b\u3002\u504f\u89c1\u8bc4\u5206\u80fd\u591f\u6839\u636e\u4e25\u91cd\u7a0b\u5ea6\u5bf9\u504f\u89c1\u8fdb\u884c\u6392\u5e8f\uff0c\u5e76\u5728\u5df2\u77e5\u504f\u89c1\u6570\u636e\u96c6\u4e0a\u5f97\u5230\u9a8c\u8bc1\u3002", "conclusion": "MineTheGap\u63d0\u4f9b\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u65b9\u6cd5\u6765\u53d1\u73b0\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u4e2d\u7684\u504f\u89c1\uff0c\u901a\u8fc7\u4e3b\u52a8\u5bfb\u627e\u66b4\u9732\u504f\u89c1\u7684\u63d0\u793a\u8bcd\uff0c\u6709\u52a9\u4e8e\u8bc6\u522b\u548c\u7f13\u89e3\u6a21\u578b\u4e2d\u7684\u793e\u4f1a\u504f\u89c1\u95ee\u9898\uff0c\u63d0\u5347\u751f\u6210\u56fe\u50cf\u7684\u591a\u6837\u6027\u3002"}}
{"id": "2512.13428", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13428", "abs": "https://arxiv.org/abs/2512.13428", "authors": ["Anika Islam", "Tasfia Tahsin", "Zaarin Anjum", "Md. Bakhtiar Hasan", "Md. Hasanul Kabir"], "title": "A Domain-Adapted Lightweight Ensemble for Resource-Efficient Few-Shot Plant Disease Classification", "comment": null, "summary": "Accurate and timely identification of plant leaf diseases is essential for resilient and sustainable agriculture, yet most deep learning approaches rely on large annotated datasets and computationally intensive models that are unsuitable for data-scarce and resource-constrained environments. To address these challenges we present a few-shot learning approach within a lightweight yet efficient framework that combines domain-adapted MobileNetV2 and MobileNetV3 models as feature extractors, along with a feature fusion technique to generate robust feature representation. For the classification task, the fused features are passed through a Bi-LSTM classifier enhanced with attention mechanisms to capture sequential dependencies and focus on the most relevant features, thereby achieving optimal classification performance even in complex, real-world environments with noisy or cluttered backgrounds. The proposed framework was evaluated across multiple experimental setups, including both laboratory-controlled and field-captured datasets. On tomato leaf diseases from the PlantVillage dataset, it consistently improved performance across 1 to 15 shot scenarios, reaching 98.23+-0.33% at 15 shot, closely approaching the 99.98% SOTA benchmark achieved by a Transductive LSTM with attention, while remaining lightweight and mobile-friendly. Under real-world conditions using field images from the Dhan Shomadhan dataset, it maintained robust performance, reaching 69.28+-1.49% at 15-shot and demonstrating strong resilience to complex backgrounds. Notably, it also outperformed the previous SOTA accuracy of 96.0% on six diseases from PlantVillage, achieving 99.72% with only 15-shot learning. With a compact model size of approximately 40 MB and inference complexity of approximately 1.12 GFLOPs, this work establishes a scalable, mobile-ready foundation for precise plant disease diagnostics in data-scarce regions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5c11\u6837\u672c\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408MobileNet\u7279\u5f81\u63d0\u53d6\u5668\u3001\u7279\u5f81\u878d\u5408\u6280\u672f\u548c\u6ce8\u610f\u529b\u589e\u5f3a\u7684Bi-LSTM\u5206\u7c7b\u5668\uff0c\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u690d\u7269\u53f6\u7247\u75c5\u5bb3\u8bc6\u522b\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u5b66\u4e60\u690d\u7269\u75c5\u5bb3\u8bc6\u522b\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u548c\u8ba1\u7b97\u5bc6\u96c6\u578b\u6a21\u578b\uff0c\u4e0d\u9002\u5408\u6570\u636e\u7a00\u7f3a\u548c\u8d44\u6e90\u53d7\u9650\u7684\u519c\u4e1a\u73af\u5883\uff0c\u9700\u8981\u5f00\u53d1\u8f7b\u91cf\u7ea7\u4e14\u9ad8\u6548\u7684\u5c11\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u57df\u9002\u5e94\u7684MobileNetV2\u548cMobileNetV3\u4f5c\u4e3a\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u7ed3\u5408\u7279\u5f81\u878d\u5408\u6280\u672f\u751f\u6210\u9c81\u68d2\u7279\u5f81\u8868\u793a\uff0c\u7136\u540e\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u589e\u5f3a\u7684Bi-LSTM\u5206\u7c7b\u5668\u8fdb\u884c\u5e8f\u5217\u4f9d\u8d56\u5efa\u6a21\u548c\u5173\u952e\u7279\u5f81\u805a\u7126\u3002", "result": "\u5728PlantVillage\u6570\u636e\u96c6\u4e0a\uff0c15-shot\u573a\u666f\u8fbe\u523098.23\u00b10.33%\uff0c\u63a5\u8fd1SOTA\u768499.98%\uff1b\u5728Dhan Shomadhan\u91ce\u5916\u6570\u636e\u96c6\u4e0a\u8fbe\u523069.28\u00b11.49%\uff1b\u6a21\u578b\u4ec5\u7ea640MB\u5927\u5c0f\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u7ea61.12 GFLOPs\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6570\u636e\u7a00\u7f3a\u5730\u533a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u79fb\u52a8\u5c31\u7eea\u7684\u7cbe\u786e\u690d\u7269\u75c5\u5bb3\u8bca\u65ad\u57fa\u7840\uff0c\u5728\u4fdd\u6301\u8f7b\u91cf\u7ea7\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u63a5\u8fd1SOTA\u7684\u6027\u80fd\u3002"}}
{"id": "2512.13440", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13440", "abs": "https://arxiv.org/abs/2512.13440", "authors": ["Thalyssa Baiocco-Rodrigues", "Antoine Olivier", "Reda Belbahri", "Thomas Duboudin", "Pierre-Antoine Bannier", "Benjamin Adjadj", "Katharina Von Loga", "Nathan Noiry", "Maxime Touzot", "Hector Roux de Bezieux"], "title": "IMILIA: interpretable multiple instance learning for inflammation prediction in IBD from H&E whole slide images", "comment": null, "summary": "As the therapeutic target for Inflammatory Bowel Disease (IBD) shifts toward histologic remission, the accurate assessment of microscopic inflammation has become increasingly central for evaluating disease activity and response to treatment. In this work, we introduce IMILIA (Interpretable Multiple Instance Learning for Inflammation Analysis), an end-to-end framework designed for the prediction of inflammation presence in IBD digitized slides stained with hematoxylin and eosin (H&E), followed by the automated computation of markers characterizing tissue regions driving the predictions. IMILIA is composed of an inflammation prediction module, consisting of a Multiple Instance Learning (MIL) model, and an interpretability module, divided in two blocks: HistoPLUS, for cell instance detection, segmentation and classification; and EpiSeg, for epithelium segmentation. IMILIA achieves a cross-validation ROC-AUC of 0.83 on the discovery cohort, and a ROC-AUC of 0.99 and 0.84 on two external validation cohorts. The interpretability module yields biologically consistent insights: tiles with higher predicted scores show increased densities of immune cells (lymphocytes, plasmocytes, neutrophils and eosinophils), whereas lower-scored tiles predominantly contain normal epithelial cells. Notably, these patterns were consistent across all datasets. Code and models to partially replicate the results on the public IBDColEpi dataset can be found at https://github.com/owkin/imilia.", "AI": {"tldr": "IMILIA\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u4f7f\u7528\u591a\u5b9e\u4f8b\u5b66\u4e60\u9884\u6d4bIBD\u7ec4\u7ec7\u5207\u7247\u4e2d\u7684\u708e\u75c7\u5b58\u5728\uff0c\u5e76\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u6a21\u5757\u81ea\u52a8\u8ba1\u7b97\u9a71\u52a8\u9884\u6d4b\u7684\u7ec4\u7ec7\u533a\u57df\u7279\u5f81\u6807\u8bb0\u7269\u3002", "motivation": "\u968f\u7740IBD\u6cbb\u7597\u76ee\u6807\u8f6c\u5411\u7ec4\u7ec7\u5b66\u7f13\u89e3\uff0c\u51c6\u786e\u8bc4\u4f30\u5fae\u89c2\u708e\u75c7\u5bf9\u4e8e\u8bc4\u4f30\u75be\u75c5\u6d3b\u52a8\u6027\u548c\u6cbb\u7597\u53cd\u5e94\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002", "method": "IMILIA\u5305\u542b\u708e\u75c7\u9884\u6d4b\u6a21\u5757\uff08\u57fa\u4e8e\u591a\u5b9e\u4f8b\u5b66\u4e60\u6a21\u578b\uff09\u548c\u53ef\u89e3\u91ca\u6027\u6a21\u5757\uff08\u5305\u62ecHistoPLUS\u7528\u4e8e\u7ec6\u80de\u68c0\u6d4b\u3001\u5206\u5272\u548c\u5206\u7c7b\uff0c\u4ee5\u53caEpiSeg\u7528\u4e8e\u4e0a\u76ae\u5206\u5272\uff09\u3002", "result": "\u5728\u53d1\u73b0\u961f\u5217\u4e2d\u4ea4\u53c9\u9a8c\u8bc1ROC-AUC\u4e3a0.83\uff0c\u5728\u4e24\u4e2a\u5916\u90e8\u9a8c\u8bc1\u961f\u5217\u4e2d\u5206\u522b\u4e3a0.99\u548c0.84\u3002\u53ef\u89e3\u91ca\u6027\u6a21\u5757\u663e\u793a\uff1a\u9ad8\u5206\u533a\u57df\u514d\u75ab\u7ec6\u80de\u5bc6\u5ea6\u589e\u52a0\uff0c\u4f4e\u5206\u533a\u57df\u4e3b\u8981\u4e3a\u6b63\u5e38\u4e0a\u76ae\u7ec6\u80de\u3002", "conclusion": "IMILIA\u80fd\u591f\u51c6\u786e\u9884\u6d4bIBD\u7ec4\u7ec7\u5207\u7247\u4e2d\u7684\u708e\u75c7\u5b58\u5728\uff0c\u5e76\u63d0\u4f9b\u751f\u7269\u5b66\u4e00\u81f4\u7684\u53ef\u89e3\u91ca\u6027\u7ed3\u679c\uff0c\u6709\u52a9\u4e8e\u8bc4\u4f30IBD\u7684\u5fae\u89c2\u708e\u75c7\u3002"}}
{"id": "2512.13454", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13454", "abs": "https://arxiv.org/abs/2512.13454", "authors": ["Arpit Jadon", "Joshua Niemeijer", "Yuki M. Asano"], "title": "Test-Time Modification: Inverse Domain Transformation for Robust Perception", "comment": "Preprint", "summary": "Generative foundation models contain broad visual knowledge and can produce diverse image variations, making them particularly promising for advancing domain generalization tasks. While they can be used for training data augmentation, synthesizing comprehensive target-domain variations remains slow, expensive, and incomplete. We propose an alternative: using diffusion models at test time to map target images back to the source distribution where the downstream model was trained. This approach requires only a source domain description, preserves the task model, and eliminates large-scale synthetic data generation. We demonstrate consistent improvements across segmentation, detection, and classification tasks under challenging environmental shifts in real-to-real domain generalization scenarios with unknown target distributions. Our analysis spans multiple generative and downstream models, including an ensemble variant for enhanced robustness. The method achieves substantial relative gains: 137% on BDD100K-Night, 68% on ImageNet-R, and 62% on DarkZurich.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5728\u6d4b\u8bd5\u65f6\u4f7f\u7528\u6269\u6563\u6a21\u578b\u5c06\u76ee\u6807\u57df\u56fe\u50cf\u6620\u5c04\u56de\u6e90\u57df\u5206\u5e03\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u751f\u6210\uff0c\u5728\u5206\u5272\u3001\u68c0\u6d4b\u548c\u5206\u7c7b\u4efb\u52a1\u4e0a\u5b9e\u73b0\u663e\u8457\u6027\u80fd\u63d0\u5347", "motivation": "\u751f\u6210\u57fa\u7840\u6a21\u578b\u5305\u542b\u5e7f\u6cdb\u7684\u89c6\u89c9\u77e5\u8bc6\u5e76\u80fd\u4ea7\u751f\u591a\u6837\u5316\u7684\u56fe\u50cf\u53d8\u4f53\uff0c\u4f46\u5728\u8bad\u7ec3\u6570\u636e\u589e\u5f3a\u65b9\u9762\u5b58\u5728\u5408\u6210\u76ee\u6807\u57df\u53d8\u4f53\u7f13\u6162\u3001\u6602\u8d35\u4e14\u4e0d\u5b8c\u6574\u7684\u95ee\u9898", "method": "\u5728\u6d4b\u8bd5\u65f6\u4f7f\u7528\u6269\u6563\u6a21\u578b\u5c06\u76ee\u6807\u56fe\u50cf\u6620\u5c04\u56de\u6e90\u57df\u5206\u5e03\uff0c\u4ec5\u9700\u6e90\u57df\u63cf\u8ff0\uff0c\u4fdd\u7559\u4efb\u52a1\u6a21\u578b\uff0c\u907f\u514d\u5927\u89c4\u6a21\u5408\u6210\u6570\u636e\u751f\u6210", "result": "\u5728\u771f\u5b9e\u5230\u771f\u5b9e\u7684\u57df\u6cdb\u5316\u573a\u666f\u4e2d\uff0c\u5206\u5272\u3001\u68c0\u6d4b\u548c\u5206\u7c7b\u4efb\u52a1\u5747\u83b7\u5f97\u4e00\u81f4\u6539\u8fdb\uff1aBDD100K-Night\u76f8\u5bf9\u63d0\u5347137%\uff0cImageNet-R\u63d0\u534768%\uff0cDarkZurich\u63d0\u534762%", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u57df\u6cdb\u5316\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\uff0c\u65e0\u9700\u5927\u89c4\u6a21\u6570\u636e\u5408\u6210\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u548c\u6a21\u578b\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b"}}
{"id": "2512.13465", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13465", "abs": "https://arxiv.org/abs/2512.13465", "authors": ["Ruiyan Wang", "Teng Hu", "Kaihui Huang", "Zihan Su", "Ran Yi", "Lizhuang Ma"], "title": "PoseAnything: Universal Pose-guided Video Generation with Part-aware Temporal Coherence", "comment": null, "summary": "Pose-guided video generation refers to controlling the motion of subjects in generated video through a sequence of poses. It enables precise control over subject motion and has important applications in animation. However, current pose-guided video generation methods are limited to accepting only human poses as input, thus generalizing poorly to pose of other subjects. To address this issue, we propose PoseAnything, the first universal pose-guided video generation framework capable of handling both human and non-human characters, supporting arbitrary skeletal inputs. To enhance consistency preservation during motion, we introduce Part-aware Temporal Coherence Module, which divides the subject into different parts, establishes part correspondences, and computes cross-attention between corresponding parts across frames to achieve fine-grained part-level consistency. Additionally, we propose Subject and Camera Motion Decoupled CFG, a novel guidance strategy that, for the first time, enables independent camera movement control in pose-guided video generation, by separately injecting subject and camera motion control information into the positive and negative anchors of CFG. Furthermore, we present XPose, a high-quality public dataset containing 50,000 non-human pose-video pairs, along with an automated pipeline for annotation and filtering. Extensive experiments demonstrate that Pose-Anything significantly outperforms state-of-the-art methods in both effectiveness and generalization.", "AI": {"tldr": "PoseAnything\u662f\u4e00\u4e2a\u901a\u7528\u7684\u59ff\u6001\u5f15\u5bfc\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u652f\u6301\u4eba\u7c7b\u548c\u975e\u4eba\u7c7b\u89d2\u8272\uff0c\u80fd\u591f\u5904\u7406\u4efb\u610f\u9aa8\u9abc\u8f93\u5165\uff0c\u5e76\u5b9e\u73b0\u4e86\u72ec\u7acb\u76f8\u673a\u8fd0\u52a8\u63a7\u5236\u3002", "motivation": "\u5f53\u524d\u59ff\u6001\u5f15\u5bfc\u89c6\u9891\u751f\u6210\u65b9\u6cd5\u4ec5\u63a5\u53d7\u4eba\u7c7b\u59ff\u6001\u4f5c\u4e3a\u8f93\u5165\uff0c\u5bf9\u5176\u4ed6\u4e3b\u4f53\u7684\u59ff\u6001\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u9650\u5236\u4e86\u5728\u52a8\u753b\u7b49\u9886\u57df\u7684\u5e94\u7528\u8303\u56f4\u3002", "method": "1. \u63d0\u51fa\u901a\u7528\u59ff\u6001\u5f15\u5bfc\u89c6\u9891\u751f\u6210\u6846\u67b6PoseAnything\uff1b2. \u5f15\u5165\u90e8\u4ef6\u611f\u77e5\u65f6\u95f4\u4e00\u81f4\u6027\u6a21\u5757\uff0c\u901a\u8fc7\u5212\u5206\u4e3b\u4f53\u90e8\u4ef6\u3001\u5efa\u7acb\u5bf9\u5e94\u5173\u7cfb\u3001\u8ba1\u7b97\u8de8\u5e27\u6ce8\u610f\u529b\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u4e00\u81f4\u6027\uff1b3. \u63d0\u51fa\u4e3b\u4f53\u548c\u76f8\u673a\u8fd0\u52a8\u89e3\u8026CFG\u7b56\u7565\uff0c\u901a\u8fc7\u5206\u522b\u6ce8\u5165\u4e3b\u4f53\u548c\u76f8\u673a\u8fd0\u52a8\u63a7\u5236\u4fe1\u606f\u5b9e\u73b0\u72ec\u7acb\u76f8\u673a\u63a7\u5236\uff1b4. \u6784\u5efaXPose\u6570\u636e\u96c6\uff0c\u5305\u542b5\u4e07\u4e2a\u975e\u4eba\u7c7b\u59ff\u6001-\u89c6\u9891\u5bf9\u3002", "result": "PoseAnything\u5728\u6548\u679c\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u80fd\u591f\u5904\u7406\u4eba\u7c7b\u548c\u975e\u4eba\u7c7b\u89d2\u8272\uff0c\u652f\u6301\u4efb\u610f\u9aa8\u9abc\u8f93\u5165\uff0c\u5e76\u5b9e\u73b0\u4e86\u72ec\u7acb\u7684\u76f8\u673a\u8fd0\u52a8\u63a7\u5236\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u9996\u4e2a\u901a\u7528\u7684\u59ff\u6001\u5f15\u5bfc\u89c6\u9891\u751f\u6210\u6846\u67b6\uff0c\u7a81\u7834\u4e86\u73b0\u6709\u65b9\u6cd5\u4ec5\u652f\u6301\u4eba\u7c7b\u59ff\u6001\u7684\u9650\u5236\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u90e8\u4ef6\u4e00\u81f4\u6027\u6a21\u5757\u548c\u8fd0\u52a8\u89e3\u8026\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u751f\u6210\u7684\u7075\u6d3b\u6027\u548c\u63a7\u5236\u7cbe\u5ea6\u3002"}}
{"id": "2512.13492", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13492", "abs": "https://arxiv.org/abs/2512.13492", "authors": ["Jiangning Zhang", "Junwei Zhu", "Teng Hu", "Yabiao Wang", "Donghao Luo", "Weijian Cao", "Zhenye Gan", "Xiaobin Hu", "Zhucun Xue", "Chengjie Wang"], "title": "Transform Trained Transformer: Accelerating Naive 4K Video Generation Over 10$\\times$", "comment": "Project page: https://zhangzjn.github.io/projects/T3-Video", "summary": "Native 4K (2160$\\times$3840) video generation remains a critical challenge due to the quadratic computational explosion of full-attention as spatiotemporal resolution increases, making it difficult for models to strike a balance between efficiency and quality. This paper proposes a novel Transformer retrofit strategy termed $\\textbf{T3}$ ($\\textbf{T}$ransform $\\textbf{T}$rained $\\textbf{T}$ransformer) that, without altering the core architecture of full-attention pretrained models, significantly reduces compute requirements by optimizing their forward logic. Specifically, $\\textbf{T3-Video}$ introduces a multi-scale weight-sharing window attention mechanism and, via hierarchical blocking together with an axis-preserving full-attention design, can effect an \"attention pattern\" transformation of a pretrained model using only modest compute and data. Results on 4K-VBench show that $\\textbf{T3-Video}$ substantially outperforms existing approaches: while delivering performance improvements (+4.29$\\uparrow$ VQA and +0.08$\\uparrow$ VTC), it accelerates native 4K video generation by more than 10$\\times$. Project page at https://zhangzjn.github.io/projects/T3-Video", "AI": {"tldr": "T3-Video\u63d0\u51fa\u4e86\u4e00\u79cdTransformer\u6539\u9020\u7b56\u7565\uff0c\u901a\u8fc7\u4f18\u5316\u524d\u5411\u903b\u8f91\u800c\u975e\u4fee\u6539\u6838\u5fc3\u67b6\u6784\uff0c\u663e\u8457\u964d\u4f4e4K\u89c6\u9891\u751f\u6210\u7684\u8ba1\u7b97\u9700\u6c42\uff0c\u5b9e\u73b010\u500d\u52a0\u901f\u540c\u65f6\u63d0\u5347\u8d28\u91cf", "motivation": "\u539f\u751f4K\u89c6\u9891\u751f\u6210\u9762\u4e34\u4e8c\u6b21\u8ba1\u7b97\u7206\u70b8\u7684\u6311\u6218\uff0c\u5168\u6ce8\u610f\u529b\u673a\u5236\u5728\u65f6\u7a7a\u5206\u8fa8\u7387\u589e\u52a0\u65f6\u8ba1\u7b97\u91cf\u6025\u5267\u589e\u957f\uff0c\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u5728\u6548\u7387\u548c\u8d28\u91cf\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861", "method": "\u63d0\u51faT3\uff08Transform Trained Transformer\uff09\u6539\u9020\u7b56\u7565\uff0c\u5f15\u5165\u591a\u5c3a\u5ea6\u6743\u91cd\u5171\u4eab\u7a97\u53e3\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u5206\u5c42\u5206\u5757\u548c\u8f74\u4fdd\u6301\u5168\u6ce8\u610f\u529b\u8bbe\u8ba1\uff0c\u4ec5\u9700\u9002\u5ea6\u8ba1\u7b97\u548c\u6570\u636e\u5373\u53ef\u5b9e\u73b0\u9884\u8bad\u7ec3\u6a21\u578b\u7684\"\u6ce8\u610f\u529b\u6a21\u5f0f\"\u8f6c\u6362", "result": "\u57284K-VBench\u4e0a\uff0cT3-Video\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff1aVQA\u63d0\u53474.29\u5206\uff0cVTC\u63d0\u53470.08\u5206\uff0c\u540c\u65f6\u5c06\u539f\u751f4K\u89c6\u9891\u751f\u6210\u52a0\u901f\u8d85\u8fc710\u500d", "conclusion": "T3\u7b56\u7565\u4e3a\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4e0d\u6539\u53d8\u9884\u8bad\u7ec3\u6a21\u578b\u6838\u5fc3\u67b6\u6784\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u4f18\u5316\u524d\u5411\u903b\u8f91\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u8d28\u91cf\u7684\u53cc\u91cd\u63d0\u5347"}}
{"id": "2512.13507", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13507", "abs": "https://arxiv.org/abs/2512.13507", "authors": ["Siyan Chen", "Yanfei Chen", "Ying Chen", "Zhuo Chen", "Feng Cheng", "Xuyan Chi", "Jian Cong", "Qinpeng Cui", "Qide Dong", "Junliang Fan", "Jing Fang", "Zetao Fang", "Chengjian Feng", "Han Feng", "Mingyuan Gao", "Yu Gao", "Qiushan Guo", "Boyang Hao", "Qingkai Hao", "Bibo He", "Qian He", "Tuyen Hoang", "Ruoqing Hu", "Xi Hu", "Weilin Huang", "Zhaoyang Huang", "Zhongyi Huang", "Siqi Jiang", "Wei Jiang", "Yunpu Jiang", "Zhuo Jiang", "Ashley Kim", "Jianan Kong", "Zhichao Lai", "Shanshan Lao", "Ai Li", "Feiya Li", "Gen Li", "Huixia Li", "JiaShi Li", "Liang Li", "Ming Li", "Tao Li", "Xian Li", "Xiaojie Li", "Xiaoyang Li", "Xingxing Li", "Yameng Li", "Yifu Li", "Yiying Li", "Chao Liang", "Ying Liang", "Zhiqiang Liang", "Wang Liao", "Yalin Liao", "Heng Lin", "Kengyu Lin", "Shanchuan Lin", "Xi Lin", "Zhijie Lin", "Feng Ling", "Fangfang Liu", "Gaohong Liu", "Jiawei Liu", "Jie Liu", "Shouda Liu", "Shu Liu", "Sichao Liu", "Songwei Liu", "Xin Liu", "Xue Liu", "Yibo Liu", "Zikun Liu", "Zuxi Liu", "Junlin Lyu", "Lecheng Lyu", "Qian Lyu", "Han Mu", "Xiaonan Nie", "Jingzhe Ning", "Xitong Pan", "Yanghua Peng", "Lianke Qin", "Xueqiong Qu", "Yuxi Ren", "Yuchen Shen", "Guang Shi", "Lei Shi", "Yan Song", "Yinglong Song", "Fan Sun", "Li Sun", "Renfei Sun", "Zeyu Sun", "Wenjing Tang", "Zirui Tao", "Feng Wang", "Furui Wang", "Jinran Wang", "Junkai Wang", "Ke Wang", "Kexin Wang", "Qingyi Wang", "Rui Wang", "Sen Wang", "Shuai Wang", "Tingru Wang", "Weichen Wang", "Xin Wang", "Yanhui Wang", "Yue Wang", "Yuping Wang", "Yuxuan Wang", "Ziyu Wang", "Guoqiang Wei", "Wanru Wei", "Di Wu", "Guohong Wu", "Hanjie Wu", "Jian Wu", "Jie Wu", "Ruolan Wu", "Xinglong Wu", "Yonghui Wu", "Ruiqi Xia", "Liang Xiang", "Fei Xiao", "XueFeng Xiao", "Pan Xie", "Shuangyi Xie", "Shuang Xu", "Jinlan Xue", "Bangbang Yang", "Ceyuan Yang", "Jiaqi Yang", "Runkai Yang", "Tao Yang", "Yang Yang", "Yihang Yang", "ZhiXian Yang", "Ziyan Yang", "Yifan Yao", "Zilyu Ye", "Bowen Yu", "Chujie Yuan", "Linxiao Yuan", "Sichun Zeng", "Weihong Zeng", "Xuejiao Zeng", "Yan Zeng", "Chuntao Zhang", "Heng Zhang", "Jingjie Zhang", "Kuo Zhang", "Liang Zhang", "Liying Zhang", "Manlin Zhang", "Ting Zhang", "Weida Zhang", "Xiaohe Zhang", "Xinyan Zhang", "Yan Zhang", "Yuan Zhang", "Zixiang Zhang", "Fengxuan Zhao", "Huating Zhao", "Yang Zhao", "Hao Zheng", "Jianbin Zheng", "Xiaozheng Zheng", "Yangyang Zheng", "Yijie Zheng", "Jiexin Zhou", "Kuan Zhu", "Shenhan Zhu", "Wenjia Zhu", "Benhui Zou", "Feilong Zuo"], "title": "Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model", "comment": "Seedance 1.5 pro Technical Report", "summary": "Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.", "AI": {"tldr": "Seedance 1.5 pro\u662f\u4e00\u4e2a\u7528\u4e8e\u539f\u751f\u8054\u5408\u97f3\u9891-\u89c6\u9891\u751f\u6210\u7684\u57fa\u7840\u6a21\u578b\uff0c\u91c7\u7528\u53cc\u5206\u652f\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\uff0c\u901a\u8fc7\u8de8\u6a21\u6001\u8054\u5408\u6a21\u5757\u548c\u591a\u9636\u6bb5\u6570\u636e\u7ba1\u9053\u5b9e\u73b0\u5353\u8d8a\u7684\u89c6\u542c\u540c\u6b65\u548c\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u89c6\u9891\u751f\u6210\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u4e3a\u7edf\u4e00\u7684\u89c6\u542c\u751f\u6210\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u9700\u8981\u4e13\u95e8\u7684\u57fa\u7840\u6a21\u578b\u6765\u5b9e\u73b0\u539f\u751f\u3001\u8054\u5408\u7684\u97f3\u9891-\u89c6\u9891\u751f\u6210\uff0c\u4ee5\u6ee1\u8db3\u4e13\u4e1a\u7ea7\u5185\u5bb9\u521b\u4f5c\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u53cc\u5206\u652f\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\uff0c\u96c6\u6210\u8de8\u6a21\u6001\u8054\u5408\u6a21\u5757\u548c\u4e13\u95e8\u7684\u591a\u9636\u6bb5\u6570\u636e\u7ba1\u9053\u3002\u5b9e\u65bd\u7ec6\u81f4\u7684\u8bad\u7ec3\u540e\u4f18\u5316\uff0c\u5305\u62ec\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u4e0a\u7684\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u57fa\u4e8e\u591a\u7ef4\u5956\u52b1\u6a21\u578b\u7684\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\uff08RLHF\uff09\u3002\u5f15\u5165\u52a0\u901f\u6846\u67b6\u5c06\u63a8\u7406\u901f\u5ea6\u63d0\u534710\u500d\u4ee5\u4e0a\u3002", "result": "\u6a21\u578b\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u89c6\u542c\u540c\u6b65\u548c\u751f\u6210\u8d28\u91cf\uff0c\u5177\u5907\u7cbe\u786e\u7684\u591a\u8bed\u8a00\u548c\u65b9\u8a00\u5507\u5f62\u540c\u6b65\u3001\u52a8\u6001\u7535\u5f71\u7ea7\u6444\u50cf\u673a\u63a7\u5236\u4ee5\u53ca\u589e\u5f3a\u7684\u53d9\u4e8b\u8fde\u8d2f\u6027\uff0c\u6210\u4e3a\u4e13\u4e1a\u7ea7\u5185\u5bb9\u521b\u4f5c\u7684\u5f3a\u5927\u5f15\u64ce\u3002", "conclusion": "Seedance 1.5 pro\u901a\u8fc7\u521b\u65b0\u7684\u67b6\u6784\u8bbe\u8ba1\u3001\u7cbe\u7ec6\u7684\u8bad\u7ec3\u4f18\u5316\u548c\u9ad8\u6548\u7684\u63a8\u7406\u52a0\u901f\uff0c\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u529f\u80fd\u5f3a\u5927\u7684\u539f\u751f\u8054\u5408\u97f3\u9891-\u89c6\u9891\u751f\u6210\u57fa\u7840\u6a21\u578b\uff0c\u73b0\u5df2\u53ef\u5728\u706b\u5c71\u5f15\u64ce\u5e73\u53f0\u4e0a\u8bbf\u95ee\u4f7f\u7528\u3002"}}
{"id": "2512.13534", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13534", "abs": "https://arxiv.org/abs/2512.13534", "authors": ["Marianne Rakic", "Siyu Gai", "Etienne Chollet", "John V. Guttag", "Adrian V. Dalca"], "title": "Pancakes: Consistent Multi-Protocol Image Segmentation Across Biomedical Domains", "comment": "Accepted at NeurIPS 2025. Code available at: https://github.com/mariannerakic/Pancakes", "summary": "A single biomedical image can be meaningfully segmented in multiple ways, depending on the desired application. For instance, a brain MRI can be segmented according to tissue types, vascular territories, broad anatomical regions, fine-grained anatomy, or pathology, etc. Existing automatic segmentation models typically either (1) support only a single protocol, the one they were trained on, or (2) require labor-intensive manual prompting to specify the desired segmentation. We introduce Pancakes, a framework that, given a new image from a previously unseen domain, automatically generates multi-label segmentation maps for multiple plausible protocols, while maintaining semantic consistency across related images. Pancakes introduces a new problem formulation that is not currently attainable by existing foundation models. In a series of experiments on seven held-out datasets, we demonstrate that our model can significantly outperform existing foundation models in producing several plausible whole-image segmentations, that are semantically coherent across images.", "AI": {"tldr": "Pancakes\u6846\u67b6\u80fd\u591f\u4e3a\u533b\u5b66\u56fe\u50cf\u81ea\u52a8\u751f\u6210\u591a\u79cd\u8bed\u4e49\u4e00\u81f4\u7684\u591a\u6807\u7b7e\u5206\u5272\u65b9\u6848\uff0c\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u53ea\u80fd\u5904\u7406\u5355\u4e00\u5206\u5272\u534f\u8bae\u6216\u9700\u8981\u624b\u52a8\u63d0\u793a\u7684\u95ee\u9898", "motivation": "\u540c\u4e00\u533b\u5b66\u56fe\u50cf\u53ef\u4ee5\u6839\u636e\u4e0d\u540c\u5e94\u7528\u9700\u6c42\u8fdb\u884c\u591a\u79cd\u6709\u610f\u4e49\u7684\u5206\u5272\uff08\u5982\u7ec4\u7ec7\u7c7b\u578b\u3001\u8840\u7ba1\u533a\u57df\u3001\u89e3\u5256\u7ed3\u6784\u7b49\uff09\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u8981\u4e48\u53ea\u652f\u6301\u5355\u4e00\u534f\u8bae\uff0c\u8981\u4e48\u9700\u8981\u4eba\u5de5\u63d0\u793a\uff0c\u7f3a\u4e4f\u81ea\u52a8\u751f\u6210\u591a\u79cd\u8bed\u4e49\u4e00\u81f4\u5206\u5272\u65b9\u6848\u7684\u80fd\u529b", "method": "\u63d0\u51faPancakes\u6846\u67b6\uff0c\u5f15\u5165\u65b0\u7684\u95ee\u9898\u8868\u8ff0\uff0c\u80fd\u591f\u5728\u672a\u89c1\u8fc7\u7684\u56fe\u50cf\u57df\u4e2d\u81ea\u52a8\u751f\u6210\u591a\u4e2a\u5408\u7406\u534f\u8bae\u7684\u591a\u6807\u7b7e\u5206\u5272\u56fe\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u5173\u56fe\u50cf\u95f4\u7684\u8bed\u4e49\u4e00\u81f4\u6027", "result": "\u57287\u4e2a\u4fdd\u7559\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u751f\u6210\u591a\u4e2a\u8bed\u4e49\u4e00\u81f4\u7684\u6574\u56fe\u5206\u5272\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7840\u6a21\u578b", "conclusion": "Pancakes\u6846\u67b6\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u7840\u6a21\u578b\u65e0\u6cd5\u5b9e\u73b0\u7684\u65b0\u95ee\u9898\uff0c\u80fd\u591f\u81ea\u52a8\u4e3a\u533b\u5b66\u56fe\u50cf\u751f\u6210\u591a\u79cd\u8bed\u4e49\u4e00\u81f4\u7684\u5206\u5272\u65b9\u6848\uff0c\u5177\u6709\u91cd\u8981\u7684\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c"}}
{"id": "2512.13573", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13573", "abs": "https://arxiv.org/abs/2512.13573", "authors": ["Tao Zhang", "Ziqi Zhang", "Zongyang Ma", "Yuxin Chen", "Bing Li", "Chunfeng Yuan", "Guangting Wang", "Fengyun Rao", "Ying Shan", "Weiming Hu"], "title": "MMhops-R1: Multimodal Multi-hop Reasoning", "comment": "Acceped by AAAI 2026", "summary": "The ability to perform multi-modal multi-hop reasoning by iteratively integrating information across various modalities and external knowledge is critical for addressing complex real-world challenges. However, existing Multi-modal Large Language Models (MLLMs) are predominantly limited to single-step reasoning, as existing benchmarks lack the complexity needed to evaluate and drive multi-hop abilities. To bridge this gap, we introduce MMhops, a novel, large-scale benchmark designed to systematically evaluate and foster multi-modal multi-hop reasoning. MMhops dataset comprises two challenging task formats, Bridging and Comparison, which necessitate that models dynamically construct complex reasoning chains by integrating external knowledge. To tackle the challenges posed by MMhops, we propose MMhops-R1, a novel multi-modal Retrieval-Augmented Generation (mRAG) framework for dynamic reasoning. Our framework utilizes reinforcement learning to optimize the model for autonomously planning reasoning paths, formulating targeted queries, and synthesizing multi-level information. Comprehensive experiments demonstrate that MMhops-R1 significantly outperforms strong baselines on MMhops, highlighting that dynamic planning and multi-modal knowledge integration are crucial for complex reasoning. Moreover, MMhops-R1 demonstrates strong generalization to tasks requiring fixed-hop reasoning, underscoring the robustness of our dynamic planning approach. In conclusion, our work contributes a challenging new benchmark and a powerful baseline model, and we will release the associated code, data, and weights to catalyze future research in this critical area.", "AI": {"tldr": "\u63d0\u51fa\u4e86MMhops\u57fa\u51c6\u6d4b\u8bd5\u548cMMhops-R1\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u63d0\u5347\u591a\u6a21\u6001\u591a\u8df3\u63a8\u7406\u80fd\u529b\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u52a8\u6001\u63a8\u7406\u8def\u5f84\u89c4\u5212", "motivation": "\u73b0\u6709MLLM\u4e3b\u8981\u5c40\u9650\u4e8e\u5355\u6b65\u63a8\u7406\uff0c\u7f3a\u4e4f\u8bc4\u4f30\u591a\u6a21\u6001\u591a\u8df3\u63a8\u7406\u80fd\u529b\u7684\u590d\u6742\u57fa\u51c6\uff0c\u9700\u8981\u89e3\u51b3\u591a\u6a21\u6001\u4fe1\u606f\u8fed\u4ee3\u6574\u5408\u548c\u5916\u90e8\u77e5\u8bc6\u5229\u7528\u7684\u6311\u6218", "method": "\u63d0\u51faMMhops\u57fa\u51c6\u6d4b\u8bd5\u5305\u542bBridging\u548cComparison\u4e24\u79cd\u4efb\u52a1\u683c\u5f0f\uff0c\u5e76\u5f00\u53d1MMhops-R1\u591a\u6a21\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u52a8\u6001\u63a8\u7406\u8def\u5f84\u89c4\u5212\u3001\u76ee\u6807\u67e5\u8be2\u5236\u5b9a\u548c\u591a\u5c42\u6b21\u4fe1\u606f\u5408\u6210", "result": "MMhops-R1\u5728MMhops\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u52a8\u6001\u89c4\u5212\u548c\u591a\u6a21\u6001\u77e5\u8bc6\u6574\u5408\u5bf9\u590d\u6742\u63a8\u7406\u7684\u91cd\u8981\u6027\uff0c\u540c\u65f6\u5728\u56fa\u5b9a\u8df3\u6570\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b", "conclusion": "\u8be5\u5de5\u4f5c\u8d21\u732e\u4e86\u5177\u6709\u6311\u6218\u6027\u7684\u65b0\u57fa\u51c6\u548c\u5f3a\u5927\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5c06\u53d1\u5e03\u76f8\u5173\u4ee3\u7801\u3001\u6570\u636e\u548c\u6743\u91cd\u4ee5\u63a8\u52a8\u8be5\u5173\u952e\u9886\u57df\u7684\u7814\u7a76"}}
{"id": "2512.13597", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13597", "abs": "https://arxiv.org/abs/2512.13597", "authors": ["Christophe Bolduc", "Julien Philip", "Li Ma", "Mingming He", "Paul Debevec", "Jean-Fran\u00e7ois Lalonde"], "title": "Lighting in Motion: Spatiotemporal HDR Lighting Estimation", "comment": null, "summary": "We present Lighting in Motion (LiMo), a diffusion-based approach to spatiotemporal lighting estimation. LiMo targets both realistic high-frequency detail prediction and accurate illuminance estimation. To account for both, we propose generating a set of mirrored and diffuse spheres at different exposures, based on their 3D positions in the input. Making use of diffusion priors, we fine-tune powerful existing diffusion models on a large-scale customized dataset of indoor and outdoor scenes, paired with spatiotemporal light probes. For accurate spatial conditioning, we demonstrate that depth alone is insufficient and we introduce a new geometric condition to provide the relative position of the scene to the target 3D position. Finally, we combine diffuse and mirror predictions at different exposures into a single HDRI map leveraging differentiable rendering. We thoroughly evaluate our method and design choices to establish LiMo as state-of-the-art for both spatial control and prediction accuracy.", "AI": {"tldr": "LiMo\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65f6\u7a7a\u5149\u7167\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u4e0d\u540c\u66dd\u5149\u4e0b\u7684\u955c\u9762\u548c\u6f2b\u53cd\u5c04\u7403\u4f53\u6765\u9884\u6d4b\u9ad8\u9891\u7ec6\u8282\u548c\u51c6\u786e\u7167\u5ea6\uff0c\u7ed3\u5408\u6df1\u5ea6\u548c\u65b0\u51e0\u4f55\u6761\u4ef6\u8fdb\u884c\u7a7a\u95f4\u63a7\u5236\uff0c\u6700\u7ec8\u5408\u6210HDRI\u5730\u56fe\u3002", "motivation": "\u73b0\u6709\u7684\u5149\u7167\u4f30\u8ba1\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u5b9e\u73b0\u771f\u5b9e\u7684\u9ad8\u9891\u7ec6\u8282\u9884\u6d4b\u548c\u51c6\u786e\u7684\u7167\u5ea6\u4f30\u8ba1\uff0c\u7279\u522b\u662f\u5728\u65f6\u7a7a\u573a\u666f\u4e2d\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u6355\u6349\u590d\u6742\u5149\u7167\u7ec6\u8282\u53c8\u80fd\u51c6\u786e\u4f30\u8ba1\u5149\u7167\u5f3a\u5ea6\u7684\u7efc\u5408\u65b9\u6cd5\u3002", "method": "1. \u57fa\u4e8e\u8f93\u51653D\u4f4d\u7f6e\u751f\u6210\u4e0d\u540c\u66dd\u5149\u4e0b\u7684\u955c\u9762\u548c\u6f2b\u53cd\u5c04\u7403\u4f53\u96c6\u5408\uff1b2. \u5728\u5927\u89c4\u6a21\u5b9a\u5236\u5ba4\u5185\u5916\u573a\u666f\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u73b0\u6709\u6269\u6563\u6a21\u578b\uff1b3. \u5f15\u5165\u65b0\u7684\u51e0\u4f55\u6761\u4ef6\uff08\u573a\u666f\u4e0e\u76ee\u68073D\u4f4d\u7f6e\u7684\u76f8\u5bf9\u4f4d\u7f6e\uff09\u6765\u8865\u5145\u6df1\u5ea6\u4fe1\u606f\u7684\u4e0d\u8db3\uff1b4. \u901a\u8fc7\u53ef\u5fae\u5206\u6e32\u67d3\u5c06\u4e0d\u540c\u66dd\u5149\u7684\u6f2b\u53cd\u5c04\u548c\u955c\u9762\u9884\u6d4b\u7ec4\u5408\u6210\u5355\u4e2aHDRI\u5730\u56fe\u3002", "result": "LiMo\u5728\u7a7a\u95f4\u63a7\u5236\u548c\u9884\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\uff0c\u80fd\u591f\u540c\u65f6\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u9ad8\u9891\u7ec6\u8282\u9884\u6d4b\u548c\u51c6\u786e\u7684\u7167\u5ea6\u4f30\u8ba1\u3002", "conclusion": "LiMo\u901a\u8fc7\u7ed3\u5408\u6269\u6563\u5148\u9a8c\u3001\u65b0\u51e0\u4f55\u6761\u4ef6\u548c\u591a\u66dd\u5149\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u65f6\u7a7a\u5149\u7167\u4f30\u8ba1\u4e2d\u7684\u9ad8\u9891\u7ec6\u8282\u548c\u7167\u5ea6\u51c6\u786e\u6027\u7684\u53cc\u91cd\u6311\u6218\uff0c\u4e3a\u573a\u666f\u7167\u660e\u63d0\u4f9b\u4e86\u5168\u9762\u800c\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13604", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13604", "abs": "https://arxiv.org/abs/2512.13604", "authors": ["Jianxiong Gao", "Zhaoxi Chen", "Xian Liu", "Junhao Zhuang", "Chengming Xu", "Jianfeng Feng", "Yu Qiao", "Yanwei Fu", "Chenyang Si", "Ziwei Liu"], "title": "LongVie 2: Multimodal Controllable Ultra-Long Video World Model", "comment": "Project Page: https://vchitect.github.io/LongVie2-project/", "summary": "Building video world models upon pretrained video generation systems represents an important yet challenging step toward general spatiotemporal intelligence. A world model should possess three essential properties: controllability, long-term visual quality, and temporal consistency. To this end, we take a progressive approach-first enhancing controllability and then extending toward long-term, high-quality generation. We present LongVie 2, an end-to-end autoregressive framework trained in three stages: (1) Multi-modal guidance, which integrates dense and sparse control signals to provide implicit world-level supervision and improve controllability; (2) Degradation-aware training on the input frame, bridging the gap between training and long-term inference to maintain high visual quality; and (3) History-context guidance, which aligns contextual information across adjacent clips to ensure temporal consistency. We further introduce LongVGenBench, a comprehensive benchmark comprising 100 high-resolution one-minute videos covering diverse real-world and synthetic environments. Extensive experiments demonstrate that LongVie 2 achieves state-of-the-art performance in long-range controllability, temporal coherence, and visual fidelity, and supports continuous video generation lasting up to five minutes, marking a significant step toward unified video world modeling.", "AI": {"tldr": "LongVie 2\u662f\u4e00\u4e2a\u4e09\u9636\u6bb5\u8bad\u7ec3\u7684\u7aef\u5230\u7aef\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u5f15\u5bfc\u3001\u9000\u5316\u611f\u77e5\u8bad\u7ec3\u548c\u5386\u53f2\u4e0a\u4e0b\u6587\u5f15\u5bfc\uff0c\u5b9e\u73b0\u4e86\u53ef\u63a7\u3001\u9ad8\u8d28\u91cf\u3001\u65f6\u95f4\u4e00\u81f4\u7684\u957f\u89c6\u9891\u751f\u6210\uff0c\u652f\u6301\u957f\u8fbe5\u5206\u949f\u7684\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u57fa\u4e8e\u9884\u8bad\u7ec3\u89c6\u9891\u751f\u6210\u7cfb\u7edf\u6784\u5efa\u89c6\u9891\u4e16\u754c\u6a21\u578b\u662f\u5b9e\u73b0\u901a\u7528\u65f6\u7a7a\u667a\u80fd\u7684\u91cd\u8981\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u6b65\u9aa4\u3002\u4e16\u754c\u6a21\u578b\u9700\u8981\u5177\u5907\u4e09\u4e2a\u57fa\u672c\u5c5e\u6027\uff1a\u53ef\u63a7\u6027\u3001\u957f\u671f\u89c6\u89c9\u8d28\u91cf\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "method": "\u91c7\u7528\u6e10\u8fdb\u5f0f\u65b9\u6cd5\uff1a1) \u591a\u6a21\u6001\u5f15\u5bfc\uff1a\u6574\u5408\u7a20\u5bc6\u548c\u7a00\u758f\u63a7\u5236\u4fe1\u53f7\uff0c\u63d0\u4f9b\u9690\u5f0f\u4e16\u754c\u7ea7\u76d1\u7763\uff1b2) \u8f93\u5165\u5e27\u7684\u9000\u5316\u611f\u77e5\u8bad\u7ec3\uff1a\u5f25\u5408\u8bad\u7ec3\u4e0e\u957f\u671f\u63a8\u7406\u4e4b\u95f4\u7684\u5dee\u8ddd\uff1b3) \u5386\u53f2\u4e0a\u4e0b\u6587\u5f15\u5bfc\uff1a\u5bf9\u9f50\u76f8\u90bb\u7247\u6bb5\u95f4\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u4ee5\u786e\u4fdd\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "LongVie 2\u5728\u957f\u8ddd\u79bb\u53ef\u63a7\u6027\u3001\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u652f\u6301\u957f\u8fbe5\u5206\u949f\u7684\u8fde\u7eed\u89c6\u9891\u751f\u6210\u3002\u540c\u65f6\u63d0\u51fa\u4e86\u5305\u542b100\u4e2a\u9ad8\u5206\u8fa8\u7387\u4e00\u5206\u949f\u89c6\u9891\u7684LongVGenBench\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "LongVie 2\u4ee3\u8868\u4e86\u5411\u7edf\u4e00\u89c6\u9891\u4e16\u754c\u5efa\u6a21\u8fc8\u51fa\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u751f\u6210\u4e2d\u7684\u53ef\u63a7\u6027\u3001\u89c6\u89c9\u8d28\u91cf\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u6311\u6218\u3002"}}
{"id": "2512.13609", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.13609", "abs": "https://arxiv.org/abs/2512.13609", "authors": ["Shweta Mahajan", "Shreya Kadambi", "Hoang Le", "Munawar Hayat", "Fatih Porikli"], "title": "Do-Undo: Generating and Reversing Physical Actions in Vision-Language Models", "comment": null, "summary": "We introduce the Do-Undo task and benchmark to address a critical gap in vision-language models: understanding and generating physically plausible scene transformations driven by real-world actions. Unlike prior work focused on object-level edits, Do-Undo requires models to simulate the outcome of a physical action and then accurately reverse it, reflecting true cause-and-effect in the visual world. We curate a large-scale dataset of reversible actions from real-world videos and design a training strategy enforcing consistency for robust action grounding. Our experiments reveal that current models struggle with physical reversibility, underscoring the importance of this task for embodied AI, robotics, and physics-aware generative modeling. Do-Undo establishes an intuitive testbed for evaluating and advancing physical reasoning in multimodal systems.", "AI": {"tldr": "Do-Undo\u4efb\u52a1\u548c\u57fa\u51c6\u65e8\u5728\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u548c\u751f\u6210\u7531\u771f\u5b9e\u4e16\u754c\u52a8\u4f5c\u9a71\u52a8\u7684\u7269\u7406\u5408\u7406\u573a\u666f\u8f6c\u6362\u65b9\u9762\u7684\u5173\u952e\u7f3a\u9677\uff0c\u8981\u6c42\u6a21\u578b\u6a21\u62df\u7269\u7406\u52a8\u4f5c\u7684\u7ed3\u679c\u5e76\u51c6\u786e\u53cd\u8f6c\u5b83\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u7269\u7406\u573a\u666f\u8f6c\u6362\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u7279\u522b\u662f\u7f3a\u4e4f\u5bf9\u771f\u5b9e\u4e16\u754c\u52a8\u4f5c\u5f15\u8d77\u7684\u56e0\u679c\u5173\u7cfb\u7684\u7406\u89e3\u3002\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u5bf9\u8c61\u7ea7\u7f16\u8f91\uff0c\u800cDo-Undo\u9700\u8981\u6a21\u578b\u7406\u89e3\u7269\u7406\u52a8\u4f5c\u7684\u53ef\u9006\u6027\uff0c\u8fd9\u5bf9\u4e8e\u5177\u8eabAI\u3001\u673a\u5668\u4eba\u548c\u7269\u7406\u611f\u77e5\u751f\u6210\u5efa\u6a21\u81f3\u5173\u91cd\u8981\u3002", "method": "1. \u4ece\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u4e2d\u7b56\u5212\u5927\u89c4\u6a21\u53ef\u9006\u52a8\u4f5c\u6570\u636e\u96c6\uff1b2. \u8bbe\u8ba1\u8bad\u7ec3\u7b56\u7565\u4ee5\u589e\u5f3a\u52a8\u4f5c\u57fa\u7840\u7684\u4e00\u81f4\u6027\uff1b3. \u8981\u6c42\u6a21\u578b\u6a21\u62df\u7269\u7406\u52a8\u4f5c\u7684\u7ed3\u679c\u5e76\u51c6\u786e\u53cd\u8f6c\u8be5\u52a8\u4f5c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u6a21\u578b\u5728\u5904\u7406\u7269\u7406\u53ef\u9006\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u8fd9\u7a81\u663e\u4e86\u8be5\u4efb\u52a1\u7684\u91cd\u8981\u6027\u3002Do-Undo\u4e3a\u8bc4\u4f30\u548c\u63a8\u8fdb\u591a\u6a21\u6001\u7cfb\u7edf\u4e2d\u7684\u7269\u7406\u63a8\u7406\u80fd\u529b\u5efa\u7acb\u4e86\u76f4\u89c2\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002", "conclusion": "Do-Undo\u4efb\u52a1\u586b\u8865\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7269\u7406\u573a\u666f\u8f6c\u6362\u7406\u89e3\u65b9\u9762\u7684\u5173\u952e\u7a7a\u767d\uff0c\u4e3a\u5177\u8eabAI\u3001\u673a\u5668\u4eba\u548c\u7269\u7406\u611f\u77e5\u751f\u6210\u5efa\u6a21\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u5f3a\u8c03\u4e86\u7269\u7406\u63a8\u7406\u80fd\u529b\u5bf9\u591a\u6a21\u6001\u7cfb\u7edf\u53d1\u5c55\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2512.13636", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2512.13636", "abs": "https://arxiv.org/abs/2512.13636", "authors": ["Haoyu Fu", "Diankun Zhang", "Zongchuang Zhao", "Jianfeng Cui", "Hongwei Xie", "Bing Wang", "Guang Chen", "Dingkang Liang", "Xiang Bai"], "title": "MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning", "comment": "16 pages, 12 figures, 6 tables; Project Page: https://xiaomi-mlab.github.io/MindDrive/", "summary": "Current Vision-Language-Action (VLA) paradigms in autonomous driving primarily rely on Imitation Learning (IL), which introduces inherent challenges such as distribution shift and causal confusion. Online Reinforcement Learning offers a promising pathway to address these issues through trial-and-error learning. However, applying online reinforcement learning to VLA models in autonomous driving is hindered by inefficient exploration in continuous action spaces. To overcome this limitation, we propose MindDrive, a VLA framework comprising a large language model (LLM) with two distinct sets of LoRA parameters. The one LLM serves as a Decision Expert for scenario reasoning and driving decision-making, while the other acts as an Action Expert that dynamically maps linguistic decisions into feasible trajectories. By feeding trajectory-level rewards back into the reasoning space, MindDrive enables trial-and-error learning over a finite set of discrete linguistic driving decisions, instead of operating directly in a continuous action space. This approach effectively balances optimal decision-making in complex scenarios, human-like driving behavior, and efficient exploration in online reinforcement learning. MindDrive achieves strong closed-loop performance on the challenging Bench2Drive benchmark, with a Driving Score (DS) of 78.04 and a Success Rate (SR) of 55.09%. To the best of our knowledge, this is the first work to demonstrate the effectiveness of online reinforcement learning for the VLA model in autonomous driving.", "AI": {"tldr": "MindDrive\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684VLA\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u6620\u5c04\u5230\u79bb\u6563\u8bed\u8a00\u51b3\u7b56\u7a7a\u95f4\u6765\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u63a2\u7d22\u6548\u7387\u95ee\u9898\uff0c\u5728Bench2Drive\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u8303\u5f0f\u4e3b\u8981\u4f9d\u8d56\u6a21\u4eff\u5b66\u4e60\uff0c\u5b58\u5728\u5206\u5e03\u504f\u79fb\u548c\u56e0\u679c\u6df7\u6dc6\u95ee\u9898\u3002\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u867d\u7136\u80fd\u901a\u8fc7\u8bd5\u9519\u5b66\u4e60\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f46\u5728\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u63a2\u7d22\u6548\u7387\u4f4e\u4e0b\uff0c\u963b\u788d\u4e86\u5176\u5728VLA\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faMindDrive\u6846\u67b6\uff0c\u4f7f\u7528\u4e00\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u914d\u5907\u4e24\u5957\u4e0d\u540c\u7684LoRA\u53c2\u6570\uff1a\u4e00\u4e2a\u4f5c\u4e3a\u51b3\u7b56\u4e13\u5bb6\u8fdb\u884c\u573a\u666f\u63a8\u7406\u548c\u9a7e\u9a76\u51b3\u7b56\uff0c\u53e6\u4e00\u4e2a\u4f5c\u4e3a\u52a8\u4f5c\u4e13\u5bb6\u5c06\u8bed\u8a00\u51b3\u7b56\u52a8\u6001\u6620\u5c04\u4e3a\u53ef\u884c\u8f68\u8ff9\u3002\u901a\u8fc7\u5c06\u8f68\u8ff9\u7ea7\u5956\u52b1\u53cd\u9988\u5230\u63a8\u7406\u7a7a\u95f4\uff0c\u5b9e\u73b0\u5728\u6709\u9650\u79bb\u6563\u8bed\u8a00\u9a7e\u9a76\u51b3\u7b56\u4e0a\u7684\u8bd5\u9519\u5b66\u4e60\u3002", "result": "\u5728Bench2Drive\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMindDrive\u53d6\u5f97\u4e86\u9a7e\u9a76\u8bc4\u520678.04%\u548c\u6210\u529f\u738755.09%\u7684\u5f3a\u95ed\u73af\u6027\u80fd\u3002\u8fd9\u662f\u9996\u4e2a\u8bc1\u660e\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u5728\u81ea\u52a8\u9a7e\u9a76VLA\u6a21\u578b\u4e2d\u6709\u6548\u6027\u7684\u5de5\u4f5c\u3002", "conclusion": "MindDrive\u901a\u8fc7\u5c06\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u6620\u5c04\u5230\u79bb\u6563\u8bed\u8a00\u51b3\u7b56\u7a7a\u95f4\uff0c\u6709\u6548\u5e73\u8861\u4e86\u590d\u6742\u573a\u666f\u4e0b\u7684\u6700\u4f18\u51b3\u7b56\u3001\u7c7b\u4eba\u9a7e\u9a76\u884c\u4e3a\u548c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u9ad8\u6548\u63a2\u7d22\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76VLA\u8303\u5f0f\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13639", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13639", "abs": "https://arxiv.org/abs/2512.13639", "authors": ["Michal Nazarczuk", "Thomas Tanay", "Arthur Moreau", "Zhensong Zhang", "Eduardo P\u00e9rez-Pellitero"], "title": "Charge: A Comprehensive Novel View Synthesis Benchmark and Dataset to Bind Them All", "comment": "Project page: https://charge-benchmark.github.io/", "summary": "This paper presents a new dataset for Novel View Synthesis, generated from a high-quality, animated film with stunning realism and intricate detail. Our dataset captures a variety of dynamic scenes, complete with detailed textures, lighting, and motion, making it ideal for training and evaluating cutting-edge 4D scene reconstruction and novel view generation models. In addition to high-fidelity RGB images, we provide multiple complementary modalities, including depth, surface normals, object segmentation and optical flow, enabling a deeper understanding of scene geometry and motion. The dataset is organised into three distinct benchmarking scenarios: a dense multi-view camera setup, a sparse camera arrangement, and monocular video sequences, enabling a wide range of experimentation and comparison across varying levels of data sparsity. With its combination of visual richness, high-quality annotations, and diverse experimental setups, this dataset offers a unique resource for pushing the boundaries of view synthesis and 3D vision.", "AI": {"tldr": "\u63d0\u51fa\u7528\u4e8e\u65b0\u89c6\u89d2\u5408\u6210\u7684\u5168\u65b0\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u5305\u542b\u52a8\u6001\u573a\u666f\u3001\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u652f\u6301\u4e09\u79cd\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u573a\u666f", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u3001\u591a\u6a21\u6001\u3001\u52a8\u6001\u573a\u666f\u7684\u65b0\u89c6\u89d2\u5408\u6210\u6570\u636e\u96c6\uff0c\u9700\u8981\u4e3a4D\u573a\u666f\u91cd\u5efa\u548c\u65b0\u89c6\u89d2\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u66f4\u597d\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\u8d44\u6e90", "method": "\u4ece\u9ad8\u8d28\u91cf\u52a8\u753b\u7535\u5f71\u4e2d\u751f\u6210\u6570\u636e\u96c6\uff0c\u63d0\u4f9bRGB\u56fe\u50cf\u3001\u6df1\u5ea6\u3001\u8868\u9762\u6cd5\u7ebf\u3001\u7269\u4f53\u5206\u5272\u548c\u5149\u6d41\u7b49\u591a\u79cd\u6a21\u6001\u6570\u636e\uff0c\u7ec4\u7ec7\u6210\u5bc6\u96c6\u591a\u89c6\u89d2\u3001\u7a00\u758f\u76f8\u673a\u548c\u5355\u76ee\u89c6\u9891\u4e09\u79cd\u57fa\u51c6\u6d4b\u8bd5\u573a\u666f", "result": "\u521b\u5efa\u4e86\u4e00\u4e2a\u89c6\u89c9\u4e30\u5bcc\u3001\u6807\u6ce8\u8d28\u91cf\u9ad8\u3001\u5b9e\u9a8c\u8bbe\u7f6e\u591a\u6837\u7684\u65b0\u89c6\u89d2\u5408\u6210\u6570\u636e\u96c6\uff0c\u4e3a3D\u89c6\u89c9\u7814\u7a76\u63d0\u4f9b\u4e86\u72ec\u7279\u8d44\u6e90", "conclusion": "\u8be5\u6570\u636e\u96c6\u7ed3\u5408\u4e86\u89c6\u89c9\u4e30\u5bcc\u6027\u3001\u9ad8\u8d28\u91cf\u6807\u6ce8\u548c\u591a\u6837\u5316\u5b9e\u9a8c\u8bbe\u7f6e\uff0c\u4e3a\u63a8\u8fdb\u89c6\u89d2\u5408\u6210\u548c3D\u89c6\u89c9\u9886\u57df\u8fb9\u754c\u63d0\u4f9b\u4e86\u72ec\u7279\u8d44\u6e90"}}
{"id": "2512.13665", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13665", "abs": "https://arxiv.org/abs/2512.13665", "authors": ["Wenhan Chen", "Sezer Karaoglu", "Theo Gevers"], "title": "Grab-3D: Detecting AI-Generated Videos from 3D Geometric Temporal Consistency", "comment": null, "summary": "Recent advances in diffusion-based generation techniques enable AI models to produce highly realistic videos, heightening the need for reliable detection mechanisms. However, existing detection methods provide only limited exploration of the 3D geometric patterns present in generated videos. In this paper, we use vanishing points as an explicit representation of 3D geometry patterns, revealing fundamental discrepancies in geometric consistency between real and AI-generated videos. We introduce Grab-3D, a geometry-aware transformer framework for detecting AI-generated videos based on 3D geometric temporal consistency. To enable reliable evaluation, we construct an AI-generated video dataset of static scenes, allowing stable 3D geometric feature extraction. We propose a geometry-aware transformer equipped with geometric positional encoding, temporal-geometric attention, and an EMA-based geometric classifier head to explicitly inject 3D geometric awareness into temporal modeling. Experiments demonstrate that Grab-3D significantly outperforms state-of-the-art detectors, achieving robust cross-domain generalization to unseen generators.", "AI": {"tldr": "\u63d0\u51faGrab-3D\u6846\u67b6\uff0c\u5229\u75283D\u51e0\u4f55\u4e00\u81f4\u6027\u68c0\u6d4bAI\u751f\u6210\u89c6\u9891\uff0c\u901a\u8fc7\u6d88\u5931\u70b9\u5206\u6790\u51e0\u4f55\u6a21\u5f0f\u5dee\u5f02\uff0c\u5728\u9759\u6001\u573a\u666f\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6548\u679c\u663e\u8457", "motivation": "\u968f\u7740\u6269\u6563\u6a21\u578b\u751f\u6210\u903c\u771f\u89c6\u9891\u7684\u80fd\u529b\u63d0\u5347\uff0c\u9700\u8981\u53ef\u9760\u7684\u68c0\u6d4b\u673a\u5236\u3002\u73b0\u6709\u65b9\u6cd5\u5bf9\u751f\u6210\u89c6\u9891\u4e2d\u76843D\u51e0\u4f55\u6a21\u5f0f\u63a2\u7d22\u6709\u9650\uff0c\u5b58\u5728\u68c0\u6d4b\u4e0d\u8db3\u7684\u95ee\u9898", "method": "\u4f7f\u7528\u6d88\u5931\u70b9\u4f5c\u4e3a3D\u51e0\u4f55\u6a21\u5f0f\u7684\u663e\u5f0f\u8868\u793a\uff0c\u63d0\u51faGrab-3D\u6846\u67b6\uff1a\u5305\u542b\u51e0\u4f55\u4f4d\u7f6e\u7f16\u7801\u3001\u65f6\u7a7a\u51e0\u4f55\u6ce8\u610f\u529b\u673a\u5236\u548cEMA\u51e0\u4f55\u5206\u7c7b\u5668\u5934\uff0c\u5728\u9759\u6001\u573a\u666f\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3", "result": "Grab-3D\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u68c0\u6d4b\u5668\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u751f\u6210\u5668\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u8de8\u57df\u6cdb\u5316\u80fd\u529b", "conclusion": "\u901a\u8fc7\u663e\u5f0f\u5efa\u6a213D\u51e0\u4f55\u65f6\u95f4\u4e00\u81f4\u6027\uff0cGrab-3D\u4e3aAI\u751f\u6210\u89c6\u9891\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2512.13674", "categories": ["cs.CV", "cs.CL", "cs.GR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2512.13674", "abs": "https://arxiv.org/abs/2512.13674", "authors": ["Yiyi Cai", "Xuangeng Chu", "Xiwei Gao", "Sitong Gong", "Yifei Huang", "Caixin Kang", "Kunhang Li", "Haiyang Liu", "Ruicong Liu", "Yun Liu", "Dianwen Ng", "Zixiong Su", "Erwin Wu", "Yuhan Wu", "Dingkun Yan", "Tianyu Yan", "Chang Zeng", "Bo Zheng", "You Zhou"], "title": "Towards Interactive Intelligence for Digital Humans", "comment": null, "summary": "We introduce Interactive Intelligence, a novel paradigm of digital human that is capable of personality-aligned expression, adaptive interaction, and self-evolution. To realize this, we present Mio (Multimodal Interactive Omni-Avatar), an end-to-end framework composed of five specialized modules: Thinker, Talker, Face Animator, Body Animator, and Renderer. This unified architecture integrates cognitive reasoning with real-time multimodal embodiment to enable fluid, consistent interaction. Furthermore, we establish a new benchmark to rigorously evaluate the capabilities of interactive intelligence. Extensive experiments demonstrate that our framework achieves superior performance compared to state-of-the-art methods across all evaluated dimensions. Together, these contributions move digital humans beyond superficial imitation toward intelligent interaction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4ea4\u4e92\u5f0f\u667a\u80fd\u65b0\u8303\u5f0fMio\u6846\u67b6\uff0c\u901a\u8fc7\u4e94\u4e2a\u4e13\u4e1a\u6a21\u5757\u5b9e\u73b0\u4eba\u683c\u5bf9\u9f50\u8868\u8fbe\u3001\u81ea\u9002\u5e94\u4ea4\u4e92\u548c\u81ea\u6211\u8fdb\u5316\uff0c\u5efa\u7acb\u4e86\u65b0\u7684\u8bc4\u4f30\u57fa\u51c6\u5e76\u5728\u6240\u6709\u7ef4\u5ea6\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u6570\u5b57\u4eba\u4e3b\u8981\u505c\u7559\u5728\u8868\u9762\u6a21\u4eff\u9636\u6bb5\uff0c\u7f3a\u4e4f\u771f\u6b63\u7684\u667a\u80fd\u4ea4\u4e92\u80fd\u529b\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5b9e\u73b0\u4eba\u683c\u5bf9\u9f50\u8868\u8fbe\u3001\u81ea\u9002\u5e94\u4ea4\u4e92\u548c\u81ea\u6211\u8fdb\u5316\u7684\u6570\u5b57\u4eba\u65b0\u8303\u5f0f\u3002", "method": "\u63d0\u51fa\u4e86Mio\uff08\u591a\u6a21\u6001\u4ea4\u4e92\u5168\u606f\u5316\u8eab\uff09\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u5305\u542b\u4e94\u4e2a\u4e13\u4e1a\u6a21\u5757\uff1a\u601d\u8003\u8005\u3001\u8bf4\u8bdd\u8005\u3001\u9762\u90e8\u52a8\u753b\u5e08\u3001\u8eab\u4f53\u52a8\u753b\u5e08\u548c\u6e32\u67d3\u5668\uff0c\u5c06\u8ba4\u77e5\u63a8\u7406\u4e0e\u5b9e\u65f6\u591a\u6a21\u6001\u5177\u8eab\u5316\u76f8\u7ed3\u5408\u3002", "result": "\u5efa\u7acb\u4e86\u65b0\u7684\u4ea4\u4e92\u667a\u80fd\u8bc4\u4f30\u57fa\u51c6\uff0c\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u5728\u6240\u6709\u8bc4\u4f30\u7ef4\u5ea6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u4e9b\u8d21\u732e\u63a8\u52a8\u6570\u5b57\u4eba\u4ece\u8868\u9762\u6a21\u4eff\u5411\u667a\u80fd\u4ea4\u4e92\u53d1\u5c55\uff0c\u5b9e\u73b0\u4e86\u4eba\u683c\u5bf9\u9f50\u8868\u8fbe\u3001\u81ea\u9002\u5e94\u4ea4\u4e92\u548c\u81ea\u6211\u8fdb\u5316\u7684\u4ea4\u4e92\u5f0f\u667a\u80fd\u65b0\u8303\u5f0f\u3002"}}
{"id": "2512.13677", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13677", "abs": "https://arxiv.org/abs/2512.13677", "authors": ["Xiaohu Huang", "Hao Zhou", "Qiangpeng Yang", "Shilei Wen", "Kai Han"], "title": "JoVA: Unified Multimodal Learning for Joint Video-Audio Generation", "comment": "Project page: \\url{https://visual-ai.github.io/jova}", "summary": "In this paper, we present JoVA, a unified framework for joint video-audio generation. Despite recent encouraging advances, existing methods face two critical limitations. First, most existing approaches can only generate ambient sounds and lack the capability to produce human speech synchronized with lip movements. Second, recent attempts at unified human video-audio generation typically rely on explicit fusion or modality-specific alignment modules, which introduce additional architecture design and weaken the model simplicity of the original transformers. To address these issues, JoVA employs joint self-attention across video and audio tokens within each transformer layer, enabling direct and efficient cross-modal interaction without the need for additional alignment modules. Furthermore, to enable high-quality lip-speech synchronization, we introduce a simple yet effective mouth-area loss based on facial keypoint detection, which enhances supervision on the critical mouth region during training without compromising architectural simplicity. Extensive experiments on benchmarks demonstrate that JoVA outperforms or is competitive with both unified and audio-driven state-of-the-art methods in lip-sync accuracy, speech quality, and overall video-audio generation fidelity. Our results establish JoVA as an elegant framework for high-quality multimodal generation.", "AI": {"tldr": "JoVA\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u89c6\u9891-\u97f3\u9891\u8054\u5408\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u8de8\u6a21\u6001\u4ea4\u4e92\uff0c\u65e0\u9700\u989d\u5916\u5bf9\u9f50\u6a21\u5757\uff0c\u5e76\u5f15\u5165\u57fa\u4e8e\u9762\u90e8\u5173\u952e\u70b9\u68c0\u6d4b\u7684\u5634\u90e8\u533a\u57df\u635f\u5931\u6765\u63d0\u5347\u5507\u8bed\u540c\u6b65\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a1) \u5927\u591a\u53ea\u80fd\u751f\u6210\u73af\u5883\u97f3\uff0c\u7f3a\u4e4f\u751f\u6210\u4e0e\u5507\u90e8\u8fd0\u52a8\u540c\u6b65\u7684\u4eba\u7c7b\u8bed\u97f3\u7684\u80fd\u529b\uff1b2) \u73b0\u6709\u7edf\u4e00\u89c6\u9891-\u97f3\u9891\u751f\u6210\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u663e\u5f0f\u878d\u5408\u6216\u6a21\u6001\u7279\u5b9a\u5bf9\u9f50\u6a21\u5757\uff0c\u589e\u52a0\u4e86\u67b6\u6784\u590d\u6742\u6027\u5e76\u524a\u5f31\u4e86\u539f\u59cbTransformer\u7684\u7b80\u6d01\u6027\u3002", "method": "JoVA\u91c7\u7528\u89c6\u9891\u548c\u97f3\u9891token\u5728\u6bcf\u5c42Transformer\u4e2d\u7684\u8054\u5408\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u76f4\u63a5\u9ad8\u6548\u7684\u8de8\u6a21\u6001\u4ea4\u4e92\uff0c\u65e0\u9700\u989d\u5916\u5bf9\u9f50\u6a21\u5757\u3002\u540c\u65f6\u5f15\u5165\u57fa\u4e8e\u9762\u90e8\u5173\u952e\u70b9\u68c0\u6d4b\u7684\u5634\u90e8\u533a\u57df\u635f\u5931\uff0c\u5728\u8bad\u7ec3\u671f\u95f4\u589e\u5f3a\u5bf9\u5173\u952e\u5634\u90e8\u533a\u57df\u7684\u76d1\u7763\uff0c\u800c\u4e0d\u5f71\u54cd\u67b6\u6784\u7b80\u6d01\u6027\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cJoVA\u5728\u5507\u8bed\u540c\u6b65\u51c6\u786e\u6027\u3001\u8bed\u97f3\u8d28\u91cf\u548c\u6574\u4f53\u89c6\u9891-\u97f3\u9891\u751f\u6210\u4fdd\u771f\u5ea6\u65b9\u9762\u4f18\u4e8e\u6216\u4e0e\u6700\u5148\u8fdb\u7684\u7edf\u4e00\u548c\u97f3\u9891\u9a71\u52a8\u65b9\u6cd5\u76f8\u7ade\u4e89\u3002", "conclusion": "JoVA\u88ab\u786e\u7acb\u4e3a\u4e00\u4e2a\u4f18\u96c5\u7684\u9ad8\u8d28\u91cf\u591a\u6a21\u6001\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u7b80\u6d01\u7684\u67b6\u6784\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u89c6\u9891-\u97f3\u9891\u8054\u5408\u751f\u6210\u3002"}}
{"id": "2512.13680", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13680", "abs": "https://arxiv.org/abs/2512.13680", "authors": ["Tianye Ding", "Yiming Xie", "Yiqing Liang", "Moitreya Chatterjee", "Pedro Miraldo", "Huaizu Jiang"], "title": "LASER: Layer-wise Scale Alignment for Training-Free Streaming 4D Reconstruction", "comment": "16 pages", "summary": "Recent feed-forward reconstruction models like VGGT and $\u03c0^3$ achieve impressive reconstruction quality but cannot process streaming videos due to quadratic memory complexity, limiting their practical deployment. While existing streaming methods address this through learned memory mechanisms or causal attention, they require extensive retraining and may not fully leverage the strong geometric priors of state-of-the-art offline models. We propose LASER, a training-free framework that converts an offline reconstruction model into a streaming system by aligning predictions across consecutive temporal windows. We observe that simple similarity transformation ($\\mathrm{Sim}(3)$) alignment fails due to layer depth misalignment: monocular scale ambiguity causes relative depth scales of different scene layers to vary inconsistently between windows. To address this, we introduce layer-wise scale alignment, which segments depth predictions into discrete layers, computes per-layer scale factors, and propagates them across both adjacent windows and timestamps. Extensive experiments show that LASER achieves state-of-the-art performance on camera pose estimation and point map reconstruction %quality with offline models while operating at 14 FPS with 6 GB peak memory on a RTX A6000 GPU, enabling practical deployment for kilometer-scale streaming videos. Project website: $\\href{https://neu-vi.github.io/LASER/}{\\texttt{https://neu-vi.github.io/LASER/}}$", "AI": {"tldr": "LASER\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u5c06\u79bb\u7ebf3D\u91cd\u5efa\u6a21\u578b\u8f6c\u6362\u4e3a\u6d41\u5f0f\u5904\u7406\u7cfb\u7edf\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u5c3a\u5ea6\u5bf9\u9f50\u89e3\u51b3\u65f6\u95f4\u7a97\u53e3\u95f4\u7684\u6df1\u5ea6\u5c3a\u5ea6\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u5b9e\u73b0\u5b9e\u65f6\u6d41\u5f0f\u89c6\u9891\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709\u7684\u524d\u9988\u91cd\u5efa\u6a21\u578b\uff08\u5982VGGT\u548c\u03c0\u00b3\uff09\u867d\u7136\u91cd\u5efa\u8d28\u91cf\u4f18\u79c0\uff0c\u4f46\u7531\u4e8e\u4e8c\u6b21\u5185\u5b58\u590d\u6742\u5ea6\u65e0\u6cd5\u5904\u7406\u6d41\u5f0f\u89c6\u9891\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u90e8\u7f72\u3002\u73b0\u6709\u7684\u6d41\u5f0f\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u91cd\u65b0\u8bad\u7ec3\uff0c\u4e14\u65e0\u6cd5\u5145\u5206\u5229\u7528\u6700\u5148\u8fdb\u79bb\u7ebf\u6a21\u578b\u7684\u51e0\u4f55\u5148\u9a8c\u3002", "method": "\u63d0\u51faLASER\u6846\u67b6\uff0c\u901a\u8fc7\u8de8\u8fde\u7eed\u65f6\u95f4\u7a97\u53e3\u7684\u9884\u6d4b\u5bf9\u9f50\u5c06\u79bb\u7ebf\u91cd\u5efa\u6a21\u578b\u8f6c\u6362\u4e3a\u6d41\u5f0f\u7cfb\u7edf\u3002\u4e3a\u4e86\u89e3\u51b3\u7b80\u5355\u76f8\u4f3c\u53d8\u6362\u5bf9\u9f50\u56e0\u5355\u76ee\u5c3a\u5ea6\u6a21\u7cca\u5bfc\u81f4\u7684\u5c42\u6df1\u5ea6\u9519\u4f4d\u95ee\u9898\uff0c\u5f15\u5165\u4e86\u5206\u5c42\u5c3a\u5ea6\u5bf9\u9f50\u65b9\u6cd5\uff1a\u5c06\u6df1\u5ea6\u9884\u6d4b\u5206\u5272\u4e3a\u79bb\u6563\u5c42\uff0c\u8ba1\u7b97\u6bcf\u5c42\u5c3a\u5ea6\u56e0\u5b50\uff0c\u5e76\u5728\u76f8\u90bb\u7a97\u53e3\u548c\u65f6\u95f4\u6233\u95f4\u4f20\u64ad\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLASER\u5728\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u548c\u70b9\u4e91\u56fe\u91cd\u5efa\u65b9\u9762\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728RTX A6000 GPU\u4e0a\u4ee514 FPS\u8fd0\u884c\uff0c\u5cf0\u503c\u5185\u5b58\u4ec56 GB\uff0c\u80fd\u591f\u5904\u7406\u5343\u7c73\u7ea7\u6d41\u5f0f\u89c6\u9891\u3002", "conclusion": "LASER\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5c06\u9ad8\u8d28\u91cf\u79bb\u7ebf3D\u91cd\u5efa\u6a21\u578b\u8f6c\u6362\u4e3a\u5b9e\u7528\u7684\u6d41\u5f0f\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5185\u5b58\u548c\u8bad\u7ec3\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u5927\u89c4\u6a21\u6d41\u5f0f\u89c6\u9891\u91cd\u5efa\u7684\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2512.13683", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13683", "abs": "https://arxiv.org/abs/2512.13683", "authors": ["Lu Ling", "Yunhao Ge", "Yichen Sheng", "Aniket Bera"], "title": "I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners", "comment": null, "summary": "Generalization remains the central challenge for interactive 3D scene generation. Existing learning-based approaches ground spatial understanding in limited scene dataset, restricting generalization to new layouts. We instead reprogram a pre-trained 3D instance generator to act as a scene level learner, replacing dataset-bounded supervision with model-centric spatial supervision. This reprogramming unlocks the generator transferable spatial knowledge, enabling generalization to unseen layouts and novel object compositions. Remarkably, spatial reasoning still emerges even when the training scenes are randomly composed objects. This demonstrates that the generator's transferable scene prior provides a rich learning signal for inferring proximity, support, and symmetry from purely geometric cues. Replacing widely used canonical space, we instantiate this insight with a view-centric formulation of the scene space, yielding a fully feed-forward, generalizable scene generator that learns spatial relations directly from the instance model. Quantitative and qualitative results show that a 3D instance generator is an implicit spatial learner and reasoner, pointing toward foundation models for interactive 3D scene understanding and generation. Project page: https://luling06.github.io/I-Scene-project/", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u7f16\u7a0b\u9884\u8bad\u7ec3\u76843D\u5b9e\u4f8b\u751f\u6210\u5668\u6765\u5b66\u4e60\u573a\u666f\u7ea7\u7a7a\u95f4\u5173\u7cfb\uff0c\u5b9e\u73b0\u5bf9\u65b0\u5e03\u5c40\u548c\u7269\u4f53\u7ec4\u5408\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u65e0\u9700\u4f9d\u8d56\u6709\u9650\u573a\u666f\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5b66\u4e60\u76843D\u573a\u666f\u751f\u6210\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u6709\u9650\u7684\u573a\u666f\u6570\u636e\u96c6\uff0c\u5bfc\u81f4\u5bf9\u65b0\u5e03\u5c40\u7684\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u3002\u7814\u7a76\u8005\u5e0c\u671b\u5229\u7528\u9884\u8bad\u7ec33D\u5b9e\u4f8b\u751f\u6210\u5668\u4e2d\u8574\u542b\u7684\u53ef\u8fc1\u79fb\u7a7a\u95f4\u77e5\u8bc6\uff0c\u7a81\u7834\u6570\u636e\u96c6\u9650\u5236\u3002", "method": "\u91cd\u65b0\u7f16\u7a0b\u9884\u8bad\u7ec3\u76843D\u5b9e\u4f8b\u751f\u6210\u5668\uff0c\u5c06\u5176\u8f6c\u53d8\u4e3a\u573a\u666f\u7ea7\u5b66\u4e60\u5668\u3002\u91c7\u7528\u6a21\u578b\u4e2d\u5fc3\u7684\u7a7a\u95f4\u76d1\u7763\u66ff\u4ee3\u6570\u636e\u96c6\u76d1\u7763\uff0c\u89e3\u9501\u751f\u6210\u5668\u7684\u53ef\u8fc1\u79fb\u7a7a\u95f4\u77e5\u8bc6\u3002\u63d0\u51fa\u4ee5\u89c6\u89d2\u4e3a\u4e2d\u5fc3\u7684\u573a\u666f\u7a7a\u95f4\u8868\u793a\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u89c4\u8303\u7a7a\u95f4\uff0c\u5b9e\u73b0\u5168\u524d\u9988\u3001\u53ef\u6cdb\u5316\u7684\u573a\u666f\u751f\u6210\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u5e03\u5c40\u548c\u65b0\u9896\u7684\u7269\u4f53\u7ec4\u5408\u3002\u5373\u4f7f\u5728\u8bad\u7ec3\u573a\u666f\u7531\u968f\u673a\u7ec4\u5408\u7684\u7269\u4f53\u6784\u6210\u65f6\uff0c\u7cfb\u7edf\u4ecd\u80fd\u6d8c\u73b0\u51fa\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002\u5b9a\u91cf\u548c\u5b9a\u6027\u7ed3\u679c\u8868\u660e\uff0c3D\u5b9e\u4f8b\u751f\u6210\u5668\u5177\u6709\u9690\u5f0f\u7684\u7a7a\u95f4\u5b66\u4e60\u548c\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "3D\u5b9e\u4f8b\u751f\u6210\u5668\u53ef\u4ee5\u4f5c\u4e3a\u9690\u5f0f\u7684\u7a7a\u95f4\u5b66\u4e60\u5668\u548c\u63a8\u7406\u5668\uff0c\u4e3a\u4ea4\u4e92\u5f0f3D\u573a\u666f\u7406\u89e3\u548c\u751f\u6210\u7684\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002\u8be5\u65b9\u6cd5\u8bc1\u660e\u4e86\u4ece\u7eaf\u51e0\u4f55\u7ebf\u7d22\u4e2d\u63a8\u65ad\u90bb\u8fd1\u6027\u3001\u652f\u6491\u5173\u7cfb\u548c\u5bf9\u79f0\u6027\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2512.13687", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2512.13687", "abs": "https://arxiv.org/abs/2512.13687", "authors": ["Jingfeng Yao", "Yuda Song", "Yucong Zhou", "Xinggang Wang"], "title": "Towards Scalable Pre-training of Visual Tokenizers for Generation", "comment": "Our pre-trained models are available at https://github.com/MiniMax-AI/VTP", "summary": "The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.", "AI": {"tldr": "\u89c6\u89c9\u5206\u8bcd\u5668\uff08VAE\uff09\u7684\u6f5c\u5728\u7a7a\u95f4\u8d28\u91cf\u5bf9\u751f\u6210\u6a21\u578b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u91cd\u5efa\u8bad\u7ec3\u504f\u5411\u4f4e\u7ea7\u4fe1\u606f\uff0c\u5bfc\u81f4\u9884\u8bad\u7ec3\u6269\u5c55\u95ee\u9898\u3002VTP\u6846\u67b6\u901a\u8fc7\u8054\u5408\u4f18\u5316\u56fe\u50cf-\u6587\u672c\u5bf9\u6bd4\u3001\u81ea\u76d1\u7763\u548c\u91cd\u5efa\u635f\u5931\uff0c\u5b9e\u73b0\u7406\u89e3\u9a71\u52a8\u751f\u6210\uff0c\u663e\u8457\u6539\u5584\u6269\u5c55\u6027\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u91cd\u5efa\u7684\u89c6\u89c9\u5206\u8bcd\u5668\u8bad\u7ec3\u5b58\u5728\u6839\u672c\u7f3a\u9677\uff1a\u66f4\u597d\u7684\u50cf\u7d20\u7ea7\u7cbe\u5ea6\u5e76\u4e0d\u5e26\u6765\u66f4\u9ad8\u8d28\u91cf\u7684\u751f\u6210\uff0c\u5bfc\u81f4\u5927\u91cf\u8ba1\u7b97\u6295\u5165\u5728\u5206\u8bcd\u5668\u9884\u8bad\u7ec3\u4e0a\u5374\u65e0\u6cd5\u6709\u6548\u63d0\u5347\u751f\u6210\u6027\u80fd\uff0c\u8fd9\u88ab\u79f0\u4e3a\"\u9884\u8bad\u7ec3\u6269\u5c55\u95ee\u9898\"\u3002", "method": "\u63d0\u51faVTP\u7edf\u4e00\u89c6\u89c9\u5206\u8bcd\u5668\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u9996\u6b21\u8054\u5408\u4f18\u5316\u4e09\u79cd\u635f\u5931\uff1a\u56fe\u50cf-\u6587\u672c\u5bf9\u6bd4\u635f\u5931\u3001\u81ea\u76d1\u7763\u635f\u5931\u548c\u91cd\u5efa\u635f\u5931\uff0c\u4ee5\u6784\u5efa\u80fd\u7b80\u6d01\u8868\u793a\u9ad8\u5c42\u8bed\u4e49\u7684\u6f5c\u5728\u7a7a\u95f4\u3002", "result": "\u5927\u89c4\u6a21\u7814\u7a76\u8868\u660e\uff1a1\uff09\u7406\u89e3\u662f\u751f\u6210\u7684\u5173\u952e\u9a71\u52a8\u529b\uff1b2\uff09VTP\u5177\u6709\u66f4\u597d\u7684\u6269\u5c55\u6027\uff0c\u751f\u6210\u6027\u80fd\u968f\u8ba1\u7b97\u3001\u53c2\u6570\u548c\u6570\u636e\u6709\u6548\u63d0\u5347\u3002VTP\u5728ImageNet\u4e0a\u8fbe\u523078.2%\u96f6\u6837\u672c\u51c6\u786e\u7387\u548c0.36 rFID\uff0c\u751f\u6210\u6536\u655b\u901f\u5ea6\u5feb4.1\u500d\uff0c\u4ec5\u901a\u8fc7\u589e\u52a0VTP\u9884\u8bad\u7ec3\u8ba1\u7b97\u91cf\u5c31\u80fd\u5b9e\u73b065.8%\u7684FID\u6539\u8fdb\u3002", "conclusion": "VTP\u6846\u67b6\u901a\u8fc7\u8054\u5408\u591a\u4efb\u52a1\u5b66\u4e60\u89e3\u51b3\u4e86\u89c6\u89c9\u5206\u8bcd\u5668\u7684\u9884\u8bad\u7ec3\u6269\u5c55\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u7406\u89e3\u9a71\u52a8\u751f\u6210\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89c6\u89c9\u5206\u8bcd\u5668\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002"}}
